[00:00:43] *** Joins: Fremen (~Fremen@78.190.110.216)
[00:00:48] <Fremen> Hello everyone
[00:01:53] <Fremen> I use tensorflow with python but I find official documentation to be insufficient for code referencing. Do you know any other good sources for tensorflow reference and code checking?
[00:08:02] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[00:09:43] <mefistofeles> Fremen: do you have a specific example of what is lacking in the official docs?
[00:09:49] <mefistofeles> because without that is hard to recommend something else
[00:09:53] <mefistofeles> I'd think
[00:21:17] <Fremen> mefistofeles, for example, I use eurosat dataset from tensorflow dataset library. However it returns something called Optioned_Dataset, different from standard tensorflow dataset and I was not able to find a method to cast it into a standard dataset. The only option I can think right now is to stop using default dataset and download from other sources and use "create dataset from folder" method or something similar.
[00:22:15] *** Quits: jlrnick (~josephler@2a01cb040a15940000d35e859ebfbf6e.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[00:22:39] <mefistofeles> Fremen: that doesn't sound like a lack of documentation but rather a limitation of the API/dataset-object itself
[00:24:19] <Fremen> mefistofeles, I assumed this was a trivial operation? Just cast a special dataset format to the default one? Is this too complex to implement?
[00:25:33] <Fremen> In any case, I assume I need to use the second method I proposed to get a default dataset? Because tensorflow datasets did not return such an object or maybe I am doing something wrong? :)
[00:36:53] *** Joins: georgios (~georgios@user/georgios)
[00:55:03] *** Joins: mcint (mcint@user/mcint)
[00:56:37] <mefistofeles> Fremen: yeah, it should have the option, but maybe that's why they have a different dataset object in the first place, maybe not everything is compatible... I don't really know :)
[00:57:48] <Fremen> mefistofeles, I was not able to find concrete data about what makes this dataset format different from the other one, thats why I felt it was not sufficient to be honest :). Anyway thanks for the help, see you later.
[00:58:00] *** Quits: Fremen (~Fremen@78.190.110.216) (Quit: Leaving)
[00:59:22] <mefistofeles> oh they left
[00:59:39] <mefistofeles> was about to ask what version they were using, because I cannot find anything with `Optionated_dataset` in the current API
[01:20:40] <Sheilong> I am trying to run a gridsearchcv with tensorflow on a NVIDIA GeForce RTX 2080 SUPER with 8gb memory. However, tensorflow allocates a lot of memory and I get "CUDA_ERROR_OUT_OF_MEMORY";
[01:21:17] <Sheilong> On the other hand, when I was running the same code in a CPU on my main machine which also have 8gb memory it was running just fine.
[01:25:13] <mefistofeles> Sheilong: it's likely that with the GPU you are running more jobs in parallel, so there's more memory getting allocated
[01:25:55] <Sheilong> mefistofeles: I just pass n_jobs=-1 to gridsearchcv
[01:26:11] <Sheilong> I am using kerasRegressor with that
[01:26:26] <mefistofeles> Sheilong: exactly, that's the maximum, I don't know what n_jobs=-1 does in a GPU though
[01:37:35] <HuntsMan> Sheilong: its really impossible to say anything without more information
[01:38:47] <Sheilong> HuntsMan: I was just gave access to a remote machine with two GPU's. Before that I was running my models on my local machine. But now it seems that to run a grid search on the GPU is a nightmare compared when I was running at my local machine.
[01:39:03] <HuntsMan> Sheilong: also TF allocates all memory by default, to manage memory itself
[01:39:09] <HuntsMan> Sheilong: nah it is not, assuming you do things right
[01:39:18] <HuntsMan> maybe you are doing something wrong
[01:39:46] <HuntsMan> Sheilong: GPUs also do not have exactly 8GB of RAM, if you check it could be a bit less
[01:40:04] <Sheilong> I was manage to run it on the GPU, however it always complain about something in the GPU. However it took 3.5 minutes to run the grid there meanwhile it was taking 5 minutes on my local machine.
[01:40:26] <HuntsMan> complains about what exactly?
[01:40:27] <Sheilong> nvtop shows 8.4gb for each GPU.
[01:40:36] <HuntsMan> see? not exactly 8 GB :D
[01:41:41] <Sheilong> : )
[01:43:23] <Sheilong> things like 2022-01-10 19:01:32.116507: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:442 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support
[01:43:48] <Sheilong> 2022-01-10 18:53:12.310634: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[01:48:04] <HuntsMan> by 2.2Kb hahaha
[01:50:24] <Sheilong> lol
[01:50:51] <Sheilong> I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
[01:51:52] <Sheilong> HuntsMan: Have you ever ran a GridSearch with tensorflow on a machine with GPU access?
[01:53:14] <HuntsMan> Sheilong: yes
[01:53:31] <HuntsMan> Sheilong: I means Information, just an informative message
[01:53:34] <Sheilong> HuntsMan: did you set n_jobs=-1?
[01:58:50] <Sheilong> HuntsMan: Along with GPU memory. It seems that the notebook was allocating 27Gb out og 62gb available on the main memory.
[01:59:47] <Sheilong> However the same code, the same project, runs just fine on my local machine which has only 8gb of mem and no GPU.
[02:00:35] <Sheilong> Also, the dateset is very small. It has only 500 rows...
[02:11:18] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:24:53] <HuntsMan> Sheilong: I ran four jobs I think
[02:40:02] *** Quits: palasso (~palasso@user/palasso) (Read error: Connection reset by peer)
[02:44:20] *** Quits: spinningCat (~spinningC@about/web/muscles) (Ping timeout: 256 seconds)
[02:52:11] <octav1a> Hello,
[02:53:37] <octav1a> I was looking into a large model running on pytorch. I have access to a large number of machines with cuda and nccl . However, not all of  the GPUs are of the same model. Is it necessary that all of the cards have the same specs to train a model in tandem?
[03:04:15] *** Joins: CaCode (~CaCode@user/cacode)
[03:11:04] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[03:12:21] <Sheilong> Even limiting the total memory it still uses all memory of the GPU lol
[03:19:04] <HuntsMan> limiting what and how?
[03:22:22] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: TAG)
[03:25:45] <Sheilong> HuntsMan: limiting the amount of memory tensorflow should use.
[03:25:51] <Sheilong> https://www.tensorflow.org/guide/gpu
[03:25:52] <SigmoidFroid> ⇒  Use a GPU | TensorFlow Core
[03:27:11] *** Joins: Malvolio (~Malvolio@user/malvolio)
[03:27:30] <Sheilong> HuntsMan: I killed all python processes and the memory of the GPU gets down to zero. Then I restarted the jupyter notebook and ran a single baseline on a dataset of just 500 samples. Then I look again at the nvtop, and the memory that was zero just grown up to 7.5gb when running a simple model.
[03:28:56] <HuntsMan> Sheilong: don't throw a tutorial to me, show me the code you actually executed
[03:29:33] <Sheilong> HuntsMan: Sorry.
[03:31:02] <Sheilong> Now, without limiting the amount of memory Tensorflow is executing. I am not running gridsearch anymore. I just ran a basic model and it is allocating almost 8gb of memory of the GPU.
[03:31:36] <Sheilong> Even after the model ran, nvtop shows that all the memory is still allocated by the notebook.
[03:31:51] <Sheilong> s/notebook/python3
[03:32:13] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 240 seconds)
[03:32:40] <HuntsMan> which tells me you are not limiting VRAM correctly
[03:33:50] <Sheilong> HuntsMan: I am not anymore. But why runing a basic model on a dataset with shape (523, 37) allocates all that memory?
[03:34:42] <HuntsMan> Sheilong: also TF allocates all memory by default, to manage memory itself <-
[03:34:45] <HuntsMan> I already said why
[03:35:41] *** Joins: `Tim (~zenguin@user/zenguin)
[03:36:09] <Sheilong> HuntsMan: Limiting the memory with this code snippet it only allocates 1.6gb
[03:36:36] <Sheilong> https://paste.ofcode.org/eSg2daYCmscPDFHQgLajPi
[03:36:38] <SigmoidFroid> ⇒  Paste ofCode
[03:36:54] <Sheilong> However, if I ever try to run a grid search I get CUDA_ERROR_OUT_OF_MEMORY
[03:37:25] <Sheilong> I'll try to limit n_jobs to see if I can get around this problem
[03:37:47] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[03:40:55] <Sheilong> Even with n_jobs=4 It still ran out of memory
[03:44:02] <mefistofeles> hmm
[03:45:50] <mefistofeles> Sheilong: feel free to share a minimal working example that reproduces your issue
[03:45:57] <mefistofeles> without code it's hard to tell at this point
[03:46:18] <Sheilong> mefistofeles: I could share the code, but I am not allowed to  share the data
[03:47:49] <mefistofeles> okay, that helps, but you also can use other datasets and check the memory consumption
[03:47:54] <mefistofeles> I have to run, bbl, maybe
[03:49:51] <Sheilong> mefistofeles: Okay. I will manage to get a minimal working example with a synthetic dataset that I could share
[03:52:17] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 240 seconds)
[03:52:39] <HuntsMan> Sheilong: set njobs to 1
[04:00:36] *** Quits: CaCode (~CaCode@user/cacode) (Quit: Leaving)
[04:04:48] *** Quits: maxyz (~maxy@80.254.172.91) (Ping timeout: 256 seconds)
[04:28:18] *** Quits: marcello42 (~mp@2001:1a81:132d:e100:15d5:e4d8:bd65:5509) (Ping timeout: 268 seconds)
[04:29:46] *** Joins: marcello42 (~mp@2001:1a81:1345:5c00:cd6:90d6:47fd:9bae)
[04:51:31] <Sheilong> n_jobs to 1 is taking forever to run...
[04:52:32] <Sheilong> The CPU model is a Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz
[04:53:03] <Sheilong> Also there are two gpus NVIDIA GeForce RTX 2080 SUPER
[04:55:51] <HuntsMan> but it runs
[05:00:12] <Sheilong> Yes. But never finished. It was running more than 30 minutes already
[05:00:46] <Sheilong> I am giving up on using that GPU.
[05:00:55] <Sheilong> I will force tensorflow to use the CPU instead
[05:01:17] <HuntsMan> have you ever run the grid search to the end?
[05:01:24] <HuntsMan> how many param combinations are there?
[05:05:26] <Sheilong> HuntsMan: I did locally on my machine. And it took only 10 minutes to ran. There are 108 param combinations. Given that every combination runs on a k fold with k = 3; There are a total of 324  models.
[05:06:47] <Sheilong> My local machine is an Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz which is a lot weak compared with the Xeon or the GPUs  I was running.
[05:49:14] *** Quits: marcello42 (~mp@2001:1a81:1345:5c00:cd6:90d6:47fd:9bae) (Quit: WeeChat 3.4)
[07:04:11] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[07:25:18] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[07:27:23] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[07:56:39] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[08:38:16] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[08:40:56] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[09:08:43] *** Joins: miique (~miique@181.46.139.166)
[09:30:04] *** Quits: Codaraxis_ (~Codaraxis@user/codaraxis) (Ping timeout: 256 seconds)
[10:39:01] *** Joins: maxyz (~maxy@80.254.172.91)
[10:53:02] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Quit: Leaving)
[11:08:17] *** Quits: maxyz (~maxy@80.254.172.91) (Quit: maxyz)
[11:08:31] *** Joins: maxyz (~maxy@80.254.172.91)
[11:14:20] *** Quits: etolier (~somewhere@122.199.46.54) (Ping timeout: 256 seconds)
[11:17:19] *** Joins: etolier (~somewhere@122.199.46.54)
[11:30:48] *** Joins: miique_ (~miique@181.46.139.166)
[11:32:37] *** Quits: miique (~miique@181.46.139.166) (Ping timeout: 240 seconds)
[11:33:10] *** Joins: manti7 (~manti7@176.10.104.94)
[11:36:54] *** Joins: palasso (~palasso@user/palasso)
[11:58:11] <lericson> hodapp_: it was incorrectly defined, supposed to be QW_Q (KW_K)^2 and the result follows
[12:05:52] <lericson> ergh ^T not ^2
[12:33:01] *** Quits: Malvolio (~Malvolio@user/malvolio) (Ping timeout: 240 seconds)
[12:48:23] *** Joins: Malvolio (~Malvolio@user/malvolio)
[13:00:45] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[13:06:30] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[13:23:38] *** Quits: gloomy (~gloomy@2001:470:69fc:105::ce00) (Quit: You have been kicked for being idle)
[13:51:22] *** Joins: spinningCat (~spinningC@94.54.210.37)
[13:51:56] *** Quits: spinningCat (~spinningC@94.54.210.37) (Changing host)
[13:51:56] *** Joins: spinningCat (~spinningC@about/web/muscles)
[13:57:54] *** Joins: jlrnick (~josephler@gw-wifi.lipn.univ-paris13.fr)
[14:21:45] *** Quits: jlrnick (~josephler@gw-wifi.lipn.univ-paris13.fr) (Ping timeout: 256 seconds)
[15:04:43] <hodapp_> lericson: if treating Q and K as matrices in that formulation, they are something like t x d and each weight matrix is d x 1; Q*W_Q and K*W_K are both O(t*d) but the subsequent multiply is O(t^2), so O(2*t*d + t^2); if rearranged as you propose then it is O(d + t*d + t^2) instead
[15:05:31] <hodapp_> but in most transformers, t is much larger than d, and that O(t^2) is the bottleneck that dominates everything and that you'll see mentioned in papers
[15:08:32] <hodapp_> (where t is the sequence length)
[15:11:56] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[15:18:47] <taeaad> Is there a way to measure the "size" of a LightGBM model in order to know whether using a GPU would provide a speedup?
[15:19:21] <taeaad> I have a model that worked fine on CPU, but I'm making the model significantly larger and am now wondering whether I should switch to a GPU based setup.
[15:19:33] <taeaad> (Cloud based...)
[15:30:19] *** Joins: jlrnick (~josephler@gw.lipn.univ-paris13.fr)
[16:09:00] *** Quits: spinningCat (~spinningC@about/web/muscles) (Ping timeout: 256 seconds)
[16:09:16] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[16:56:25] <lericson> hodapp_: i can see how it asymptotically doesn't really matter, but this should no doubt remove computations yes?
[17:15:07] <hodapp_> lericson: it isn't about it asymptotically not mattering, it's about it practically not mattering
[17:21:52] <hodapp_> and if you look at (for instance) https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py#L45 which handles the weights this is likely weirder implement: multiplying parameters together isn't a thing normally done and it may not handle it properly with the gradients
[17:21:53] <SigmoidFroid> ⇒  vit-pytorch/vit.py at main · lucidrains/vit-pytorch · GitHub
[17:22:43] <hodapp_> and it may have implications on numerical stability that are problematic
[17:31:18] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0d8-5.dhcp.inet.fi)
[17:34:07] <dostoyevsky2> taeaad: Often the cpu size is the same on the gpu, as it provides the same data types, but also half precision where the floats are just 16bit, so they could be 50% of a model using 32bit floats
[17:43:49] *** Joins: `Tim (~zenguin@user/zenguin)
[17:45:36] *** Joins: jokoon (~jokoon@136.252.163.126)
[17:46:11] <jokoon> What is the easiest way to label image with text, using AI? Are there pretrained NN freely available?
[17:46:38] <hodapp_> like, image captioning?
[17:46:42] <jokoon> yes
[17:47:15] <hodapp_> I don't know offhand but there are probably some pretrained one for some limited domain (e.g. something like imagenet)
[17:47:37] <jokoon> isn't imagenet just a set of labeled images?
[17:47:59] <lericson> aren't computers just sets of bits? :p
[17:48:27] <jokoon> haha
[17:49:11] <dostoyevsky2> jokoon: https://paperswithcode.com/task/image-captioning
[17:49:12] <SigmoidFroid> ⇒  Papers with Code - Image Captioning
[17:50:45] <jokoon> My goal is just to caption image, without having to train a NN
[17:52:58] <jokoon> seems like those links you gave me requires training
[18:00:35] <dostoyevsky2> https://preview.redd.it/3y8rmpeds6681.jpg?width=840&auto=webp&s=093b996e9b33fbb32a95463964abff21df77a910
[18:00:36] <SigmoidFroid> ⇒  (: No title)
[18:02:06] <jokoon> Usually how large would such data be?
[18:02:10] <dostoyevsky2> jokoon: Nah, e.g.:  https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing
[18:02:11] <SigmoidFroid> ⇒  Google Colaboratory
[18:04:14] *** Quits: SigmoidFroid (~SigmoidFr@ns508678.ip-192-99-15.net) (Remote host closed the connection)
[18:04:15] *** Joins: etolier_ (~somewhere@122.199.45.244)
[18:04:23] *** Joins: SigmoidFroid (~SigmoidFr@ns508678.ip-192-99-15.net)
[18:05:04] <dostoyevsky2> https://preview.redd.it/3y8rmpeds6681.jpg?width=840&auto=webp&s=093b996e9b33fbb32a95463964abff21df77a910
[18:05:09] <SigmoidFroid>  ⇒  (JPEG) May contain: trench_coat (82.7%) 62K
[18:05:31] <jokoon> 523MB
[18:05:37] *** Quits: etolier (~somewhere@122.199.46.54) (Ping timeout: 240 seconds)
[18:05:53] <jokoon> those are called the "model weights"
[18:18:23] *** Joins: sinaowolabi_ (~SinaOwola@160.152.185.94)
[18:20:58] *** Quits: sinaowolabi_ (~SinaOwola@160.152.185.94) (Client Quit)
[18:27:27] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[18:32:50] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0d8-5.dhcp.inet.fi) (Read error: Connection reset by peer)
[18:34:18] *** Joins: DnzAtWrk (~DnzAtWrk@1408.pc.puv.fi)
[18:50:48] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[18:52:09] *** Joins: Snyp (~Snyp@49.207.207.174)
[19:07:35] *** Joins: n0p (~n0p@static.139.237.203.116.clients.your-server.de)
[19:29:17] *** Joins: mnl (~mnl@user/mnl)
[19:30:36] *** Quits: DnzAtWrk (~DnzAtWrk@1408.pc.puv.fi) (Read error: Connection reset by peer)
[19:32:16] <Sheilong> Hi guys. About the problem I was having yesterday, the grid search is running just fine on google colab.
[19:39:22] <Sheilong> here is a minimal working code that reproduces what I am trying to do https://paste.ofcode.org/zitaQPMWafdaPfx6sUUXvH
[19:39:23] <SigmoidFroid> ⇒  Paste ofCode
[19:39:38] <Sheilong> I am not having those issues running this code on google colab.
[19:41:55] *** Quits: jlrnick (~josephler@gw.lipn.univ-paris13.fr) (Ping timeout: 256 seconds)
[19:47:59] <mefistofeles> Sheilong: oh okay, interesting
[19:49:11] <mefistofeles> Sheilong: so that veyr code is having problem with your local gpu setup, correct?
[19:50:34] *** Quits: jokoon (~jokoon@136.252.163.126) (Remote host closed the connection)
[19:50:53] <Sheilong> mefistofeles: It is a remote GPU that I have access through ssh. When running the gridsearch, if I set the cv to -1 it gives cuda out of memory error; if I set it to 4/3/2/1 for instance it gives a SIGABART error; If I set it to 1 it takes forever to run.
[19:58:57] <mefistofeles> Sheilong: I get an out of memory status with my local GPU (small one)
[19:59:26] <Sheilong> Colab might be using CPU only then. However I set it up to use GPU.
[20:04:01] <mefistofeles> Sheilong: I think you can check the GPU usage in colab
[20:06:50] <Sheilong> I just tested on kaggle. The GPU is enabled but it is running on the CPU instead.
[20:07:28] <Sheilong> Well, it just showed 20% of usage on GPU against 156% on CPU
[20:07:29] <mefistofeles> I see, yes, I don't really know if it makes sense to use -1 as jobs for GPU, tbh
[20:07:37] <mefistofeles> Sheilong: well, then it should be using GPU
[20:08:32] <Sheilong> So what would you recommend to run a grid search? Run it on the CPU instead?
[20:08:48] *** Quits: Snyp (~Snyp@49.207.207.174) (Quit: Textual IRC Client: www.textualapp.com)
[20:09:41] <mefistofeles> Sheilong: well, it appears that the bottleneck is not in the GPU, but I'm not that familiar with tensorflow to recommend anything, sorry
[20:14:31] <mefistofeles> Sheilong: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/#comment-414306
[20:14:32] <SigmoidFroid> ⇒  How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras
[20:16:27] <mefistofeles> Sheilong: I guess if what you have is only one GPU it doesn't make sense to have more than 1 in n_jobs
[20:16:51] <mefistofeles> or you have to tune tensorflow to not use the full GPU resources and split the GPU in fractions
[20:16:59] <Sheilong> mefistofeles: there are two GPU's, but setting n_jobs=2 gives sigabart error.
[20:58:34] <HuntsMan> Sheilong: setting njobs=2 does not automatically mean each process will use one GPU, you need to setup that manually
[21:05:22] *** Quits: etolier_ (~somewhere@122.199.45.244) (Ping timeout: 256 seconds)
[21:07:50] *** Joins: etolier (~somewhere@122.199.45.244)
[21:12:17] *** Quits: etolier (~somewhere@122.199.45.244) (Ping timeout: 240 seconds)
[21:17:59] *** Joins: etolier (~somewhere@202-65-84-142.ip4.superloop.com)
[21:29:54] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[21:43:48] *** Joins: SiegeLord (~sl@user/siegelord)
[21:52:49] *** Joins: loprox-_- (~loprox-_-@p508e0b54.dip0.t-ipconnect.de)
[21:54:17] *** Quits: loprox-_- (~loprox-_-@p508e0b54.dip0.t-ipconnect.de) (Remote host closed the connection)
[21:55:00] *** Quits: mnl (~mnl@user/mnl) (Quit: cya~)
[22:00:52] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[22:04:48] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[22:17:20] *** Quits: miique_ (~miique@181.46.139.166) (Ping timeout: 256 seconds)
[22:17:44] *** Joins: miique (~miique@181.46.139.166)
[22:30:35] *** Joins: miique_ (~miique@181.46.139.166)
[22:32:37] *** Quits: miique (~miique@181.46.139.166) (Ping timeout: 240 seconds)
[22:39:27] *** Joins: georgios (~georgios@user/georgios)
[22:55:17] *** Joins: miique (~miique@181.46.139.166)
[22:55:18] *** Quits: miique_ (~miique@181.46.139.166) (Ping timeout: 256 seconds)
[23:14:53] *** Joins: marcello42 (~mp@2001:1a81:1345:5c00:cd6:90d6:47fd:9bae)
[23:27:02] *** Quits: marcello42 (~mp@2001:1a81:1345:5c00:cd6:90d6:47fd:9bae) (Ping timeout: 252 seconds)
[23:31:25] *** Quits: PTapioK (~PTapioK@84-231-166-137.elisa-mobile.fi) (Ping timeout: 256 seconds)
[23:33:15] *** Joins: PTapioK (~PTapioK@84-231-5-115.elisa-mobile.fi)
[23:35:25] *** Quits: PTapioK (~PTapioK@84-231-5-115.elisa-mobile.fi) (Read error: Connection reset by peer)
[23:54:47] *** Joins: PTapioK (~PTapioK@84-231-163-229.elisa-mobile.fi)
[23:57:50] *** Quits: miique (~miique@181.46.139.166) (Read error: Connection reset by peer)
[23:59:18] *** Joins: miique (~miique@181.46.139.166)
