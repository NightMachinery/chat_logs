[00:04:29] *** Joins: pulseaudio (~pulseaudi@223.223.137.136)
[00:07:29] <Bushmaster> i will do that, i just created my first table 
[00:07:38] <Bushmaster> lot of work tonight for me 
[00:08:00] <Bushmaster> if stackover flow keeps censoring it, i will open it in tor browser 
[00:08:18] <Bushmaster> but for now I need to make these tables and paste bin you 
[00:11:54] *** Joins: The_Blode_ (~Blode@user/the-blode/x-7164444)
[00:13:07] *** Quits: The_Blode (~Blode@user/the-blode/x-7164444) (Ping timeout: 256 seconds)
[00:17:14] <Bushmaster> now I have a question Xgc  product names are like these in many rows with various punctuation #10-4 1/8" x 9 1/2" Premium Diagonal Seam Envelopes
[00:18:09] <Bushmaster> any suggestion what data type can work for these kind of weird product names? I did not carry out any data cleaning by writing python codes but can do so 
[00:37:33] *** Quits: vnf (~vnf@85.174.194.51) (Quit: Leaving)
[00:40:14] *** Joins: internecine1 (~interneci@ip68-98-149-5.dc.dc.cox.net)
[00:41:26] *** Quits: internecine (~interneci@ip68-98-149-5.dc.dc.cox.net) (Ping timeout: 272 seconds)
[00:44:53] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Quit: maret)
[00:54:14] *** Quits: rgrinberg (~textual@177.248.156.216) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[00:54:38] *** Quits: Xgc (~Xgc@user/xgc) (Read error: Connection reset by peer)
[00:55:17] *** Joins: Xgc (~Xgc@user/xgc)
[00:57:58] <Xgc> Bushmaster: VARCHAR(...) is probably fine. Just use a width that is appropriate to hold any of your values.
[01:00:31] <Bushmaster> all table creation is done, now time for importing the data 
[01:03:30] *** Joins: acidjnk (~acidjnk@p200300d0c7049f70498c262e6d47c9a7.dip0.t-ipconnect.de)
[01:06:47] *** Quits: pulseaudio (~pulseaudi@223.223.137.136) (Remote host closed the connection)
[01:07:06] *** Joins: pulseaudio (~pulseaudi@223.223.137.136)
[01:15:12] <Bushmaster> man how on earth somebody can work as SQL developer
[01:15:17] <Bushmaster> its pain in a**s
[01:15:44] <Bushmaster> i created customer table with constraint cusatomer_id as primary key and named the constrain customer_key
[01:15:56] <Bushmaster> i then realized i missed one of the columns during importing 
[01:16:13] <Bushmaster> i then deleted the table and then created back again with right column added
[01:16:22] <Bushmaster> and now it wont import at all saying :
[01:16:42] <Bushmaster> ERROR:  duplicate key value violates unique constraint "customer_key_2"
[01:16:42] <Bushmaster> DETAIL:  Key (customer_id_pk)=(CG-12520) already exists.
[01:17:06] <Bushmaster> so i cant drop the constraint cos I already deleted the first table 
[01:17:12] <Bushmaster> how i solve this mess
[01:20:08] *** Joins: rgrinberg (~textual@2806:102e:18:79a2:8cad:c2b5:666b:e6e7)
[01:25:12] <Bushmaster> because I used those customer_id as primary key, PostgreSQL wont allow them as primary key because these customer_id is not unique, many rows have same customer_id 
[01:25:14] *** Quits: pulseaudio (~pulseaudi@223.223.137.136) (Ping timeout: 250 seconds)
[01:26:15] <thumbs> you have to remove the duplicates first.
[01:31:26] <Bushmaster> i cant cos this customer tables, each customer appears many times because this table came from data dump csv table where customers purchased history is linked with sales table 
[01:32:44] <Bushmaster> I just simply do not understand how someone can create several tables from one large table where each rows is basically sales information, I cant just make each rows unique because its sales data 
[02:02:03] *** dob1_ is now known as dob1
[02:03:48] <thumbs> consider using a linking table then
[02:05:19] *** Quits: internecine1 (~interneci@ip68-98-149-5.dc.dc.cox.net) (Ping timeout: 256 seconds)
[02:08:43] <Xgc> Bushmaster: The sales history table should not have a primary key of just (customer_id).  That would be for the customers table only.
[02:09:29] <Xgc> Bushmaster: You could import that raw data into a staging table and then build / populate the other tables from this raw data.
[02:11:22] <Xgc> Bushmaster: I thought you said there was a sales_id value or something similar.  That would be a possible candidate as the primary key.
[02:13:01] <Xgc> Bushmaster: The row_id in the original data you showed could be that primary key of the sales table.
[02:18:51] <Bushmaster> Xgc it is hard to explain and as I suggested before, it would be best I can share all the tables with you 
[02:18:55] *** Quits: triberio13 (~triberio1@195.53.32.158) (Quit: Konversation terminated!)
[02:19:05] <Bushmaster> i spent pretty much all day in these and little or no progress 
[02:19:08] <Xgc> Bushmaster: But you never shared them.
[02:19:35] <Bushmaster> I understand what you saying but like I mentioned it is hard for me to explain by typing all these 
[02:19:56] <Bushmaster> I suggest you look into the main dump data csv file and then all the tables I made from it
[02:20:04] <Xgc> Bushmaster: You said you had made some tables and were going to share them.  I've been waiting.
[02:20:06] *** Joins: nsk_nyc (~nsk_nyc@network45-238-142-host-234.inethn.net)
[02:20:16] <Bushmaster> I am willing to share with you by sharing my server in which you can go and download the data 
[02:20:25] <Bushmaster> there is no other way I can share it 
[02:20:32] *** Quits: \mSg (mSg@user/msg/x-0285936) (Ping timeout: 246 seconds)
[02:20:50] <Bushmaster> yes like I said, I can share with you hence I wanted to send you my server credential in private
[02:21:33] <Xgc> Bushmaster: It's not necessary to share everything (all rows or all tables).  Just a sample will be fine.  A few rows from each table that refer to each other.
[02:22:20] *** Joins: internecine1 (~interneci@ip68-98-149-5.dc.dc.cox.net)
[02:24:04] <Bushmaster> thumbs, yes i can sure create relations but it wont comply with first three normal forms
[02:24:27] <Bushmaster> Xgc i can do that but as you noticed the tables get all break up in paste bin
[02:24:39] <Bushmaster> let send you again, it just too much work again
[02:26:52] <Xgc> Bushmaster: I showed you an example, like this: https://dbfiddle.uk/?rdbms=mysql_8.0&fiddle=5d8bbeeb860852242749689fa0fc5e21
[02:26:59] <Xgc> Bushmaster: You can do the same thing.
[02:27:26] <Xgc> Bushmaster: That's the data you showed initially.
[02:27:51] <Bushmaster> yes
[02:27:53] *** Joins: vnf (~vnf@188.162.140.42)
[02:28:01] <Bushmaster> but what you showing me before normalizing 
[02:28:04] <Bushmaster> its the main table 
[02:28:09] <Xgc> https://dbfiddle.uk/?rdbms=mysql_8.0&fiddle=c0622f08c8602642264f909a6a3b59c0
[02:28:10] <Bushmaster> we going in circle here 
[02:28:22] <Xgc> Bushmaster: Sure, because you provided no other data to me.
[02:28:37] <Xgc> Bushmaster: This is how you should provide your test case for review.
[02:29:17] <Bushmaster> well okay so the whole data dump of messy table, you want me to stage it as one big table in SQL server?
[02:29:25] <Xgc> Bushmaster: There are no circles.  We have not advanced beyond that.  That was the last detail available.  I edited it for the fiddle.
[02:29:28] <Bushmaster> i know what you saying 
[02:29:54] <Xgc> Bushmaster: I don't care how you do it.  I'm showing one trivial way to create a test case.
[02:30:58] <Bushmaster> but the current issue is i have already normalized the tables and current issue is customer ID which is repeated god knows how many times cannot be used as primary key, on the other hand,. if not use as primary key, then what
[02:31:03] <Xgc> If you have an easier way, that's fine with me.  But downloading some mass of data which can't be processed directly isn't a great idea.
[02:31:50] <Xgc> Bushmaster: I gave you the answer for that.  You seemed to have lost the sales_id (row_id) for the sales table.
[02:32:15] <Bushmaster> I do not think there is any way for this kind of data to be normalized, it is possible to create relations for sure but it wont conform normal form
[02:32:15] <Xgc> Bushmaster: You tried to use customer_id instead, which is a mistake.
[02:32:28] *** Quits: rgrinberg (~textual@2806:102e:18:79a2:8cad:c2b5:666b:e6e7) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[02:32:28] <Bushmaster> no no
[02:32:29] <Xgc> Bushmaster: It's trivial.  :)
[02:32:36] *** Quits: vnf (~vnf@188.162.140.42) (Ping timeout: 260 seconds)
[02:32:45] <Bushmaster> customer_id is customer table where is it used as primary key
[02:33:03] <Xgc> Bushmaster: Your main problem is being unable to ask a question with proper detail (or a clear test case).
[02:33:07] <Bushmaster> sales table has sales_id which I created from 1 to 10,000 or whatever
[02:33:25] <Bushmaster> yes it could be, not everyone is good at everything 
[02:33:41] <Xgc> Bushmaster: You can't have two matching customer rows (same customer_id) in the customers table.  If you have that, you made a mistake.
[02:33:44] *** Joins: kristijonas_ (~kristijon@cl-78-158-23-234.fastlink.lt)
[02:34:22] <Xgc> Bushmaster: You said (above) that customer_id was not unique, which is why you got errors when importing.
[02:34:30] <Bushmaster> well i cant have one unique customer_id either, because then it wont match with sales table data because each customer is linked with sales 
[02:34:37] <Xgc> Bushmaster: Focus on one table at a time.
[02:34:54] <Xgc> Bushmaster: You just made the same mistake (again).
[02:35:14] <Xgc> Bushmaster: The customer table DOES NOT link to the sales table!!!
[02:35:26] <Xgc> Bushmaster: Sales has the customer_id reference.
[02:35:37] <Bushmaster> okay 
[02:35:56] *** Quits: kristijonas (~kristijon@cl-78-158-23-234.fastlink.lt) (Ping timeout: 246 seconds)
[02:36:02] <Xgc> Bushmaster: So you're stuck on the same topic we spent hours discussing.
[02:37:12] <Xgc> Bushmaster: sales table has references to everything else.  Nothing refers to sales.
[02:37:37] <Xgc> Bushmaster: A Customers table has one row per customer (period).
[02:37:48] <Bushmaster> in some way yes, but in customer table, for example customer name, state etc are repeated many times and that is because this is subset of original csv table where customer data and sales are in same rows 
[02:38:02] <Xgc> Bushmaster: A Products table has one row per product (product_id), period.
[02:38:04] <Bushmaster> well no
[02:38:39] <Bushmaster> customer table has customer name and customer id repeated many times because it came from same original table where customer name and sales price were in same row 
[02:39:04] <Bushmaster> fine so how i go about getting rid of these repeation in each table
[02:39:12] <Xgc> Bushmaster: That's a mistake.  STOP doing that.
[02:39:47] <Xgc> Bushmaster: INSERT INTO customers SELECT DISTINCT customer_id, customer_name, ... FROM raw_staging_table;
[02:39:55] <Bushmaster> which one was mistake? all I saying is how the original table was before I segmented it
[02:40:32] *** Joins: MrZeus_ (~MrZeus@81.92.205.23)
[02:40:34] <Xgc> Bushmaster: Almost everything.  You tried to insert all rows from the staging data into customers, without making each row unique for the insert, like I just showed above.
[02:40:35] <Bushmaster> excellent idea
[02:41:15] <Bushmaster> you are professional 
[02:41:20] <Xgc> I've done this sort of thing many many times over several decades.  So it's easy for me.  I suspect most everything will be entirely new for you.  So don't worry about it.
[02:41:58] <Bushmaster> well I can write few SQL codes mainly DML and some DDL yes
[02:41:59] <Xgc> Of course, my cases were much more complicated, with hierarchical structures, etc.
[02:42:13] <Bushmaster> nothing else, this is new to me but this is MUST needed skill 
[02:42:47] <Bushmaster> okay, so basically here me out okay
[02:43:02] <Xgc> Bushmaster: That's why I tried to show you a simple stackoverflow answer I wrote, to give you a little taste of some non-trivial (complicated for you), but relatively simple SQL.
[02:43:17] <Bushmaster> first bring that long data dump csv file into a table what you call it raw staging table
[02:43:25] <Xgc> Correct.
[02:44:13] <Bushmaster> and then create other tables and then use that insert command above
[02:44:16] <Xgc> Now you can simply INSERT INTO tablex (...) SELECT ... FROM staging ...;  to populate most of your data.
[02:44:45] <Bushmaster> is that subquery or something
[02:44:50] <Xgc> Just be sure to use DISTINCT or the equivalent so you don't attempt to create duplicates.
[02:44:55] <Bushmaster> i thought insert into values
[02:44:59] <Xgc> The (...) is the insert column list.
[02:45:33] <Xgc> Bushmaster: You will probably want to be specific about the columns and order in all your SQL.
[02:46:16] <Bushmaster> so for example if I am creating customer table, from that raw table, the command will look like this:
[02:46:25] <Xgc> INSERT INTO products (product_id, product_name) SELECT DISTINCT product_id, product_name FROM staging;
[02:47:03] <Xgc> INSERT INTO customers (customer_id, customer_name) SELECT DISTINCT customer_id, customer_name FROM staging;
[02:47:14] <Xgc> Add other columns as you wish.
[02:47:50] <Bushmaster> okay
[02:47:58] <Bushmaster> and this would be real first stage 
[02:48:03] <Xgc> You'll just want to create the tables first... or in MySQL (and a few others), you can: CREATE TABLE customers AS SELECT DISTINCT customer_id, customer_name FROM staging;
[02:48:26] <Xgc> Right.  You can always refine the process and redo easily.
[02:48:55] <Bushmaster> and then how about constraints, second stage? alter table customer add constraint customer_id primary key?
[02:49:13] <Xgc> By doing all this in direct SQL, you can always drop your new tables and rerun the procedure.
[02:49:49] <Bushmaster> excellent excellent, i spent time doing all these in spreadsheet all day by sepratign table 
[02:49:51] <Xgc> Bushmaster: Step by step.  You might be able to recreate your tables from scratch with constraints and then do the above again.
[02:50:07] <Xgc> or just ALTER TABLE later to create the constraints.
[02:50:11] <Bushmaster> okay, will you be around tomorrow 
[02:50:48] <Bushmaster> i need to sleep, i spent all day in these, but i guess its paying off, this is one time skill, like learning bicycle, once i can do this, i can do it again 
[02:50:58] <Xgc> Your structure is simple enough that the constraints can be done at table creation time.  There don't appear to be any problems with the data I saw.  Just create the child tables first.
[02:51:29] <Xgc> Create sales last, which will then be referring to all the child tables you've just imported.
[02:52:08] <Xgc> Bushmaster: Good luck.  We're usually around.  Just ask questions and wait.
[02:52:30] <Bushmaster> so customer table, product tables, shipment tables are child table
[02:52:35] <Xgc> Yes.
[02:53:05] <Xgc> Well, be careful with shipment.  That might be one table that refers to sales. It's hard to tell.
[02:53:13] <Bushmaster> sales table is parent table cos it is holding all the foreign keys of customer table, product table etc
[02:53:36] <Xgc> Bushmaster: For instance, a large amazon order might be shipped in multiple shipments, each shipment referring to the same sales order.
[02:54:06] <Xgc> shipments are not shared between unrelated sales orders.
[02:54:28] <Xgc> Not in this context.  You haven't really explained your concept of shipment yet.
[02:54:41] <Bushmaster> ship_id_pk	ship_date	ship_mode
[02:54:57] <Bushmaster> these are the three columns when I made it as child table
[02:55:01] <Xgc> Bushmaster: So you think two unrelated sales will refer to the same shipment row???
[02:55:04] <Bushmaster> ship_id I made it 
[02:55:11] <Bushmaster> indexed it from 1 - 10,000 
[02:55:19] <Xgc> Bushmaster: Pay attention.
[02:55:35] <Bushmaster> ship date and ship mode came from main sales parent table before segmenting it 
[02:55:44] <Xgc> Bushmaster: Pay attention.
[02:55:57] <Xgc> Bushmaster: Think about my question?
[02:56:00] <Xgc> Bushmaster: Think about my question!
[02:56:08] <Bushmaster> i understand but you know the sales table 
[02:56:18] <Xgc> Bushmaster: Think. Stop typing.
[02:56:21] <Bushmaster> there i made sales_id indexing from 1 - 10,000 
[02:56:28] <Bushmaster> so it matches with ship_id
[02:56:47] <Xgc> Bushmaster: That's immaterial and wrong.
[02:57:04] <Xgc> Bushmaster: It's possible that the final sales table will have no ship_id.
[02:57:29] <Xgc> Bushmaster: You can do that.  But then a sales can not be shipped in 2 loads.
[02:57:30] <Bushmaster> no, two unrelated sales wont have same ship mode 
[02:58:04] <Xgc> Bushmaster: What if you have a sales and only some items are available this month and a few will be delayed until next month?
[02:58:28] <Xgc> Bushmaster: I'm not discussing ship_mode.
[02:58:35] <Xgc> Bushmaster: But that is important too.
[02:58:56] <Bushmaster> that question i have no answer 
[02:59:06] <Xgc> Bushmaster: It's possible, in a normal sales system, that a single sales is related to multiple shipments, with different dates and contents.
[02:59:19] <Bushmaster> yes
[02:59:44] <Xgc> Bushmaster: So, if you don't need to support that, it's ok to keep the ship_id (same as sales_id probably) in the sales table.
[03:00:01] <Bushmaster> and that child table has shipment date same but ship mode different like some are standard ship mode, some are faster delivery etc
[03:00:03] <Xgc> In fact, there is no ship_id.  It's just a sales_id.
[03:01:13] <Bushmaster> in the original table everything started out as order_id 
[03:01:18] <Bushmaster> which i did not use
[03:01:24] <Xgc> You could also have a shipment_type table, which is just the distinct list of kinds of shipments supported, (truck, train, air, etc)
[03:01:54] <Bushmaster> so yeah, i can bring that order_id as primary key in ship table as in distinct 
[03:01:55] <Xgc> Maybe even a shipper list, UPS, FEDEX, etc.
[03:02:12] <Bushmaster> and then use that as foreign key in sales table, is that what you think right?
[03:02:51] <Xgc> Right.  If it's always 1 sales_id per 1 shipment, you can use sales_id for ship_id in the shipments table.
[03:02:51] <Bushmaster> that data dump only have information about delivery mode like standard delivery, faster deliver etc
[03:03:23] <Bushmaster> no idea
[03:03:53] <Xgc> Is sales different than orders in your system.  This is the first time you've mentioned orders.
[03:04:06] <Bushmaster> all I know the main table the one what you calling as raw table every rows refer to sales with all kind of data in it
[03:04:50] <Xgc> Bushmaster: Ok. Get some sleep.  You'll do better when not so tired.
[03:04:52] <Bushmaster> well here are the columns of the raw table 
[03:05:11] <Bushmaster> Row ID	Order ID	Order Date	Ship Date	Ship Mode	Customer ID	Customer Name	Segment	Country/Region	City	State	Postal Code	Region	Product ID	Category	Sub-Category	Product Name	Sales	Quantity	Discount	Profit
[03:05:32] <Xgc> Bushmaster: I have all that.  I'm more familiar with it than you.
[03:05:36] <Bushmaster> this came from raw table, original table okay
[03:05:57] <Bushmaster> yes okay, i need to lie down
[03:06:01] <Xgc> I'm probably a step or two ahead of you, which is why you're struggling.
[03:06:09] <Bushmaster> but yes, i think tomorrow I can make the real work 
[03:06:25] <Bushmaster> today was basically brainstorming session but i guess I learnt a lot from you 
[03:06:44] <Xgc> Very good session.  I hope you can save the IRC logs.
[03:07:23] <Xgc> I can send them to you tomorrow, if you care to have them.
[03:07:31] <Bushmaster> well, I will need to copy and paste the relevant chuncks of the irc but i keep my machine on 24 horus anyway so i remain stay in this channel 
[03:07:35] *** Joins: Joanna (uid57472@id-57472.lymington.irccloud.com)
[03:08:06] <Bushmaster> all I need to do is bring that data dump csv flat file right into database 
[03:08:37] <Bushmaster> just gonna copy table from path with (format csv, header) 
[03:08:58] <Bushmaster> and what did you say staging area?
[03:09:13] <Bushmaster> you mean i need to create a table called stage?
[03:10:29] <Xgc> Bushmaster: Sure.  I do that all the time.
[03:10:41] <Xgc> Whatever name you like.
[03:11:46] <Bushmaster> i really like that SQL command you mentioned 
[03:11:49] <Bushmaster> INSERT INTO customers SELECT DISTINCT customer_id, customer_name, ... FROM raw_staging_table
[03:12:29] <Bushmaster> i been banging my head looking at that data dump in spreadsheet and thinking how I can make each values distinct in Excel 
[03:12:30] <Xgc> Right.  Just a simple example.  You can add other columns as needed.
[03:13:18] *** Quits: acidjnk (~acidjnk@p200300d0c7049f70498c262e6d47c9a7.dip0.t-ipconnect.de) (Ping timeout: 260 seconds)
[03:13:22] <Bushmaster> so data dump main table can be called raw_staging_table and child tables can be called staging_customer, staging_product etc 
[03:14:42] <Xgc> Bushmaster: https://dpaste.com/683FGAPB4
[03:14:50] <Bushmaster> let me check
[03:15:40] <Xgc> Bushmaster: Play with how you create tables.  You don't lose anything by testing.  If you don't like the first tables you create, drop them and tweak the process as you see fit.
[03:16:03] <Bushmaster> how did you filter out the log 
[03:16:04] <Xgc> Bushmaster: It will never be right the first time.
[03:16:16] <Xgc> Bushmaster: grep
[03:16:23] <Bushmaster> that irc log only have communication between you and I, how did you filter it
[03:16:50] <Bushmaster> grep in Linux command 
[03:17:03] <Xgc> Bushmaster: Basically this, with a little adjustment at the end: grep 'Bushmaster\|Xgc' "#sql.log" > bush.txt
[03:17:47] <Bushmaster> amazing
[03:18:06] <Bushmaster> it says copy to clipboard i clicked it no idea where to take it 
[03:18:43] <Xgc> Bushmaster: You can paste your clipboard in any text editor.
[03:18:56] <Xgc> As long as your clipboard was large enough to hold the data.
[03:19:05] <Bushmaster> hold on
[03:20:27] <Bushmaster> okay done and saved it
[03:23:25] <Bushmaster> these are my tables I created today and it did not work because of those redundancy in customer_id when I tried to import it 
[03:23:28] <Bushmaster> https://dpaste.com/6DAR29DAU
[03:24:18] <Xgc> Right. ... and you know why.
[03:26:06] <Bushmaster> well not until you gave me the secret recipe about insert into customers select distinct customer_id from staging_raw_table
[03:26:53] <Bushmaster> i been baging my head all day thinking if I destroy the original fabric of customer Id, customer name data, then it will destroy the sales fabric 
[03:27:50] <Bushmaster> but you explained to me, sales table has all the reference id of customer table, so how it can destroy sales links, it cant, hence no problem getting rid of duplication from customer tables
[03:28:22] <Bushmaster> i was all bundled up with two major issues, which now beginning to come very clear
[03:28:54] *** Quits: internecine1 (~interneci@ip68-98-149-5.dc.dc.cox.net) (Quit: Leaving)
[03:29:44] <Bushmaster> one was duplication of customer names refers to each sales she or he made and i cant destroy that duplication cos it refers back to sales, until you explained to me how can it destroy cos sales table already has all the customer_id as foreign key which all it needs to pull record and match from customer table 
[03:30:40] <Bushmaster> the other issue i was bundled up was how i can eliminate the duplication, cos i was only thinking of Excel or Python functions like replace()
[03:31:29] <Bushmaster> all these can be done in one fell swoop through that exciting code of distinct 
[03:31:46] <Xgc> It can take a little time to think in terms of SQL and relational databases.
[03:32:16] <Bushmaster> without breaking any links from sales_table referencing customer data through customer_id as its foriegn key 
[03:32:54] *** Quits: kn100 (~kn100@wireguard/tunneler/kn100) (Remote host closed the connection)
[03:33:00] <Bushmaster> you telling me, at one point today, i felt like i go out and walk to clear my head or simply forget about learning database design concept
[03:33:15] <Bushmaster> but i clinged on 
[03:33:20] <Bushmaster> now i am feeling lot better
[03:33:25] <Bushmaster> i can gave good sleep 
[03:34:30] *** Joins: kn100 (~kn100@wireguard/tunneler/kn100)
[03:34:35] <Xgc> https://dbfiddle.uk/?rdbms=mysql_8.0&fiddle=d5cc9c6add3bb32c5341ee689625e41d
[03:34:41] <Bushmaster> let me check
[03:35:07] <Xgc> I've adjusted it slightly for the MySQL test case I created.
[03:35:44] <Xgc> The original data doesn't contain all the columns in your new tables.
[03:35:48] <Bushmaster> you know this command 
[03:35:49] <Bushmaster> INSERT INTO customers (customer_id_pk, customer_first_name)
[03:35:50] <Bushmaster> SELECT DISTINCT `Customer ID`, `Customer Name` FROM test;
[03:35:55] <Bushmaster> you typed there
[03:36:00] <Xgc> Yes.
[03:36:13] <Bushmaster> it looks like cross between inster into values and subquery
[03:36:43] <Bushmaster> so you just replaced values with select statement 
[03:36:46] <Xgc> Bushmaster: It has a similar look.
[03:37:21] <Xgc> In a more typical INSERT, the VALUES clause is the source of the data.  In this case, the SELECT is the source.
[03:37:22] <Bushmaster> i really did not know insert statement can take select statement as values 
[03:37:48] <Xgc> Bushmaster: Stick around. You'll learn a few more things.
[03:38:03] <Bushmaster> yeah well these are easy to learn yes
[03:38:33] <Bushmaster> but its the table design from seeminly disparate data sources coming as sales data dump that is ciritcal 
[03:39:28] <Bushmaster> you know that statement insert with select, does it work with SQL Server as well 
[03:39:35] <Xgc> Well, you had several problems to solve and didn't really know how to solve any of them.  That can be rather confusing, until you see how to break down the work.
[03:39:36] <Bushmaster> i know T-SQL is weird 
[03:40:04] <Xgc> Bushmaster: Every major database supports it.
[03:40:45] <Bushmaster> well i had no idea how to break the csv data, i have this data for nearly 3 months now, i looked at it every afetr few weeks and never got my head around how on earth i can break it down like you see proper tables in AdventureWork2012 
[03:42:17] <Xgc> Every step will seem simple, once you see it. But it's tough to figure it all out from the documentation when you are just starting.
[03:43:39] <Xgc> Bushmaster: The example from stackoverflow I was trying to show you contains lots of interesting features that you can use.  It can take months or years to find them on your own.
[03:44:40] <Xgc> Common Table Expressions (which supports recursion), Window Functions, Derived Tables, etc.
[03:45:17] <Bushmaster> i will address these this week, StackOverflow is censored in my machine, i will fix it
[03:45:39] <Xgc> I believe it's probably just that new FILTER UI option.
[03:45:55] <Bushmaster> I have data of 19502 rows of all US cities but data came in as insert into values then cities, population from 2010 till 2019
[03:46:08] <Bushmaster> i did that at ease to bring it into PostgreSQL 
[03:46:11] <Xgc> If you mouse over one of the hidden lines, I bet it uncovers the data.
[03:46:18] <Bushmaster> and then pulled top 100 cities 
[03:47:22] <Xgc> Your eyes are getting heavy. ...  Sleep ...
[03:47:31] <Bushmaster> it came with latitude and longitude hence I utilized PostGIS extension to create geomtery shape file which i then plugged into QGIS to create vector Spatial map of all 100 US cities with high population 
[03:47:45] <Bushmaster> here is them maps 
[03:47:47] <Bushmaster> https://imgur.com/a/AZdY1e6
[03:47:51] <Bushmaster> yes I am off to bed 
[03:48:01] <Bushmaster> i am tired and I am stretching my brain too far 
[04:25:02] *** Quits: AmR (~AmREiSa@156.207.152.245) (Quit: Konversation terminated!)
[04:47:38] *** Joins: Filohuhum (~dante@gateway/tor-sasl/filohuhum)
[05:03:16] *** Joins: Rathskalon (uid65285@id-65285.lymington.irccloud.com)
[05:48:05] *** Joins: rgrinberg (~textual@177.248.158.166)
[05:49:43] *** Quits: The_Blode_ (~Blode@user/the-blode/x-7164444) (Ping timeout: 260 seconds)
[06:04:14] *** Quits: rgrinberg (~textual@177.248.158.166) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[06:40:09] *** Joins: agrosant (~agrosant@79.103.182.92.dsl.dyn.forthnet.gr)
[06:51:33] *** Quits: MrZeus_ (~MrZeus@81.92.205.23) (Ping timeout: 260 seconds)
[07:18:04] *** Quits: nsk_nyc (~nsk_nyc@network45-238-142-host-234.inethn.net) (Read error: Connection reset by peer)
[07:19:57] *** Joins: irontom (~user@69.174.103.168)
[07:26:42] *** Joins: nsk_nyc (~nsk_nyc@45.238.142.234)
[07:35:36] *** Joins: rgrinberg (~textual@177.248.158.166)
[07:57:18] *** Quits: Joanna (uid57472@id-57472.lymington.irccloud.com) (Quit: Connection closed for inactivity)
[08:00:12] *** Joins: herjazz (~herjazz@p10295192-ipngn22201marunouchi.tokyo.ocn.ne.jp)
[08:02:52] *** Quits: rgrinberg (~textual@177.248.158.166) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[08:09:50] *** Quits: irontom (~user@69.174.103.168) (Ping timeout: 272 seconds)
[08:15:12] *** Joins: Joanna (uid57472@id-57472.lymington.irccloud.com)
[08:36:48] *** Joins: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr)
[09:01:45] *** Joins: rgrinberg (~textual@177.248.158.166)
[09:13:02] *** Quits: Rathskalon (uid65285@id-65285.lymington.irccloud.com) (Quit: Connection closed for inactivity)
[09:55:39] *** Joins: \mSg (mSg@user/msg/x-0285936)
[10:27:18] *** Quits: Joanna (uid57472@id-57472.lymington.irccloud.com) (Quit: Connection closed for inactivity)
[10:27:36] *** Joins: acidjnk (~acidjnk@p200300d0c7049f701951913ce7bf0925.dip0.t-ipconnect.de)
[10:33:45] *** Quits: \mSg (mSg@user/msg/x-0285936) (Read error: Connection reset by peer)
[10:38:45] *** Quits: rgrinberg (~textual@177.248.158.166) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[10:41:47] *** Joins: maret (~maret@nat-88-212-37-89.antik.sk)
[10:42:05] *** Quits: reset (~reset@user/reset) (Quit: reset)
[11:07:19] *** Quits: cthulchu (~Cthulchu@69-172-160-196.cable.teksavvy.com) (Ping timeout: 260 seconds)
[11:11:06] *** Joins: lehinsun (~ttytwiste@188.113.176.117)
[11:13:51] *** Quits: lehinsun (~ttytwiste@188.113.176.117) (Remote host closed the connection)
[11:14:35] *** Quits: Filohuhum (~dante@gateway/tor-sasl/filohuhum) (Ping timeout: 240 seconds)
[11:14:37] *** Joins: lehinsun (~ttytwiste@188.113.176.117)
[11:33:55] *** Quits: acidjnk (~acidjnk@p200300d0c7049f701951913ce7bf0925.dip0.t-ipconnect.de) (Ping timeout: 260 seconds)
[11:34:16] *** Joins: The_Blode (~Blode@user/the-blode/x-7164444)
[11:56:45] *** Quits: z8z (~x@ac255238.ppp.asahi-net.or.jp) (Quit: Quitting)
[12:12:43] *** Parts: az (az@user/azz) (Leaving)
[12:31:38] *** Quits: lehinsun (~ttytwiste@188.113.176.117) (Ping timeout: 246 seconds)
[12:33:01] *** Joins: grooverman (~grooverma@179.97.175.157)
[12:34:26] *** Quits: grooverman_ (~grooverma@179.97.175.157) (Ping timeout: 246 seconds)
[12:36:27] *** Quits: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr) (Ping timeout: 260 seconds)
[12:56:29] *** Joins: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr)
[13:02:57] *** Quits: Atque (~Atque@user/atque) (Remote host closed the connection)
[13:25:18] *** Quits: The_Blode (~Blode@user/the-blode/x-7164444) (Ping timeout: 260 seconds)
[13:26:45] *** Quits: nsk_nyc (~nsk_nyc@45.238.142.234) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[14:03:30] *** Joins: pulseaudio (~pulseaudi@223.223.141.228)
[14:04:07] *** Joins: The_Blode (~Blode@user/the-blode/x-7164444)
[14:08:51] *** Quits: The_Blode (~Blode@user/the-blode/x-7164444) (Ping timeout: 260 seconds)
[14:11:50] *** Joins: The_Blode (~Blode@user/the-blode/x-7164444)
[14:14:32] *** Quits: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr) (Ping timeout: 246 seconds)
[14:15:53] *** Quits: pulseaudio (~pulseaudi@223.223.141.228) (Remote host closed the connection)
[14:20:58] *** Joins: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr)
[14:24:50] *** Joins: The_Blode_ (~Blode@user/the-blode/x-7164444)
[14:25:23] *** Quits: The_Blode (~Blode@user/the-blode/x-7164444) (Ping timeout: 246 seconds)
[14:36:57] *** Joins: pulseaudio (~pulseaudi@223.223.141.228)
[14:46:29] *** Joins: triberio13 (~triberio1@195.53.32.158)
[14:46:44] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[14:48:44] *** Quits: pulseaudio (~pulseaudi@223.223.141.228) (Remote host closed the connection)
[15:18:58] *** Joins: Bushycat (~IceChat95@user/bushmaster)
[15:19:48] <Bushycat> Howdy Folks! I am gonna continue my work where I left off, normalizing table, obviously I am doing it both in windows SQL SEWrver first and then other machine in Linux where I will do it in PostgreSQL 
[15:21:07] <Bushmaster> Howdy Folks, this is Buyshycat aka Bushmaster from Linux machine, I am the same dude who is Bushycat so that no confusion occur, I am working in both Linux and Widdows hence two IRC clients 
[15:36:32] *** Joins: The_Blode (~Blode@user/the-blode/x-7164444)
[15:37:08] *** Quits: The_Blode_ (~Blode@user/the-blode/x-7164444) (Ping timeout: 246 seconds)
[15:55:02] *** Quits: shibboleth (~shibbolet@user/shibboleth) (Quit: shibboleth)
[16:00:35] *** Quits: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr) (Ping timeout: 246 seconds)
[16:07:13] *** Joins: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr)
[16:17:22] *** Joins: Joanna (uid57472@id-57472.lymington.irccloud.com)
[16:35:38] *** Joins: wakeup (~wakeup@user/wakeup)
[16:49:52] <Bushycat> Hi I am still stuck, here is the SQL codes I wrote and I am getting error, can anyone check and let me know what's wrong here https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=18e08fe957d2dc044f0b33eab23b630c 
[16:50:23] <Bushycat> Xgc Hey, are you around
[16:51:21] <Xgc> Bushycat: Add the create table statements in the first panels.
[16:53:43] <Xgc> Bushycat: For a test case, follow my example.  It needs to contain the create table statements, and insert statements which provide the setup, so that your INSERTs can be tested with data and produce the error you don't understand.
[16:55:34] <Xgc> Bushycat: The bottom line is to find the bad data.  Your SQL is assuming the same customer over many orders contains exactly the same detail.
[16:56:54] <Xgc> Bushycat: If some customer is found to contain different detail in different orders, the SELECT DISTINCT ... will not collapse those rows into one single customer, leading to attempting to insert the same customer_id more than once.
[16:57:47] <Xgc> Bushycat: There are other ways to handle the INSERT ... SELECT ...; when you have bad data.
[16:58:06] <Xgc> Bushycat: Are you around?
[16:58:35] <Bushycat> okay, so you mentioned add create table statements in the first panel, not sure what it means
[16:59:03] <Bushycat> yes I am reading your lines and of course trying to understand each lines you type hence the delay
[16:59:50] <Xgc> Bushycat: https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=350f470ea6a23a5694784f9fd6aa583b
[17:00:01] <Bushycat> let me check 
[17:00:12] <Xgc> Bushycat: See the tables (create table statements) at the top?
[17:00:36] <Xgc> Bushycat: Now edit them so they are correct.
[17:02:55] <Bushycat> https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=efcbac4cd4a03565df11528c995dbc66
[17:03:27] *** Joins: z8z (~x@ac255238.ppp.asahi-net.or.jp)
[17:03:38] <Bushycat> the raw table I created through SSIS hence I do not have the script for that, SSIS imported all the 9994 rows right into SQL Server 
[17:04:07] <Xgc> Bushycat: Use your SQL Server tools to find the CREATE TABLE statement for lalwani_superstore_raw_stg.
[17:04:54] <Xgc> Bushycat: Once the table is created by your tools, you can find the corresponding create table statement.
[17:06:43] <Bushycat> I can try, does it not have something like show create table statement in SQL Server like MySQL?
[17:08:12] <Xgc> Bushycat: Yes.  I think it called "generate script" or something like that.
[17:09:46] <Xgc> I haven't used SSIS in many years.  I don't remember at all.
[17:10:37] <Bushycat> https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=02c905217ddaa996b52be068efd56244
[17:12:10] <Bushycat> what I do not understand when I run distinct for raw table it works fine and give me result 
[17:13:14] <Xgc> Bushycat: The SELECT is fine.  It's just that the result has some customers which show up in more than one result row, causing the INSERT to fail due to the PRIMARY KEY constraint.
[17:13:20] <Bushycat> here I added another line https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=ff44c153a5751609329e3c42b35f743b
[17:14:36] <Xgc> Bushycat: You need to edit your fiddle to remove the error.  Like this: https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=9170811f7fa4503b7bb0662eb4c87e6a
[17:15:08] <Xgc> Bushycat: Now add INSERT statements into lalwani_superstore_raw_stg that are causing the problems.
[17:15:46] <Bushycat> well in raw table of course customers and customer id is repeated many times, but that is why you brought that distinct command so that SQL can pick only one from these repeats so no idea why it cant pick now
[17:15:59] <Xgc> Bushycat: The other way to approach this is to remove the primary key constraint in stg_customers, so that the insert succeeds.  Then use SQL to find the duplicate customers.
[17:16:37] <Xgc> Bushycat: Focus.  I know all that.  The DISTINCT in your SELECT is expecting that all customer detail is IDENTICAL.  It's NOT!  That's the problem.
[17:16:47] <Bushycat> hold on, one thing at a time, i do not want my brain gets too warmed up as I have lot of work to do with this 
[17:17:03] <Bushycat> let mew check the link you posted 
[17:17:14] <Xgc> Bushycat: I'm focused in just the one problem. You are getting distracted by old detail.
[17:17:53] <Xgc> Do not add any detail that I did not ask you to provide.
[17:18:38] <Xgc> <Bushycat> well in raw table of course customers and customer id is repeated many times,   (DON'T wander into this territory.  I know all this.  It's not the problem.)
[17:19:11] <Xgc> The problem is your assumptions are wrong about what customer detail exists in the raw data.
[17:19:38] *** Joins: masber (~masber@213.55.224.23)
[17:19:45] <Xgc> If you're ready to find the problem, there's a SIMPLE way to find it.
[17:19:55] <Bushycat> I am listening 
[17:20:19] <Xgc> 1) Remove the PRIMARY KEY constraint in the target customers table.
[17:20:25] <Bushycat> I am not sure how to correct or edit fiddle at this point, that fiddle site is mainly for sharing SQL quries right
[17:20:54] <Bushycat> okay let me try 
[17:20:58] <Xgc> Bushycat: Don't get distracted.  The data is not in the fiddle.  You have a data issue.  Let's find it.
[17:21:41] <Xgc> Bushycat: You could add the data to the fiddle.  But you don't know which data is bad, yet.  I'll help you find it.
[17:22:35] <Xgc> Let me know when that's done.
[17:22:44] <Bushycat> i removed that constrain 
[17:23:15] <Xgc> Now run the procedure to insert from staging to customers.
[17:23:25] <Bushycat> I really like to see which data is bad, i mean that distinct command when I ran in raw data worked fine, it picked up 700 or so customer ID 
[17:23:54] <Bushycat> but with insert statement it cant pick up, i do not understand that 
[17:24:18] <Xgc> Bushycat: When you're dealing with a relatively large amount of data, it's often difficult to notice problems.
[17:24:35] <Bushycat> well I do not even know what is the problem
[17:24:38] <Xgc> Bushycat: I will help you identify the problem customer.
[17:24:50] <Xgc> Bushycat: Please just do what I asked.
[17:25:06] <Bushycat> well how could a customer is problem? You know that distinct code, let me post it here 
[17:25:17] <Xgc> Bushycat: STOP!!!
[17:25:23] <Xgc> Bushycat: Don't waste time.
[17:25:25] <Bushycat> select count(distinct [Customer ID]) as total_count
[17:25:25] <Bushycat> from lalwani_superstore_raw_stg;
[17:25:28] <Bushycat> select count(distinct [Customer ID]) as total_count
[17:25:28] <Bushycat> from lalwani_superstore_raw_stg;
[17:25:43] <Xgc> Why do you insist on wasting time?
[17:25:54] <Bushycat> okay okay
[17:26:05] <Xgc> I don't want you to do anything unless told to.
[17:26:18] <Xgc> No comments.  No nothing.
[17:26:23] <Xgc> Are you ready?
[17:26:41] <Bushycat> I wait further instruction from you, i was just trying to tell you that distinct command picked up 793 distinct customer_id hence customer
[17:26:55] <Bushycat> okay ready
[17:26:56] <Xgc> I will repeat my direction.
[17:27:01] <Bushycat> okay
[17:27:13] <Xgc> <Xgc> 2) Now run the procedure to insert from staging to customers.
[17:27:47] <Xgc> Do this from scratch, starting with an empty customers table.
[17:28:22] <Xgc> Tell me when that is done.
[17:30:09] <Bushycat> just confirming this one you want me to run again with distinct in it ? https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=4e0eb479f5a383af1dc06068f076efdd
[17:30:11] <Xgc> To empty customers: DELETE FROM customers;
[17:30:42] <Xgc> Bushycat: Yes.  DELETE FROM customers;  so it contains no rows.  Then run the INSERT ... SELECT DISTINCT ...;
[17:31:14] <Xgc> Use the real table name.  I shortened it here, because the other is annoying.
[17:31:45] <Xgc> stg_customers is not that bad.  I didn't want to type the other.
[17:31:56] *** Quits: herjazz (~herjazz@p10295192-ipngn22201marunouchi.tokyo.ocn.ne.jp) (Quit: leaving)
[17:31:57] <Xgc> Tell me when the INSERT is complete.
[17:32:14] <Bushycat> it took it this time, told me 4910 rows affected 
[17:32:31] <Xgc> No errors, correct?
[17:33:50] *** Quits: wakeup (~wakeup@user/wakeup) (Quit: Client closed)
[17:33:53] <Bushycat> well no error in execution but when I look at the result, its all repeating 
[17:33:55] <Xgc> Bushycat: Now run the following to find problem customers:  SELECT customer_id, COUNT(*) AS n FROM stg_customers GROUP BY customer_id HAVING COUNT(*) > 1 ORDER BY customer_id;
[17:34:12] *** Joins: irontom (~user@69.174.103.168)
[17:34:13] <Xgc> Bushycat: That will list only the bad customers.
[17:34:44] <Bushycat> before I do anything can I ask something
[17:34:49] <Xgc> Sure.
[17:35:39] <Xgc> Maybe you should wait until after you see the bad customers.
[17:35:39] <Bushycat> i ran select * from stg_customer, it returned 4910 result and customer_id and customer names are all repeated many many times, so that distinct comman in insert statement did not work 
[17:35:52] <Xgc> No.  You didn't.
[17:36:09] <Xgc> You ran: select * from stg_customers;
[17:36:39] <Xgc> Bushycat: Please now follow my direction.  I'll answer that question in a moment.
[17:36:50] <Bushycat> okay
[17:37:13] <Xgc> Bushycat: The result will show each duplicated customer, along with how many times it was duplicated.]
[17:37:29] <Xgc> Well, how many times it was found.  n-1 are duplicates.
[17:38:13] <Xgc> Bushycat: Tell me how many total rows are in this result.
[17:40:46] <Bushycat> oh yes very interesting result indeed
[17:41:03] <Bushycat> it tells me the frequency of each customer_id 
[17:41:14] <Xgc> Yes.  How many customers were found?
[17:41:16] <Bushycat> and the frequency is high 
[17:41:59] <Bushycat> 780 rows 
[17:42:09] <Xgc> 4) SELECT COUNT(DISTINCT customer_id) AS total FROM stg_customers;
[17:42:55] <Bushycat> I have done that before tha I was telling you and the result was like 793 or something
[17:43:03] <Bushycat> let me run your code 
[17:43:08] *** Quits: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr) (Ping timeout: 246 seconds)
[17:43:10] <Xgc> Run it again.  I need to know the exact value.
[17:44:16] <Xgc> I hope you're not retyping everything every time.
[17:44:42] <Xgc> Copy / paste / execute ... should take a few seconds.
[17:45:13] <Bushycat> I had to retype, its windows machine and I could not manage to get hold of any good but free IRC client, the one best suited me is called IceChat which is free but it wont allow me to copy and paste 
[17:45:57] <Xgc> I'm not sure what you mean by that.  HexChat is nice.  But there are many.
[17:46:23] <Bushycat> 793
[17:46:45] <Xgc> You need to learn how to use your irc client.  Copy/paste is in all clients.  But you can also log the chat history and then view/edit the log.
[17:47:06] <Bushycat> HexChat is not free in windows machine, it has a month trial and then it asks money $19.99 to be specific, I run hexChat in other machine, the one I communicated with you yesterday and its free in Linux
[17:47:36] <Bushycat> well usually I donto work in windows machine other than SQL Server, PowerBI etc
[17:47:45] <Bushycat> 793 
[17:47:58] <Bushycat> its the same as raw table customer_id count anywya 
[17:48:19] <Bushycat> so what is happening, 13 extra means what?
[17:48:21] <Xgc> "HexChat is an IRC client based on XChat, but unlike XChat it’s completely free for both Windows and Unix-like systems. Since XChat is open source, it’s perfectly legal. "
[17:48:58] <Bushycat> Well, its binary, no windows installer, i found
[17:49:07] <Xgc> Bushycat: You have 13 valid customers.  The others have duplicates.
[17:49:20] <Xgc> Bushycat: Just run it.
[17:49:24] <Bushycat> let me think a bit
[17:49:50] <Xgc> Don't worry about hexchat now. That's another problem you may want to solve.
[17:50:10] <Xgc> Bushycat: Now, let's look at one customer.
[17:50:17] <Xgc> Bushycat: Are you ready?
[17:50:56] <Xgc> If you have enough detail, I'll stop.  If not, please pay attention.
[17:51:30] <Xgc> The delays are annoying.
[17:52:11] <Xgc> Ok. I'll check back later.
[17:52:32] <Bushycat> i am trying to understand your group by with having cluase command, it sure gives me the frequency of each customer_id but in SQL server, in bottom right panel it gives me 780 rows, my question is 780s rows does not mean adds of all the frequency is it?
[17:53:16] <Xgc> The solution will be relatively trivial. I am trying to walk you through it all.  Too much waiting.  Feel free to review.  We can revisit this later.
[17:53:26] <Bushycat> does 780 rows mean same as if I add the frequency?
[17:54:13] <Bushycat> can we have 10 minutes break, my brain needs some break
[17:55:19] <Xgc> 25 minutes until half time.  Then I'll have about 15 minutes if you're ready to continue.  Then we can continue at the end of the match, before the next match.
[17:58:19] *** Joins: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr)
[18:00:43] <Bushycat> i am still looking at your group by command with having cluase
[18:00:53] <Bushycat> it does not gives me 780 as result
[18:01:10] <Bushycat> it just gives me the distribution for each customer_id 
[18:01:26] <Bushycat> so 780 is row count 
[18:13:56] *** Joins: rgrinberg (~textual@177.248.158.166)
[18:13:56] <Xgc> Bushycat: It ignored the customers with only one customer row, to focus only on the duplicated cases.
[18:14:30] <Xgc> HAVING COUNT(*) > 1 .. means just those where customer_id was found 2 or more times.
[18:17:33] *** Quits: irontom (~user@69.174.103.168) (Ping timeout: 260 seconds)
[18:19:55] <Xgc> 5) SELECT customer_id, COUNT(*) AS n FROM stg_customers GROUP BY customer_id HAVING COUNT(*) > 1 ORDER BY COUNT(*);   -- Choose the first customer_id.
[18:20:39] <Xgc> 6) SELECT * FROM stg_customers WHERE customer_id = 'above id';   -- Review these rows to see why they are not identical.
[18:22:32] <Xgc> You could use the same COUNT(DISTINCT colname) for each stg_customers column to find the columns of interest.
[18:23:12] <Xgc> 7) SELECT customer_id, COUNT(*) AS n, COUNT(DISTINCT colname) AS cx FROM stg_customers GROUP BY customer_id HAVING COUNT(*) > 1 ORDER BY COUNT(*);   -- Choose the first customer_id.
[18:24:02] <Xgc> Add a select list item for each column name that is likely to contain non-unique detail for each customer.
[18:25:02] <Xgc> Some customer names could be mistyped, some addresses could be different, etc.
[18:26:21] <Xgc> The solution is to decide how to calculate the best/correct value.  You may choose to use MAX(customer_name), etc (for each problem columns) when inserting each customer.
[18:26:38] <Bushycat> well
[18:26:53] <Bushycat> I only understood the first statement 
[18:27:49] <Xgc> Bushycat: Do you understand step 5?
[18:28:04] <Bushycat> yes
[18:28:11] <Bushycat> but before I go into it 
[18:28:18] <Bushycat> I need to discuss something 
[18:28:18] <Xgc> Do it. Let me know when that's done.
[18:28:44] <Xgc> Feel free to ask.  I have about 5 or 10 minutes.
[18:29:17] <Bushycat> I think its 4117 customers who are duplicated 
[18:29:22] <Bushycat> not 13
[18:29:46] <Xgc> Bushycat: No. There are 793 customers.
[18:30:07] <Xgc> 780 of them have duplicates.
[18:30:31] <Xgc> Those duplicates are why you see the 4117 rows.
[18:32:04] <Bushycat> okay I will need to think about it more but for now let just go ahead with all the instructions from you 
[18:34:53] <Bushycat> okay the first one gives me five result with same names of all the customers 
[18:35:34] <Bushycat> there city, state etc are different but first and last name are same
[18:36:50] <Bushycat> I am not sure about 6, 7, and other instructions above
[18:36:59] <Xgc> Bushycat: Here's solution #1: https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=8ed032a278f36676f7987e01d146512f
[18:37:00] <Bushycat> no 6 is fine 
[18:37:09] <Bushycat> let me check 
[18:37:14] <Xgc> Bushycat: We can discuss why later.
[18:37:39] <Xgc> See the last 2 panels.  DELETE all the rows first.  Then execute the last panel.
[18:38:11] <Xgc> That should insert 793 rows.
[18:38:41] <Xgc> Bushycat: If that works, you can re-add the primary key constraint and rerun.
[18:39:04] <Bushycat> delete from command, what is it? truncate you mean?
[18:39:23] <Xgc> Bushycat: Deletes all existing rows, transactionally.
[18:39:44] <Xgc> In many databases, truncate is not transactional.
[18:39:55] <Xgc> You can do that too, if you wish.
[18:40:59] <Bushycat> okay deleted 
[18:41:11] <Bushycat> let me read that important insert comman
[18:41:15] <Xgc> Now the last panel.
[18:41:43] <Bushycat> what about primary key
[18:42:02] <Bushycat> can we not add primary key constraint first 
[18:42:27] <Xgc> Sure.
[18:43:04] <Xgc> You can always skip steps.  It will generate unique customer_id rows.
[18:43:16] <Bushycat> let me see i can add that, is it same as dropping command
[18:43:25] <Xgc> If you were less sure, you might do it one step at a time.
[18:43:49] <Bushycat> but we need PK because of sales table 
[18:43:51] <Xgc> Just DROP the table and CREATE TABLE again.
[18:44:09] <Bushycat> right 
[18:44:12] <Xgc> Bushycat: One step at a time.  Solve this one.  Then move on.
[18:44:48] <Xgc> Bushycat: It's fine to think ahead, but that was causing you to struggle yesterday.  Too many balls to juggle.
[18:45:17] <Xgc> If you skip 3 steps, you might not understand what broke down.
[18:46:08] <Xgc> This solution was obvious at the start of our conversation yesterday, when you first reported customer issues.
[18:46:41] <Xgc> But it took time for you to understand.  I'm not sure you quite get it yet.  I just skipped ahead without explaining, since that was taking too much time.
[18:47:10] <Bushycat> The INSERT statement conflicted with the CHECK constraint "check_city". The conflict occurred in database "MyProjects", table "dbo.stg_customers".
[18:47:10] <Bushycat> The statement has been terminated.
[18:47:20] <Xgc> We can discuss why when you're ready.
[18:47:27] <Bushycat> I am gonna fix this, I am gonna remove that check constrain 
[18:47:46] <Xgc> Bushycat: Remove the CHECK constraints.  Don't add them until you understand the data.
[18:48:12] <Xgc> That was a mistake from the beginning.
[18:49:36] <Bushycat> it worked this time, 793 rows afected 
[18:49:53] <Bushycat> you know your work right at the tip of your finger, dont you
[18:50:29] <Xgc> Well, it took time for me too.
[18:50:31] <Bushycat> I have many questions but I wont articulate it now cos brain is heated up 
[18:51:36] <Bushycat> for example I ran query like select count(*) - count(distinct(customer_id)) and it gives me over 4000 result, i figured they are the repeated customers
[18:51:57] <Xgc> Most of the SQL questions have been answered for me.  I'm usually working on data issues, how to extract what I need in complex hierarchies, in a reasonable way, with acceptable performance.
[18:52:20] <Bushycat> i understood your group by command with having clause but i simply did not understand how that tranlsted to 780 rows 
[18:52:32] <Xgc> and often with data problems to solve.
[18:53:07] <Xgc> Bushycat: The GROUP BY customer_id tells you how many rows will be produced, one row per customer_id.
[18:53:24] <Bushycat> well you have lot of brain I tell you that much, it appears SQL or these stuffs what you know or what I am doing now is lot harder than lets say web application development in Python or PHP
[18:53:45] <Xgc> Bushycat: Without the HAVING, that would produce 793 rows.  With the HAVING, we removed 13 rows from the result, the ones without duplicates.
[18:54:59] <Bushycat> yes yes I know all that I understood group by and having cluase but you know the numbers, it was not summery result in terms of sum of (sum) it just tells each customer with how many times they appeared, so i am not sure how that without adding them all up we know it 780 rows cos surely row count
[18:54:59] <Bushycat> has nothing to do with summing up all these customer
[18:54:59] <Bushycat>  frequency sitting inside each customer_id 
[18:55:21] <Xgc> Bushycat: It seems easier for me because I don't have SQL questions. I've solved most of those.  I only need to understand your data.  You have SQL and data questions about your problem.  Much more to deal with.
[18:56:22] <Xgc> Bushycat: 780 came from the row count alone.  There was no magic.
[18:57:01] <Xgc> 793 came from the SELECT COUNT(DISTINCT customer_id) FROM stg_customers;
[18:57:14] <Bushycat> hard to explain but I will review these script, as it appears this script will be backbone for any future work 
[18:57:28] <Bushycat> yes I understand 793
[18:57:40] <Bushycat> I understood group by as well
[18:57:58] <Bushycat> i just di dnot understand 780 row count and how it relates with group by having statement 
[18:58:09] <Xgc> "so i am not sure how that without adding them all up we know it 780 rows"  ... That was just the row count.
[18:58:31] <Xgc> The row count of the GROUP BY ... HAVING ... result.
[18:58:38] <Bushycat> okay
[18:58:41] <Xgc> You understood some of that, but not all of it.
[18:58:43] <Bushycat> did not know that
[18:58:59] <Bushycat> yes I understood some of the group by 
[18:59:05] <Bushycat> i understood having cluase for sure
[18:59:32] <Bushycat> cos having clause distate you wanted the frequency of occuring each customer_id more than one time
[18:59:40] <Bushycat> dictate 
[19:02:02] <Bushycat> so the original problem as you detected was when it tried to add primary key athe distinct command could not able to do it because with one customer_id there were multiple customer names
[19:02:25] <Bushycat> so now what is this max command 
[19:03:38] <Bushycat> i know max means miximum but how that applies with string values and why you brought to fix issue
[19:03:47] <Xgc> Bushycat: 1) SELECT DISTINCT col1, col2, col3 FROM ...;  produces a row for each distinct (col1, col2, col3) triple.
[19:04:18] <Xgc> Bushycat: So your result for SELECT DISTINCT customer_id, other columns, ...   was not one row per customer_id.
[19:04:45] <Xgc> Bushycat: That would only be true if all the other customer related detail were identical over all the customer rows.
[19:05:00] <Xgc> over all the staging rows.
[19:05:39] <Xgc> For some customers, maybe the city detail was different.  Maybe the address detail was different.  Each difference would produce another result for that customer.
[19:06:09] <Bushycat> no idea cant process it in brain 
[19:06:12] <Xgc> Only 13 customers had all their detail exactly the same over all staging orders.
[19:07:56] <Xgc> Bushycat: In staging: If customer_id = 'CUST01' had name of 'CUST NAME 01' in one order, and also 'CUST NAME 01b' in another order (in staging), SELECT DISTINCT customer_id, customer_name FROM staging;  would produce 2 rows in the result, for the same customer_id.
[19:08:38] <Xgc> SELECT DISTINCT ... operates over all columns selected.
[19:09:24] <Xgc> Bushycat: It's possible you'll need to practice some of this..
[19:13:56] <Xgc> Bushycat: Check the last few panels.  I added a test to show the GROUP BY logic.  https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=8700aee69f1e4fbafa688aa0c158fea4
[19:15:06] <Xgc> Bushycat: We hoped each customer detail in staging was the same per customer.  But it's not.  So we have to pick one value for each column per customer, to guarantee that only one row is produced per customer.
[19:15:36] <Xgc> MAX(colname) is how we chose the one value per customer.
[19:16:47] <Xgc> As long as the data type is usable with MAX(...), we can do this for any columns.
[19:17:07] <Bushycat> i am creating another table for product category 
[19:17:21] <Bushycat> for insert do i need to add group by like you did 
[19:18:00] <Bushycat> https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=c9da1b8e57a7eea6c383183005ef376c
[19:18:32] <Xgc> Bushycat: If you have a similar problem, sure.
[19:19:17] <Xgc> Bushycat: Think about your SQL.  It looks wrong.
[19:19:52] <Xgc> Bushycat: Are you suggesting that product_category_id is the product_id?
[19:19:53] <Bushycat> what looks wrong
[19:20:15] <Bushycat> yes 
[19:20:30] <Bushycat> product_category_id is in stg_product table 
[19:20:44] <Bushycat> and product_id is in raw table
[19:21:03] <Xgc> Bushycat: Is a product related to one and only one category or multiple categories?
[19:22:08] <Bushycat> no idea
[19:22:19] <Bushycat> but i ran it and it says 1862 affected 
[19:22:31] <Bushycat> means i did not run into same issue like customer table 
[19:23:19] <Bushycat> so what it means, each product_category_id is not related to multiple products category right
[19:23:35] <Xgc> Bushycat: Do you have a category table?
[19:23:41] <Bushycat> we had that problem with custmer
[19:23:51] <Bushycat> i just made one now 
[19:23:58] <Bushycat> and insert statement worked out 
[19:24:32] <Xgc> Bushycat: That means a product can be associated with more than one category.  We can't really answer the question properly without knowing if that is expected and ok.
[19:24:38] *** Quits: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr) (Ping timeout: 260 seconds)
[19:25:01] <Xgc> Bushycat: Maybe some of the categories are mistakes / typos.
[19:26:30] <Xgc> Bushycat: <Bushycat> no idea  ... until you can answer that question, you have an open question.  That needs to be answered / resolved.  You can't just blindly create a mess.
[19:27:18] <Xgc> Bushycat: Now we can do this for customer, because we don't really care (at the moment) about mistyped addresses or names.  We just pick one and move on.  But category could be important.
[19:28:08] <Xgc> For instance, if you thought the customer_id(s) could be wrong, you would have to resolve that first.  The same goes for category.
[19:28:32] <Xgc> or what you called category_id.
[19:29:46] <Bushycat> i jusr ran into same issue this time with products 
[19:30:13] <Bushycat> Msg 2627, Level 14, State 1, Line 145
[19:30:13] <Bushycat> Violation of PRIMARY KEY constraint 'product_key'. Cannot insert duplicate key in object 'dbo.stg_products'. The duplicate key value is (FUR-BO-10002213).
[19:30:13] <Bushycat> The statement has been terminated.
[19:30:43] <Xgc> Right. Try to use the same process to solve this one.
[19:31:00] <Bushycat> well it already mentioned which one is causing issue
[19:31:34] <Xgc> Remove the constraints;  Insert;  Check the data for duplication;  Identify the duplicates;  Resolve the problem with an identical approach.
[19:33:00] <Xgc> Bushycat: Nope. You stopped too early.  You only know you have a problem because some products have various other column values and are not identical per product across all staging data.  Maybe this is a category issue.  You never answered that one.
[19:33:40] <Xgc> Bushycat: If a product can have multiple categories, you can't have that column in the products table.
[19:34:44] <Xgc> Product probably has fewer columns compared to customers.  This should be easier to review.
[19:35:18] <Bushycat> yes yes
[19:35:23] <Bushycat> you are the BOSS
[19:35:29] <Bushycat> let me paste bin
[19:36:02] <Bushycat> how on earth you already know all these, without even looking at anything, i cant figure out
[19:37:39] <Bushycat> https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=beeab9f186ade9603a7729d369b6a0d4
[19:38:10] <Bushycat> so this gives me 10 rows with same product ID but 10 different products hence it cant add primary key right
[19:40:43] <Xgc> Not yet.  That's right.
[19:42:17] <Xgc> Bushycat: Well, that one doesn't tell you there are duplicates.  Use DISTINCT
[19:42:44] <Xgc> Bushycat: and make sure you select all the columns you are trying to stuff into the products table.
[19:43:06] <Bushycat> head is warmed up again 
[19:43:09] <Bushycat> cant process 
[19:43:20] <Bushycat> it appeared this project needs lot more time than I thought
[19:43:55] <Bushycat> 1862 rows affected
[19:44:01] <Bushycat> i followed what you did and it worked
[19:44:19] <Bushycat> but conceptually I am not sure what this max doing 
[19:44:59] <Xgc> Bushycat: Check the last 2 panels of the fiddle: https://dbfiddle.uk/?rdbms=sqlserver_2019&fiddle=60ede6a7f632271297f3c3eb4ced0a3e
[19:45:17] <Xgc> Bushycat: Run that last query against your data.
[19:46:51] *** Joins: irontom (~user@69.174.103.168)
[19:47:36] *** Joins: fabic (~fabic@lfbn-reu-1-379-8.w92-130.abo.wanadoo.fr)
[19:47:43] <Bushycat> I will call it a day my friend, my head brain cant process any more, but good progress today, i may spend more time over the week, but I really need to do my other courses cos I cant just spend all day in this you see
[19:48:21] <Bushycat> but one thing I am sure, I wont look for SQL developer jobs
[19:48:34] <Bushycat> its not my thing, its really hard as far I can tell
[19:50:01] <Xgc> Bushycat: It takes time, even when it might become your thing.
[19:50:52] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Read error: Connection reset by peer)
[19:50:55] <Xgc> There's a bunch of theory behind this.  With some understanding of theory and practice, it's not that bad.
[19:51:10] *** Joins: maret_ (~maret@nat-88-212-37-89.antik.sk)
[19:51:11] <Bushycat> okay
[19:51:22] <Bushycat> i mean i enjoy it but my brain does gets very tired
[19:51:34] <Xgc> I've been there.
[19:52:23] <Bushycat> for example I am learning PHP, Python, PowerBI, and I learnt HTML, CSS, and also learning JavaScript etc, and of course I learnt SQL coding, but this is what I expereince over the last couple of days I found most complex than anything else 
[19:53:26] <Xgc> SQL is very different from the rest.  Most of the others (accept css/html) are nearly identical in concepts / general structure.
[19:53:34] <Xgc> except
[19:54:01] <Bushycat> and for you as if its all you already knew whats coming, i cant understand that, you know exactly what issues in the table and whereas I am scratching my head all day just to understand which rows and which columns related to which distnct and which group by etc 
[19:55:00] <Bushycat> yeah same for loop, if conditionals, while loop, functions and classes for Python, PHP, JavaScript more or less and nothing is as hard as this one I am telling you 
[19:55:42] *** Joins: MrZeus_ (~MrZeus@185.206.227.135)
[19:56:47] <Bushycat> PowerBI, Tableau etc are like girls things, even kindergarten kids can crate graphs in Tableau 
[19:57:06] <Bushycat> I am telling you among all, this is most hardest in my opinion 
[19:57:17] <Xgc> We tend to think procedurally / naturally.  We don't think declaratively, typically. SQL is declarative ... except for the procedural constructs.
[19:57:26] *** Joins: reset (~reset@user/reset)
[19:58:12] <Bushycat> oh yeah, coding is not hard cos as you said I know how exactly the computer will read one line of code and check correct or not and then move on to next line of code, so its lot easy than this SQL stuff 
[19:58:27] <Xgc> That's why it tends to be difficult to pick up.  Our minds have to twist in odd ways to think the right way.
[19:59:58] <Xgc> We can do it.  It just takes a little time.  Eventually, it becomes easy.  We even develop translations from procedural thought to declarative thought.... and like magic, it's easy.
[20:00:02] <Bushycat> i can code like x = 1 while x < 10 x = x + 1 print ("SQL is Hard, Python is Easy") and this while loop will run it 10 times or so, easy stuff
[20:01:22] <Bushycat> that code is easy cos I can see as you said computer will read one line at a time whereas in SQL i have no idea after writing snippet of SQL codes, what it gonna throw at me 
[20:04:48] *** Joins: pulseaudio (~pulseaudi@2409:4060:2e98:20fb:6ca2:924:b016:91a)
[20:06:15] *** Quits: pulseaudio (~pulseaudi@2409:4060:2e98:20fb:6ca2:924:b016:91a) (Remote host closed the connection)
[20:14:31] *** Joins: palasso (~palasso@user/palasso)
[20:16:49] *** Quits: rgrinberg (~textual@177.248.158.166) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[20:21:24] *** Joins: MrZeus__ (~MrZeus@194.37.96.119)
[20:22:55] *** Quits: kristijonas_ (~kristijon@cl-78-158-23-234.fastlink.lt) (Remote host closed the connection)
[20:23:04] *** Joins: MrZeus (~MrZeus@86.106.136.215)
[20:24:29] *** Quits: MrZeus_ (~MrZeus@185.206.227.135) (Ping timeout: 246 seconds)
[20:25:53] *** Quits: MrZeus__ (~MrZeus@194.37.96.119) (Ping timeout: 246 seconds)
[20:49:29] *** Joins: wakeup (~wakeup@user/wakeup)
[20:58:52] *** Joins: The_Blode_ (~Blode@user/the-blode/x-7164444)
[20:59:29] *** Quits: The_Blode (~Blode@user/the-blode/x-7164444) (Ping timeout: 246 seconds)
[21:13:22] *** Joins: acidjnk (~acidjnk@p200300d0c7049f70e9fe474f3be10533.dip0.t-ipconnect.de)
[21:18:56] *** Joins: nsk_nyc (~nsk_nyc@network45-238-142-host-234.inethn.net)
[21:24:54] *** Joins: vnf (~vnf@85.174.194.51)
[21:25:51] *** Quits: vnf (~vnf@85.174.194.51) (Read error: Connection reset by peer)
[21:26:07] *** Joins: vnf (~vnf@85.174.194.51)
[21:33:47] *** Quits: irontom (~user@69.174.103.168) (Ping timeout: 246 seconds)
[22:11:42] *** Joins: rvalue- (~rvalue@user/rvalue)
[22:11:56] *** Quits: rvalue (~rvalue@user/rvalue) (Ping timeout: 265 seconds)
[22:15:34] *** rvalue- is now known as rvalue
[22:22:42] *** Joins: cthulchu (~Cthulchu@69-172-160-196.cable.teksavvy.com)
[22:25:26] *** Quits: wakeup (~wakeup@user/wakeup) (Quit: Client closed)
[22:34:30] *** Joins: cerill (~cerill@h-155-4-198-153.A785.priv.bahnhof.se)
[22:36:03] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[22:37:06] *** Quits: Joanna (uid57472@id-57472.lymington.irccloud.com) (Quit: Connection closed for inactivity)
[23:07:22] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[23:08:50] *** Joins: palasso (~palasso@user/palasso)
[23:16:41] *** Quits: maret_ (~maret@nat-88-212-37-89.antik.sk) (Quit: maret_)
[23:17:23] *** Quits: cliluw (~cliluw@47.147.77.43) (Ping timeout: 260 seconds)
[23:17:47] *** Joins: cliluw (~cliluw@47.147.77.43)
[23:19:38] *** Joins: maret (~maret@nat-88-212-37-89.antik.sk)
[23:29:54] *** Joins: The_Blode (~Blode@user/the-blode/x-7164444)
[23:31:18] *** Joins: Rashad (~textual@2a01:9700:13d6:d000:f9a4:ee1a:ded0:358b)
[23:31:58] *** Quits: Rashad (~textual@2a01:9700:13d6:d000:f9a4:ee1a:ded0:358b) (Client Quit)
[23:32:05] *** Quits: The_Blode_ (~Blode@user/the-blode/x-7164444) (Ping timeout: 246 seconds)
[23:41:45] *** Quits: vnf (~vnf@85.174.194.51) (Quit: Leaving)
[23:46:42] *** Joins: pulseaudio (~pulseaudi@103.42.175.103)
