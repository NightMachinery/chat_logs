[00:03:05] *** Quits: darkstarx (~darkstard@50.39.115.145) (Read error: Connection reset by peer)
[00:03:53] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 268 seconds)
[00:04:14] *** Joins: sudomann (~sudomann@98.219.211.182)
[00:05:57] *** Joins: darkstarx (~darkstard@50.39.115.145)
[00:06:53] *** Quits: darkstarx (~darkstard@50.39.115.145) (Remote host closed the connection)
[00:07:20] *** Joins: darkstarx (~darkstard@50.39.115.145)
[00:13:02] *** Quits: goldfish (~goldfish@user/goldfish) (Quit: Leaving)
[00:13:51] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 250 seconds)
[00:17:00] *** Quits: boON (~boON@user/boon) (Quit: boON)
[00:20:07] *** Joins: hellstabber (~hellstabb@178.233.16.12)
[00:22:25] *** Joins: sudomann (~sudomann@98.219.211.182)
[00:28:43] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 252 seconds)
[00:33:57] *** Quits: Tach (~Tach@user/tach) (Quit: Tach)
[00:36:50] *** Quits: hellstabber (~hellstabb@178.233.16.12) (Quit: My MacBook Air has gone to sleep. ZZZzzz‚Ä¶)
[00:54:10] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Quit: Leaving)
[00:54:39] *** Joins: thiras (~thiras@user/thiras)
[00:54:44] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[01:03:22] *** Joins: sudomann (~sudomann@98.219.211.182)
[01:08:54] *** Joins: PanickedKernel (~PanickedK@user/panickedkernel)
[01:10:48] <PanickedKernel> Hey all. I'm using docker-compose to manage a few things. In 1 docker-compose.yaml file I have a few services. Some of the services depend on one of the services and I've set depends_on and it seems to work. But if I update the everything and goto restart the services I a error:
[01:11:07] <PanickedKernel> ERROR: for service1  Cannot start service service1: No such container: XXXXXXXXXXXXXXXX
[01:11:34] <PanickedKernel> I;m pretty new to docker, am I missing something obvious to handle this?
[01:12:18] <PanickedKernel> I've only found if I completely rm the images and containers and rebuild everything it works again until a update
[01:14:05] *** Quits: OZ8AAZ (~OZ8AAZ@77.240.70.168) (Quit: Leaving...)
[01:15:06] <rawtaz> PanickedKernel: its hard to tell when we dont know anything about your configuration or setup or how you run it, more than what you said.
[01:15:25] <rawtaz> but try the following: when you want to make a change; run docker-compose down, make the change, then run docker-compose up.
[01:16:03] <PanickedKernel> rawtaz, Ah. When I updated I did: docker-compose pull then docker-compose up -d
[01:16:12] <PanickedKernel> I didn't use docker-compose down
[01:17:45] <rawtaz> pull is what you use when you wnat to pull the images you are referring to in your compose file
[01:17:50] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 256 seconds)
[01:19:20] <PanickedKernel> rawtaz, Doing that would update the apps though right?
[01:19:30] <PanickedKernel> Basic config I'm using: https://pastebin.com/uzTrn7DS
[01:19:49] <PanickedKernel> I was looking to update the apps
[01:19:51] <rawtaz> it would update the images
[01:19:58] <rawtaz> which is what i presume you mean by the apps
[01:20:18] <rawtaz> so yeah
[01:20:18] <PanickedKernel> rawtaz, Correct. 
[01:20:42] <rawtaz> then you of course need to restart stuff, and i just meant that the most safe way to do that (in that it completely takes down and then recreates stuff) is doing down and up
[01:21:32] <PanickedKernel> rawtaz, I'm new to using docker. I thought I had read that to update things you just need to run pull then up to rebuild everything.
[01:21:56] <PanickedKernel> But it seems that when the vpn image is updated, the other images depending on that freak out.
[01:22:10] <rawtaz> well that should generally work, but of course it depends on what changes have been made and so on
[01:22:32] <rawtaz> that makes sense because the container that was the old vpn one has then been replaced by a new one
[01:22:44] <rawtaz> so when the other ones reference the old one's ID that no longer exist
[01:23:28] <PanickedKernel> rawtaz, That seems EXACTLY what is happening. I kinda expected the other images to automatically get updated as well due to the depends_on.
[01:23:33] <PanickedKernel> Like a cascading update
[01:24:43] *** Quits: AnapodoPsalidaki (~AnapodoPs@2a02:587:291b:fbb7:b2d6:2c91:bf3f:91b4) (Quit: Leaving)
[01:25:19] <traderjoe> rawtaz: you were talking about disney in the disney channel bruh... just fyi... :)
[01:27:02] <rawtaz> yeah i know
[01:27:05] <rawtaz> :)
[01:27:07] <rawtaz> sorry
[01:28:23] *** Quits: thiras (~thiras@user/thiras) (Remote host closed the connection)
[01:29:56] *** Joins: thiras (~thiras@user/thiras)
[01:41:56] *** Quits: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[01:43:18] *** Quits: iomari891 (~iomari891@197.210.53.147) (Ping timeout: 260 seconds)
[01:51:59] *** Quits: gopar (~gopar@c-67-164-79-22.hsd1.ca.comcast.net) (Remote host closed the connection)
[01:52:19] *** Joins: gopar (~gopar@c-67-164-79-22.hsd1.ca.comcast.net)
[01:57:57] *** Joins: sudomann (~sudomann@98.219.211.182)
[02:05:44] <programmerq> PanickedKernel‚ñ∏ what's the exact error message you get with that compose file? You redacted the container name, so it's hard to tell what the problem is when it's blacked out like that.
[02:06:05] <programmerq> but in generall, docker-compose pull && docker-compose up -d is a 100% expected-to-work and normal workflow.
[02:06:20] <programmerq> 'docker-compose down' for an update is bad advice, not sure why that was suggested.
[02:07:21] <programmerq> PanickedKernel‚ñ∏ I do notice that in your paste, you have a yaml indentation problem, but I wouldn't expect it to cause that issue.
[02:07:41] <programmerq> If you changed the container_name in the compose file since the last time 'docker-compose up -d' was run, it may be getting in the way.
[02:08:15] <programmerq> normally, you wouldn't need or want to override the container_name like that
[02:08:54] <PanickedKernel> programmerq, The error was:
[02:08:57] <PanickedKernel> ERROR: for qbittorrent  Cannot start service qbittorrent: No such container: 17fcd74c4960707f25c31f89d3aa3ba2f9279cc5dc759d51980c0c6233c66fad
[02:09:01] *** Quits: rpthms (~rpthms@user/rpthms) (Remote host closed the connection)
[02:09:16] <PanickedKernel> qBittorrent used depends_on for pia-wireguard
[02:09:29] <programmerq> there's no qbittorrent container in the pasted compose file
[02:09:51] <PanickedKernel> Right, I didn't post the full config. s/prowlarr/qbittoreent
[02:10:28] <programmerq> if you run 'docker ps -a', do you see that container id? I think what might be happening is the pia-wireguard container was recreated, and the existing qbittorrent container still has a reference to the old container id.
[02:10:44] <programmerq> the depends_on only cares that the service is started and isn't aware of container mode networking, unfortunately.
[02:10:49] <PanickedKernel> programmerq, That is *exactly* what I suspect as well
[02:11:25] <PanickedKernel> Unfortunately I did compose down and recreated everything, so right now everything is again working as expected.
[02:11:28] <programmerq> I've tinkered with container mode networking a bit-- I find it's really only useful for quick one-off troubleshooting type usecases. I did run a similar type of setup for a hamachi-like setup on a gameserver
[02:11:39] <programmerq> I think I ended up doing: docker-compose up -d --pull --force-recreate
[02:12:12] <PanickedKernel> I'm gonna write that down for the next update and see if it helps. 
[02:12:36] <gebbione> --force-recreate indeed
[02:12:39] <rawtaz> programmerq: why do you think docker-compose down is a bad thing to advice?
[02:12:39] <PanickedKernel> I expected depends_on to be "smarter" and kinda cascade
[02:12:45] <programmerq> one pattern that's worth pointing out is that a kubernetes pod can be one or more containers that share a network namespace. The whole pod restarts when it restarts, which means every single container in that pod does the trick
[02:12:48] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 256 seconds)
[02:13:09] <gebbione> rawtaz, just takes longer. You can recreate and should be the same as down and up again
[02:13:17] <programmerq> rawtaz: that's basically saying "get rid of everything including images and volumes" which is way way overkill when you just need new images swapped in. docker-compose up is what you want when updating an existing project.
[02:13:20] *** Joins: sudomann (~sudomann@98.219.211.182)
[02:13:32] <programmerq> but if you have data you care about in a named volume, down will kill it.
[02:13:37] *** Joins: rpthms (~rpthms@user/rpthms)
[02:13:41] <gebbione> only with -v
[02:13:49] <programmerq> ah yes, they did add that
[02:13:58] <gebbione> down will just kill containers, not named volumes
[02:14:56] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Read error: Connection reset by peer)
[02:15:02] <rawtaz> programmerq: well, fair enough i see your point. i was just trying to give them the most fool-proof thing they could possibly do. i didnt think there was any data loss involved (and still dont). it was just a way to isolate if they had done something terribly wrong that would make the stuff not work at all
[02:15:33] <programmerq> it's fine to point out big hammer last resorts, just disclaim it as such üëç
[02:15:58] <gebbione> is a rancher question too border line for here?
[02:16:05] <rawtaz> ill try to clarify that better next time :)
[02:16:14] <programmerq> gebbione go for it. I won't be much help, but it can't hurt.
[02:16:20] <rawtaz> +1
[02:16:28] <rawtaz> Gopher it, as we say :>
[02:16:46] <gebbione> it is actually published here
[02:16:47] <gebbione> https://forums.rancher.com/t/letsencrypt-ssl-multiple-domains-usinv-ame/36433
[02:16:47] <PanickedKernel> I think that's the first time I'ver ever seen a actual emoji on irc :o
[02:16:56] <PanickedKernel> (The thumbs up)
[02:17:11] <programmerq> I like the occasional emoji every once in a while.
[02:17:13] <gebbione> basically on legacy rancher i use a loadbalancer to expose port 80 entry points for letsencrypt acme
[02:17:45] <programmerq> and yes, I agree with you PanickedKernel that the container mode networking in compose should be smarter. It's arguably a bug, but it doesn't get used terribly often.
[02:18:12] <gebbione> i checked the go code but i am not a go dev so i could not figure out if what i am doing is supposed to work with multiple subdomains as currently it is not and only works for the first domain only in the 'port_rules'
[02:18:13] <PanickedKernel> Nothing wrong with using them :) Just never seen a actual emoji. Just stuff like :) :< \o/ >:|
[02:18:18] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 260 seconds)
[02:18:58] <programmerq> personally, I'm considering adding a VPN egress vlan on my home network and then I can have any docker machine that can trunk get a macvlan network called 'vpn' and the containers would just egress over that.
[02:18:59] <gebbione> so basically a cert for app.domain.com is generated, but for other.domain.com doesnt
[02:19:39] <PanickedKernel> programmerq, Totally. It works well until well it doesn't. Other then this issue it's met all my needs.
[02:20:18] <PanickedKernel> My home network runs on shoestring and hope lol
[02:21:02] <PanickedKernel> Frankenstein'ed "server" running parts from 10 different machines that reboots if anything slightly touches it lol
[02:21:13] <PanickedKernel> But it works(tm)
[02:23:11] *** Quits: gebbione (~gebbione@cpc152037-finc21-2-0-cust213.4-2.cable.virginm.net) (Quit: Leaving)
[02:30:02] *** Quits: Sven_vB (~sven@user/sven-vb/x-2094958) (Remote host closed the connection)
[02:30:36] *** Joins: Sven_vB (~sven@user/sven-vb/x-2094958)
[02:32:01] *** Joins: sudomann (~sudomann@98.219.211.182)
[02:34:25] *** Joins: slice (~slice@user/slice)
[02:35:34] *** Joins: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net)
[02:36:10] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[02:36:20] *** Quits: zer0bitz (~zer0bitz@dsl-hkibng32-54fbfb-173.dhcp.inet.fi) (Read error: Connection reset by peer)
[02:36:36] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 256 seconds)
[02:37:09] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Quit: maret)
[02:39:26] *** Quits: rardiol (~quassel@user/rardiol) (Ping timeout: 256 seconds)
[02:42:17] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[02:52:52] *** Quits: [diablo] (~diablo]@user/diablo/x-9068044) (Quit: Ping timeout (120 seconds))
[02:53:22] *** Joins: [diablo] (~diablo]@user/diablo/x-9068044)
[02:55:57] *** Joins: Tach (~Tach@user/tach)
[03:08:47] *** Quits: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[03:10:20] *** Quits: Tach (~Tach@user/tach) (Quit: Tach)
[03:14:04] *** Joins: CodeSpelunker (~CodeSpelu@user/codespelunker)
[03:17:01] *** Quits: Enitin (~Enitin@82.102.22.85) (Ping timeout: 252 seconds)
[03:20:19] *** Quits: Agrius (uid483993@id-483993.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[03:22:25] *** Joins: pyrrhus (~pyrrhus@user/pyrrhus)
[03:22:43] *** Joins: Enitin (~Enitin@82.102.22.84)
[03:41:25] *** Quits: lantech19446 (~lantech19@144.202.10.138) (Ping timeout: 250 seconds)
[03:44:31] *** Joins: tetrapod (~tetrapod@user/tetrapod)
[03:57:37] *** Joins: jazzy (~jaziz@user/jaziz)
[04:02:55] *** Quits: nvmd (~nvmd@user/nvmd) (Quit: Later, nerds.)
[04:05:02] *** Quits: CodeSpelunker (~CodeSpelu@user/codespelunker) (Ping timeout: 240 seconds)
[04:21:49] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Quit: Leaving)
[04:22:08] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[04:24:40] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[04:25:10] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[04:27:48] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 268 seconds)
[04:28:42] *** Joins: hellstabber (~hellstabb@178.233.16.12)
[04:29:37] *** Quits: lilgopher (~textual@2601:241:8000:38f0:fc6d:baa:b0b5:9af8) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[04:30:30] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Read error: Connection reset by peer)
[04:30:45] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[04:30:55] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Read error: Connection reset by peer)
[04:31:42] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[04:32:23] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Remote host closed the connection)
[04:33:00] *** Joins: frittro-mob (~frittro@user/frittro)
[04:36:09] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[04:40:02] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Remote host closed the connection)
[04:40:30] *** Quits: sehrope (~sehrope@23-24-81-162-static.hfc.comcastbusiness.net) (Remote host closed the connection)
[04:40:50] *** Joins: sehrope (~sehrope@23-24-81-162-static.hfc.comcastbusiness.net)
[04:40:53] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[04:43:13] *** Quits: frittro-mob (~frittro@user/frittro) (Quit: Ka kite an≈ç au i a koe.)
[04:43:28] *** Joins: frittro-mob (~frittro@user/frittro)
[04:45:06] *** Quits: nexgen (~nexgen@li614-178.members.linode.com) (Remote host closed the connection)
[04:45:23] *** Joins: nexgen (~nexgen@li614-178.members.linode.com)
[04:50:09] *** Quits: Brainium (~brainium@user/brainium) (Quit: Konversation terminated!)
[04:57:26] *** traderjoe is now known as njka
[04:59:45] *** Joins: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net)
[05:03:46] *** Quits: hellstabber (~hellstabb@178.233.16.12) (Quit: My MacBook Air has gone to sleep. ZZZzzz‚Ä¶)
[05:06:19] *** Joins: hellstabber (~hellstabb@178.233.16.12)
[05:10:21] *** Joins: [diablo]5 (~diablo]@user/diablo/x-9068044)
[05:12:15] *** Quits: frittro-mob (~frittro@user/frittro) (Remote host closed the connection)
[05:12:22] *** Quits: [diablo] (~diablo]@user/diablo/x-9068044) (Ping timeout: 260 seconds)
[05:12:22] *** [diablo]5 is now known as [diablo]
[05:12:31] *** Joins: frittro-mob (~frittro@user/frittro)
[05:15:24] *** Quits: PanickedKernel (~PanickedK@user/panickedkernel) (Quit: Leaving)
[05:23:28] *** Quits: hellstabber (~hellstabb@178.233.16.12) (Quit: Textual IRC Client: www.textualapp.com)
[05:26:08] *** Quits: very_sneaky (~very_snea@user/very-sneaky/x-7432109) (Quit: very_sneaky)
[05:30:04] *** Joins: CodeSpelunker (~CodeSpelu@user/codespelunker)
[05:31:23] *** Joins: very_sneaky (~very_snea@user/very-sneaky/x-7432109)
[05:33:29] *** Quits: gopar (~gopar@c-67-164-79-22.hsd1.ca.comcast.net) (Remote host closed the connection)
[05:38:24] *** Quits: dalan (~dalan@194-193-164-106.tpgi.com.au) (Quit: Ping timeout (120 seconds))
[05:38:35] *** Quits: minimal (~minimal@user/minimal) (Quit: Leaving)
[05:38:44] *** Joins: dalan (~dalan@194-193-164-106.tpgi.com.au)
[05:40:21] *** Joins: thiras (~thiras@user/thiras)
[05:47:43] *** Quits: tsal (~tsal@user/tsal) (Ping timeout: 252 seconds)
[05:51:54] *** Quits: jkovac1 (~jkovac1@user/jkovac1) (Quit: The Lounge - https://thelounge.chat)
[05:52:05] *** Joins: jkovac1 (~jkovac1@user/jkovac1)
[05:52:51] *** Joins: tsal (~tsal@user/tsal)
[05:57:58] *** Quits: CodeSpelunker (~CodeSpelu@user/codespelunker) (Remote host closed the connection)
[05:58:21] *** Joins: CodeSpelunker (~CodeSpelu@user/codespelunker)
[05:59:33] *** Joins: artok (~azo@mobile-access-5d6a99-113.dhcp.inet.fi)
[05:59:35] *** Quits: CodeSpelunker (~CodeSpelu@user/codespelunker) (Remote host closed the connection)
[06:00:55] *** Quits: frittro-mob (~frittro@user/frittro) (Read error: Connection reset by peer)
[06:01:08] *** Joins: frittro-mob (~frittro@user/frittro)
[06:04:40] *** Quits: leitz (~LeamHall@cpe-173-172-58-72.tx.res.rr.com) (Quit: nappy time)
[06:09:43] *** Quits: frittro-mob (~frittro@user/frittro) (Ping timeout: 252 seconds)
[06:10:46] *** Joins: frittro-mob (~frittro@user/frittro)
[06:11:22] *** Quits: frittro-mob (~frittro@user/frittro) (Client Quit)
[06:12:02] *** Joins: sudomann (~sudomann@98.219.211.182)
[06:14:56] *** Joins: gopar (~gopar@c-67-164-79-22.hsd1.ca.comcast.net)
[06:15:30] *** Quits: pyrrhus (~pyrrhus@user/pyrrhus) (Quit: pyrrhus)
[06:15:37] *** Quits: gopar (~gopar@c-67-164-79-22.hsd1.ca.comcast.net) (Remote host closed the connection)
[06:19:34] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 260 seconds)
[06:20:15] *** Joins: dox (~dox@dox.la)
[06:28:25] *** Joins: Czernobog (~Czernobog@user/czernobog)
[06:29:40] *** Quits: sehrope (~sehrope@23-24-81-162-static.hfc.comcastbusiness.net) (Read error: Connection reset by peer)
[06:30:59] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 252 seconds)
[06:34:16] *** Quits: rvalue (~rvalue@user/rvalue) (Quit: ZNC - https://znc.in)
[06:34:32] *** Joins: rvalue (~rvalue@user/rvalue)
[06:34:53] *** Joins: sudomann (~sudomann@98.219.211.182)
[06:39:31] *** Quits: dinowilliam (~dinowilli@168.194.162.233) (Ping timeout: 250 seconds)
[06:50:04] *** Joins: dmalteseknight (~dmaltesek@user/dmalteseknight)
[06:52:44] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 256 seconds)
[07:12:21] *** Quits: cornduck (~cornduck@gateway/tor-sasl/cornduck) (Ping timeout: 276 seconds)
[07:13:59] *** Joins: cornduck (~cornduck@gateway/tor-sasl/cornduck)
[07:21:47] *** Joins: sudomann (~sudomann@98.219.211.182)
[07:23:47] *** Quits: jazzy (~jaziz@user/jaziz) (Ping timeout: 252 seconds)
[07:25:59] *** Joins: jazzy (~jaziz@user/jaziz)
[07:27:14] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 260 seconds)
[07:59:15] *** Quits: slice (~slice@user/slice) (Quit: zzz)
[08:00:41] *** Joins: lilgopher (~textual@2601:241:8000:38f0:2013:ba19:4db0:9b26)
[08:01:04] *** Joins: kenwoodfox (~quassel@user/kenwoodfox)
[08:03:38] *** Quits: Church (~aleph@pool-98-116-232-112.nycmny.fios.verizon.net) (Ping timeout: 268 seconds)
[08:10:45] *** Quits: gschanuel21 (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[08:10:55] *** Joins: gschanuel217 (~gschanuel@user/gschanuel)
[08:42:04] *** Quits: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[08:46:46] *** Joins: sehrope (~sehrope@23-24-81-162-static.hfc.comcastbusiness.net)
[08:58:40] *** Quits: gschanuel217 (~gschanuel@user/gschanuel) (Quit: Ping timeout (120 seconds))
[08:59:01] *** Joins: gschanuel21 (~gschanuel@user/gschanuel)
[09:02:52] *** Joins: Church (~aleph@pool-98-116-232-112.nycmny.fios.verizon.net)
[09:07:06] *** Quits: artok (~azo@mobile-access-5d6a99-113.dhcp.inet.fi) (Ping timeout: 260 seconds)
[09:27:32] *** Quits: sudoforge (~sudoforge@wireguard/tunneler/sudoforge) (Ping timeout: 240 seconds)
[09:32:27] *** Joins: sudomann (~sudomann@98.219.211.182)
[09:37:54] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 260 seconds)
[09:50:40] *** Joins: artok (~azo@mobile-access-5672d1-115.dhcp.inet.fi)
[09:51:32] *** Quits: andycooper (uid246432@id-246432.helmsley.irccloud.com) (Quit: Connection closed for inactivity)
[09:57:37] *** Joins: slice (~slice@user/slice)
[10:02:57] *** Joins: maret (~maret@nat-88-212-37-89.antik.sk)
[10:19:01] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Quit: maret)
[10:24:22] *** Quits: slice (~slice@user/slice) (Quit: zzz)
[10:24:23] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[10:25:15] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[10:34:19] *** Quits: Sven_vB (~sven@user/sven-vb/x-2094958) (Killed (NickServ (GHOST command used by Sven_vB_)))
[10:34:22] *** Joins: Sven_vB_ (~sven@user/sven-vb/x-2094958)
[10:35:19] *** Quits: AtomicElephants (~AtomicEle@user/atomicelephants) (Quit: Leaving)
[10:37:35] *** Sven_vB_ is now known as Sven_vB
[10:43:13] *** Quits: lilgopher (~textual@2601:241:8000:38f0:2013:ba19:4db0:9b26) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[10:45:13] *** Quits: divine (~divine@2001:470:8247:1::31) (Ping timeout: 250 seconds)
[10:46:02] *** Joins: divine (~divine@2001:470:8247:1::31)
[10:59:37] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[10:59:58] *** Quits: jkovac1 (~jkovac1@user/jkovac1) (Quit: The Lounge - https://thelounge.chat)
[11:00:09] *** Joins: jkovac1 (~jkovac1@user/jkovac1)
[11:04:32] *** Quits: Maxattax (~max@50-195-160-193-static.hfc.comcastbusiness.net) (Ping timeout: 240 seconds)
[11:08:57] *** Joins: n9nes (~n9nes@user/n9nes)
[11:09:03] *** Quits: jkovac1 (~jkovac1@user/jkovac1) (Quit: The Lounge - https://thelounge.chat)
[11:09:15] *** Joins: jkovac1 (~jkovac1@user/jkovac1)
[11:15:37] *** Quits: kevr (~kevr@user/kevr) (Remote host closed the connection)
[11:16:03] *** Joins: kevr (~kevr@user/kevr)
[11:43:31] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[11:43:59] *** Joins: maret (~maret@195.12.158.102)
[11:53:18] <very_sneaky> does anybody know specifically what the dod do to images that are uploaded into their iron bank? what qualifies them as "hardened"?
[11:55:28] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[11:56:09] <very_sneaky> scratch that, turned out it was pretty easy to google. They have a container hardening guide: https://dl.dod.cyber.mil/wp-content/uploads/devsecops/pdf/Final_DevSecOps_Enterprise_Container_Hardening_Guide_1.1.pdf
[12:10:27] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[12:13:07] *** Joins: ninjabreakbot (~ninjabrea@192-0-170-208.cpe.teksavvy.com)
[12:20:06] *** Quits: ninjabreakbot (~ninjabrea@192-0-170-208.cpe.teksavvy.com) (Changing host)
[12:20:06] *** Joins: ninjabreakbot (~ninjabrea@user/ninjabreakbot)
[12:30:58] *** Joins: slice (~slice@user/slice)
[12:31:38] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[12:32:56] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[12:36:43] *** Joins: iomari891 (~iomari891@197.210.77.169)
[12:37:27] *** Joins: jmd (~user@de.cellform.com)
[12:50:50] *** Quits: memoryleak (~memorylea@77-58-142-250.dclient.hispeed.ch) (Ping timeout: 265 seconds)
[12:52:27] *** Joins: usr123 (~usr123@124.253.2.79)
[12:54:18] <usr123> Hello All, I'm getting this error when trying to get kafka up with docker-compose: https://dpaste.org/7e4u
[12:59:40] <jmd> There's something peculiar about "docker image ls".  I can't seem to get it to work in backquotes.
[13:00:33] <ikke> jmd: in what way?
[13:03:04] <jmd> ikke: I expect  id=$(docker image ls blah:blha --format='{{.ID}}') echo $id to print the id of the image
[13:06:11] *** Quits: yjftsjthsd3 (~yjftsjths@162.127.123.34.bc.googleusercontent.com) (Ping timeout: 245 seconds)
[13:06:15] <ikke> works for me
[13:06:39] <s17> no for me
[13:06:52] <jmd> Odd.  Must be my shell or something.
[13:09:41] *** Joins: gebbione (~gebbione@cpc152037-finc21-2-0-cust213.4-2.cable.virginm.net)
[13:10:16] <gebbione> hi folks, sharing this question again if anyone knows. https://forums.rancher.com/t/letsencrypt-ssl-multiple-domains-usinv-ame/36433 . The obvious answer is probably to have many services for each domain but sounds wasteful
[13:10:31] *** Quits: gebbione (~gebbione@cpc152037-finc21-2-0-cust213.4-2.cable.virginm.net) (Quit: Leaving)
[13:17:54] *** Joins: thiras (~thiras@user/thiras)
[13:25:01] *** Joins: Tach (~Tach@user/tach)
[13:26:33] *** Joins: yjftsjthsd3 (~yjftsjths@162.127.123.34.bc.googleusercontent.com)
[13:31:25] *** Quits: yjftsjthsd3 (~yjftsjths@162.127.123.34.bc.googleusercontent.com) (Ping timeout: 250 seconds)
[13:34:15] <jmd> How can I list just those containers pertaining to a particular image?
[13:34:57] *** Joins: yjftsjthsd3 (~yjftsjths@162.127.123.34.bc.googleusercontent.com)
[13:45:48] <ikke> jmd: with docker image inspect <container> --format '{{.Config.Image}}' you can find the image
[13:47:06] *** Quits: artok (~azo@mobile-access-5672d1-115.dhcp.inet.fi) (Ping timeout: 260 seconds)
[13:49:39] *** Joins: artok (~azo@mobile-access-b04815-253.dhcp.inet.fi)
[13:52:47] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[13:58:34] *** Joins: zer0bitz (~zer0bitz@dsl-hkibng32-54fbfb-173.dhcp.inet.fi)
[14:01:15] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[14:12:26] *** Quits: artok (~azo@mobile-access-b04815-253.dhcp.inet.fi) (Read error: Connection reset by peer)
[14:15:33] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[14:25:04] <jmd> I have created an image with "docker load < somefile.tar"  How can I modify somefile.tar in a sensible way?
[14:29:31] *** Joins: rsx (~dummy@ppp-188-174-129-166.dynamic.mnet-online.de)
[15:07:44] *** Quits: slice (~slice@user/slice) (Quit: zzz)
[15:20:06] <simplicity> Why are you creating images like that? Best avoided if you don't have a specific use case.
[15:21:13] <jmd> simplicity: Yeah I know.  I'm following an instruction from a third party.
[15:21:31] <jmd> How would you recommend creating an image?
[15:21:49] <simplicity> Using a Dockerfile
[15:22:46] *** Joins: night_wulfe_ (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[15:23:12] <jmd> Is there an easy way to get a Dockerfile from this tar ?
[15:23:32] <ikke> jmd: depends on what's in the tar
[15:23:35] *** Quits: bencc1 (~bencc1@2a00:a040:197:3af:1d4c:50a6:eb64:4733) (Quit: Leaving)
[15:23:51] <ikke> but one thing you can do is use ADD to copy the contents from the tar on top of another image
[15:23:57] <ikke> (which could be FROM scratch)
[15:24:45] <aminvakil> jmd: checkout dive (https://github.com/wagoodman/dive)
[15:24:57] <jmd> When I run tar -tf on it there's a ton of files like f03e76858b66122b4176b3cee3a311beaf34712a976a8579029e7eac4bd2feb3/layer.tar
[15:24:58] <jmd>  
[15:25:11] <ikke> So it's already an image
[15:25:42] <ikke> Then import it with docker load, use a dockerfile to build on top of it
[15:26:02] *** Quits: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 260 seconds)
[15:26:13] <jmd> How will that improve anything?
[15:26:37] <ikke> improve in what sense?
[15:26:39] <simplicity> Then you create a Dockerfile that uses COPY to change the files you need to change.
[15:26:53] <ikke> You basically have a base image, and you want to modify
[15:26:58] <jmd> ikke: Well why do you suggest doing that?
[15:27:07] <ikke> jmd: you have a tar file containing an image
[15:27:12] <ikke> you use docker load to load that image
[15:27:25] <ikke> after that, it's just normal docker flow to build on top of it
[15:27:40] *** Quits: usr123 (~usr123@124.253.2.79) (Read error: Connection reset by peer)
[15:27:45] <jmd> I see (I think)
[15:27:56] <jmd> How should that DockerFile look then?
[15:27:58] <ikke> Instead of using docker pull <baseimage> (or having docker pull it automatically), you use docker load
[15:28:03] <ikke> You need to know the name of the image
[15:28:05] *** Joins: usr123 (~usr123@124.253.110.228)
[15:28:15] <jmd> Yeah I know that.
[15:28:22] <ikke> then you do FROM <name>
[15:28:29] <ikke> or FROM <name>:<tag>
[15:28:38] <jmd> in the DockerFile ?
[15:28:40] <ikke> yes
[15:29:15] <jmd> and then docker create ?
[15:29:37] <ikke> docker build
[15:30:02] <ikke> You need to add more to the Dockerfile to modify things
[15:30:04] <jmd> Then I'll end up with two images.
[15:30:16] <ikke> yes, that's typically how you work with docker
[15:30:28] <ikke> images are immutable, you build on top of them
[15:30:38] <jmd> ok
[15:31:01] <jmd> So the minimal DockerFile is "FROM foo:bar"
[15:31:02] <ikke> Note that the new image will only contain the modifications you make on top of the base image
[15:31:15] <ikke> Yes, but just doing that does not make a lot of sense
[15:31:28] <ikke> (except for just trying out)
[15:31:33] <jmd> Right.
[15:31:46] <jmd> Then what do I do with that DockerFile
[15:32:12] <ikke> https://docs.docker.com/engine/reference/builder/
[15:32:28] <ikke> You can use commands like COPY, RUN etc to do things in the image
[15:35:12] *** Quits: jazzy (~jaziz@user/jaziz) (Quit: Beddie bye-bye)
[15:43:32] *** Quits: ses (uid38151@id-38151.hampstead.irccloud.com) (Quit: Connection closed for inactivity)
[15:46:04] *** Joins: Anticom (~anticom@ip-88-153-48-20.hsi04.unitymediagroup.de)
[15:49:07] *** Joins: leitz (~LeamHall@cpe-173-172-58-72.tx.res.rr.com)
[15:57:28] <jmd> Why is it, that inside a container I cannot run a shell script hosted outside?
[15:58:53] <ikke> Because the filesystem in the container is isolated
[15:59:22] <jmd> But I have mounted that filesystem
[15:59:45] <jmd> I can read the file ok, but get a strange error if I try to execute it.
[16:00:06] <jmd> tv_build: 5: export: -march: bad variable name
[16:06:51] *** Quits: ultima (~ultima@23.81.113.231) (Ping timeout: 256 seconds)
[16:08:34] *** Joins: ultima (~ultima@23.81.113.231)
[16:10:16] *** Quits: bn_work (uid268505@id-268505.uxbridge.irccloud.com) (Quit: Connection closed for inactivity)
[16:12:00] *** Quits: sehrope (~sehrope@23-24-81-162-static.hfc.comcastbusiness.net) (Remote host closed the connection)
[16:21:27] <rawtaz> jmd: if you have a problem, dont just provide a small part of the relevant info about it. show the exact command youre trying to run and all of its output
[16:21:31] <rawtaz> (pastebin of course)
[16:22:14] *** Quits: ultima (~ultima@23.81.113.231) (Ping timeout: 256 seconds)
[16:24:13] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[16:26:49] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[16:42:15] *** Joins: vlm (~vlm@user/vlm)
[17:01:03] *** Quits: Enitin (~Enitin@82.102.22.84) (Remote host closed the connection)
[17:02:42] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[17:02:49] *** Joins: Enitin (~Enitin@82.102.22.84)
[17:03:21] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[17:03:24] *** Joins: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net)
[17:05:50] *** Quits: rsx (~dummy@ppp-188-174-129-166.dynamic.mnet-online.de) (Quit: rsx)
[17:11:34] *** Joins: ultima (~ultima@23.81.113.229)
[17:20:17] *** Joins: Maxattax (~max@50-195-160-193-static.hfc.comcastbusiness.net)
[17:21:10] *** Quits: ultima (~ultima@23.81.113.229) (Ping timeout: 256 seconds)
[17:25:13] *** Quits: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[17:25:37] *** Joins: liefer4 (~liefer@user/liefer)
[17:27:42] *** Quits: usr123 (~usr123@124.253.110.228) (Read error: Connection reset by peer)
[17:27:53] *** Quits: liefer (~liefer@user/liefer) (Ping timeout: 268 seconds)
[17:27:54] *** liefer4 is now known as liefer
[17:28:11] *** Joins: usr123 (~usr123@27.255.222.228)
[17:37:18] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[17:37:55] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[17:42:00] *** Joins: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net)
[17:44:05] <jmd> Is there a --force or --overwrite option to stop docker run whinging about container names already in use?
[17:45:32] *** Quits: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net) (Client Quit)
[17:51:43] *** Joins: ultima (~ultima@23.81.113.231)
[17:54:32] *** Joins: sehrope (~sehrope@23-24-81-162-static.hfc.comcastbusiness.net)
[17:59:17] <rawtaz> can you rephrase that? :) 
[17:59:28] <rawtaz> what do you mean by "whinging about container names"?
[18:03:49] <Anticom> jmd: container names have to be unique since it's its identifier. Even if the container isn't currently running, doesn't mean it isn't present anymore. Look up your existing containers using `docker ps -a` to show stopped containers as well
[18:04:21] *** Quits: dmalteseknight (~dmaltesek@user/dmalteseknight) (Quit: WeeChat 3.3)
[18:05:01] <Anticom> jmd: and regarding your first question: a) you of course have to specify the path to the script inside your container not the path on your host matchine. Additionally if someone is complaining about -march that's probably you're trying to use a binary that was build on your host machine. -march is a flag that ususally specifies the system architecture
[18:05:28] <jmd> Anticom: I understand that.  But it's a PITA to have to always stop and remove a container before building something.
[18:06:40] <Anticom> If you want your container to be removed when it stops, just add `--rm` flag to `docker run`. #rtfm `docker run --help`
[18:06:41] <jmd> Yes it was complaining about -march.  If I had been running a binary then I could understand it, but this is a shell script.
[18:07:07] <Anticom> jmd: in that case what rwtaz said: share the script, otherwise we can't possibly help you out
[18:07:25] <Anticom> rawtaz*
[18:14:28] *** Quits: Anticom (~anticom@ip-88-153-48-20.hsi04.unitymediagroup.de) ()
[18:22:01] *** Joins: minimal (~minimal@user/minimal)
[18:24:21] *** Joins: sudoforge (~sudoforge@wireguard/tunneler/sudoforge)
[18:26:27] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[18:28:08] *** Quits: thiras (~thiras@user/thiras) (Quit: Leaving)
[18:32:32] *** Quits: jmd (~user@de.cellform.com) (Remote host closed the connection)
[18:45:39] *** Quits: yjftsjthsd3 (~yjftsjths@162.127.123.34.bc.googleusercontent.com) (Quit: Ping timeout (120 seconds))
[19:03:30] *** Quits: Deknos (~someone@user/menace) (Ping timeout: 260 seconds)
[19:15:08] *** Quits: cX1n (~cX1n@107.159.97.244) (Remote host closed the connection)
[19:17:52] <null23> hi again... my brain hurts... i just wanted to get https to my app.... i got wan<>switch<>docker-host with a reverse-proxy container( traefik )... I put A-record from one.com to point to my wan ip... but i get 404 but the cert seems to work... i have also port 80 and 443 but it dosnt work.. help?
[19:18:11] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[19:18:13] *** Joins: armin (~armin@unix.porn)
[19:25:12] *** Quits: sudoforge (~sudoforge@wireguard/tunneler/sudoforge) (Quit: 404)
[19:28:11] *** Quits: usr123 (~usr123@27.255.222.228) (Read error: Connection reset by peer)
[19:28:36] *** Joins: usr123 (~usr123@27.255.248.3)
[19:36:56] *** Joins: liefer9 (~liefer@user/liefer)
[19:39:14] *** Quits: liefer (~liefer@user/liefer) (Ping timeout: 268 seconds)
[19:39:15] *** liefer9 is now known as liefer
[19:41:06] *** Quits: maret (~maret@195.12.158.102) (Read error: Connection reset by peer)
[19:41:51] *** Joins: maret (~maret@195.12.158.102)
[19:45:28] *** Joins: lilgopher (~textual@2601:241:8000:38f0:2013:ba19:4db0:9b26)
[19:48:18] *** Quits: cornduck (~cornduck@gateway/tor-sasl/cornduck) (Ping timeout: 276 seconds)
[19:54:24] *** Joins: pkx (~pkx@user/pkx)
[19:54:26] *** Joins: cornduck (~cornduck@gateway/tor-sasl/cornduck)
[19:54:49] *** Quits: CombatVet (~c4@user/combatvet) (Remote host closed the connection)
[19:55:06] *** Joins: CombatVet (~c4@user/combatvet)
[19:59:07] *** Quits: mavhq (~quassel@mapp-14-b2-v4wan-161519-cust401.vm15.cable.virginm.net) (Ping timeout: 252 seconds)
[20:10:32] *** Quits: ninjabreakbot (~ninjabrea@user/ninjabreakbot) (Quit: bye)
[20:12:29] *** Joins: mavhq (~quassel@mapp-14-b2-v4wan-161519-cust401.vm15.cable.virginm.net)
[20:13:48] *** Joins: luva6 (~luva@37.120.197.52)
[20:16:51] *** Quits: maret (~maret@195.12.158.102) (Quit: maret)
[20:16:57] *** Quits: luva (~luva@37.120.197.46) (Ping timeout: 265 seconds)
[20:16:57] *** luva6 is now known as luva
[20:28:39] *** Joins: DoofusCanadensis (~DoofusCan@2604:3d09:47c:f970:7440:8136:6a93:eaac)
[20:29:14] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[20:41:53] *** Quits: noarb (~noarb@user/noarb) (Read error: Connection reset by peer)
[20:43:50] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[20:45:04] *** Joins: noarb (~noarb@user/noarb)
[20:52:10] *** Joins: gschanuel218 (~gschanuel@user/gschanuel)
[20:55:35] *** Quits: gschanuel21 (~gschanuel@user/gschanuel) (Ping timeout: 252 seconds)
[20:55:36] *** gschanuel218 is now known as gschanuel21
[20:56:52] *** Joins: supay (adas@adas.shelltalk.net)
[20:59:32] <supay> hey, i want to add https://github.com/mitchellkrogza/nginx-ultimate-bad-bot-blocker to the base nginx image.. so do i just do that manually by getting a shell into the nginx container, and then committing with my changes? or is there are different/better/standard way to achieve this?
[21:01:10] *** Joins: jmd (~user@de.cellform.com)
[21:03:04] *** Joins: AtomicElephants (~AtomicEle@user/atomicelephants)
[21:03:53] <jmd> If I don't give a --name when I do "docker run", where does it get those names from?
[21:08:43] <DoofusCanadensis> internal list. want the code?
[21:10:08] <jmd> Not really!
[21:10:25] <DoofusCanadensis> https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go
[21:10:29] <DoofusCanadensis> too late ;-)
[21:10:32] <supay> lol
[21:10:47] <supay> nice!
[21:10:52] <supay> i love how they commented explanations too
[21:11:07] <DoofusCanadensis> actually I love looking at that list because they tell who each person is
[21:11:13] <supay> yeah!
[21:11:28] <jmd> So the next question is, "Is the name really necessary?"
[21:11:30] <supay> nice easter egg. thanks for sharing DoofusCanadensis :)
[21:11:45] <supay> jmd, human referenceable..
[21:11:57] <DoofusCanadensis> and unique
[21:12:01] <supay> if you're a robot, you might love a nice alphanumeric string
[21:12:20] <DoofusCanadensis> there is a hash id for each container, but humans... they don't read those so good
[21:12:24] <jmd> the ID is more unique
[21:12:34] <DoofusCanadensis> both are required to be unique
[21:12:41] <DoofusCanadensis> within their own lists
[21:13:08] <DoofusCanadensis> you can call your container joes_nginx or let it come up with sneaky_benz
[21:13:10] <supay> jmd, solve this captcha please..
[21:13:44] <DoofusCanadensis> the id is absolutely unique, true. docker just requires that the human readable container name also be unique within that list 
[21:14:14] <supay> DoofusCanadensis, could you help with my noob query above, please!
[21:14:26] <supay> [22:59:31]  <supay> hey, i want to add https://github.com/mitchellkrogza/nginx-ultimate-bad-bot-blocker to the base nginx image.. so do i just do that manually by getting a shell into the nginx container, and then committing with my changes? or is there are different/better/standard way to achieve this?
[21:14:40] <DoofusCanadensis> thank you for repeating it
[21:14:59] <supay> haha, of course! :)
[21:15:13] <DoofusCanadensis> the proper way: build a Dockerfile that starts FROM the nginx image you're currently using and adds your modules
[21:15:40] <DoofusCanadensis> manually modifying a container and committing, while doable, is not repeatable
[21:16:17] *** Joins: sudomann (~sudomann@98.219.211.182)
[21:16:22] <supay> DoofusCanadensis, understood! thanks a ton. will do that
[21:23:29] *** Joins: ninjabreakbot (~ninjabrea@user/ninjabreakbot)
[21:26:15] <rawtaz> jmd: instead of going on about something that isnt a problem to anyone else, what is the actual problem you are having and that makes you ask about the naming of containers?
[21:26:52] <jmd> rawtaz: There's no problem.  I just wish to understand things better.
[21:27:33] <jmd> So instead of going on about what other people are saying why don't you desist from saying things if you can't say anything useful?
[21:27:35] <rawtaz> ok :) thats indeed a good thing
[21:28:27] <jmd> rawtaz: ok I apologize for that last comment.
[21:28:28] *** Quits: usr123 (~usr123@27.255.248.3) (Read error: Connection reset by peer)
[21:28:40] *** Joins: Lenkov45 (~Lenkov45@156.146.59.171)
[21:28:46] <rawtaz> jmd: stop fucking around. the reason i wrote what i did was that your previous visit in here before the current one wasnt exactly exemplary. you come across as someone who complains about stuff in an irrational way and without giving enough context in what you write and ask.
[21:28:50] <rawtaz> sure, whatever
[21:28:52] *** Joins: usr123 (~usr123@49.156.123.26)
[21:28:59] <rawtaz> looks like we came across on the wrong side of a fence lol
[21:29:20] <rawtaz> have a good day, i will add you to my ignore list so you dont have to see more of me
[21:33:18] *** Quits: Lenkov45 (~Lenkov45@156.146.59.171) (Ping timeout: 260 seconds)
[21:46:54] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 268 seconds)
[21:47:21] *** Joins: sudomann (~sudomann@98.219.211.182)
[21:53:02] *** Quits: noarb (~noarb@user/noarb) (Quit: ZNC 1.8.2 - https://znc.in)
[21:54:42] *** Joins: self (~self@user/hackers)
[21:54:45] *** self is now known as Guest9978
[21:54:57] *** Guest9978 is now known as hackers
[21:55:33] *** Quits: Sven_vB (~sven@user/sven-vb/x-2094958) (Remote host closed the connection)
[21:56:09] *** Joins: Sven_vB (~sven@user/sven-vb/x-2094958)
[21:57:57] *** Joins: noarb (~noarb@user/noarb)
[22:07:57] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 250 seconds)
[22:24:14] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[22:24:29] <null23> solved it... it must be a bug or something... i change certs back and forth and now it work perfect..
[22:24:45] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[22:26:00] *** Joins: sudomann (~sudomann@98.219.211.182)
[22:33:41] *** Joins: bencc1 (~bencc1@2a00:a040:197:3af:dea5:b03e:b19a:7e61)
[22:33:55] <bencc1> is it possible to make container see only 1 or to cpus?
[22:34:11] <ikke> Probably with cgroups
[22:34:12] <bencc1> tried with "--cpus=1" but cat /proc/cpuinfo still show all the cpus of the host
[22:35:00] <rawtaz> what problem are you trying to solve?
[22:35:08] <bencc1> some tools like ffmpeg create threads based on the number of cpus on the machine
[22:35:32] <bencc1> I want to create multiple containers on the host so I want each container to use less resources
[22:35:57] <rawtaz> makes sense
[22:36:12] <rawtaz> can you tell ffmpeg how many threads to create?
[22:36:14] <BtbN> You can limit how many threads FFmpeg uses directly.
[22:36:31] <bencc1> I'm doing this for ffmpeg but I have other tools that I don't know how to limit
[22:36:33] <rawtaz> generally speaking https://docs.docker.com/config/containers/resource_constraints/ lists the options you have for restricting stuff
[22:36:40] <bencc1> I'm also using chrome xvfb and pulseaudio
[22:36:44] <rawtaz> also https://docs.docker.com/engine/reference/run/
[22:37:10] <bencc1> but limiting cpu with --cpus=1 still exposes the real number of cpus to the container
[22:37:22] *** Quits: Czernobog (~Czernobog@user/czernobog) (Read error: Connection reset by peer)
[22:37:24] <bencc1> so the program still creates large number of threads
[22:37:28] <rawtaz> yeah
[22:37:37] <BtbN> The most brutal way to force a CPU limit is --cpuset-cpus, but that has some averse effects, since it pins it to exactly those cores
[22:37:37] <bencc1> I have a bottleneck in my container that I can't find
[22:37:51] <bencc1> what's the downside?
[22:37:56] <bencc1> of --cpuset-cpus?
[22:38:41] <bencc1> I only run the docker daemon and containers on this host
[22:39:06] <BtbN> It pins to precise cores/threads
[22:39:16] <bencc1> why is it bad?
[22:39:21] *** Quits: hackers (~self@user/hackers) (Ping timeout: 252 seconds)
[22:39:23] *** Joins: zitter (~danilo@host-79-22-181-75.retail.telecomitalia.it)
[22:39:33] <BtbN> Could be a bad cores of that CPU. And removes freedom from the scheduler.
[22:39:44] *** Joins: Czernobog (~Czernobog@user/czernobog)
[22:40:08] <BtbN> And I don't think it'll even affect the nproc result? It might tho, easy enough to try.
[22:42:31] <bencc1> it doesnt affect the result
[22:42:44] <bencc1> just checked and I still get the host cores in the container
[22:43:32] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[22:44:23] <rawtaz> silly question perhaps but have you tried simply creating a text file with whatever you want that /proc/cpuinfo to contain, and then bind mount that into the container?
[22:44:43] <BtbN> That's not how nproc operates.
[22:51:15] <rawtaz> was worth a shot
[22:55:24] <bencc1> it would be nice to limit chrome resources but I'm not sure how
[22:57:52] *** Joins: dudek (~dudek@185.150.236.103)
[23:13:40] *** Quits: iomari891 (~iomari891@197.210.77.169) (Quit: WeeChat 3.3)
[23:16:40] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[23:17:29] <rawtaz> bencc1: i think this is a generic linux question at this point. if theres a way to do it with linux, thats your chance. otherwise not going to happen. has nothing to do with docker it seems
[23:19:02] *** Quits: sudomann (~sudomann@98.219.211.182) (Ping timeout: 240 seconds)
[23:20:32] *** Quits: useful_idiot (~useful_id@gateway/vpn/pia/usefulidiot/x-43226899) (Ping timeout: 240 seconds)
[23:20:43] <bencc1> ok
[23:21:01] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[23:21:40] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[23:21:59] <minimal> I'm assuming docker is simply mounting the host machine's /proc/cpuinfo into the containers rather than providing a "custom" version of that pseudo-file
[23:22:45] *** Joins: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se)
[23:23:33] *** Quits: Mattiaslndstrm (~Mattiasln@c213-103-137-235.bredband.tele2.se) (Client Quit)
[23:23:35] <ikke> /proc is a virtual fs. I would assume things like namespaces / cgroups should affect what is returned
[23:23:36] <minimal> bencc1: does the "nproc" command inside the container give a different answer?
[23:24:06] <rawtaz> according to BtbN nproc doesnt work like that (presumably looking at /proc/cpuinfo to determine stuff)
[23:24:37] <minimal> ikke: that's what I'm wondering, whether there is anything in place to transform rather than just "passthrough" the info
[23:24:48] <bencc1> minimal: nproc gives me the reduced number when using --cpuset-cpus=0-1
[23:25:23] <BtbN> That hard-pins it to those exact physical cores/threads though
[23:25:34] <BtbN> so those will get extra hot
[23:25:44] *** Joins: sudomann (~sudomann@98.219.211.182)
[23:25:51] <ikke> minimal: what I mean that even if docker bindmounts /proc into the container, the kernel should be aware of it
[23:26:00] <bencc1> but "--cpus=1" doesn't affect nproc
[23:26:39] <ikke> minimal: just like all processes in /proc are a view of what's in the container
[23:27:55] <bencc1> what happens if I use --cpuset-cpus=0-3 for 2 containers?
[23:28:00] <bencc1> both will share the same cores?
[23:28:16] <bencc1> if that's the case I can device my host to multiple "islands"
[23:28:28] *** Quits: usr123 (~usr123@49.156.123.26) (Read error: Connection reset by peer)
[23:28:30] <minimal> ikke: right, I expect the process inside docker is filtered from /proc based on the PID namespec but suspect that (some) other stuff inside /proc like cpuinfo may not be "adjusted". I guess the only way to know is to look at the kernel code for /proc/cpuinfo
[23:28:40] <minimal> s/namespec/namespace/
[23:28:51] *** Joins: usr123 (~usr123@124.253.4.235)
[23:30:02] <bencc1> https://github.com/moby/moby/issues/20770
[23:30:18] <ikke> Yeah, that might be true, especially because things like htop show stats from the host by default
[23:31:40] *** Joins: useful_idiot (~useful_id@gateway/vpn/pia/usefulidiot/x-43226899)
[23:33:23] <minimal> ikke: yeah the fundamental differences between virtualisation and containerisation, with a shared kernel for containers its harder to hide some stuff from each container
[23:33:59] <ikke> Yes, certainly
[23:37:20] <minimal> bencc1: for the Ubuntu bug linked by that docker issue: "/proc/cpuinfo showing what is the container affinity. /proc/cpuinfo shows all the cpus available in the system but it does not have anything to do with affinity of container."
[23:38:05] <bencc1> ideally I would want to limit number of cores the container see without using affinity
[23:38:33] <bencc1> what happens if several containers use the same cores with affinity? will it work or does a container assume it has full control of the cores?
[23:38:35] <minimal> bencc1: spin up a VM then to run Docker with the required number of cores :-)
[23:40:29] <tabakhase> its not even just "harder" - go as far as "not supported" / "outside the scope"
[23:40:48] <minimal> bencc1: with affinity I think you are simply restricting that container to using the defined cores, you are *not* giving it exclusive use of them however
[23:40:50] *** Quits: usr123 (~usr123@124.253.4.235) (Ping timeout: 256 seconds)
[23:41:42] <bencc1> minimal: so it might work for me. I can try for example dividing 32 cores to groups of 8 cores
[23:41:48] <bencc1> and put N containers on each group
[23:45:23] <BtbN> Doing that is very inefficient though
[23:45:37] <BtbN> better to let the Scheduler handle it, and just set fair share parameters
[23:45:45] <minimal> I think one of the potential problems with doing that, which BtbN alluded to earlier, is that as your workload across all the containers is unlikely to be well balanced at all times there's a risk that some individual cores on the CPU die should get far hotter than others (hotspots)
[23:46:03] *** Quits: zitter (~danilo@host-79-22-181-75.retail.telecomitalia.it) (Read error: Connection reset by peer)
[23:46:35] <bencc1> still, I have a bottleneck
[23:46:47] <bencc1> a server with 8 cores is able to run N containers
[23:47:03] <bencc1> a server with 4*8 cores can't run 4*N containers
[23:47:20] <BtbN> You're probably IO bound then. Or the bigger CPU is just slower.
[23:47:25] <bencc1> I sometimes see spikes in the number of running processes and the cpu goes to 100%
[23:47:41] <bencc1> the bigger CPU is ryzen 3 vs ryzen 2
[23:47:43] <gurki> bencc1: so how about you do some proper evaluation. what exactly is happening, whats the cache bw, whats the memory bw, whats the io bw
[23:47:46] <bencc1> with higher clock speed
[23:47:49] <gurki> guessing wont help you
[23:47:52] <minimal> or perhaps its not 4*8 cores in a single CPU but multiple CPUs?
[23:47:54] <bencc1> faster nvme drive
[23:47:58] <bencc1> dedicated vs VM
[23:48:01] <BtbN> an 8 core Ryzen will be significantly faster than a 32 core one, per individual core.
[23:48:06] <bencc1> so the bigger machine is better in all aspects
[23:48:18] <gurki> zen is a nightmare regarding scaling since they present as a bunch of numa nodes
[23:48:26] <gurki> so its really easy to mess sth up
[23:48:35] <BtbN> Uhm, Zen CPUs present a single Numa Node
[23:48:39] <BtbN> that's what's no nice about them
[23:48:50] <BtbN> Even the big 64 core ones. One single Numa-Node.
[23:48:54] <bencc1> BtbN: cloud VM with 8 ryzen 2 hyper-threads 2GHz vs dedicated with 32 ryzen 3 hyperthreads 4GHz
[23:49:01] *** Quits: ninjabreakbot (~ninjabrea@user/ninjabreakbot) (Ping timeout: 252 seconds)
[23:49:14] <BtbN> "3 Hyperthreads" makes no sense.
[23:49:19] *** Joins: ses (uid38151@id-38151.hampstead.irccloud.com)
[23:49:20] <gurki> BtbN: i am literally using some zen machine right now which presents as multiple numa nodes
[23:49:23] <gurki> :)
[23:49:29] <bencc1> I didn't say 3 hyperthreads
[23:49:39] <BtbN> gurki, I have literally 30 of those machines right here, which right now presents a single Numa node :)
[23:49:48] <minimal> bencc1: "cloud VM" - so not a physical machine and no idea what the underlying physical machine actually is...
[23:50:06] <bencc1> minimal: I know the cpu spec of the host machine
[23:50:06] <gurki> which one? ryzen, tr or epyc?
[23:50:21] <bencc1> minimal: and I know the dedicated is better in all aspects
[23:50:31] <bencc1> so my bottleneck is in software
[23:50:55] <bencc1> I had an issue with ffmpeg creating 90 threads because it see 32 cores
[23:50:56] <minimal> yes I saw your discussion on the ffmpeg channel earlier this week
[23:51:02] <BtbN> gurki, Ryzens and Epycs.
[23:51:03] <bencc1> I've fixed it but still have a bottleneck
[23:51:17] <bencc1> minimal: are you spying on me? :)
[23:51:22] <gurki> BtbN: then your mainboard does weirdness. using epycs here
[23:51:28] <gurki> they certainly do not present as one by default
[23:51:32] <minimal> nope, just watch the ffmpeg channel in general :-)
[23:51:44] <BtbN> AMD literally advertises that as feature. You're just straight up wrong with your claim, or are confused what a Numa-Node is.
[23:52:02] <BtbN> Yes, they do have multiple CCXs. But that's not Numa-Nodes.
[23:52:47] <BtbN> https://bpa.st/LG5Q
[23:54:22] <minimal> I used to work with some pretty huge bare metal clusters but thankfully didn't need to dig very far into NUMA stuff
[23:54:46] <gurki> https://developer.amd.com/wp-content/resources/56308-NUMA%20Topology%20for%20AMD%20EPYC%E2%84%A2%20Naples%20Family%20Processors.PDF
[23:54:57] <gurki> BtbN: i found the reason for this confusion. it -is- configurable
[23:54:57] <BtbN> Our old Cluster was Intel, with multiple CPUs and Numa-Nodes per CPU
[23:55:01] <BtbN> it was a pain
[23:55:15] <gurki> wasnt aware. guess i learned sth today.
[23:55:16] <gurki> :)
[23:55:17] <BtbN> IB card was on a different CPU than your Code? Shit slowed to a crows.
[23:55:21] <BtbN> *crawl
[23:55:48] <BtbN> gurki, that's outdated documentation
[23:55:59] <BtbN> Naples is the very first gen Epyc
[23:56:08] <BtbN> Rome and higher literally only have a single Numa-Node.
[23:56:49] <minimal> BtbN: when I said pretty huge I meant it, 22+ node machines each with terrabytes of RAM, terrabytes of SSDs, and large numbers of cores. Was all Intel stuff, think the AMD Zen was only coming out at that time
[23:57:05] <minimal> s/machines/clusters/
[23:57:23] <gurki> https://downloads.dell.com/manuals/common/dell-emc-dfd-numa-amd-epyc-2ndgen.pdf
[23:57:25] <gurki> no
[23:57:25] <BtbN> We have several thousand nodes with Rome Epycs at work.
[23:57:45] <gurki> several thousand nodes set up this way *
[23:57:51] <minimal> BtbN: we did have some HP Superdomes which were quite weird
[23:57:54] <gurki> i will not argue that
[23:58:49] <BtbN> Each CPU being a single Numa-Node also makes sense. Given that Memory access and PCIe IO goes via a single central IO controller
[23:59:18] <gurki> dell even gives hints which numa configuration to use for your rome epyc for which workload
[23:59:19] *** Quits: useful_idiot (~useful_id@gateway/vpn/pia/usefulidiot/x-43226899) (Ping timeout: 250 seconds)
[23:59:21] <gurki> :>
[23:59:43] <BtbN> Well, they're wrong.
[23:59:55] <gurki> lol
