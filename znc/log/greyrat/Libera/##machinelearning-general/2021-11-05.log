[00:00:37] <dostoyevsky2> So ungrateful
[00:02:13] <dostoyevsky2> Samian: I think not many people use cudnn directly these days.  Just via pytorch/tf, but they don't use the function you mentioned
[00:03:13] <Samian> not yet they don't
[00:03:53] <Samian> dostoyevsky2  I'm trying to remember what you specialize in again. You seem to be all over the place, sort of like me
[00:05:28] <dostoyevsky2> Samian: reminds me of my job interviews... I made my CV a javascript page with 100 variables... I just activate those things the customer wants and then the CV generates itself..  and I also only talk about those things if I am invited for the interview
[00:07:12] <Klinda> when you apply the softmax, for each sample you have the features that has some values... what do they represent?
[00:08:11] <Samian> Klinda  are you asking what the output of the softmax represents? or what the input represents?
[00:08:31] <Klinda> I just take the (sample,feature) matrix
[00:08:42] <Klinda> then subtract for each element
[00:08:46] <Klinda> the max value in it
[00:09:07] <Klinda> then for each sample divide each element of that sample for the sum of it
[00:09:13] <Klinda> and then do the exponential
[00:09:22] <Samian> I don't know what a " (sample,feature) matrix" is
[00:10:02] <Klinda> you have in each row an observation with the features as columns
[00:10:28] <Klinda> # numerical stability
[00:10:29] <Klinda>     score = np.exp(x - np.max(x))
[00:10:31] <Klinda>     # calculate probability for each sample and each class
[00:10:32] <Klinda>     score = [ y/np.sum(y) for y in score ]
[00:10:34] <Klinda>     return np.array(score)
[00:11:36] <Klinda> so what's the meaning of data
[00:11:52] <Klinda> I mean yes you do these calculus, and each feature what represent
[00:12:11] <Samian> softmax output probabilities between mutually exclusive classifications.  like say you trained a neural net that output an array of 3 number. The first number is probability the image is a picture of Obama. Second number is probability the image is a picture of Trump. The third number is probability the image is a picture of another person.  The 3 numbers should sum to 1.
[00:15:42] <Klinda> I don't understand
[00:16:20] <Klinda> because what I see are only feature for each sample multiply by a y_oneshot
[00:17:11] <Samian> the softmax activation function converts "logits" (aka "activation values" aka the numbers that are the output of one layer and input of the next layer) into a probabilities between 0 and 1.
[00:18:06] <Klinda> so for example if you a sample with feature size = 0.1 and x_feature = 0.5 you will do like [0,0,1] * [0.1, 0.5]
[00:18:19] <Samian> Klinda  I'm not understanding you. Why not start by providing an example of something you're training a neural network to do
[00:18:54] <Samian> I gave an example of identifying which human the image a picture of.
[00:19:18] <Samian> notice that I can say the problem without typing any numbers or even mentioning AI
[00:21:56] <Klinda> I only see features that has some probability for each example
[00:26:11] <Samian> Klinda  how do you know whether I'm even interpreting that sentence to mean what you're trying to communicate? specifically, why do you think that I understand what you mean by "features".  If there's a decent chance that the word features in your mind is assigned to one idea and the word features in my mind is assigned to a different idea, why did you not clarify what you mean by features so the chance of misunderstanding is reduced?
[00:28:00] <Samian> If a question is misunderstood, then the person trying to answer basically answers a different question from the one the person asked.  So, first step to answering a question is to make sure to know what the question is.
[00:28:44] <Samian> Likewise, your initial goal should be to make sure your question is properly understood. If it isn't, how can the person help you?
[00:28:58] *** Quits: causative (~halberd@user/causative) (Remote host closed the connection)
[00:29:30] <Klinda> imagine you have a dataset composed as this http://en.wikipedia.org/wiki/Special:Search?go=Go&search=0.1,0.9
[00:29:31] <SigmoidFroid> ⇒  Search - Wikipedia
[00:29:35] <Klinda> ups
[00:29:37] <Samian> meh I'm getting back to work.
[00:29:56] *** Joins: causative (~halberd@user/causative)
[00:29:58] <Klinda> [ [ 0.1 , 0.9] ]
[00:30:23] <Klinda> you have one example with two features
[00:30:39] <Klinda> you just did all the processes of softmax
[00:30:44] <Klinda> then what does it represent?
[00:31:05] <Klinda> that the second feaure has more probability than the first
[00:31:09] <Klinda> of doing what?
[00:33:02] <dostoyevsky2> Klinda: Could be anything.  What did you train the weights for?
[00:33:35] <Klinda> just for learning
[00:33:39] <Klinda> nothing more
[00:34:05] <Samian> dostoyevsky2  he didn't even clarify whether he's talking about a neural network or something else.
[00:34:14] <Klinda> I am just a beginner
[00:34:21] <Klinda> I am doing regression
[00:34:27] <Klinda> with k classes
[00:34:48] <dostoyevsky2> Samian: I think Klinda just started at university and also a beginner at English probably ;-)
[00:35:48] <Klinda> NN maybe some day I will train one
[00:36:11] <Klinda> it's just Multinomial Classification
[00:41:15] <Klinda> so no one can answer?
[00:41:57] <Klinda> I don't know what I train the weights for
[00:42:13] <cslr> how many iterations (gradient updates of weights) does it take for NN reinforcement learning to learn something? I try to create agent that would wander succesfully around in roguelike environments but I cannot get any improvements. I'm using own RL (DQN) implementation and minihack python/gym environment.
[00:42:44] <dostoyevsky2> cslr: Realistically?  Too many... it's pretty slow
[00:43:25] <cslr> ok
[00:43:58] <dostoyevsky2> cslr: but if you study e.g. deep mind implementations of people who wrote RL books, you see that they do a lot of optimizations to speed up things, but those depend on the task
[00:45:26] <cslr> yes I'm profiling my C++ code to make it faster but most of the time is used in computing gradients of neural networks (jacobian, value gradient, mse gradient etc)
[00:45:58] <cslr> next I will move from double to float maybe it makes things faster.
[00:46:10] <Samian> cslr  dqn is the faster.  on-line actor critic is the about 2 orders of magnitude slower.
[00:46:16] <Samian> *fastest
[00:49:21] <cslr> I'm using old algorithm for RL. Maybe I need to change algorithm to newer one to get better results.
[00:49:27] <Klinda> https://imgur.com/a/XAeF8LD
[00:49:28] <SigmoidFroid> ⇒  Imgur: The magic of the Internet
[00:49:34] <Klinda> maybe this is more clear
[00:49:42] <Klinda> with the loss what do you discover here?
[00:54:18] <dostoyevsky2> Klinda: what does the first line say in the screenshot you gave?
[00:55:30] <Klinda> 0 is cat, 1 is dog, 2 is bird, we are considering the dog
[00:55:46] <Klinda> with p(x), right?
[00:56:32] <dostoyevsky2> Klinda: so the context here is cat, dog, bird...  that's your training data ... so the numbers of softmax refer to that
[00:56:58] <dostoyevsky2> Klinda: if the first line wasn't there it could mean anything
[00:58:29] <Klinda> what does it mean that cat has a probability of 0.23 ?
[00:59:34] <dostoyevsky2> Klinda: what does z(x) say?
[01:00:02] <Klinda> 1,2,0.5
[01:00:16] <dostoyevsky2> Klinda: cat:1, dog:2, bird:0.5
[01:00:26] <Klinda> yes
[01:00:49] <dostoyevsky2> but p(x) says: cat: 0, dog:1, bird: 0 ... so-called one hot encoding
[01:01:06] <dostoyevsky2> but z(x), the prediction isn't just that
[01:01:38] <dostoyevsky2> that's what the loss is expressing
[01:03:09] <Klinda> "the prediction isn't just that"
[01:03:11] <Klinda> what do you mean?
[01:04:31] <dostoyevsky2> z(x) says it's 23% chance it's a cat and 14% chance it's a bird... only 63% chance it's a dog... so we would have expected 100% dog
[01:07:19] <Klinda> but z(x) should be interpreted has 3 training examples?
[01:08:21] <dostoyevsky2> Klinda: one hot encoding just describes one training sample in a vector with all categories
[01:11:12] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[01:11:39] <Samian> Klinda  watch this video https://www.youtube.com/watch?v=r0Ogt-q956I . You're asking questions that will be answered by watching that video and others like it
[01:11:41] <SigmoidFroid> ⇒  Deep Learning: A Crash Course - YouTube
[01:11:49] <Klinda> yes the [0,1,0] I understand, but how z(x) is calculated ? why there is a vector of something says is cat, one is dog and one a bird ?
[01:12:31] <Klinda> thanks Samian
[01:13:36] <Klinda> all I know it's having a dataset in a shape of (n_traing_examples, n_features)... so each feature vector has some probability in it, the y_onehot encoding just pick one feature
[01:16:19] <Klinda> so one problem is that what if you  have n_features and not only 1 (described in the imgur I posted) ?
[01:16:41] <Klinda> btw I will watch that youtube video
[01:16:47] <Klinda> maybe I understand better
[01:17:05] <coraxx> Klinda: I'm already watching it now ;-)
[01:17:58] <Klinda> 1 semester in 3 hours
[01:18:01] <Klinda> let's go
[01:20:46] <Samian> Klinda  you're using the word features wrong
[01:21:17] <Samian> why would you assume you're using the word correctly if you're just starting?
[01:22:31] <Samian> Klinda why hasn't you asked "what does the word feature mean in machine learning?"
[01:22:58] <coraxx> Samian: he's busy taking a 3 hours online youtube course
[01:24:15] <Klinda> features for me are the characteristics of the observed example
[01:24:28] <Klinda> in the training set you have a label attached to it
[01:24:48] <Klinda> cause you know before in which class it belongs
[01:25:15] <Samian> coraxx  I'd watch it at 1.5x speed so it's less than 3 hours.
[01:27:13] <Klinda> because I know what the features are
[01:27:19] <Klinda> you act like I am stupid
[01:29:05] <Klinda> I think maybe that's a correlation between the number of classes and the number of features
[01:29:15] <Klinda> they have to be the same
[01:29:32] <Samian> What you call "features" are basically the columns in a database. Like a database where each record is a person. There may be columns for age, height, weight, address, phone number and so on.  These are called "attributes".
[01:30:52] <Klinda> yes I know
[01:36:13] <Samian> The problem with using the word features is that the word features is used to mean something else when talking about the internals of neural networks.
[01:38:25] *** Joins: Klinda_PING (~superleag@user/klinda)
[01:38:54] <Klinda_PING> yup I got disconnect
[01:39:05] *** Quits: Klinda (~superleag@user/klinda) (Killed (NickServ (GHOST command used by Klinda_PING)))
[01:39:12] *** Klinda_PING is now known as Klinda
[01:41:27] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[01:46:58] *** Quits: causative (~halberd@user/causative) (Remote host closed the connection)
[01:47:00] *** Quits: jinsun (~quassel@user/jinsun) (Read error: Connection reset by peer)
[01:47:55] *** Joins: causative (~halberd@user/causative)
[01:48:26] *** Joins: jinsun (~quassel@user/jinsun)
[01:49:11] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[01:50:13] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) ()
[01:51:15] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[01:52:20] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 268 seconds)
[02:58:46] *** Quits: DnzAtWrk (~DnzAtWrk@88-115-149-152.elisa-laajakaista.fi) (Ping timeout: 260 seconds)
[03:00:14] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[03:00:15] *** Quits: causative (~halberd@user/causative) (Ping timeout: 268 seconds)
[03:02:37] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 268 seconds)
[03:02:44] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[03:03:40] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[03:12:18] *** Joins: causative (~halberd@user/causative)
[03:13:30] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[03:17:11] *** Quits: brand0 (~brandon@user/brand0) (Ping timeout: 245 seconds)
[03:27:00] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[03:27:26] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[03:32:30] <Klinda> coraxx: did you finish?
[03:33:07] <coraxx> Klinda: I'm starting on the second half in a minute.
[03:33:22] <coraxx> Klinda: he's really good
[03:34:35] <Klinda> tomorrow I will continue
[03:34:41] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:34:42] <Klinda> now I go to bed, goodnight :)
[03:34:56] <coraxx> Klinda: g'nite :-)
[03:34:57] <Klinda> and thanks to all
[03:35:38] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[03:36:47] <Samian> coraxx  you have no idea how many videos I went through to find the few gems I bookmarked
[03:36:55] <Samian> coraxx  most ml videos are trash
[03:37:50] <Samian>  Does this chart make sense to anyone? https://imgur.com/a/7KtJpcL (https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetSeqDataDescriptor).   I can't tell if contiguous storage is following the arrow trail, or the traditional left to right, next row and repeat.
[03:37:51] <SigmoidFroid> ⇒  Imgur: The magic of the Internet
[03:37:52] <SigmoidFroid> ⇒  API Reference :: NVIDIA Deep Learning cuDNN Documentation
[03:37:56] <coraxx> Samian: TOTALLY agree ...many of them a missing the "keypoints" (the overview that allows us to remember and understand which information goes where in our understanding)
[03:39:40] <Samian> Your welcome for the free gems.  I didn't have the benefit of someone giving me the best links. I had to find them manually after months and years
[03:40:07] <coraxx> Samian: well it is VERY much appreciated :-)
[03:40:19] <Samian> awe thanks man :-)
[03:41:08] <mefistofeles> hah
[03:41:08] *** Joins: sinaowolabi__ (~SinaOwola@169.159.127.183)
[03:41:32] <coraxx> mefistofeles: :-) hey there
[03:42:01] <mefistofeles> coraxx: hey!
[03:43:38] *** Quits: jhc (~jhc@141.98.252.251) (Ping timeout: 260 seconds)
[03:43:41] *** [_] is now known as [itchyjunk]
[03:46:55] *** Joins: jhc (~jhc@141.98.252.251)
[03:47:10] <[itchyjunk]> https://www.isomorphiclabs.com/blog
[03:47:11] <SigmoidFroid> ⇒  https://assets-global.website-files.com/61542ea294e1166d3ee124d4/6183f7f5d43cdfc0a53071c8_isolabs-image-opengraph.png
[03:47:15] <mefistofeles> Samian: they want a data layout that follows the arrows, so I think you are mixing two levels there
[03:47:23] <[itchyjunk]> didn't realize google was this confident about their AI's already
[03:47:41] <mefistofeles> I mean, the diagram is just a way of visualizing the beams, batches and time, but it doesn't correspond to anything in memory
[03:48:39] <mefistofeles> so you don't have to assume anything about memory allocation by that diagram, other than you want to navigate memory as the arrows are suggesting
[03:49:06] <Samian> [itchyjunk]  which do you find more elite? Google Deep Mind or Facebook Research?
[03:49:42] <Samian> mefistofeles  I saw the answer to my question as I asked it actually.
[03:50:11] <mefistofeles> Samian: where? Can you share that?
[03:50:31] <[itchyjunk]> Samian, deepmind probably.
[03:50:43] <[itchyjunk]> fb was more focused in NLP i think?
[03:51:00] <[itchyjunk]> i've not really kept up with FB stuff
[03:51:11] <[itchyjunk]> do they publish their stuff somewhere specific?
[03:51:17] <[itchyjunk]> lecunn is still head there right?
[03:52:24] <Samian> [itchyjunk]  nowhere specific, not that I know.  I do believe pytorch is overtaking tensorflow.
[03:53:01] <mefistofeles> I have colleagues that have worked in both FB research and Deepmind
[03:53:03] <Samian> [itchyjunk]  fb is researching in reinforcement learning too
[03:53:16] <mefistofeles> they kinda hate FB research and love DeepMind, basically
[03:53:28] <Samian> whaaat? Can you provide more details?
[03:53:42] <Hunts> Samian: why the question?
[03:53:46] <Samian> What did they say about the differences between FB research and DeepMind ?
[03:54:27] <mefistofeles> Samian: from what I can tell it just boils down to being a data monkey or an actual scientist that works with data, DeepMind is more for the latter
[03:54:34] <mefistofeles> FB pays better, apparently
[03:54:35] <[itchyjunk]> yes i hear pytorch is doing well
[03:54:41] <Samian> Hunts just conversating. I never worked at Google. I'm curious what I'm missing out on, and whether the grass is really greener on the other side.
[03:55:03] <[itchyjunk]> Samian, deepmind is really acadamic
[03:55:20] <mefistofeles> yeah, DeepMind is the place to be if you really want to do research in the field
[03:55:29] <[itchyjunk]> fb is like a research department of a company
[03:55:33] <mefistofeles> FB research, despite its name, is more about applying what's already known
[03:55:41] <[itchyjunk]> and now isometric company i guess
[03:55:48] <[itchyjunk]> if you're into biophysics type stuff
[03:55:59] <Samian> yes, fb is highly weighted toward apply what's already known
[03:56:02] <[itchyjunk]> they seem to be headhunting pretty agressively already
[03:56:48] <mefistofeles> MS research on the other hand is also doing a lot of actual research in the field
[03:56:58] <Samian> I'm frustrated whenever I learn something new in ML and after I see it, after I see how simple the idea is, I'm like "why didn't I think of that!?"
[03:57:27] <Samian> ML is a young field so there's opportunity to be the first to discover something. It's not 600 years old like calculus where everything to be discovered already has
[04:01:36] <Hunts> Samian: you are probably not missing anything
[04:01:58] <Samian> Two example: generative adversarial networks: The simple idea is freezing the latter part of a network while forcing the early part to produce a certain output by training it to always true.   Second example: self attention: The simple idea is to use one set of activation values as though they were weights that another set of activation values forward pass through.
[04:06:25] <Samian> in other words, self attention is basically the idea of generating multiple feature maps and treating some of the maps as weights of the neural network.  THat's a pretty cool idea
[04:07:09] <Samian> It's a neural network that generates itself as it uses itself!
[04:08:55] <mefistofeles> Samian: the best ideas are always the simple ones, and also the hardest one to actually have
[04:08:58] <mefistofeles> :)
[04:09:02] <mefistofeles> occam's razor
[04:10:14] <mefistofeles> I think the problem is that the idea is just the tip of the iceberg, everyone can have good simple ideas, but almost no one can actually make the work to actually prove the concept/idea, that's the hard part
[04:10:19] <Samian> occam's razor is more of a principle on how to evaluate evidence in terms of what's more likely than what given a certain set of facts
[04:10:54] <Samian> or it means more?
[04:11:12] <Samian> mefistofeles  yeah, backing up an idea with evidence is harder than the idea itself.
[04:11:31] <mefistofeles> hmm I guess it can become philosophical, but I see occam's razor as simply "the simples model that explains the observations"
[04:11:35] <mefistofeles> *simplest
[04:11:59] <Samian> But coming across the idea is like winning the lottery.  Doing the work is guaranteed for the person who is determined to do it.
[04:12:29] <mefistofeles> Samian: I actually think otherwise, everyday I see people coming with great ideas that they just never work hard enough to implement
[04:12:50] <mefistofeles> only the ones that actually work hard, regardless of their idea being better or not, are the ones making the breakthroughs
[04:14:16] <Samian> I hear you. I'm saying for the one person out there who is obsessed with wanting to discover something great. For such a person whether they will do the work is no question. It's all about coming across that golden idea
[04:15:06] <mefistofeles> Samian: yeah, I also think being motivated by "doing something great" is a good way of thinking nothing is worth the work, so it's commonly a blocker
[04:16:03] <mefistofeles> the usual, how does it go? 1% inspiration 99% transpiration (?) or something like that
[04:16:20] <Samian> certainly. It reminds me of song writing. The person trying to force out a hit record never does. The person who goes with the flow ends up having 1 million views on youtube
[04:17:12] <Samian> mefistofeles  Einstein said that. One Percent Inspiration, Ninety-Nine Percent Perspiration
[04:17:25] <mefistofeles> yes
[04:22:26] <causative> you have to have a great idea, and put in the work - doesn't matter how much work you put in on a mediocre idea, it won't become great
[04:23:02] <mefistofeles> causative: the catch is that you don't really know that beforehand, you have to put the work to see if it's actually a good idea or not
[04:23:24] <mefistofeles> and the more work you do, the better you get at thinking better ideas
[04:23:40] <mefistofeles> that's the whole point of studying the fundamentals
[04:23:49] <mefistofeles> that many people want to skip :P
[04:24:07] <causative> depends on the type of work you do - if all you are doing is incremental optimizations on existing ideas, you don't get better at thinking of better ideas
[04:24:29] <causative> you have to be constantly *trying* to find something great, or at least different, not just an incremental optimization
[04:24:42] <mefistofeles> hmm, I don't think there's a recipe
[04:25:02] <mefistofeles> and I have examples where people doing the same eventually realize how these things are "wrong" and come up with alternatives and such
[04:25:06] <mefistofeles> but yeah, it really varies
[04:25:39] <mefistofeles> you DO have to know fairly well what the current ideas and state of the art is before aiming for something great, I guess that's my point
[04:25:45] <mefistofeles> and that requires a lot of work
[04:26:13] <causative> yeah you do get better and you can't expect your first idea or your tenth idea to work out until you work it out
[04:28:31] <causative> it's like simulated annealing temperature - if you want to find a new minimum you have to have a high temperature, where you seek out and explore ideas that are more different from the current minimum
[04:29:03] <mefistofeles> causative: hmmm, but that's mostly due to a sampling problem, not because are fundamentally different
[04:29:04] <causative> then you lower the temperature to optimize the idea
[04:29:17] <Samian> The history of neural nets shows that a great idea can be dismissed for decades until finally it can be tested and proven
[04:29:22] <mefistofeles> higher temperature -> higher energy -> less energy barrier -> more states sampled
[04:30:15] <causative> higher temperature leads simulated annealing to range far away from the current value
[04:30:32] <causative> lower temperature leads simulated annealing to seek a local minimum near the current value
[04:30:42] <mefistofeles> sure, but that's jsut a sampling problem
[04:30:49] <mefistofeles> nothing fundamentally different in the theory or idea
[04:30:54] <causative> a brilliant new idea has to first come from a high temperature search where you break out of the local minimum in search of significantly different solutions
[04:31:10] <causative> then followed by a low temperature search where you optimize the new and different idea
[04:31:26] <mefistofeles> I'm not really sure how simulated annealing applies to ideas here, tbh
[04:32:11] <causative> the simulated annealing search space in this metaphor is the space of ideas
[04:32:26] <mefistofeles> yeah, I guess
[04:32:45] <causative> and the objective function is how good the idea is
[04:32:55] <mefistofeles> yet again, ideas are not really well-defined in terms of energy or even some optimization problem
[04:33:10] <causative> well, it is only a metaphor
[04:33:14] <mefistofeles> ok yes
[04:33:17] <mefistofeles> I understand
[04:34:12] <mefistofeles> causative: I guess you can actually do that for different techniques or algorithms though
[04:34:19] <mefistofeles> people are already doing that, btw
[04:38:07] <causative> high temperature is a bias towards novelty even if the novel idea seems bad at first, low temperature is a bias towards optimizing what's already known to be good
[04:40:57] <Hunts> everything seems to be simple in retrospective :)
[04:41:41] <mefistofeles> Hunts: exactly :)
[04:42:00] <mefistofeles> no true knowledge is trivial
[04:42:37] <mefistofeles> I always find it amazing when they say "Newtonian physics" is a one-semester course....
[04:43:15] <mefistofeles> holy molly, it took Newton his whole life to come up with it, and he was a genius...
[04:43:27] <mefistofeles> applies to everything, btw
[05:13:37] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Remote host closed the connection)
[05:14:58] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[05:31:14] *** Quits: sinaowolabi__ (~SinaOwola@169.159.127.183) (Ping timeout: 268 seconds)
[05:43:09] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[05:45:15] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[05:50:15] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Remote host closed the connection)
[05:55:49] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[05:57:15] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Remote host closed the connection)
[06:02:52] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[06:04:15] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Remote host closed the connection)
[06:08:18] *** Quits: energizer (~energizer@user/energizer) (Ping timeout: 260 seconds)
[06:08:57] *** Quits: causative (~halberd@user/causative) (Ping timeout: 268 seconds)
[06:09:48] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[06:12:57] *** Joins: energizer (~energizer@user/energizer)
[06:18:07] *** Quits: trace987 (~trace@91.66.153.65) (Ping timeout: 268 seconds)
[06:28:50] *** Joins: causative (~halberd@user/causative)
[06:45:51] *** Quits: _ohm (~research@user/ohm/x-5690770) (Ping timeout: 268 seconds)
[07:08:58] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[07:11:04] *** Joins: brand0 (~brandon@user/brand0)
[07:11:26] <Samian> How does multi-head attention add any value to CNNs used for computer vision? I thought CNNs had attention built into it. If a part of an image is unimportant, the activation values covering that part of the image will be 0.
[07:14:04] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[07:16:05] *** Quits: akevinhuang2 (~thekevinh@user/thekevinhuang) (Ping timeout: 268 seconds)
[07:27:56] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
[07:35:54] *** Quits: causative (~halberd@user/causative) (Ping timeout: 268 seconds)
[08:09:07] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Ping timeout: 268 seconds)
[08:33:32] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[08:41:17] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[09:46:35] *** Quits: Samian (~s@user/samian) (Ping timeout: 264 seconds)
[10:02:30] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Ping timeout: 260 seconds)
[10:19:53] *** Quits: shoky_ (uuuggg@141.226.193.67) ()
[10:23:43] *** Joins: manti7 (~manti7@176.10.104.94)
[10:29:54] *** Joins: palasso (~palasso@user/palasso)
[10:58:58] *** Joins: Samian (~s@user/samian)
[11:44:03] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-209.dhcp.inet.fi)
[11:46:06] *** Quits: jerome- (~jerome@78.193.84.130) (Ping timeout: 260 seconds)
[12:13:52] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[12:34:08] *** Joins: yuri6037[m] (~yuri6037m@2001:470:69fc:105::1:2bf5)
[12:34:59] *** Parts: yuri6037[m] (~yuri6037m@2001:470:69fc:105::1:2bf5) ()
[12:35:04] *** Joins: Klinda (~superleag@user/klinda)
[12:35:14] *** Joins: yuri6037 (~quassel@yuristudio.net)
[12:36:35] <yuri6037> Hi, I've ran a machine learning app yesterday and it blew up SSH completely. I've just tried again the same pytorch app and somehow it does it again. Is it a known buf of pytorch to blow up SSH servers?
[12:37:08] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[12:37:35] <yuri6037> By blowing up I mean SSH is completely inaccessible when connecting it just blocks local terminal windows forever
[12:37:52] *** Joins: jlrnick (~josephler@2a01cb040a1594007c89c79693ecade1.ipv6.abo.wanadoo.fr)
[12:39:43] <yuri6037> btw, here is the code that blows up SSH: https://github.com/Yuri6037/SRGAN/blob/master/train.py
[12:39:44] <SigmoidFroid> ⇒  SRGAN/train.py at master · Yuri6037/SRGAN · GitHub
[12:48:37] <stefan-_> yuri6037, you could put it inside a screen session https://linuxize.com/post/how-to-use-linux-screen/
[12:48:38] <SigmoidFroid> ⇒  (: No title)
[12:51:20] <yuri6037> I know how to use screen the problem is not wether I can use screen or not  the problem is that while I was running this code the SSH server blew up and connecting to it just results now in nothing, it just block my terminal window
[13:02:04] <Hunts> yuri6037: can the system be accesed locally while running this code?
[13:02:57] <yuri6037> No the machine is a tencent cloud machine provided by my university
[13:04:27] <Hunts> this sounds more like the machine does not have enough RAM to run this code
[13:04:37] <Hunts> then it starts swapping and becomes very slow
[13:04:44] <Hunts> almost grinding to a halt
[13:10:21] <yuri6037> But how is it possible that it was stabilized at 22% last time I checked and was 66% training of 1st epoch
[13:10:57] <Hunts> what are these percentages?
[13:12:26] <yuri6037> percentage coming from the XFCE task manager which I always keep open to check CPU and RAM usage. I also keep another terminal window to periodically monitor nvidia-smi
[13:12:44] <Hunts> but percentage of what?
[13:12:47] <Hunts> memory?
[13:12:52] <yuri6037> memory yes
[13:13:04] <yuri6037> XFCE task manager shows to graphs: CPU and RAM
[13:13:07] <Hunts> how much RAM does your VM have? and GPU RAM?
[13:13:25] <yuri6037> 32 Gb of RAM and 16 Gb of VRAM (Tesla T4)
[13:13:56] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-209.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:14:07] <yuri6037> That would be surprising this network can't run with 32 GB of RAM considering I've ran many other networks which are EVEN more consuming than this one.
[13:14:34] *** Joins: _ohm (~research@user/ohm/x-5690770)
[13:14:35] <Hunts> it is not just about the network
[13:14:40] <yuri6037> I've ran network which consumes up to 15Gb of VRAM this one only consumes 10Gb of VRAM
[13:14:46] <Hunts> did you reduce the batch size to 1 to see if that fixes the issue?
[13:14:59] <Hunts> VRAM would not make this problem, but RAM would
[13:15:02] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-567363-170.dhcp.inet.fi)
[13:15:17] <Hunts> SRGAN was trained with a NVIDIA Tesla M40 which has 24 GB of RAM
[13:16:35] <yuri6037> the validation batch size is always 1 as the code expects 3 dimensions not 4 so it can't work with batches on validation but training runs at batch 300
[13:16:50] *** Quits: Codaraxis (~Codaraxis@user/codaraxis) (Ping timeout: 268 seconds)
[13:17:17] <yuri6037> and as I said last time I've checked it was 66% of training of first epoch
[13:17:43] <yuri6037> and stabilized at 22% RAM usage and 10Gb of VRAM usage
[13:17:56] <Hunts> yeah but you do not see spikes in RAM usage that could trigger this
[13:18:25] <Hunts> the behavior you describe exactly matches what would happen when running close to filling all RAM (not VRAM)
[13:18:28] <yuri6037> All I can tell is it probably happened while validating first epoch
[13:18:34] <Hunts> does this VM have swap?
[13:18:47] <yuri6037> I have no idea how can I check?
[13:19:03] <yuri6037> This VM has been set up by my university
[13:19:58] <Hunts> swapon -s
[13:20:50] <Hunts> also 32 GB is not really that much for system RAM
[13:21:20] <yuri6037> "swapon -s" returns nothing at all
[13:21:58] <Hunts> mmmm
[13:22:06] <yuri6037> 32GB not really that much, well locally I only have 16 Gb of RAM and my older worstation which now serves as a gaming machine is 24Gb of RAM
[13:22:44] <Hunts> loading many batches of some datasets can fill that amount of RAM
[13:22:47] <Hunts> and that is just the dataset
[13:24:36] <yuri6037> Yeah I know loading dataset can fill RAM pretty badly. My dataset is 1600 images of 1280x720 training and 200 images of 1280x720 validation and finally 200 images of 1280x720 for testing only (that is testing once network has been saved as pth/pt/pb)
[13:26:33] <yuri6037> For now the worst network I've ran was SAN which pumped up to almost 30% of RAM and 15Gb of VRAM and required a big patch to use image decomposition otherwise no matter how hard I tried testing/training was failling instantly with CUDA out of memory
[13:30:15] <Hunts> and what image size are you using for training?
[13:31:06] <yuri6037> I've just told you 1280x720
[13:31:41] <yuri6037> and my image decomposition decomposes this in squares of 80 if I remember
[13:32:30] <Hunts> training size sounds like 80x80
[13:33:22] <yuri6037> well yeah regions where 80x80 and batch was 300 (can't put more otherwise CUDA out of memory)
[13:33:43] <Hunts> are you storing all the patches into RAM?
[13:34:21] <yuri6037> Yes I'm preloading every HR squares in RAM (well that because pytorch forces me to do this, pytorch does not support image decomposition)
[13:34:36] <Hunts> that could be a memory hog
[13:34:48] <yuri6037> but as I said it was at 22%
[13:35:08] <yuri6037> how could 22% RAM usage cause this crash
[13:35:13] <Hunts> it does not
[13:35:20] <Hunts> but your server is freezing uip
[13:35:21] <yuri6037> the preload is done before even running the training
[13:35:35] <Hunts> and you would not see spikes in memory usage due to the freezing
[13:35:37] <yuri6037> so all bacthes where already in RAM when the crash occured
[13:37:02] <yuri6037> I made sure to preload all HR images before even reaching the main epoch loop
[13:37:25] <Hunts> what if you don't?
[13:37:30] <Hunts> load them on the fly
[13:38:13] <yuri6037> I can't as I said I can't do that because pytorch doesn't support it and that would require a HUGE change like a rewrite of the entire init logic (a bit like what I've done for SAN)
[13:38:38] <Hunts> pytorch doesn't support it? that is bullshit
[13:39:01] <yuri6037> and I had also preloaded HR and even LR images for SAN and it did not crash
[13:39:59] <yuri6037> That's because my image decomposition does not use a fixed size region size the regions is dynamically computed per each image. The only way would be to have batches pre-made but pytorch bakes batches itself so I have to give it region by region
[13:40:22] <yuri6037> but in order to give it region by region I need to know all regions in advance which I can't know without preloading
[13:41:20] <yuri6037> Pytorch would need a dynamic DatasetLoader which dynamically calls len then getitem then calls current_image_region_len then get_region
[13:42:35] <yuri6037> but it does not change the fact that SAN also did preload all HR images and even did preload the LR images and no it never crashed it trained the 20 epochs without any issue. I could even test the network locally (well by forcing batch size to 4 but still I could)
[13:43:34] <yuri6037> And also as I already explained: when this SRGAN was running all HR images where already preloaded in RAM
[13:44:16] <Hunts> but it is always freezing at the same point, during validation?
[13:44:24] <yuri6037> so the only explanation that some other internal part of SRGAN while validating caused a sudden memory overload but I can't find anything in the code which did that
[13:46:10] <yuri6037> Yes in both cases yesterday when I checked at 7 PM it was at 70% training 1st epoch. When I cam back a hour later, Remmina (local VNC client) had completely crashed (indicates network failure, yeah reminna handles network failure by segfaulting). When I tried again to re-connect Remmina I got a strange error saying timed out. When I tried to call ssh command on a terminal locally it was stuck never connecting
[13:46:41] <Klinda> morning Hunts
[13:47:30] <Hunts> yuri6037: so you were not actually monitoring everything in real-time?
[13:48:03] <yuri6037> I was until I had to go have dinner (yesterday) and today breakfist
[13:48:31] <yuri6037> I just left it for one hour without monitoring that's all.
[13:50:32] <yuri6037> I'm pretty sure it happened after training.
[13:50:50] <Hunts> after training?
[13:53:15] <yuri6037> Yes as I said when I left it it was finishing first epoch (70% yesterday) and (66% today)
[13:54:01] *** Quits: Hunts (~hunts@p4fc9a453.dip0.t-ipconnect.de) (Quit: Konversation terminated!)
[13:54:32] <yuri6037> I'll try running again the software this time I'll try to keep htop running on a separate SSH seesion hoping that ssh client/gnome terminal does not handle network failure/RAM overload by segfaulting
[13:56:38] *** Joins: HuntsMan (~hunts@p4fc9a453.dip0.t-ipconnect.de)
[13:56:56] *** Joins: tomeaton17 (~tomeaton1@2a0c:5bc0:40:3e3a:7c5f:1645:8476:a5d4)
[13:58:45] <yuri6037> Htop indicates right now 8.20Gb of RAM used over 31.3Gb available
[13:58:59] <yuri6037> Network is training first epoch 1%
[15:04:23] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[15:07:11] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-567363-170.dhcp.inet.fi) (Quit: Leaving)
[15:31:26] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[15:34:51] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[16:02:06] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[16:11:34] *** Joins: gareppa (~gareppa@user/gareppa)
[16:12:15] *** Quits: gareppa (~gareppa@user/gareppa) (Client Quit)
[16:49:21] <yuri6037> Thanks to a postmarket OS phone constantly monitoring htop I found out it's indeed validation which is causing issues: once it enters validation it immediatly jumps to 16Gb of RAM consumption and keeps increasing. I had to CTRL-C it at 24.4Gb
[16:53:20] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[16:58:17] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[17:04:50] <yuri6037> Thanks for giving me this idea of constant monitoring
[17:07:48] *** Quits: Luyin (~daisy@user/luyin) (Quit: WeeChat 3.3)
[17:11:10] *** Joins: Luyin (~daisy@user/luyin)
[17:16:40] <tomeaton17> Do you have a very large validation dataset?
[17:22:26] <HuntsMan> yuri6037: told you ;)
[17:23:06] <yuri6037> The problem is not how large the validation dataset is I found what code is causing the issue
[17:23:51] <yuri6037> "val_images.extend(" this line forces python keep in CPU memory all validation images upscaled by network + original
[17:32:31] *** Joins: shoky (uuuggg@141.226.193.67)
[17:34:17] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 256 seconds)
[17:48:41] *** Joins: `Tim (~zenguin@user/zenguin)
[18:08:02] *** Quits: xs (~xs@user/xs/x-9591474) (Ping timeout: 265 seconds)
[18:08:15] *** Joins: xs (~xs@user/xs/x-9591474)
[18:30:23] *** Quits: yuri6037 (~quassel@yuristudio.net) (Ping timeout: 256 seconds)
[19:03:33] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Remote host closed the connection)
[19:04:37] *** Quits: knipster50 (~knipster@user/knipster) (Ping timeout: 268 seconds)
[19:07:06] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[19:23:53] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[19:28:13] *** Quits: tomeaton17 (~tomeaton1@2a0c:5bc0:40:3e3a:7c5f:1645:8476:a5d4) (Quit: Client closed)
[19:35:24] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[19:51:33] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[19:58:54] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Ping timeout: 260 seconds)
[20:04:40] <[itchyjunk]> mefistofeles, i can't figure this mathplotlib.animation thing at all. if i want to draw one line then another line, how do i do it? i am looking at this sin function animation and trying to repurpose it but nothing is working so far
[20:05:21] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[20:06:02] <mefistofeles> [itchyjunk]: what's the code and the objective?
[20:06:28] <[itchyjunk]> long term goal is to animamte travelling salesman paths being chosen
[20:06:35] <[itchyjunk]> https://bpa.st/4XAA
[20:06:36] <SigmoidFroid> ⇒  (: No title)
[20:06:46] <mefistofeles> use syntax highlight, next time :P
[20:06:49] <[itchyjunk]> short term goal is to get a line and another line to show up in animation
[20:06:58] <[itchyjunk]> no js, can't select anything else
[20:07:23] <mefistofeles> [itchyjunk]: ok, why aren't you calling matplotlib's animation module at all?
[20:07:41] <[itchyjunk]> oops wait, it should be. let me repaste
[20:08:11] <[itchyjunk]> https://bpa.st/SCIQ
[20:08:12] <SigmoidFroid> ⇒  (: No title)
[20:08:19] <[itchyjunk]> it calls the animation.FuncAnimation
[20:08:27] <[itchyjunk]> https://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/
[20:08:28] <SigmoidFroid> ⇒  Matplotlib Animation Tutorial | Pythonic Perambulations
[20:08:32] <[itchyjunk]> is the tutorial i was trying to follow
[20:09:38] <mefistofeles> ok, that tutorial is pretty outdated, but let me see
[20:10:01] <[itchyjunk]> oh i dont mimnd followin something else, that was first result
[20:10:14] <[itchyjunk]> the matplotlib code examples themselves were too hard to follow
[20:10:38] <mefistofeles> yeah, basically that blog post is what eventually transformed into matpotlib's animation module
[20:11:13] <mefistofeles> [itchyjunk]: https://matplotlib.org/stable/gallery/animation/simple_anim.html
[20:11:14] <SigmoidFroid> ⇒  Animated line plot — Matplotlib 3.4.3 documentation
[20:12:57] <[itchyjunk]> okay thats a simpler code, i might be able to play with that one
[20:13:58] *** Joins: sinaowolabi__ (~SinaOwola@41.58.86.159)
[20:14:05] <mefistofeles> [itchyjunk]: so basically you just want a single parameter. Maybe it can handle multiple parameters but I'll try with a single one for now
[20:14:38] <[itchyjunk]> yeah i think i figured out the greedy algo for 3 points and i just want to animate that
[20:15:37] <mefistofeles> [itchyjunk]: https://matplotlib.org/stable/gallery/animation/animate_decay.html
[20:15:38] <SigmoidFroid> ⇒  Decay — Matplotlib 3.4.3 documentation
[20:16:14] <[itchyjunk]> yeah that's the one i wasn't sure about
[20:16:32] <[itchyjunk]> i looked at that then switched to the blog because it seemed easier
[20:19:55] <mefistofeles> ah ok
[20:20:23] <[itchyjunk]> hmm it seems to depend on i, which gets passed inside the FuncAnimation
[20:21:00] <[itchyjunk]> not sure how i make my thing rely on that i
[20:21:44] <mefistofeles> [itchyjunk]: in the example that would be t
[20:21:51] <mefistofeles> in the decay example
[20:22:10] <mefistofeles> and it has a bit of magic with the itertools.count() but that's just something that gives a sequence of int, a counter
[20:25:51] *** Joins: CaCode (~CaCode@user/cacode)
[20:26:23] *** Joins: yuri6037 (~quassel@yuristudio.net)
[20:33:11] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[20:39:16] *** Joins: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com)
[20:49:18] *** Quits: Klinda (~superleag@user/klinda) (Ping timeout: 260 seconds)
[20:55:24] *** Joins: Klinda (~superleag@user/klinda)
[21:10:12] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[21:12:54] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 268 seconds)
[21:15:17] *** Joins: `Tim (~zenguin@user/zenguin)
[21:19:58] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[21:26:12] *** Quits: yuri6037 (~quassel@yuristudio.net) (Quit: https://quassel-irc.org - Chat comfortably. Anywhere.)
[21:31:50] *** Joins: SiegeLord (~sl@user/siegelord)
[21:35:35] *** Joins: causative (~halberd@user/causative)
[21:35:59] *** Joins: yuri6037 (~quassel@yuristudio.net)
[21:36:58] *** Quits: sinaowolabi__ (~SinaOwola@41.58.86.159) (Ping timeout: 260 seconds)
[21:50:37] *** Joins: CaCode_ (~CaCode@user/cacode)
[21:53:11] *** Quits: CaCode (~CaCode@user/cacode) (Ping timeout: 264 seconds)
[22:07:15] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[22:12:35] *** Joins: CaCode- (~CaCode@user/cacode)
[22:15:11] *** Quits: CaCode_ (~CaCode@user/cacode) (Ping timeout: 268 seconds)
[22:21:23] *** Quits: jlrnick (~josephler@2a01cb040a1594007c89c79693ecade1.ipv6.abo.wanadoo.fr) (Ping timeout: 264 seconds)
[22:53:24] *** Quits: dostoyevsky2 (~sck@user/dostoyevsky2) (Ping timeout: 268 seconds)
[22:53:26] *** Quits: xs (~xs@user/xs/x-9591474) (Ping timeout: 260 seconds)
[22:53:42] *** Joins: xs (~xs@user/xs/x-9591474)
[22:54:55] *** Joins: dostoyevsky2 (~sck@user/dostoyevsky2)
[23:03:50] *** Quits: hagrid (~tomek-r@194.33.117.9) (Quit: WeeChat 2.8)
[23:58:29] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
