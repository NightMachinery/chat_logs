[00:00:21] <hodapp> Pandas dataframes are likely going to be much quicker for all manners of bulk numerical or vectorized operation
[00:01:10] <clime> quicker than postgres? idk probly because dataframes are already in memory
[00:01:42] <hodapp> relational DBs can scale very well but they often bring a lot more overhead than a lightweight in-memory thing
[00:01:59] <hodapp> and there's a reason that SQLite often outperforms enterprise DBs for common workloads
[00:04:14] <clime> memory-backed sqlite dbs seems to be pretty cool, haven't tried it yet though
[00:05:49] <clime> btw there is a cool extension for PostgreSQL called TimescaleDB for time-series - i am thinking about using that but will see, i would need write all the operations in SQL, lots of work :)
[00:07:36] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[00:18:58] <hodapp> FWIW I think you can have Pandas target a SQL database instead of in-memory
[00:19:05] <hodapp> but I have no clue if this works well with something like TimescaleDB
[00:19:24] <clime> ok
[00:19:46] <hodapp> https://docs.dask.org/en/stable/dataframe-sql.html take a look at something like this too. something like Dask is probably insane overkill for your use-case though
[00:19:47] <SigmoidFroid> ⇒  Dask Dataframe and SQL — Dask documentation
[00:27:28] *** Quits: sinaowolabi_ (~SinaOwola@160.152.104.223) (Ping timeout: 250 seconds)
[00:30:22] <clime> ok, thank you!
[00:30:36] <hodapp> sure, good luck
[00:42:37] <dostoyevsky2> clime: Have you looked into apache arrow?  I thought it was interesting, as it lets you use database/SQL constructs as an API
[00:44:43] <mefistofeles> https://github.com/rmcelreath/stat_rethinking_2022 interesting
[00:44:44] <SigmoidFroid> ⇒  GitHub - rmcelreath/stat_rethinking_2022: Statistical Rethinking course winter 2022
[00:45:39] <mefistofeles> I missed the convo, but if you have something working with pandas, transitioning to dask should be pretty straightforward, since they have pandas objects/dataframes analogues
[00:46:37] <dostoyevsky2> Also https://duckdb.org/ -- they say it's focussed on OLAP/analytics, whereas something like sqlite is focussed on transactions/OLTP
[00:46:37] <SigmoidFroid> ⇒  DuckDB - An in-process SQL OLAP database management system
[00:46:47] <clime> dostoyevsky2: that would be potentially cool because i could transition to postgres if needed at any point
[00:46:56] <mefistofeles> clime: I feel you, the pandas API is not great, to put it lightly
[00:47:40] <dostoyevsky2> mefistofeles: it's literally data.frame from R without NA ;-)
[00:47:45] <clime> mefistofeles: i am not sure how i would evaluate it. just sometimes i am bit suprised by something but it might be just my inexperience...
[00:48:43] <mefistofeles> dostoyevsky2: exactly, I blame R for that one, evne if it's not as messy as the R api
[00:48:46] <mefistofeles> :P
[00:48:54] <mefistofeles> but it also comes with a lot of features and power
[00:49:19] <mefistofeles> there are some cleaner "alternatives" that can help, such as xarray
[00:49:31] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[00:49:50] <clime> like e.g. df[() & ()] <- need for parenthesis around the expression otherwise the behavior is semantically wrong i think? also not sure about that & operator - evokes in me binary and...so there are lots of these things that are very intuitive for me but otherwise it seems quite fast when i get it right
[00:49:51] <mefistofeles> clime: it does require experience to master python or, in that regard, any tool or programming language... programming is hard
[00:51:04] <dostoyevsky2> clime: the only time I've used Rust so far was in inferrence with Rayon... was easy to run  my dot products on an shm data structure and scale it up to all available processors with the Rayon lib
[00:51:16] <clime> (also dates are pain, there is python datetime, pandas Timepoint, then numpy's datetime64, and idk what else) - it's also numpy and pandas and native python are all mixed on one code and they all have something like an array but it's a bit different each time etc :)
[00:55:07] <mefistofeles> yeah, these are things you will encounter everywhere. If anything, python helps making it easier, imho. I think one of the issues nowadays is that people get bombarded with the "programming is easy" and "python is friendly" too much, it's not as easy or friendly, because reasons. Because information theory laws must be followed :)
[00:56:03] <dostoyevsky2> > What I Do: Director, Department of Human Behavior
[00:56:06] <clime> dostoyevsky2: this seems to implement the Apache Arrow memory format: https://github.com/pola-rs/polars - i looked at it yesterday - it seemed quite interesting
[00:56:07] <SigmoidFroid> ⇒  GitHub - pola-rs/polars: Fast multi-threaded DataFrame library in Rust | Python | Node.js
[00:56:53] <mefistofeles> yeah, that's one of the alternatives to pandas dataframes
[01:10:57] <taeaad> dostoyevsky2: Looks like there was an incorrect value. The RMSE must have been so high because most of the values hover between 0 and say 100, and then one entry was like 60m. I assume this entry is what caused the very high RMSE values. I am curious to know if the model would still be accurate (for general prediction) while having that super outlyer bogus value, but hey, aint nobody got time for that.
[01:16:54] <dostoyevsky2> taeaad: did you normalize the data before running it through the model?
[01:17:41] <dostoyevsky2> This lecture mefistofeles seems interesting: https://www.youtube.com/playlist?list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN
[01:17:42] <SigmoidFroid> ⇒  Statistical Rethinking 2022 - YouTube
[01:17:55] <dostoyevsky2> mefistofeles mentioned, even
[01:19:57] <dostoyevsky2> taeaad: also it's always good to do some basic exploratory data analysis (EDA) before using the data...  Kaggle is very famous for making sure you can't just use any model before doing the EDA step, as they will always put in some outliers or make the test data different in some ways from the validation data
[01:21:05] <taeaad> dostoyevsky2: Eh no? These are sales data so they are unit sales. The "error" is a data error that previously wasn't there since I filtered it out. I forgot about the filter since there previously was a R EDA intermediary step.
[01:21:53] <taeaad> Do you mean the competitions on Kaggle have deliberate oddities in the data?
[01:22:57] <taeaad> Interestingly it seems like without the error (a single line in the training_df) it now learns faster...
[01:24:44] <taeaad> But yeah, this would be picked up by any basic analysis. I was trying to avoid too much R intermediary scripts since at this point most of the functionality is in my Python code already. But it was literally just an filter to take out certain codes that I didn't realise also removed an error that I wasn't aware of.
[01:26:46] <taeaad> It reminds me of one time when a friend/colleague asked a farmer: "Are you sure your yield was 100 000 tons on that hectare?" Their EDA always started with column and row min/mean/max in Excel. Picks up 90% of errors :)
[01:26:50] <dostoyevsky2> taeaad: they don't say it openly but I've seen it many times...  when you do the EDA steps, draw histograms etc, look at standard deviations, etc you often find discrepancies between the different data sets, very much like when you work with real data
[01:27:13] <hodapp> it might be more accurate to say that they just don't clean the data for you
[01:27:18] <hodapp> and most 'real' data is... not perfect
[01:27:54] <taeaad> dostoyevsky2: I have basic familiarity with Kaggle, but don't do the competitions. I'll look out for that in the future.
[01:29:56] *** Joins: sinaowolabi_ (~SinaOwola@160.152.124.9)
[01:30:17] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) ()
[01:31:50] <dostoyevsky2> hodapp: They have to make sure the competition is fair and there are no data leakages, which can be very hard to do... but in the last step they might reapply some of those problems they found that wouldn't cause data leakage
[01:50:13] <dostoyevsky2> and for the petfinder competition the data leak was that someone wrote a crawler for the pet finder website and then tried to find those pets mentioned in the test data and hence knew whether they would have been adopted... so the guy who won those USD 25k didn't really have any meaningful insights into the test data (I think he got disqualified before that)
[01:50:33] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:18:30] *** Joins: marcello1 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3)
[02:42:56] *** Quits: marcello1 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3) (Read error: Connection reset by peer)
[02:42:59] *** Joins: marcello2 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3)
[03:06:42] *** Quits: marcello2 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3) (Read error: Connection reset by peer)
[03:16:11] *** Quits: palasso (~palasso@user/palasso) (Read error: Connection reset by peer)
[03:16:24] *** Joins: ivanzep (~ivanzep@2806:2f0:8000:950f:f1e5:10ff:7651:a62e)
[03:41:36] *** Quits: marcello42 (~mp@2001:1a81:1204:8a00:ea0c:2047:6cd8:7a12) (Ping timeout: 250 seconds)
[03:43:40] *** Joins: marcello42 (~mp@2001:1a81:121a:ec00:31d0:9741:cc4a:a42b)
[04:41:10] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[04:44:48] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[04:45:32] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[04:52:37] *** Quits: marcello42 (~mp@2001:1a81:121a:ec00:31d0:9741:cc4a:a42b) (Ping timeout: 240 seconds)
[05:41:49] *** Quits: _ohm (~research@user/ohm/x-5690770) (Quit: Konversation terminated!)
[07:13:35] *** Joins: fltrz (~ik@94.105.125.89.dyn.edpnet.net)
[07:18:21] <fltrz> hi, I have been interested in ML for a long time, mostly from a theory and math perspective, so was reading the 'under the hood' papers. I never actually familiarized myself with any of the toolkits that blackbox this stuff under the hood. Since I am going to apply for a job soon I decided to enroll in one of the online courses (ML A to Z handson Python and R), mathematically its bog
[07:18:27] <fltrz> simple for now, and its all happening under the hood, so learning the API's more than anything else. The course is on udemy. I use Linux. Most of the lectures use DRM, and the minimalist browsers I prefer don't support it. Do any of the popular recommendable online courses use a platform without DRM? its annoying to grab the recreational sim-cardless secondhand smartphone to watch a
[07:18:33] <fltrz> lecture on a small screen...
[07:19:03] <fltrz> I'd rather direct my money to a linux friendly platform...
[07:20:35] <fltrz> also the video format doesn't really add a lot of value... don't understand why they can't make the spoken word into coherent written paragraphs in the notebooks etc...
[07:23:12] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[07:25:37] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 240 seconds)
[07:29:11] *** [_] is now known as [itchyjunk]
[07:34:29] <mefistofeles> fltrz: probably not what you are looking for, and I know this also doesn't answer the specific question, I don't have a suggestion on a online course that is DRM-free or something like that. On the other hand, my suggestion is to separate life from work. If you are doing this for work or professional reasons, you should be able to just use a DRM browser for professional reasons
[07:35:08] <fltrz> thats true, but I don't have a work laptop yet, I'm to go an apply soon
[07:35:52] <fltrz> I don't want to arrive all loftyheaded and say I'm all theory, when I'm realistic and know I'll have to get my hands dirty
[07:36:23] <mefistofeles> fltrz: yeah, so use a ML approach, optimize your time, sounds like just using a DRM-enabled browser is time efficient in this case
[07:36:39] <mefistofeles> feel free to use it just for the course, that's on you
[07:38:00] <fltrz> well, thats what I'm doing with the smartphone (for the DRM thing), was just wondering if someone could recommend an alternative platform / course
[07:38:53] <mefistofeles> yeah, I cannot, I've never used any of those platforms... other than kaggle, maybe
[08:00:25] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[08:01:06] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[08:22:17] *** Quits: sinaowolabi_ (~SinaOwola@160.152.124.9) (Ping timeout: 240 seconds)
[09:13:23] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[09:39:25] *** Quits: Jong (~Jong@2620:10d:c090:400::5:263a) (Ping timeout: 240 seconds)
[09:52:45] <fltrz> the course is also chock-full of superstitions and mantras, instead of saying we do X for numerical stability, or we do Y to maintain numerical precision because something that should work identically from a mathematical perspective won't on a pragmatic computer...
[09:53:19] <fltrz> if they said the right thing at least the uninformed student has a term to look up
[09:55:50] <fltrz> also, they contradict theirself all the time, like first replacing missing values by column average... but when rescaling features they fit the rescaling on the training set and then apply the same rescaling on the test set... its quite harmless since the math is linear, but if thats their reason... why did they tolerate 'leaking' the train+test mean into the missing values?
[09:56:59] <fltrz> I feel like I'm being trained to fear the (in this case unfounded) wrath of math
[09:58:32] <mefistofeles> fltrz: it's just a way of dealing with missing data, you can just discard those as well... but any decision will have a bias (smaller or greater, depends)
[09:58:45] <mefistofeles> and yes, sometimes it's just "try it and see"...
[09:59:23] <mefistofeles> you don't need the mathematical justification, maybe there isn't even one at all, that happens
[09:59:45] <fltrz> its just dumb to fear global rescaling, its equivalent to selecting new units
[10:02:03] <mefistofeles> well, sometimes that doesn't work
[10:02:05] <mefistofeles> it depends
[10:02:11] <fltrz> but then again, I guess the course is OKish, I didn't enroll to learn the math (studied physics), just to get familiar with the API's and frameworks... and I guess it prepares me for future brainfarts at work
[10:02:43] <mefistofeles> sure, that's the state of the art so far
[10:02:59] <mefistofeles> most of the field lacks real mathematical structure foundation
[10:03:29] <mefistofeles> not that things are unjustified, but it's just not that well unified, I guess
[10:03:51] <fltrz> a lot of it is like programmers slowly reinventing statistical physics, or Volterra/Wiener representations for non-linear systems
[10:04:09] <mefistofeles> yeah, it feels that way, but you have to consider the real difference
[10:04:28] <mefistofeles> the statistical methods might be the same, but the objective is really different, that is new enough
[10:04:43] <mefistofeles> that is, predicting or fitting or forecasting
[10:05:06] <mefistofeles> whereas in statistical physics is more about dimensionality reduction and general descriptors
[10:05:44] <mefistofeles> but there is a significant overlap, indeed
[10:05:47] <fltrz> apart from the reinventing the wheel nonrigorously ("lets make a wheel from dirt"), there is the facet of didactic reasoning that is being formalized, which you don't see when studying statistical physics or volterra representations... like having a training and test set etc...
[10:06:11] <mefistofeles> fltrz: you are looking it at it the wrong way
[10:06:23] <mefistofeles> but I don't blame you, most physicists and mathematicians struggle with that :P
[10:07:07] <mefistofeles> they are good at recognizing same patters and methods they use in their own fields and already think they know better, but I suggest a humble approach, ML is very different
[10:07:19] <mefistofeles> even if at first it doesn't look like
[10:08:23] <mefistofeles> gotta go get some sleep, see you
[10:08:24] <fltrz> I learnt a *lot* of interesting things by reading the math heavier ML theoretical papers, like RM-AD etc, which wasn't taught to me in physics curricula
[10:09:05] <fltrz> but this course here is just codemonkeys nonsense
[10:09:51] <fltrz> to be fair Im just in the data preprocessing chapter, and the instructors themselves mention its the boring part, so perhaps things become better in the later sections, when it actually interests the instructors themselves
[10:11:43] *** Quits: ivanzep (~ivanzep@2806:2f0:8000:950f:f1e5:10ff:7651:a62e) (Remote host closed the connection)
[10:50:02] *** Joins: BinarySavior_ (~BinarySav@josh.vet)
[10:50:08] *** Quits: BinarySavior (~BinarySav@idlerpg/player/BinarySavior) (Ping timeout: 256 seconds)
[11:11:40] *** Joins: jlrnick (~josephler@2a01cb040a1594002c07b52d65ea1062.ipv6.abo.wanadoo.fr)
[11:12:04] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[11:13:14] *** Joins: sinaowolabi_ (~SinaOwola@160.152.124.9)
[11:21:12] *** Joins: manti7 (~manti7@176.10.104.94)
[11:30:15] *** Joins: sinaowolabi__ (~SinaOwola@160.152.124.9)
[11:31:31] *** Joins: palasso (~palasso@user/palasso)
[11:37:41] *** Joins: Jong (~Jong@2620:10d:c090:400::5:4f54)
[11:41:55] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[11:59:51] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[12:03:01] <lericson> machine learning is perhaps best described as virtual snail racing
[12:03:17] <lericson> well dl anyway
[12:05:19] <lericson> fltrz: what's RM-AD?
[12:24:31] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Read error: Connection reset by peer)
[12:25:56] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi)
[12:33:38] *** Joins: Codaraxis (~Codaraxis@user/codaraxis)
[12:36:40] *** Quits: Codaraxis_ (~Codaraxis@user/codaraxis) (Ping timeout: 256 seconds)
[12:40:55] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi) (Read error: Connection reset by peer)
[12:46:41] *** Quits: dstein64 (~dstein64@dannyadam.com) (Ping timeout: 256 seconds)
[12:48:52] *** Joins: dstein64 (~dstein64@dannyadam.com)
[12:55:02] *** Joins: `Tim (~zenguin@user/zenguin)
[12:57:00] *** Quits: `Tim (~zenguin@user/zenguin) (Client Quit)
[13:00:57] *** Quits: sinaowolabi__ (~SinaOwola@160.152.124.9) (Ping timeout: 240 seconds)
[13:01:37] *** Quits: sinaowolabi_ (~SinaOwola@160.152.124.9) (Ping timeout: 240 seconds)
[13:02:10] *** Quits: dstein64 (~dstein64@dannyadam.com) (Ping timeout: 256 seconds)
[13:03:41] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi)
[13:04:00] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:04:18] *** Joins: dstein64 (~dstein64@dannyadam.com)
[13:06:51] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi)
[13:07:50] <taeaad> Does early stop rounds have some sort of threshold concept of being "close"?
[13:08:44] <taeaad> Or does it strictly mean that RMSE is not improving; i.e. RMSE_t2 <= RMSE_t1 the score is either starting to get worse or is strictly equal after n rounds.
[13:08:44] *** Joins: `Tim (~zenguin@user/zenguin)
[13:09:14] <taeaad> I would think some models would "improve" indefinitely by simply overfitting.
[13:11:46] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:12:53] *** Joins: sinaowolabi__ (~SinaOwola@41.58.241.86)
[13:13:21] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi)
[13:14:36] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df083-245.dhcp.inet.fi) (Read error: No route to host)
[13:15:48] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[13:16:05] *** Joins: sinaowolabi_ (~SinaOwola@41.58.241.86)
[13:16:54] *** Quits: BinarySavior_ (~BinarySav@josh.vet) (Ping timeout: 256 seconds)
[13:18:15] *** Joins: BinarySavior (~BinarySav@idlerpg/player/BinarySavior)
[13:33:58] *** Quits: dstein64 (~dstein64@dannyadam.com) (Ping timeout: 250 seconds)
[13:42:41] <clime> taeaad: i do earlyStop on validation set, you do it on training set? i always wondered if that is possible...
[13:44:06] <taeaad> clime: Eh, I'm not sure now, will need to check code.
[13:45:18] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:46:46] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[13:47:40] <taeaad> clime: It is on the validation set, as you say. But the validation set is last couple of weeks of the training set.
[13:49:34] <clime> right, i think when you start overfitting on training set, results on validation set should get worse so maybe this is taken care of already...
[13:49:58] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:50:38] <taeaad> Oh, so your point is that since the validation set is a (small) subset of large training corpus, it's unlikely to just keep improving indefinitely.
[13:51:37] <taeaad> I would agree with that. If the validation period = training period (and it's possible in the library you use, like you are wondering) then it would just keep running indefinitely until it gets sort of maximally (or asymptotically) overfititng.
[13:53:02] *** Quits: sinaowolabi__ (~SinaOwola@41.58.241.86) (Ping timeout: 250 seconds)
[13:53:08] <taeaad> I think I need to learn more about how one chooses the validation subset. Does that mean that the validation subset isn't actually used for training?
[13:53:22] <clime> exactly
[13:53:44] *** Quits: sinaowolabi_ (~SinaOwola@41.58.241.86) (Ping timeout: 256 seconds)
[13:55:08] <taeaad> I find that somewhat surprising. Don't you want to use all possible data for training? Say it's something that is time critical, and you use 7 days for validation, then when you predict you add those 7 days back in, right, but the model hasn't adapted thus to the last 7 days. Can't you have a sort of second consolidation step?
[13:55:49] *** Joins: dstein64 (~dstein64@dannyadam.com)
[13:57:34] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[13:59:11] <clime> ye, that is my thinking too for time-series it might be important to keep that piece of data of training but haven't seen but so far excluding it for validation brought better results for me, but maybe some "consolidation" as you say might be beneficial...
[14:01:30] <taeaad> I mean, at least in theory. One could by luck have a dataset that doesn't need to train on that period. But I guess this means one should do cross validation with different validation subsets. I previously only thought cross validation is for checking the model on historical data.
[14:04:42] <taeaad> Or well, using historical data as a impetus for model adjustments. But choosing the validation set thus seems to be another component to CV. Changing the validation set without changing the prediction horizon should impact your score you get from whatever metric you are using.
[14:05:26] *** Joins: sinaowolabi__ (~SinaOwola@160.152.124.9)
[14:22:40] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[14:24:02] *** Joins: `Tim (~zenguin@user/zenguin)
[14:24:19] <taeaad> I suspect there should be a theorem or theorems about separation of training into used training set and validation set and convergence and other implications. For example if you by accident overlap training and validation sets that perhaps it would be guaranteed (depending on your algorithm) to converge (overfit) on the intersection.
[14:26:50] <taeaad> If you take linear regression by contrast, I think the analogy is that the training set determines what kind of regression you do (e.g. pick linear) and the validation set actually is just ommited from model selection (linear) but when you add back those points you redo the linear regression on the full train + valid. So in my mind the analogy is that you do "model selection" without the validation period. Ofc with decision trees the
[14:26:51] <taeaad> analogy is imperfect but I think this should be a basic intuition around it.
[14:30:06] *** Joins: Scarecr0w (scarecr0w@user/scarecr0w)
[14:33:22] *** Joins: Codaraxis_ (~Codaraxis@user/codaraxis)
[14:33:31] *** Quits: Codaraxis (~Codaraxis@user/codaraxis) (Read error: Connection reset by peer)
[14:37:32] *** Quits: Codaraxis_ (~Codaraxis@user/codaraxis) (Read error: Connection reset by peer)
[14:37:57] *** Joins: Codaraxis_ (~Codaraxis@user/codaraxis)
[14:46:30] <hodapp> this is a lot of how N-fold cross validation might be done on much smaller models that are easy to train
[14:47:01] <hodapp> the cross-validation is done more to validate the settings than the model itself, but the "real" model might be trained on everything
[14:47:33] <hodapp> but with deep learning with much longer training cycles, that isn't practical and there is no good way to just add in the validation data at the end
[14:53:57] *** Joins: marcello42 (~mp@2001:1a81:121a:ec00:31d0:9741:cc4a:a42b)
[14:55:57] *** Joins: sinaowolabi_ (~SinaOwola@160.152.124.9)
[14:59:36] *** Joins: sheb (~sheb@31.223.228.71)
[15:01:02] *** Quits: jlrnick (~josephler@2a01cb040a1594002c07b52d65ea1062.ipv6.abo.wanadoo.fr) (Ping timeout: 240 seconds)
[15:34:05] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[16:10:38] *** Quits: Codaraxis_ (~Codaraxis@user/codaraxis) (Quit: Leaving)
[16:19:45] *** Quits: sinaowolabi_ (~SinaOwola@160.152.124.9) (Ping timeout: 256 seconds)
[16:25:08] <clime> well, i also think that n-fold cross-validation might not be that great for time-series because validating on some intermediate data segment (between two training segments) may give you much better results than validating on the last segment. I.e. it might be easier to predict past data from future data (+some past data) than to predict future data only from past data. I think at
[16:25:11] <clime> some point I tried this in practice (basically trying to predict past from future and it was easier than predicting future from past) and got this result but I would need to experiments more to confirm that so don't take my word on that - one would expect it will be the same difficulty either way...
[16:37:42] *** Quits: sinaowolabi__ (~SinaOwola@160.152.124.9) (Ping timeout: 250 seconds)
[16:40:44] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[16:48:31] *** Joins: sinaowolabi__ (~SinaOwola@41.58.202.132)
[16:56:16] <taeaad> Well, a simple example would be bubbles in stock prices. If you validate on the first sliver based on say 80% that contain most of the peak it should be clear that there should be a low early price. Conversely, you can't predict a bubble (in general).
[16:57:43] *** Joins: georgios (~georgios@user/georgios)
[17:02:17] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 240 seconds)
[17:26:30] <hodapp> you also don't want n-fold cross-val on time-series for something like stocks, you want roll-forward cross validation
[17:27:57] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[17:33:31] <taeaad> hodapp: Right. So I think the question is, once you've done CV and decided how to build the model, in your final training run for the model you'll use to predict, is there any way to use the validation subset of your training dataset? We know it doesn't get used in a normal call to say lgbm.estimator as the validation set is the one producing the RMSE... But could one perhaps have a second model that trains on that historical data that
[17:33:31] <taeaad> you never used in training?
[17:34:52] *** Joins: jlrnick (~josephler@2a01cb040a1594002c07b52d65ea1062.ipv6.abo.wanadoo.fr)
[17:38:20] <fltrz> lericson: plain old reverse mode autodiff
[17:38:36] <lericson> ah
[17:46:23] *** Joins: sinaowolabi_ (~SinaOwola@160.152.0.223)
[18:00:24] <hodapp> taeaad: on models where CV is already feasible this is just kind of already done
[18:01:29] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 268 seconds)
[18:07:04] <taeaad> hodapp: So, let's say you have a call like this:
[18:07:06] <taeaad> https://pastebin.com/raw/BknAvSX6
[18:07:07] <SigmoidFroid> ⇒  (: No title)
[18:07:32] <taeaad> Does valid_data get used in training the model? Or just for RMSE calculations?
[18:12:15] <Hunts> what do you mean by "used in training the model"?
[18:16:43] <taeaad> Hunts: The point made above by clime was that you don't score RMSE on your whole training set; you score it on the validation subset of the training set. My response to that was then to point out that you then don't use the validation subset in training the model; you use it only in scoring the model. Note that these are time series data that we were talking about.
[18:18:29] <clime> i am not sure about how you evaluate RMSE in this particular case. I was stating that if you evaluate a stopEarly condition on RMSE computed on a validation set, then you should be well protected against overfitting...
[18:22:12] <taeaad> clime: Oh, I see. Then I misunderstood slightly. I thought RMSE was scored _only_ on the a validation set, irrespective of early stopping conditions.
[18:22:54] <clime> i would expect that the rmse will be evaluated twice and separately for training set and for validation set and that you can condition early stopping as you see appropriate but idk the details...
[18:23:00] *** Quits: sinaowolabi_ (~SinaOwola@160.152.0.223) (Ping timeout: 250 seconds)
[18:27:37] *** Quits: Malvolio (~Malvolio@user/malvolio) (Ping timeout: 240 seconds)
[18:30:01] <clime> maybe it's described somewhere in the docs of the framework :)
[18:30:11] <Hunts> precisely
[18:36:49] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[18:39:00] <taeaad> Alright.
[18:44:43] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[18:56:48] *** Quits: sinaowolabi__ (~SinaOwola@41.58.202.132) (Ping timeout: 250 seconds)
[18:56:54] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[18:58:14] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[19:03:25] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Read error: Connection reset by peer)
[19:04:22] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[19:11:50] *** Joins: sinaowolabi__ (~SinaOwola@160.152.179.228)
[19:21:55] <clime> guys, i know this is off-topic but do you sometimes struggle with wasting time watching your model train/evaluate? e.g. I know I should instead prepare batch of automated tests and let it run e.g. several hours, which will give me overall view of the model performance but often I cannot help myself but to try individual cases that give me me occassionally some quirk results that I
[19:21:58] <clime> then have hard time reproducing. I guess this is something that i need to work on (i.e. better separation of development vs. testing/evaluation)...sorry if this is too off-topic.
[19:23:29] *** Joins: SiegeLord (~sl@user/siegelord)
[19:23:30] *** Quits: jlrnick (~josephler@2a01cb040a1594002c07b52d65ea1062.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[19:23:52] <hodapp> ehh, usually if I'm watching the training curve when I don't need to it's when I'm already in a very distractable state
[19:25:04] <clime> are there cases when you "need to" watch training curve (online)?
[19:27:43] *** Quits: SiegeLord (~sl@user/siegelord) (Client Quit)
[19:32:06] <hodapp> sometimes it can be beneficial to see it in realtime, e.g. if you're trying out a new architecture or loss function or some case where you want to know as early as possible that it does or does not appear to be training properly
[19:35:49] *** Quits: sinaowolabi__ (~SinaOwola@160.152.179.228) (Ping timeout: 256 seconds)
[19:37:57] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 240 seconds)
[19:39:30] <clime> ye, ok, i think i need to create a git commit into a separate branch before i begin to train anything, so that i am sure what was the state when i got that good result. i guess more automation and more parallel work is always good too...
[19:44:04] <hodapp> it is a good idea to be able to track that, and mlflow and the like may have ways to help with that
[19:44:37] <hodapp> particularly so that if you look at an old model, or the train/val curve from it, you can query "exactly what data was this trained on?" and "exactly what revision of the code was this?"
[19:47:30] <clime> ye, that's useful, i kind of managed a bit of that myself - just need to avoid traning from a dirty tree :)
[19:50:26] *** Joins: sinaowolabi__ (~SinaOwola@160.152.179.228)
[19:50:27] <fltrz> does udemy intentionally put disinfo in their coursewares? They really stress giving feedback with Q&A section or the chat bot. Perhaps they are surreptitiously headhunting the critical thinkers who spot the intentional mistakes...
[19:57:13] <hodapp> disinfo...?
[20:11:32] *** Quits: sinaowolabi__ (~SinaOwola@160.152.179.228) (Ping timeout: 240 seconds)
[20:13:12] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Read error: Connection reset by peer)
[20:13:25] *** Joins: sinaowolabi__ (~SinaOwola@160.152.179.228)
[20:21:05] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[20:27:23] *** Quits: dstein64 (~dstein64@dannyadam.com) (Ping timeout: 256 seconds)
[20:39:30] *** Joins: dstein64 (~dstein64@dannyadam.com)
[20:52:05] <fltrz> hodapp: "don't use global scaling on dataset, use scaling on training set, or else we leak the secret global average; feature scaling AFTER splitting in train and test sets" vs "missing values? before ever splitting the dataset we just replace missing values with column average..."
[20:53:08] <fltrz> they spent more minutes contradicting themselves than providing didactic information
[20:54:11] <fltrz> might as well read the phrase "then do the thing that there because" over and over
[21:08:20] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[21:26:29] *** Joins: SiegeLord (~sl@user/siegelord)
[21:27:32] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[21:50:04] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[21:57:57] *** Quits: sinaowolabi__ (~SinaOwola@160.152.179.228) (Ping timeout: 240 seconds)
[22:04:26] *** Quits: marcello42 (~mp@2001:1a81:121a:ec00:31d0:9741:cc4a:a42b) (Ping timeout: 250 seconds)
[22:12:23] *** Joins: sinaowolabi__ (~SinaOwola@41.58.232.111)
[22:30:24] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[22:57:44] *** Quits: sinaowolabi__ (~SinaOwola@41.58.232.111) (Ping timeout: 256 seconds)
[23:10:54] *** Joins: sinaowolabi__ (~SinaOwola@160.152.179.228)
[23:26:32] *** Joins: pyeveryt_ (~pyeveryth@64-18-153-56.starry-inc.net)
[23:27:19] <pyeveryt_> hi could someone please help me with this question? I am trying to create a scatterplot of predictions vs groundtruth values https://stackoverflow.com/questions/70806829/drawing-a-45-degrees-reference-line-as-well-as-making-x-and-y-axis-equal-range
[23:27:19] <SigmoidFroid> ⇒  python - drawing a 45 degrees reference line as well as making x and y axis equal range - Stack Overflow
[23:28:38] *** Joins: jlrnick (~josephler@2a01cb040a1594006c3c30233d5dcb6a.ipv6.abo.wanadoo.fr)
[23:31:48] *** Quits: rodgort (~rodgort@static.38.6.217.95.clients.your-server.de) (Quit: Leaving)
[23:48:40] *** Quits: jlrnick (~josephler@2a01cb040a1594006c3c30233d5dcb6a.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
