[00:04:11] *** Joins: Klinda (~superleag@user/klinda)
[01:17:01] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[01:42:51] <Sheilong> Are there data augmentation techniques for tabular data?
[01:51:30] *** Joins: Karel (~Karel@ptr-9091p582mq4ma2whlxa.18120a2.ip6.access.telenet.be)
[02:04:04] *** Quits: Ikkepop (~C-Man@78.31.190.82) (Ping timeout: 260 seconds)
[02:12:22] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:12:55] *** Quits: Codaraxis (~Codaraxis@user/codaraxis) (Quit: Leaving)
[02:30:25] *** Quits: hygl (uid16621@tinside.irccloud.com) (Quit: Connection closed for inactivity)
[02:35:03] <hodapp> sorta
[02:37:04] <hodapp> problem is, it's a lot less obvious there without knowing the underlying distribution and if you knew the underlying distribution you'd very likely not need ML in the first place
[02:53:22] *** Quits: stefan-_ (~cri@42dots.de) (Ping timeout: 260 seconds)
[02:56:45] <dostoyevsky2> Sheilong: in kaggle competitons the default seems to be to replace unknown data with an average value
[02:56:57] *** Joins: stefan-_ (~cri@42dots.de)
[02:57:00] <dostoyevsky2> At least that's a common pattern I see
[03:10:23] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:20:35] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Read error: Connection reset by peer)
[03:23:55] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[03:33:00] *** Joins: jinsun__ (~quassel@user/jinsun)
[03:34:01] *** Joins: jinsun___ (~quassel@user/jinsun)
[03:36:02] *** Quits: jinsun (~quassel@user/jinsun) (Ping timeout: 240 seconds)
[03:36:54] *** Joins: Ikkepop (~C-Man@78.31.190.82)
[03:38:20] *** Quits: jinsun__ (~quassel@user/jinsun) (Ping timeout: 268 seconds)
[03:41:40] *** Quits: jerome- (~jerome@78.193.84.130) (Remote host closed the connection)
[03:43:25] *** Joins: jerome- (~jerome@78.193.84.130)
[04:06:12] *** Quits: marcello42 (~mp@p200300dfaf07ff01fa2e52f98b4dfc80.dip0.t-ipconnect.de) (Quit: WeeChat 3.3)
[04:15:26] *** jinsun___ is now known as jinsun
[04:24:29] *** Quits: Sheilong (uid293653@ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[04:31:21] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 245 seconds)
[04:37:35] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[04:43:46] *** Joins: Klinda (~superleag@user/klinda)
[05:23:02] *** research is now known as _ohm
[05:50:24] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[06:06:21] *** Quits: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com) (Quit: WeeChat 3.0.1)
[06:21:59] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
[06:53:17] <hodapp> imputation and augmentation aren't quite the same though
[07:03:54] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[07:06:58] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[07:18:16] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[07:24:12] <cloudcell> does anybody know about any algorithm for relation extraction from a piece of text that actually works? I have been trying to use code from here: https://paperswithcode.com/area/natural-language-processing/relation-extraction, so far unsuccessfully :(
[07:24:13] <SigmoidFroid> ⇒  Papers with Code - Natural Language Processing
[08:03:22] *** Joins: jinsun__ (~quassel@user/jinsun)
[08:04:44] *** Quits: jinsun (~quassel@user/jinsun) (Ping timeout: 268 seconds)
[08:19:17] *** Quits: akevinhuang2 (~thekevinh@user/thekevinhuang) (Ping timeout: 256 seconds)
[08:36:51] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
[09:11:46] *** Joins: shoky_ (uuuggg@141.226.193.67)
[09:12:46] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 260 seconds)
[09:20:45] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[09:46:16] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[10:33:09] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[10:38:25] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[10:48:19] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
[10:56:07] *** Joins: hygl (uid16621@tinside.irccloud.com)
[11:01:02] *** Joins: palasso (~palasso@user/palasso)
[11:21:42] *** Joins: manti7 (~manti7@176.10.104.94)
[11:42:09] *** Quits: immibis (~hexchat@62.156.144.218) (Ping timeout: 256 seconds)
[11:54:40] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[11:55:07] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[12:00:26] *** Quits: givemeyourpies (~givemeyou@c-66-235-2-202.sea.wa.customer.broadstripe.net) (Quit: Going offline, see ya! (www.adiirc.com))
[12:14:50] *** Joins: givemeyourpies (~givemeyou@c-66-235-2-202.sea.wa.customer.broadstripe.net)
[12:19:53] *** jinsun__ is now known as jinusun
[12:19:57] *** jinusun is now known as jinsun
[12:43:02] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[12:45:22] *** Joins: SiegeLord (~sl@user/siegelord)
[12:49:35] *** Quits: Pent (sid313808@lymington.irccloud.com) (Ping timeout: 256 seconds)
[12:49:46] *** Joins: Pent (sid313808@lymington.irccloud.com)
[13:44:38] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[13:48:59] *** Quits: stefan-_ (~cri@42dots.de) (Ping timeout: 264 seconds)
[13:52:50] *** Joins: stefan-_ (~cri@42dots.de)
[13:55:37] *** Quits: Ikkepop (~C-Man@78.31.190.82) (Ping timeout: 268 seconds)
[14:09:22] *** Joins: immibis (~hexchat@62.156.144.218)
[14:34:18] *** Joins: jlrnick (~josephler@2a01cb040a1594006432f15249eb6cfc.ipv6.abo.wanadoo.fr)
[14:49:21] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[14:54:15] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[15:30:56] *** Quits: jlrnick (~josephler@2a01cb040a1594006432f15249eb6cfc.ipv6.abo.wanadoo.fr) (Ping timeout: 245 seconds)
[16:12:50] *** Joins: Klinda (~superleag@user/klinda)
[16:37:22] <Klinda> buongiorno
[16:44:37] <hodapp> https://arxiv.org/abs/2111.06377 whaaa that's kind of epic
[16:44:39] <SigmoidFroid> ⇒  [2111.06377] Masked Autoencoders Are Scalable Vision Learners  | Facebook
[16:52:20] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[17:19:05] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[17:21:39] <Klinda> hodapp: I have to study these notions to understand the paper
[17:22:13] <Klinda> aren't convolution neural networks good in computer vision?
[17:30:17] <[itchyjunk]> CNNs are good when spatial information is useful.
[17:30:51] <[itchyjunk]> like if you take images and flatten it into a vector, you lose the spatial relationship between pixels
[17:30:57] <[itchyjunk]> and sometimes it matters
[17:31:11] <[itchyjunk]> but idk if CNN is always better, hmmm
[17:38:46] <hodapp> over the past year or so there has been a lot of work with transformers and a pretty convincing case that they outperform CNNs
[17:39:58] <[itchyjunk]> So i vaguely understand transformers take cartisian product of 2 sentences and blah blah
[17:40:09] <[itchyjunk]> but how is transformer used for images?
[17:40:20] <[itchyjunk]> you take a cartisian product of the set of pixels?
[17:40:28] <[itchyjunk]> hmm actually, that does make sense
[17:40:51] <hodapp> see the paper on ViT; IIRC, the basic method is to treat the image as a sequence of patches and then otherwise handle it almost identically to the models used for NLP
[17:40:52] <[itchyjunk]> also makes sense that it would outperform CNN's because CNN is picking an arbitrary window there the spatial information is preserved
[17:41:14] <[itchyjunk]> oh sequence of patches... hmm
[17:41:26] <[itchyjunk]> is there anything less dense than a paper that i could read for a summary?
[17:41:31] <hodapp> that isn't really true; while convolution is over a certain window, CNNs have a receptive field that as you move to deeper and deeper layers encompasses pretty much the whole image
[17:42:00] <[itchyjunk]> yes the layers are related to each other so you indiriectly do have access to global information
[17:42:14] <[itchyjunk]> but it's not garunteed that all relationships are exposed/preserved that way
[17:42:38] <[itchyjunk]> lower layer overfitting and such probably happens cause information loss in some sense
[17:42:42] <[itchyjunk]> wild guess though
[17:42:52] <hodapp> nothing guarantees that ever, I'm just talking about the window over which CNNs handle spatial information
[17:43:29] <hodapp> as for a shorter overview of ViT, I don't know offhand, but Vision Transformers are sort of the seminal paper for what path was taken for transformers over images, so anything on ViT should be relevant
[17:43:38] <[itchyjunk]> yes but previous layer could be passing in hidden bias to higher layers and poisons the whole chain
[17:43:46] <[itchyjunk]> i'll google
[17:43:52] <[itchyjunk]> someone might have bloged about it
[17:44:00] <hodapp> what "hidden bias"?
[17:44:53] <hodapp> that could happen in any neural net with multiple layers
[17:46:07] <[itchyjunk]> sure, it could. but it does in CNN
[17:46:08] <[itchyjunk]> https://theaisummer.com/vision-transformer/
[17:46:09] <SigmoidFroid> ⇒  How the Vision Transformer (ViT) works in 10 minutes: an image is worth 16x16 words | AI Summer
[17:46:11] <[itchyjunk]> looks like i was right
[17:46:51] <hodapp> what is the evidence, though, that CNNs are (1) actually suspectible to this and (2) more susceptible to this than other NNs?
[17:46:59] <[itchyjunk]> the locally restricted receptive field does seem to lose information
[17:48:33] <hodapp> the information isn't really "lost" when adjacent elements in every direction will have covered further in some direction
[17:50:52] <hodapp> further, you can see results of neural nets that explicitly *don't* have this behavior: they fail to generalize spatially on (e.g. on things like translation)
[17:53:07] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[17:54:03] <hodapp> part of why CNNs succeeded is because the use of convolution gave a way of substantially reducing model capacity, and this combined with some other regularization allowed the models to generalize far better than all else available at the time
[17:54:54] <hodapp> convolution being over a limited, fixed-size window isn't a bug, it's what let them work in the first place
[17:56:16] <[itchyjunk]> https://theaisummer.com/receptive-field/
[17:56:16] <SigmoidFroid> ⇒  Understanding the receptive field of deep convolutional networks | AI Summer
[17:56:37] <[itchyjunk]> idk seems to suggest receptive layer of n-1 impacts receptive layers of layer n
[17:56:47] <[itchyjunk]> maybe they have tricks to get around this
[17:57:12] <hodapp> obviously, the receptive fields stack across the entire network
[17:58:18] <hodapp> what would one be "getting around"?
[17:59:00] <hodapp> it is more or less by design that the further and further the layers are from the input, the less tied they are to the spatial structure of the input image, and the higher-level their semantic information
[17:59:16] <[itchyjunk]> yes but activation of layer n depends on n-1 and so on. so you are dependent on activation of lower layer for higher layers spatial relations
[17:59:53] <hodapp> yes, like I just said: the higher the layers, the less that spatial relation remains
[18:00:19] <hodapp> networks which still require that spatial information (e.g. R-CNN derivatives for object detection) usually require some adaptations to preserve it
[18:00:26] <[itchyjunk]> is conv invariant under scaling and rotation?
[18:01:16] *** Joins: `Tim (~zenguin@user/zenguin)
[18:01:39] <hodapp> somewhat, but typically requires augmentation
[18:02:29] <hodapp> convolution itself is only translation-invariant, but in the context of a larger CNN it will typically develop some level of scale and rotation invariance
[18:07:50] <hodapp> IIRC, capsule networks (it was a pretty huge paper of 2017, Hinton was on it) tried to make these transformations a bit more explicit but the effort sort of fizzled out, I forget why
[18:09:48] <hodapp> it was a huge departure architecture-wise
[18:10:13] <[itchyjunk]> explicit in what sense?
[18:10:28] <[itchyjunk]> i don't understand what transformers being sequential means
[18:12:23] <hodapp> more or less that they treat the data as 1-dimensional
[18:12:38] <[itchyjunk]> ah they work with vector only?
[18:12:48] <[itchyjunk]> weird, so you flatten the image then feed it to transformer?
[18:13:01] <[itchyjunk]> don't you lose all your spatial data then?
[18:13:18] <hodapp> they add a positional encoding to preserve spatial information
[18:13:51] <[itchyjunk]> oh like the pixel data plus positing data is all in that vector?
[18:14:02] <hodapp> think so
[18:14:13] <HuntsMan> transformers were made for NLP tasks
[18:14:16] <[itchyjunk]> interesting approach. and transformers perform better on images than CNN?
[18:14:36] <[itchyjunk]> well for attention requiring task
[18:14:45] <hodapp> [itchyjunk]: for a paper that just came out and tried to do a very 1:1 comparison, see: https://arxiv.org/abs/2111.05464
[18:14:46] <SigmoidFroid> ⇒  [2111.05464] Are Transformers More Robust Than CNNs?  | Johns HopkinsU/UCalif
[18:15:17] <[itchyjunk]> papers are hard :( trying to read this pop sci blog currently lol
[18:15:28] <hodapp> because there were plenty of papers that showed that if you take a gigantic transformer and train it on a gigantic GPU farm with a gigantic number of images, it outperforms a CNN, which some people pointed out isn't exactly a fair comparison
[18:15:43] <[itchyjunk]> alphafold2 used transformers too right?
[18:15:48] <hodapp> not sure
[18:15:54] <[itchyjunk]> i wonder what their raw data was..
[18:16:31] <[itchyjunk]> are transformers in general larger than cnn?
[18:16:46] <[itchyjunk]> i remember the gpt-3 was huge because of the number of parameters i think
[18:18:07] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[18:25:41] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[18:41:22] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Read error: Connection reset by peer)
[18:42:01] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[18:49:33] *** Quits: coraxx (~coraxx@mail.8-pm.com) (Quit: Leaving)
[19:01:59] *** Joins: tomeaton17 (~tomeaton1@92.234.2.175)
[19:02:27] <tomeaton17> whats the best way to visualise a linear classifier with more than 3 features?
[19:02:46] <tomeaton17> I want to check that the hyperplane that is chosen is reasonable
[19:18:42] <dostoyevsky2> tomeaton17: PCA can summarize  data that has too many dimensions
[19:19:57] <tomeaton17> dostoyevsky2 i wasnt looking to have to do that tbh. i will have to try to check the classifier in another way
[20:13:07] <[itchyjunk]> visualize them as hyperplanes no?
[20:16:52] <tomeaton17> [itchyjunk] well I can't do that when d>3
[20:27:11] <hodapp> [itchyjunk]: a lot of the transformers used are, yes. The first paper I posted purposely tests one with a similar number of params as a typical CNN
[20:28:48] <hodapp> tomeaton17: you can try t-SNE but opinions differ on that one. As a first pass, doing PCA and picking the top two dimensions "should" show something sensible
[20:30:04] <tomeaton17> I can try to have a look at sklearn pca. I implemented the linear classifiers myself with pocket algorithm so not sure what data formats are required for sklearn pca.
[20:30:29] <tomeaton17> hodapp never heard of t-sne. looks interesting
[20:30:48] <hodapp> PCA is very general; you can do it over pretty much any set of N-dimensional vectors
[20:31:40] <tomeaton17> I am familiar with the theory but never tried an implementation before.
[20:32:15] <hodapp> and all of the transformations are linear, so as I understand it, if you've  a plane in the N-dimensional space then in the transformed space for PCA it also has to be a plane
[20:32:51] <hodapp> I mean: if it is sklearn, numpy array or anything that behaves like one
[20:33:15] <hodapp> it doesn't need labels or column names or anything
[20:34:02] <tomeaton17> i then also need to transform my classifier as well no?
[20:37:16] <hodapp> if you want to see it in the new space, yes
[20:38:24] <hodapp> my point is more - you might see sensible separation in t-SNE but it's also nonlinear
[20:40:06] <tomeaton17> well i have already achieved the separation with a non-linear transform
[20:40:33] <hodapp> you said you've a linear classifier though
[20:40:57] <tomeaton17> yes I do have a linear classifier, but it is in non linear space
[20:41:11] <hodapp> oh, you doing SVM or something?
[20:41:46] <tomeaton17> hodapp: No, I am just practicing non linear transforms and seeing how I can separate data using it
[20:42:31] <tomeaton17> I think I will just have to trust the algorithm :)
[21:28:18] *** Quits: tomeaton17 (~tomeaton1@92.234.2.175) (Quit: Client closed)
[21:38:01] *** Quits: french_c (~LAG@mars.void.fm) (Quit: ZNC 1.7.5 - https://znc.in)
[21:49:58] *** Joins: french_c (~LAG@mars.void.fm)
[22:57:25] *** Joins: Ikkepop (~C-Man@78.31.190.82)
[23:09:58] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 260 seconds)
[23:59:07] *** Quits: Ikkepop (~C-Man@78.31.190.82) (Read error: Connection reset by peer)
