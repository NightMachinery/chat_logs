[00:00:30] <galambo> I think my opinion on covid changed dramatically when I heard greg cochran's interviews with james miller in early february
[00:01:08] <gwern> did you use the mask?
[00:03:11] <galambo> gwern, I posted this on 29 february when I finally received the mask
[00:03:22] <galambo> < galambo> I have a 7502 respirator but it looks ridiculous
[00:04:07] <galambo> I bought disposable ffp masks when I realised I could not feasibly wear it
[00:04:16] <galambo> but their prices had come up on ebay at the time
[00:05:30] <galambo> gwern, what did you do?
[00:05:37] <galambo> what did you guys do
[00:06:38] <kuudes> I isolated for 4 weeks
[00:06:40] <two2thehead> surgical masks for daily outside use, we bought cloth masks at the beginning when the surgical masks were in short supply at the beginning
[00:06:47] <kuudes> but that was only after it already was here
[00:07:01] * two2thehead checks his calendar
[00:07:08] <two2thehead> more than a year and a half of this mess
[00:07:21] <kuudes> I bought ffp3 for my elderly relative who was undergoing radiation therapy
[00:07:34] <kuudes> this is the month it seems to go through here now
[00:07:41] <kuudes> it seems they are not stopping it anymore
[00:09:07] *** Quits: topdownjimmy (~topdownji@user/topdownjimmy) (Remote host closed the connection)
[00:09:28] <kuudes> I have not yet got it myself, but my nephew got exposed in daycare
[00:09:42] <galambo> I posted this on march 2 2020: covid-19 is not a virus. it is a nanobot. it will take over the universe
[00:09:51] <kuudes> in jest?
[00:10:05] *** Joins: topdownjimmy (~topdownji@user/topdownjimmy)
[00:10:12] <kuudes> what did I say? how miscalibrated I was?
[00:11:19] *** Quits: thoros (~thoros@80-121-140-225.adsl.highway.telekom.at) (Quit: WeeChat 3.0.1)
[00:12:31] <Betawolf> I underestimated societal response, I thought we'd just have a spike in deaths and a bit of irate coverage and funding shuffling, not population-wide interventions still ongoing over a year later.
[00:12:51] <Jayson_Virissimo> Throwback to March 2020: https://github.com/flattenthecurve/guide/pull/131
[00:12:53] <Robomot> Remove mask-hoarding section by jaysonvirissimo ¬∑ Pull Request #131 ¬∑ flattenthecurve/guide ¬∑ GitHub (This section was too vague, used questionable reasoning, and seemed unjustified.)
[00:13:02] <shawwwn> galambo: fwiw, it took some years for people in general to realize that buy and hold was the winning strategy
[00:13:09] <shawwwn> I made the same mistakes
[00:13:25] <shawwwn> At least you only lost 30% of your parent‚Äôs money instead of 100%
[00:13:26] <kuudes> I managed to buy it much later, procrastrinated a couple of years
[00:13:52] <gwern> it's funny because 'HODL' is such a midwit-example. HODL / actively trade and rebalance / HODL
[00:14:40] <mst> kuudes: he won't know what you said, I restricted the extraction I did for him to only things he'd said
[00:14:55] <kuudes> oh
[00:15:06] <kuudes> sorry then, I have to grep my own logs I suppose :)
[00:15:55] <galambo> shawwwn, just to clarify I lost 30% in the transaction fees from egyptian pounds to bitcoin
[00:16:05] <mst> fairly sure we weren't showing up in each others' logs at all at the time so I don't think I can offer to parse mine anyway :D
[00:16:23] <shawwwn> galambo I hear <words>
[00:16:30] <shawwwn> And understand about as much
[00:16:45] <kuudes> indeed indeed ^_^;
[00:17:18] <mst> 'getting from a non-mainstream currency to being able to buy stuff on an exchange can involve a loss of a non-trivial percentage of the funds in the process'
[00:17:26] <mst> is how I understand it
[00:18:08] <kuudes> I would not think conversion cost from developing country currency to developed country currency of 30% would be very unexpected
[00:18:31] <kuudes> I mean, there usually are protectionist stuff that sort of makes the domestic currency to be in disparity with the foreign currency
[00:18:36] <kuudes> fudge stuff
[00:18:56] <kuudes> currency exchange tax could be a decent protectionist measure I suppose :-p
[00:19:54] <galambo> I think I bought something akin to a gift card that could be redeemed for in-game currencies for some games
[00:20:31] <galambo> then found that some weird exchange accepts this gift card thingy as payment
[00:20:37] <galambo> for bitcoin
[00:25:29] *** Joins: brand0 (~brandon@user/brand0)
[00:25:37] <feepbot> <gwern> https://www.joshwcomeau.com/css/transforms/
[00:25:38] <feepbot> CSS Transforms tutorial (The "transform‚Äù property is such a powerful part of the CSS language! In this blog post, we'll take a deep look at this property and see some of the nifty things it can do.)
[00:29:39] <Robomot> [Astral Codex Ten] Eight Hundred Slightly Poisoned Word Games - https://astralcodexten.substack.com/p/eight-hundred-slightly-poisoned-word
[00:34:26] <ggreer> https://en.wikipedia.org/wiki/Blood_on_the_Risers O_o
[00:34:27] <Robomot> Blood on the Risers - Wikipedia ("Blood Upon the Risers" is an American paratrooper song from World War II. It is associated with all current airborne units, including the 101st Airborne Division, the 82nd Airborne Division, the 173rd Airborne Brigade and 4th Brigade Combat Team (Airborne) of the 25th Infantry Division, and the 120th CTS (United States) as well as British airborne units. ‚Ä¶)
[00:34:40] <ggreer> https://www.youtube.com/watch?v=VWgsdexkv18 the whole song
[00:34:41] <Robomot> Blood On the Risers(Gory Gory What a Helluva Way to Die) - YouTube (Link to an Original Distribution Document:http://www.serviceofsupply.com/images/Keeper%20Pics/BloodonTheRisers.jpgLink to Paratroop Training Booklet circa 19...)
[00:36:08] <shawwwn> https://www.reddit.com/r/cringepics/comments/p0zcgm/cartoon_porn_has_corrupted_the_children/?utm_source=share&utm_medium=ios_app&utm_name=iossmf Reddit is good today
[00:36:21] <feepbot> Cartoon porn has corrupted the children : cringepics (21,622 votes and 442 comments so far on Reddit)
[00:36:25] <shawwwn> Thought of at least 5 of you
[00:41:26] <feepbot> <gwern> https://twitter.com/images_ai/status/1424616437361094656
[00:41:36] <|dbotdan> Images Generated By AI Machines (@images_ai, 2021-08-09 06:20): ‚Äò"junji ito horror collection"‚Äô Images: https://nitter.namazso.eu/pic/media%2FE8VBF6nXIAIG9gs.png%3Fname%3Dorig (description: a collage of a woman's face; confidence: 0.54)
[00:41:38] <pompolic> when i heard video games were moral decay i became Team Moral Decay
[00:43:30] <Obormot\Arcturus> https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4355e699-56ee-49c1-becf-278914093bfe_2574x1760.png wow
[00:43:30] <Robomot> image/png (1287x880; 5.5 MB)
[00:44:31] <Obormot\Arcturus> https://www.greaterwrong.com/posts/LbdLFiZcwWcLvLa25/covid-8-5-much-ado-about-nothing#Thinking_of_the_Children
[00:44:32] <Robomot> Covid 8/5: Much Ado About Nothing - LessWrong 2.0 viewer [Thinking of the Children] (Getting into the weeds on the CDC‚Äôs new guidance and scaremongering, and the study they cited as justifications, caused this week‚Äôs post to get rather long. ‚Ä¶)
[00:44:43] <Obormot\Arcturus> "You can decide for yourself how to think about the photo above, combined with an official request for masking _at home with one‚Äôs own children_.
[00:44:43] <Obormot\Arcturus> The difference between official school policies and young adult dystopian novels is that no one would buy this in a young adult dystopian novel, because it‚Äôs fiction and therefore has to make at least some sense.
[00:44:44] <Obormot\Arcturus> Meanwhile, as noted earlier, many teachers remain unvaccinated (40% in NY), and in most places the teachers‚Äô unions have put a stop to any talk of a mandate. "
[00:45:13] *** Quits: apoc (~apoc@49.12.13.193) (Changing host)
[00:45:13] *** Joins: apoc (~apoc@user/apoc)
[00:46:02] <RiversHaveWings> That's just a slightly more explicit version of how school is anyway lol
[00:48:54] <Obormot\Arcturus> inb4 parents approve of this *because* they don't want kids having too much contact with each other
[00:49:20] <saturn2> kids are being made miserable at school for nonsensical bureaucratic reasons? oh no
[00:50:53] <Obormot\Arcturus> Meanwhile it looks like my hairstylist has a bad enough case of covid that the hair salon is looking for a replacement... I'm gonna have to find someone else to cut my hair. This is a tragedy :(
[00:52:28] <Obormot\Arcturus> Meanwhile x2, https://apnews.com/article/lifestyle-business-health-california-coronavirus-pandemic-5ebe70407fcd94ef712c16410f32c4b1
[00:52:29] <Robomot> Bacon may disappear in California as pig rules take effect (DES MOINES, Iowa (AP) ‚Äî Thanks to a reworked menu and long hours, Jeannie Kim managed to keep her San Francisco restaurant alive during the coronavirus pandemic. ‚Ä¶)
[00:57:29] <feepbot> <gwern> https://www.animenewsnetwork.com/feature/2021-08-09/yoko-kanno-what-she-been-up-to/.175767
[00:57:30] <feepbot> Yoko Kanno: What's She Been Up To? - Anime News Network (At 32 years old, she composed one of the most iconic anime soundtracks in history. Now two decades later, what is the famed Yoko Kanno up to?)
[00:58:37] <ggreer> how much money did they give her? it must have been a lot
[00:58:58] <ggreer> > as Netflix announced this past June that Kanno will once again be taking up the composer's baton in the upcoming live-action adaptation of Cowboy Bebop.
[00:59:13] <ggreer> probably the only enjoyable part of that whole show
[01:00:20] <Jayson_Virissimo> I remember liking CB a lot, but haven't watched it since high school, and my tolerance for subpar media was much greater back then.
[01:01:16] <ggreer> I mean the live action remake
[01:01:33] <ggreer> the show itself has aged pretty well. my gf is not an anime fan but she liked it. the ending made her cry
[01:02:25] <kuudes> why are teachers against masks?
[01:02:33] <kuudes> ggreer, oh, nice :)
[01:03:11] <ggreer> she only gave the show a shot because one of her friends is in the live action remake, but she was hooked after the first couple of episodes
[01:08:12] <feepbot> <gwern> https://www.ft.com/content/d8566dc5-6818-4d9b-830b-eedcb9c8103e little did lukashenko realize that diversity is the eu's strength, and he's only weakening belarus
[01:08:13] <feepbot> Become an FT subscriber to read | Financial Times (News, analysis and comment from the Financial Times, the world ºs leading global business publication)
[01:11:34] <Obormot\Arcturus> kuudes: We've gone over this before, I think - in America, many school teachers are very stupid
[01:11:44] <Robomot> [Less Wrong [frontpage]] Eight Hundred Slightly Poisoned Word Games by Scott Alexander - https://www.greaterwrong.com/posts/kxW6q5YdTGWh5sWby/eight-hundred-slightly-poisoned-word-games
[01:12:05] <Obormot\Arcturus> (This is not the only reason they are against masks, I'm sure, and certainly not the only reason to be against masks in general, but it is a big contributor, I am sure of it)
[01:13:48] *** Quits: CoJaBo_ (~aztec@c-73-64-56-249.hsd1.pa.comcast.net) (Changing host)
[01:13:48] *** Joins: CoJaBo_ (~aztec@user/CoJaBo)
[01:15:23] <kuudes> ok, I guess
[01:18:23] <Obormot\Arcturus> kuudes: https://www.iwriteessays.com/college/college-major-iq e.g. ... note how "education" is dead last
[01:18:24] <Robomot> College Major And IQ-What Your College Major Says About Your IQ - iWriteEssays (Intelligence quotient or IQ is a score derived from one of the several standardized tests designed to assess intelligence. IQ measures raw intelligence, which means that it does not evaluate a person's ability to interact well with others, or one's knowledge or skills. ‚Ä¶)
[01:18:29] <ggreer> Jayson_Virissimo: https://myanimelist.net/animelist/ggreer?status=7&order=4&order2=0 in case you're curious what my tastes are
[01:18:30] <Robomot> ggreer's Anime List - MyAnimeList.net
[01:19:19] <Obormot\Arcturus> https://www.quora.com/What-college-majors-have-on-average-students-with-the-highest-IQ?share=1 more of the same
[01:19:20] <Robomot> What college majors have, on average, students with the highest IQ? - Quora
[01:20:57] <Obormot\Arcturus> (And "social work" is even lower than "education". And then think of the sort of power we give to social workers...)
[01:23:10] <dutchie> do you have to do "education" as a college major in the us to become a teacher? here in the uk you'd normally do a subject as an undergrad and then do a postgraduate conversion course
[01:23:37] <dutchie> for secondary education at least, not sure about for primary/elementary school
[01:24:00] <Obormot\Arcturus> Yeah you do
[01:24:08] <Obormot\Arcturus> (Usually)
[01:24:58] <mst> social work over here used to be 'degree in whatever, then conversion course' but I've no idea what it is now
[01:25:11] <Obormot\Arcturus> gwern: https://www.gwern.net/Littlewood you got some kinda typo or something in here
[01:25:11] <Robomot> Littlewood‚Äôs Law and the Global Media ¬∑ Gwern.net (Selection effects in media become increasingly strong as populations and media increase, meaning that rare datapoints driven by unusual processes such as the mentally ill or hoaxers are increasingly unreliable as evidence of anything at all and must be ignored. At scale, anything that can happen will happen a small but nonzero times.)
[01:25:23] <Obormot\Arcturus> "This was always true, but online/mainstream media and social networking, which turn over much seem have become increasingly misleading as to the state of the world by focusing on ‚Äòstories‚Äô and ‚Äòevents‚Äô rather than trends and averages ..." 
[01:25:32] <Obormot\Arcturus> That second clause seems like word salad
[01:25:43] <Obormot\Arcturus> *third clause I guess
[01:26:05] <mst> "which turn over much seem have" doesn't seem like it went to plan as a sentence fragment, no
[01:28:39] <Obormot\Arcturus> Kinda has that "has anyone even gone so far as" feel
[01:29:17] <pompolic> maybe one day we'll learn what OP meant by that
[01:29:39] <saturn2> which turn over much seem have become to do what more like?
[01:29:52] <Obormot\Arcturus> pompolic: Actually we did learn what he meant by that
[01:30:04] <Obormot\Arcturus> Or wait, maybe that was the other thing
[01:35:04] <feepbot> <gwern> https://www.nytimes.com/2021/08/09/world/australia/lab-rats-tickle.html
[01:35:04] <feepbot> ‚ÄòThere Is a Right and Wrong Way to Tickle a Rat‚Äô - The New York Times (Researchers in Australia tickled the rodents every day for a month to see if it would improve their emotional well-being.)
[01:36:02] <gwern> Obormot\Arcturus: a clause got deleted, looks like, in the markdown - double space. probably was rogiinally 'turn over much more rapidly'
[01:37:14] <Betawolf> 'seem to have become' surely?
[01:41:29] <pompolic> Obormot\Arcturus: huh? and was the other thing maybe "accidentally 40 megs of files", or however that one went?
[01:41:43] <pompolic> there's a third one that's video game themed that I'm blanking on
[01:41:57] <Obormot\Arcturus> The video game one is what I am thinking of
[01:42:08] <pompolic> nice
[01:42:32] <pompolic> you know the joke that goes like, people in [setting] saying numbers to each other and laughing
[01:42:41] <pompolic> and the narrator giving them a new number and them laughing harder
[01:42:42] *** Quits: two2thehead (~AVX0@124.195.207.172) (Quit: Leaving)
[01:43:08] <gwern> mathematician joke
[01:43:21] *** tomzx_ is now known as tomzx
[01:44:43] <pompolic> https://www.reddit.com/r/Jokes/comments/23mblf/a_man_walks_into_an_irish_pub/ found it
[01:44:46] <Robomot> A man walks into an Irish pub... : Jokes (113 votes, 15 comments. ...sits down at the bar and orders a pint. A man in the corner of the bar stands up and yells "42!" The whole bar bursts ‚Ä¶)
[01:44:56] <Jayson_Virissimo> https://www.youtube.com/watch?v=UY_zcHjARgc
[01:44:57] <Robomot> That's Numberwang! - YouTube (The numbers game that simply everyone? Is talking about. Yes! I do not own this content, ownership belongs to David Mitchell and Robert Webb.)
[01:46:13] <pompolic> i think this must be what being WinZip feels like from the inside
[01:50:23] <Jayson_Virissimo> ggreer: I haven't really watched any new anime in the last half-decade, but I might checkout out Planetes, which is on your list.
[01:50:47] <ggreer> didn't I rate that one kinda low?
[01:51:05] <Jayson_Virissimo> Is 7 low?
[01:51:50] <Jayson_Virissimo> I've seen most of the 8-10s you have there.
[01:52:28] <Obormot\Arcturus> Planetes was good
[01:52:31] <Obormot\Arcturus> Not amazing, but good
[01:57:17] * gwern listens and 'gory gory' really brings back the childhood memories. his parents used to sing that.
[01:58:46] <gwern> (we were also fond of https://www.youtube.com/watch?v=_VEE2spvSHo )
[01:58:47] <Robomot> C130 Rolling Down The Strip - Military Running Cadence - YouTube (Buy "Military Workout Remixed: Running Cadences of the U.S. Military Volume 1" on iTunes: http://georiot.co/3ccR)
[02:02:38] <Obormot\Arcturus> Ok so I tried playing that word game Scott linked
[02:02:53] <Obormot\Arcturus> And, like ... look at this list of words in one puzzle:
[02:03:12] <Obormot\Arcturus> https://dl.dropboxusercontent.com/s/jixrucp1iyrxbjy/Screen%20Shot%202021-08-09%20at%205.33.00%20PM.png?dl=0
[02:03:13] <Robomot> image/png (677x369; 123 KB)
[02:03:24] <Obormot\Arcturus> "ure"? Really?
[02:03:32] <Obormot\Arcturus> zea? nome? ern?
[02:03:56] <Obormot\Arcturus> ean? ani? lere? nie?
[02:03:58] <Obormot\Arcturus> I mean, come on
[02:04:12] <kuudes> ure seems to be a surname
[02:04:18] <gwern> '(obsolete) To use; to exercise; to inure; to accustom by practice. The French soldiers [...] from their youth have been practiced and ured in feats of arms. "‚Äù Sir T. More.' is that legit
[02:04:25] <kuudes> oh https://en.wiktionary.org/wiki/ure
[02:04:26] <Robomot> ure - Wiktionary
[02:04:48] <Obormot\Arcturus> https://www.lexic.us/definition-of/ane this is just straight up a Scottish word
[02:04:48] <Robomot> Ane: Definition with Ane Pictures and Photos (Definition of Ane with photos and pictures, translations, sample usage, and additional links for more information.)
[02:04:49] <Betawolf> 'any melody determined by inviolable rules' okay
[02:04:55] <gwern> 'synonym of aurochs'?!
[02:04:55] <kuudes> https://en.wiktionary.org/wiki/zea
[02:04:56] <Robomot> zea - Wiktionary
[02:05:13] <Obormot\Arcturus> "zea" is also not English
[02:05:31] <Obormot\Arcturus> I can't stand it when puzzle / game designers pull shit like this
[02:06:03] <Obormot\Arcturus> "Har har har, you didn't spot the Scottish word in this English word game lol"
[02:06:22] <kuudes> haa https://en.wikipedia.org/wiki/Zea_(plant)
[02:06:23] <Robomot> Zea (plant) - Wikipedia (Zea is a genus of flowering plants in the grass family. The best-known species is Z. mays (variously called maize, corn, or Indian corn), one of the most important crops for human societies throughout much of the world. Several wild species are commonly known as teosintes and are native to Mesoamerica. ‚Ä¶)
[02:06:42] <Obormot\Arcturus> Or this kind oof shit: https://www.lexic.us/definition-of/nie
[02:06:42] <Robomot> Nie: Definition with Nie Pictures and Photos (Definition of Nie with photos and pictures, translations, sample usage, and additional links for more information.)
[02:06:46] <kuudes> https://en.wikipedia.org/wiki/Bay_of_Zea
[02:06:46] <Robomot> Bay of Zea - Wikipedia (Coordinates: .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}37¬∞56‚Ä≤15‚Ä≥N 23¬∞38‚Ä≤55‚Ä≥EÔªø / Ôªø37.93750¬∞N 23.64861¬∞EÔªø / 37.93750; 23.64861 ‚Ä¶)
[02:06:54] <Obormot\Arcturus> (These are defintions they link to, note!)
[02:07:02] <Betawolf> I suppose if you can be inured then you can be ured.
[02:07:02] <kuudes> https://en.wikipedia.org/wiki/Zea_(film)
[02:07:03] <Obormot\Arcturus> "National Intelligence Estimate"???
[02:07:03] <Robomot> Zea (film) - Wikipedia (Zea is a Canadian short film, directed by Andr√© Leduc and Jean-Jacques Leduc and released in 1981.[1] Set to a recording of the Ralph Vaughan Williams composition "Fantasia on a Theme by Thomas Tallis", the five-minute film presents a close-up depiction of a kernel of corn slowly heating up in oil until it bursts into popcorn.[2] ‚Ä¶)
[02:07:28] <Obormot\Arcturus> https://www.lexic.us/definition-of/nome this is just an obscure proper name
[02:07:28] <Robomot> Nome: Definition with Nome Pictures and Photos (Definition of Nome with photos and pictures, translations, sample usage, and additional links for more information.)
[02:07:45] <Betawolf> Obormot\Arcturus: there's another meaning, I gave it above
[02:08:01] <Betawolf> really obscure, but english.
[02:08:04] <gwern> Obormot\Arcturus: 'Nome' isn't obscure. it's like the only place in alaskha anyone can name aside from Anchorage. Balto!
[02:08:08] <kuudes> well, ssc is *very* high on verbal intelligence
[02:08:21] <kuudes> so to him, there is need for cogitation, in verbal side then
[02:08:26] <pompolic> >I can't stand it when puzzle / game designers pull shit like this
[02:08:32] <Betawolf> (also flagged '[Obs.]' in my dictionary)
[02:08:32] <Obormot\Arcturus> gwern: Huh? Juneau is obviously the only place in Alaska anyone can name aside from Anchorage
[02:08:33] <pompolic> guessing the teacher's password is a disease
[02:08:45] <gwern> as scoots goes, 'nae' is absolute easy mode
[02:08:51] <Obormot\Arcturus> Who the hell has ever heard of Nome??
[02:09:06] <gwern> there's a statute in central park to saving nome!
[02:09:13] <Obormot\Arcturus> Never heard of it
[02:09:35] <pompolic> i side-eyed crossword people because i thought they confused lexical knowledge with mental acumen
[02:09:47] <gwern> you've never heard of Balto? you've never walked by the balto statute? hell, even I've been there. it's surrounded by tourists, you can't miss it
[02:10:00] <gwern> there's a whole Disney movie about it
[02:10:28] <Betawolf> Oh, there's even another one "A province or political division, as of modern Greece or ancient Egypt; a nomarchy." which frankly I don't understand.
[02:10:48] <gwern> https://en.wikipedia.org/wiki/Balto_(film) sorry, (what would become) Dreamworks, not Disney
[02:10:48] <Robomot> Balto (film) - Wikipedia (Balto is a 1995 American live-action/animated adventure film directed by Simon Wells, produced by Amblin Entertainment and distributed by Universal Pictures.[4] The film is loosely based on a true story about the dog of the same name who helped save children infected with diphtheria in the 1925 serum run to Nome. ‚Ä¶)
[02:11:20] <kuudes> ssc just has some ~100k vocabulary and his brain constantly tries to fit words to anything and in his head runs a stream of words that is even actually coherent
[02:11:26] <kuudes> unlike on us mere mortals :)
[02:12:25] <gwern> (nome incidentally is apparently also where everyone in alaska who can afford it goes to give birth)
[02:12:49] <Betawolf> they don't count, they are nomen
[02:13:11] <rmmh> just another cog in the nomen
[02:14:11] <Betawolf> Much better advisors than the ones from yemen.
[02:14:25] <Obormot\Arcturus> gwern: I have no idea what the heck you're talking about with this statue. What is "Balto"?
[02:14:27] <pompolic> kuudes: maybe it's like, he runs a sharpen filter on his thought-stream and out falls an SSC post
[02:14:46] <pompolic> and he just tunes the filter parameters every time he needs a new one
[02:15:07] <Obormot\Arcturus> ... it's about a dog?
[02:15:18] <kuudes> pompolic, about so, yes
[02:15:22] <Obormot\Arcturus> https://en.wikipedia.org/wiki/Balto ... wow. I've never heard anything about this before
[02:15:23] <Robomot> Balto - Wikipedia (Balto (1919 ‚Äì March 14, 1933) was a Siberian Husky and sled dog belonging to musher and breeder Leonhard Seppala.[ ‚Ä¶)
[02:15:37] <kuudes> I mean, he likely feels discomfort if he can't output, to blogs or to speech etc
[02:15:43] <pompolic> the predictive processing model of SSC
[02:16:41] <pompolic> the other day i learned that apparently that is a psychological issue at the extreme case
[02:16:46] <pompolic> (pressure of speech)
[02:16:55] <Obormot\Arcturus> "This might just be me, but I'm not sure: of all things, what puts me slightly off is the choice of font. Namely, serifed. It gives the whole text a somewhat dated feel. However, the subject matter is technical.
[02:16:55] <Obormot\Arcturus> It's like if The New Yorker published a piece about how to use lambda functions in Java."
[02:17:01] <gwern> pompolic: graphomania?
[02:17:06] <Obormot\Arcturus> Some people have the wildest ideas about typography...
[02:17:21] <Obormot\Arcturus> (This comment is about gwern.net obviously)
[02:17:40] <Betawolf> oh no, the serifs will get stuck in the gears and break the internet
[02:17:43] <gwern> "we must use modern progressive sans-serif fonts, and abandon the dead hand of tradition!" --some guy in 1910, probably
[02:19:20] <gwern> not that I don't get where he's coming from. it's the same as doing indents vs no-indent+empty-line. maybe it's not typographically better, but it's deeply engrained at this point that stuff online has no indents and uses blank lines for separation, and native online stuff uses sans rather than serif
[02:20:29] <Obormot\Arcturus> ">     ETA: Although, just to tweak your priors, you may want to try _The Disordered Cosmos: A Journey Into Dark Matter, Spacetime, and Dreams Deferred_, by a black female physicist named Chanda Prescod-Weinstein. (No, I haven't read it myself, nor do I plan to any time soon...)
[02:20:29] <Obormot\Arcturus> I haven't read it and don't plan to, but Prescod-Weinstein is generally regarded by physicists as being both (a) crazy, and (b) a person to fear, since she (?) is an uber-woke activist extraordinaire. Judging from snippets of her (?) talks that I've seen shared, it might contain claims on how the death of George Floyd relates to the structure of black holes, or how understanding the black female non-binary perspective is
[02:20:29] <Obormot\Arcturus>  critical to a proper understanding of relativity, or similar. Just saying - don't read it with the objective of understanding something new about science (other than, perhaps, how someone seemingly insane can avoid getting drummed out of the field because everyone else is too terrified to point out that the stuff being said is, in fact, insane). "
[02:21:15] <gwern> I like how even the title warns you she's woke and mashing in activism somewhere it doesn't belong
[02:21:23] <Obormot\Arcturus> "Here is the cover of the book. Something tells me it is not so much about the science, perhaps.
[02:21:24] <Obormot\Arcturus> ETA: I am not saying this to be nasty and racist. If the book was just called The Disordered Cosmos: Dark Matter and Spacetime, and it had a picture of a galaxy on the cover instead of a picture of (I presume) the author, I would probably buy it. I don't care about the race, religion, or ethnicity of science authors, I just care if they know science and can explain it." https://img.thriftbooks.com/api/images/i/l/2066FBF18B66B2
[02:21:24] <Obormot\Arcturus> EC3229D5D25A20BA71E9B08AA6.jpg
[02:21:24] <Robomot> 404 - File or directory not found.
[02:21:40] <gwern> ('dreams deferred' is the dogwhistle there)
[02:21:52] <Obormot\Arcturus> Yep
[02:21:59] <Obormot\Arcturus> It's even italicized on the cover!
[02:22:22] <Obormot\Arcturus> "Prescod-Weinstein was born in El Sereno in East Los Angeles, California, and went to school in the Los Angeles Unified School District.[1][2] She is of Barbadian descent on her mother's side and Russian-Jewish and Ukrainian-Jewish descent on her father's side.[3]" ... lol
[02:22:33] <Obormot\Arcturus> Russian Jews, man
[02:22:57] <gwern> also continuing the rule that elite blacks tend to be either immigrants or biracial or both
[02:23:24] <Obormot\Arcturus> "In 2016, she became the Principal Investigator on a $100,522 FQXI grant to study ‚ÄúEpistemological Schemata of Astro | Physics: A Reconstruction of Observers‚Äù seeking to answer questions regarding how to re-frame who is an "observer", to acknowledge those existing outside of the European Enlightenment framework, and how that might change knowledge production in science.[14]" ... wow
[02:23:47] <Obormot\Arcturus> "Prescod-Weinstein received the 2021 Edward A. Bouchet Award from the American Physical Society, in recognition "For contributions to theoretical cosmology and particle physics, ranging from axion physics to models of inflation to alternative models of dark energy, for tireless efforts in increasing inclusivity in physics, and for co-creating the Particles for Justice movement."[22]" ... PARTICLES FOR JUSTICE
[02:23:50] <Betawolf> what's FQXI?
[02:24:03] <Obormot\Arcturus> I mean, it makes sense
[02:24:07] <Betawolf> ah https://en.wikipedia.org/wiki/Foundational_Questions_Institute
[02:24:08] <Robomot> Foundational Questions Institute - Wikipedia (The Foundational Questions Institute, styled FQXi, is an organization that provides grants to "catalyze, support, and disseminate research on questions at the foundations of physics and cosmology."[1] It was founded in 2005 by cosmologists Max Tegmark and Anthony Aguirre,[2] who hold the positions of Scientific Directors. ‚Ä¶)
[02:24:11] <Obormot\Arcturus> If particles can suffer, why can't they be for justice
[02:24:46] <Obormot\Arcturus> "Prescod-Weinstein is queer and agender.[2]" ... of course
[02:24:46] <Betawolf> "The $6.2 million seed funding was donated by the John Templeton Foundation, whose goal is to reconcile science and religion. "
[02:25:11] <gwern> yeah, the templeton foundation sure does fund a lot of BS. good source for money, if you have no shame
[02:25:59] <Betawolf> Aaronson is a member, but also David Chalmers.
[02:27:16] <gwern> I didn't say they only fund BS, but they do fund a lot of BS. when they do good, it's more like in giving soft funds to hard STEM types to go off-reservation, as it were, and enjoy some freedom
[02:32:16] <feepbot> <gwern> https://aeon.co/essays/not-all-early-human-societies-were-small-scale-egalitarian-bands
[02:32:17] <feepbot> Not all early human societies were small-scale egalitarian bands | Aeon Essays (A grand research project created our origin myth that early human societies were all egalitarian, mobile and small-scale)
[02:37:09] <Betawolf> #notallearlyhumansocieties
[02:42:09] <feepbot> <gwern> '"...For this reason we have already contacted all the stakeholders involved to avoid more of this in the future..."' https://statmodeling.stat.columbia.edu/2021/08/09/controversy-over-facial-recognition-technology-can-expose-political-orientation-from-naturalistic-facial-images/
[02:42:10] <feepbot> Controversy over ‚ÄúFacial recognition technology can expose political orientation from naturalistic facial images‚Äù ¬´  Statistical Modeling, Causal Inference, and Social Science
[02:42:54] <ggreer> Jayson_Virissimo: ah. I think it was 8ish until the ending, which is... meh
[02:44:10] <gwern> it didn't quite seem to know where it was going
[02:44:55] <rmmh> calling people that do large-scale fish farming "hunter-gatherers" is a bit iffy
[02:49:56] <feepbot> <gwern> 'kind of amazing with TRC that since the TPU is a TPUVM, and the TPU itself is free, I can run stuff without any running costs whatsoever' /shawwwn intensifies
[02:50:25] <pompolic> gwern: graphomania, or, compulsive IRC
[02:55:26] <feepbot> <gwern> 'Georgia has been showing up to the Planning Commission hearings at 1pm, every Thursday, for a decade. What were you doing? Working? Hahaha, what a chump! Georgia Schuttish breaks into construction sites and takes pictures of the construction going on. Then she looks up the permits that were approved for that location and spends hours figuring out what totally minor thing
[02:55:26] <feepbot> the builder has done wrong. Then she writes up a report, sends it to Planning and the Department of Building Inspection (DBI), and shows up at Planning Commission to present her findings. More often than not, the Planning Commission then orders the Planning Department and DBI to investigate, levy a fine, and often halt construction.'
[02:55:26] <feepbot> https://sbuss.substack.com/p/politics-for-software-engineers-part-3
[02:55:27] <feepbot> Politics for Software Engineers, Part 3 - by Steven Buss - sbuss.dev (Please don't make me vote on everything)
[02:57:37] <rmmh> neat! https://www.npr.org/2021/08/09/1026091196/this-sweet-white-flower-is-actually-a-sneaky-carnivore-scientists-discovered
[02:57:38] <Robomot> 1st Carnivorous Plant Identified In 20 Years Grows Near Vancouver : NPR (Carnivorous plants are rare, but now botanists say they've found one that's long been overlooked. It lives just outside Vancouver, British Columbia, and in other parts of the Pacific Northwest.)
[02:58:51] <rmmh> "Graham's team was doing an unrelated project on plant genetics and noticed that the western false asphodel had a genetic deletion that's sometimes seen in carnivorous plants. The researchers started to think about the fact that this flower grew in the kind of environment that's home to various other insect-eating plants."
[02:59:02] *** Quits: ua_ (~ua@public-gprs220723.centertel.pl) (Ping timeout: 258 seconds)
[02:59:26] <rmmh> discovering a phenotype serendipitously based on clues in the genotype-- that has to be uncommon!
[03:03:47] <pompolic> reverse genetic engineering
[03:08:48] <feepbot> <gwern> https://www.quantamagazine.org/animals-can-count-and-use-zero-how-far-does-their-number-sense-go-20210809/
[03:09:14] <feepbot> Quanta Magazine (Crows recently demonstrated an understanding of the concept of zero. It‚Äôs only the latest evidence of animals‚Äô talents for numerical abstraction ‚Äî which may still differ from our own grasp of numbers.)
[03:13:48] <feepbot> <gwern> https://twitter.com/Kettukaa/status/1424789911522189326
[03:14:03] <|dbotdan> Ket (@Kettukaa, 2021-08-09 17:49): ‚ÄòWatch out when entering the woods tonight! A frightening fox cryptid has been spotted stalking unsuspecting hikers with its posse.ü¶ä | I'm having a lot of fun generating monsters from VQGAN+CLIP. The quirks of the generation process compliment what makes spooky thing "spooky".‚Äô Images: https://nitter.database.red/pic/media%2FE8XeV-KXIAQ-Rbr.png%3Fname%3Dorig
[03:14:03] <|dbotdan> (description: a dinosaur with a fox head; confidence: 0.26)
[03:16:12] <kuudes> hmm. in france apparently some people are trying to get covid so that they can get a confirmed covid passport without vaccination
[03:17:03] <kuudes> would it make sense to purposefully self-infecting covid after vaccination, to make sure one gets full immunity? but there is reinfection with different variants?
[03:18:13] *** Quits: luna-is-here (~quassel@2a02:908:f761:2a3c::a) (Ping timeout: 268 seconds)
[03:18:17] <ggreer> when did used cars get expensive? I sold my miata for like... $4500 back in 2016. now that same car would be worth close to $10k
[03:18:31] *** Joins: luna-is-here (~quassel@ip-95-223-59-176.hsi16.unitymediagroup.de)
[03:18:42] <kuudes> huh
[03:19:00] <kuudes> are there some import tariffs that cause that or something?
[03:19:34] *** CoJaBo_ is now known as CoJaBo
[03:19:35] <ggreer> my guess is high demand because the new miatas are pretty heavy and have more electronics
[03:19:36] <Gurkenglas> if we're going there, could we find a variant with r less than 1 which does little damage and put it on supermarket shelves?
[03:19:46] <kuudes> how nice https://www.reuters.com/world/africa/guinea-records-west-africas-first-marburg-virus-death-who-says-2021-08-09/
[03:19:47] <Robomot> Guinea records West Africa's first Marburg virus death, WHO says | Reuters (Health authorities in Guinea have confirmed one death from Marburg virus, a highly infectious hemorrhagic fever similar to Ebola, the World Health Organization said on Monday.)
[03:19:54] <Gurkenglas> ...in bottles, clearly labelled.
[03:20:12] <ggreer> also people probably want cars now because they don't want to use public transportation
[03:20:42] <ggreer> but a 2 door convertible is a leisure car
[03:21:24] <Gurkenglas> or are we not good enough at loss of function research to not accidentally do gain of function research
[03:22:08] <kuudes> hmm, I wonder if the new ebola and marburg virus spreads are causally related to global warming
[03:22:53] *** Quits: luna-is-here (~quassel@ip-95-223-59-176.hsi16.unitymediagroup.de) (Read error: Connection reset by peer)
[03:22:54] <kuudes> ah, it is yet another bat disease
[03:23:02] *** Joins: luna-is-here (~quassel@2a02:908:f761:2a3c::a)
[03:23:07] <kuudes> why does batman hate us this much...
[03:23:22] <Betawolf> we killed his parents
[03:23:22] <CoJaBo> On Today's Episode of Can We Enable The Squirtgun Emoji Yet‚Ñ¢: Answer, no, we apparently cannot.
[03:23:35] <ggreer> CoJaBo: ?
[03:24:18] <CoJaBo> Apparently, people are really attached to using squirtgun emoji in mostly-business-related chat apps
[03:24:38] <CoJaBo> We've disable that one for years, because of it's unfortunate compatibility issues
[03:24:41] <Betawolf> can't they just use a regular gun emoji?
[03:25:11] <CoJaBo> That's the problem; someone sends squirtgun, it shows up as a revolver on the other end lol
[03:25:31] <kuudes> hmm. now that hackers got the passport data and police data of belarus, could we try that "lets own the country with money" approach I pondered some years ago re russia?
[03:25:51] <Betawolf> you want to buy belarus?
[03:26:26] <kuudes> yes
[03:26:55] <kuudes> gdp per capita there is 526 $/mo
[03:27:11] <CoJaBo> Our announcement that we were closing our datacenter in israel said that we planned to "sell israel", because apparently people suck at writing literally anything.
[03:27:18] <kuudes> so some 450 e/mo
[03:27:40] <kuudes> so 15 e/day per person
[03:27:59] <Betawolf> but surely you wouldn't have access to that money, you'd be stuck with what you can extract through taxes
[03:28:17] <Betawolf> I guess it helps with raising capital from investors
[03:28:28] <Betawolf> you can sell ad space
[03:28:36] <kuudes> total gdp there is 57 bn$/y, ~0.15 bn$/day
[03:28:47] <kuudes> well, iraq war cost 1bn$/day
[03:28:53] <kuudes> so this would be 6 times cheaper
[03:29:20] <kuudes> the plan is simple:
[03:29:33] <kuudes> start paying everyone's salaries through facebook or gmail
[03:29:45] <Betawolf> I hate it already
[03:29:49] <kuudes> and then just switch the leadership
[03:30:07] <kuudes> ie "please continue business as usual, but anyone who is 2 steps within lukashenka is fired"
[03:30:28] <kuudes> and there is already the duly elected actual president to take over
[03:32:15] <Betawolf> It's worth a try, and you might be able to convince Elon Musk...
[03:34:13] <kuudes> haa
[03:34:15] <pompolic> are guns not safe for work
[03:34:17] <kuudes> yes, he could do it
[03:34:26] <pompolic> wait shit don't answer that
[03:34:33] <pompolic> it would be a category debate
[03:34:46] <kuudes> though lukashenka has used a nuclear plant to mine cryptocoins
[03:34:56] <kuudes> so is musk pro or against that
[03:35:14] <Betawolf> does it matter? He could steal it. 
[03:35:43] <Betawolf> I'm sure he can find a use for a nuclear plant if he doesn't want to just make more dogecoin.
[03:36:30] <kuudes> true, iirc he does not have one yet?
[03:37:01] <kuudes> then he could get a metal mask and rename the country to latveria
[03:37:22] <kuudes> and order people to be happy
[03:37:25] *** Quits: srhm (~ascii@user/srhm) (Quit: Konversation terminated!)
[03:37:50] *** Joins: srhm (~ascii@user/srhm)
[03:38:35] *** Quits: Dyo (~Dyo@host.sedf.de) (Changing host)
[03:38:35] *** Joins: Dyo (~Dyo@user/dyo)
[03:42:23] <feepbot> <gwern> https://arxiv.org/abs/2012.13169
[03:42:23] <feepbot> [2012.13169] SCC: an efficient deep reinforcement learning agent mastering the game of StarCraft II (AlphaStar, the AI that reaches GrandMaster level in StarCraft II, is a)
[03:42:27] *** Quits: Gurkenglas (~Gurkengla@dslb-002-203-144-156.002.203.pools.vodafone-ip.de) (Ping timeout: 276 seconds)
[03:44:00] <gwern> Betawolf: batman hates the killer of his parents; his parents were killed because of criminals; criminals kill because society makes them bad; society is made of all people; we are people; therefore, Batman hates us
[03:47:14] <Obormot\Arcturus> "As a lawyer, I assure you that the vast majority of attorneys are completely innumerate. Anything involving math more complicated than elementary school gets shunted off to the experts, at least in cases complicated enough to have them. I have been on calls where lawyers struggled to do basic multiplication/division without using a calculator (talking about two, two-digit numbers, not like long division or something).
[03:47:14] <Obormot\Arcturus> I hear it's even worse over in government compared to the large law firm private practice where I am."
[03:47:36] <Obormot\Arcturus> Juxtaposed with:
[03:47:38] <Obormot\Arcturus> "I believe the common one among politicians is a law degree, not an MBA."
[03:47:54] <Obormot\Arcturus> "*Cackles in patent attorney*
[03:47:54] <Obormot\Arcturus> But yes, very true generally"
[03:48:33] <Obormot\Arcturus> "I find that baffling. The law is an elite profession. To practice it, you have to have not one but two degrees, and apparently admission is really competitive. I would expect the typical attorney to have studied math all through high school, probably ending with calculus or (for the very humanities oriented) pre-calc. And somehow they managed to forget all that and fall back to grade six or maybe eight arithmetic?" "Haven't
[03:48:33] <Obormot\Arcturus>  you noticed that most elites come up through either the quantitative route or the verbal route? The lawyers almost invariably come up through the verbal route. Yes, these people are smart enough to have passed various math classes, probably algebra and geometry, but I think they do it through pure memorization, not truly understanding the subjects."
[03:48:44] <Obormot\Arcturus> "I am a tax accountant, and it has definitely been my experience that lawyers are innumerate, even tax lawyers. Some of them were even accountants before they were lawyers, but I suspect not very good accountants (that's why they become lawyers). Tax lawyers are very good at finding the exact law and cases to prove some point, but they are terrible in understanding the big picture of how this particular law affects the bottom
[03:48:44] <Obormot\Arcturus>  line of the client. You can definitely be a very good lawyer without understanding numbers very well. Two of my siblings are lawyers, neither are good with numbers."
[03:48:51] <Obormot\Arcturus> etc., etc.
[03:48:55] <kuudes> I like that currently math gives about the most score for deciding who gets into university here, regardless of the faculty...
[03:49:01] <Betawolf> Can confirm, my solicitor required a calculator to do simple subtractions. Unsurprisingly the finances in that sale were all fucked the first time she sent them through.
[03:49:18] <kuudes> but is she an attorney?
[03:49:27] <Obormot\Arcturus> She's a solicitor, he said
[03:49:41] <kuudes> hmm. do they need to have master's degree?
[03:49:46] <kuudes> I don't know english enough
[03:49:47] <Obormot\Arcturus> https://en.wikipedia.org/wiki/Solicitor
[03:49:47] <Robomot> Solicitor - Wikipedia (A solicitor is a legal practitioner who traditionally deals with most of the legal matters in some jurisdictions. A person must have legally-defined qualifications, which vary from one jurisdiction to another, to be described as a solicitor and enabled to practise there as such. ‚Ä¶)
[03:50:09] <kuudes> lawyer can be anyone, attorney needs credentials and good reputation
[03:50:27] <Obormot\Arcturus> Yeah that's not how those words work in English
[03:50:50] <Obormot\Arcturus> "lawyer" and "attorney" are synonyms
[03:51:29] <Obormot\Arcturus> "I guess the conflict I am running into here is that on the one hand, the profession of the law presents itself as an elite profession. There are highly trained, carefully selected, often highly paid people who can totally fuck us peasants over if they are overzealous or incompetent. But on the other hand, the elite is supposed to be actually capable, both narrowly and broadly. They are supposed to be literate and numerate to
[03:51:29] <Obormot\Arcturus>  a rather high standard. Any particular member of of it may be better at words than numbers or vice versa, but they really should be pretty handy with both. If they often are not then a) there is a lot more fakery around than I thought, and b) we should be holding this profession in rather less esteem than we do, since they don't uniformly measure up to elite standards." ... by god, I think he's got it
[03:51:30] <kuudes> is the profession such that it needs master's degree in law?
[03:51:46] <Obormot\Arcturus> ">     There's no math portion of the LSAT.
[03:51:46] <Obormot\Arcturus> And soon there won't be a Logic Games section either! It'll only be Reading Comprehension and Logical Reasoning (i.e. 100% verbal reasoning type questions). " ... lol :\
[03:51:58] <Betawolf> solicitors need a degree and then an additional qualification 
[03:52:03] <Obormot\Arcturus> kuudes: It needs a J.D.
[03:52:05] <Obormot\Arcturus> https://en.wikipedia.org/wiki/Juris_Doctor
[03:52:06] <Robomot> Juris Doctor - Wikipedia (The Juris Doctor degree (J.D. or JD), also known as Doctor of Law or Doctor of Jurisprudence (J.D., JD, D.Jur., or DJur), is a graduate-entry professional degree in law[1] and one of several Doctor of Law degrees. In Australia, Canada, the United States, and some other common law countries, the Juris Doctor is earned by completing law school. ‚Ä¶)
[03:54:46] <kuudes> ok, so it is at least 6 years of education after secondary education, I suppose - comparable to medical doctor if I understand correclty
[03:55:07] <gwern> hm. I wonder if there would be an opportunity for law schools to additionally select on quantitative scores
[03:55:20] <adiabatic> kuudes: in the US, you get a JD (three years) after your bachelor's (four years)
[03:55:31] <kuudes> thanks
[03:55:34] <gwern> if the LSAT is now 100% verbal, then that's pretty redundant and neglects the undoubted benefit to careers of quanitative ability
[03:55:41] <adiabatic> medical doctors here in the US are however many years med school is after you get your batchelor's
[03:56:20] <gwern> (ie if you take additional scores/info on math, then that gets you some additional information on IQ, and also math ability is doubtless useful itself downstream if you are optimizing for alumni careers, earnings, donations etc)
[03:59:18] <Obormot\Arcturus> "Much of that training exists merely to reduce competition. My view of the legal profession is that being a lawyer should be an undergraduate program rather than a master's degree." "As an attorney I agree it could be an undergraduate program. Realistically I think that would never happen, but maybe eventually 3 year programs can be converted into 2 and eventually 1 year programs which would be much more appropriate. "
[03:59:35] <Obormot\Arcturus> "I'd estimate a supermajority of the lawyers I've had similar discussions with agree that law could (and should) be an undergraduate program, and most everyone agrees that it could be two years (instead of three) without much loss. At the same time, virtually no one expects that to happen.
[03:59:35] <Obormot\Arcturus> I think that sort of thing explains a lot about how cost disease happens. "
[03:59:45] <Obormot\Arcturus> "Quite a while back, Richard Posner suggested that the third year of law school should be optional."
[04:04:35] <ggreer> it seems like employers still want to know how good potential hires are, so there must be some demand for a standardized test. but it has to be a standardized test without getting smeared as "omg racist". not sure how to give employers what they actually want while still giving them what they claim they want
[04:06:08] <Betawolf> give people standardised tests and teach the ones that pass how to lie about their ethnicity
[04:11:09] <feepbot> <gwern> https://apnews.com/article/business-health-coronavirus-pandemic-4c614d89c69b4afd3504a4d92dd574f9
[04:11:10] <feepbot> U.S. job openings hit a record 10.1 million in June (WASHINGTON (AP) ‚Äî U.S. employers posted a record 10.1 million job openings in June, another sign that the job market and economy are bouncing back briskly from last year's coronavirus shutdowns. Job openings rose from 9.5 million in May, the L [snip])
[04:16:10] <feepbot> <gwern> https://twitter.com/vetyyr/status/1424781096076693513 client/UI designer
[04:16:16] <|dbotdan> Syd Mills (@vetyyr, 2021-08-09 17:14): ‚ÄòClient sketch vs. final illustration. | üåüAnother one bc I love them! This client saw my other tweet and made their own doodle for this commission :)‚Äô Images: https://nitter.mailstation.de/pic/media%2FE8XW2pxWUAQKszy.jpg%3Fname%3Dorig (description: diagram; confidence: 0.99) | https://nitter.mailstation.de/pic/media%2FE8XW2pwWYAkDMdu.jpg%3Fname%3Dorig
[04:19:23] <Obormot\Arcturus> This artist is bad at facial expressions
[04:23:34] <Robomot> [Less Wrong [frontpage]] Book Review: The Beginning of Infinity by Sam Enright - https://www.greaterwrong.com/posts/FyRyECG7YxvAF2QTF/book-review-the-beginning-of-infinity
[04:23:46] <ggreer> also how do you twist your ribcage like that
[04:25:53] <ggreer> I'm guessing the artist is tracing some parts. it's a weird combination of fucked up and almost perfect
[04:28:15] <shawwwn> Impressive. https://www.youtube.com/watch?v=24m6MiF6MNc&list=PLRZ8JWrxXGYqY3ZWGPaG1ETMWWOnzKyqV&index=6&ab_channel=PianoDeuss
[04:28:26] <feepbot> Bad Apple!! but it's actually sad and emotional - YouTube
[04:29:08] <shawwwn> I'm glad I sat through it. I almost dismissed it after the first bit
[04:29:22] <gwern> shawwwn: https://www.youtube.com/watch?v=faE0AW2Yfgg I like this sad version of bad apple
[04:29:23] <Robomot> Halozy - 8Áï™ÁõÆ„ÅÆË∑ØÂú∞_emaru - YouTube (Epic...  It catch my heart. ;pInformations: Album : "864" from HalozyCompany: HalozyAuthor : HalozyLink to shop: http://d-stage.com/shop/detail.php?seq=67591...)
[04:30:45] <shawwwn> I... do you remember what bad apple sounds like?
[04:31:14] <shawwwn> ah
[04:31:18] <shawwwn> took long enough
[04:31:26] <shawwwn> what's this from?
[04:32:19] <gwern> as it says, one of halozy's albums
[04:32:39] <gwern> (yes yes I know the wallpaper is kancolle, but the album is still touhou)
[04:33:46] <shawwwn> no idea what any of that is but ok. song's alright
[04:34:20] <shawwwn> paino sad whips anime sad's ass imo.
[04:36:43] *** Quits: Jayson_Virissimo (~Jayson_Vi@ip98-165-142-10.ph.ph.cox.net) (Quit: Textual IRC Client: www.textualapp.com)
[04:41:23] <shawwwn> "Join us for a live demo of the next generation of OpenAI Codex, our AI that writes code.
[04:41:23] <shawwwn>  
[04:41:23] <shawwwn> 10am PT, Tues Aug 10th. twitch.tv/openai
[04:41:23] <shawwwn>  
[04:41:23] <shawwwn> OpenAI Codex translates natural language to code, as first seen in GitHub Copilot. As someone with early access to the OpenAI API, we think you‚Äôll enjoy the demo of what‚Äôs next."
[04:41:59] <shawwwn> maybe I'm getting old and ... old. I just don't find text to code that compelling 
[04:42:18] <shawwwn> nor OpenAI lately, for that matter
[04:43:05] <shawwwn> DeepMind: *breaks chess and Go, does protein folding*
[04:43:05] <shawwwn> OpenAI: we haz a jukebox
[04:43:41] <gwern> the jukebox is a lot more fun to play with than the chess/go agents (which DM didn't release anyway)
[04:44:00] <shawwwn> I'm still sad they didn't release those
[04:44:05] <shawwwn> everybody in this space sucks.
[04:44:36] <Robomot> [Less Wrong [frontpage]] Implicature Conflation by abramdemski - https://www.greaterwrong.com/posts/RuFy5Gzz6LviXqcW4/implicature-conflation
[04:44:41] <Robomot> [Less Wrong [frontpage]] Fermi Fingers by Optimization Process - https://www.greaterwrong.com/posts/mLhHdWXPxgrAwPKqd/fermi-fingers
[04:46:16] <Obormot\Arcturus> quanticle: Conversation I had with my mother today - Me: "Oh, I had some of that baked salmon with rice the other day"  Her, looking very skeptical: "Fish... with rice...?"  Me: "Yeah! It was very good"  Her: "Huh... it never occurred to me to do that..."
[04:49:31] * shawwwn finds it entirely unsurprising that Obormot's mom is also an idiot.
[04:49:54] <shawwwn> she's good at Katchi though https://www.youtube.com/watch?v=Ycg5oOSdpPQ&ab_channel=Ofenbach
[04:50:05] <feepbot> Ofenbach vs. Nick Waterhouse - Katchi (Official Video) - YouTube
[04:50:08] <pompolic> i keep realizing the ways in which i resemble my parents
[04:50:29] <pompolic> i don't fail so hard, but the pattern of idiocy is isomorphic
[04:50:54] <Obormot\Arcturus> Well, in my case it's like ... I grew up eating fish, and eating rice, but never, ever those two things together
[04:51:50] <Obormot\Arcturus> The rice in pan-Soviet cuisine is exclusively in the form of plov, plov comes from Central Asia (my grandmother learned to make it in, I think, Tajikistan), and who the hell eats fish in Tajikistan?
[04:51:55] <Obormot\Arcturus> It's literally landlocked
[04:52:01] <pompolic> live options vs knowing about live options i suppose
[04:52:10] <pompolic> i do that a lot
[04:53:35] <Robomot> [Less Wrong [frontpage]] Outline of Galef‚Äôs ‚ÄúScout Mindset‚Äù by Rob Bensinger - https://www.greaterwrong.com/posts/yFJ7vCjefBxnTchmG/outline-of-galef-s-scout-mindset
[04:54:08] <Obormot\Arcturus> https://en.wikipedia.org/wiki/Tajik_cuisine#Common_foods_and_dishes
[04:54:09] <Robomot> Tajik cuisine - Wikipedia [Common foods and dishes] (Tajik cuisine is a traditional cuisine of Tajikistan, and has much in common with Russian, Afghan, Iranian and Uzbek cuisines. Plov (pilaf) (Tajik: –ø–∞–ª–∞–≤, Uzbek: palov), also called osh (Tajik: –æ—à), is the national dish in Tajikistan, as in other countries in the region. Green tea is the national drink. ‚Ä¶)
[04:54:30] <Obormot\Arcturus> "Meals are almost always served with non, flatbread found throughout Central Asia. If a Tajik has food but not non, he will say he is out of food. If non is dropped on the ground, people will put it up on a high ledge for beggars or birds. Legend holds that one is not supposed to put non upside down because this will bring bad luck. The same holds true if anything is put on top of the non, unless it is another piece of
[04:54:31] <Obormot\Arcturus>  non.[1]"
[04:55:41] <Obormot\Arcturus> (and "non" just links to the article for naan, lol)
[04:56:01] *** Joins: ua_ (~ua@public-gprs232059.centertel.pl)
[04:59:49] <rmmh> –Ω–æ–Ω –Ω–æ–Ω –Ω–æ–Ω
[05:00:26] <pompolic> nature or nautre's non
[05:02:33] <shawwwn> lol. chess is kind of boring sometimes, but gothamchess's personality makes it exciting. https://www.youtube.com/watch?v=3GDtPtoHWxw&ab_channel=GothamChess
[05:02:37] <rmmh> Obormot\Arcturus: is standard russian plov closer to Turkish pilav than tajikistan palav? https://res.cloudinary.com/hksqkdlah/image/upload/CIO%20Web%20Articles/CI%20JF%2020/Plov/Plovmap
[05:02:37] <Robomot> image/png (479x180; 126 KB)
[05:05:35] <Obormot\Arcturus> rmmh: Uh good question I guess? I am not familiar enough with the variations to answer that, also three things: (a) it's not really "Russian" so much as it is "Soviet", (b) the plov I know best is the one my grandmother makes, and she makes it better than anyone else I've seen but also a bit differently, (c) there's probably a difference between "plov as made in generic-Soviet-cuisine eatery", "plov as made in authentic [
[05:05:35] <Obormot\Arcturus>  Tajik / Uzbek / whatever] restaurant", "plov as made by my grandmother who learned it from Tajiks in Dushanbe or wherever", "plov as made by some actual Tajiks in Dushanbe or wherever", etc.
[05:06:37] <Obormot\Arcturus> (e.g. my grandmother typically makes it with chicken, which is non-standard, though she's also done beef or lamb instead sometimes and imo that is not as good ... etc.)
[05:06:54] <rmmh> There is an old Turkish saying ‚ÄúThere are as much kinds of Plov as there cities in the Muslim world are.‚Äù
[05:06:59] <Obormot\Arcturus> Yep
[05:07:08] <SDr> *sigh*
[05:07:29] <rmmh> it would be fascinating to track the breakdowns in where different ingredients are used
[05:07:51] <Obormot\Arcturus> I was just about to say - I would venture a strong guess that each of the categories on this map are spectra, e.g. "palaw as made in Afghanistan" is not one thing but many things, varying by region or even village
[05:07:56] <SDr> hey #lesswrong  what's current next-best alternative to an iphone SE?  small form-factor, g suite (gmail/calendar), sms, incoming call filtering, and does not require >30 mins setup to do so?
[05:08:07] <SDr> and, yeah, _not apple_.
[05:09:11] <shawwwn> SDr: I quite like the iPhone 12
[05:09:19] <shawwwn> if you can find a non-apple variant, I suggest you get one
[05:09:31] <shawwwn> sounds like it can do all the things you want
[05:09:37] <Obormot\Arcturus> rmmh: What I would want to see is a systematic definition and mapping of plovspace, locating all distinctive recipes in all regions of all relevant countries in said space
[05:09:44] <SDr> shawwwn, non-apple variant? how do you mean?
[05:10:14] <shawwwn> well, it sounds like the iPhone 12 meets every single one of your criteria, except "apple." So if you can find a non-apple version, you should get one
[05:10:41] <SDr> yes, okay, very good. hence the question: what's a non-apple smartphone that can do that?
[05:10:58] <shawwwn> well, personally, I like the iPhone 12 for that
[05:11:17] <shawwwn> it seems to be able to do all that, and a few other neat things
[05:14:19] <ggreer> gotta wait for the pixel 6 to come out in a couple months
[05:16:10] <rmmh> Obormot\Arcturus: recipespace, a 256-dimensional vector of the most common possible ingredients
[05:19:55] <ggreer> https://i.redd.it/3cpe9ts9cag71.jpg
[05:19:56] <Robomot> image/jpeg (1078x1348; 196 KB)
[05:21:50] <shawwwn> gwern: arg, what's AJBrock's twitter?
[05:22:03] <shawwwn> It's on my tongue
[05:22:09] <gwern> ajmooch
[05:22:13] <shawwwn> the hell
[05:22:30] <shawwwn> is his name Brock, or am I just hallucinating as usual and you happened to decipher it?
[05:22:49] <shawwwn> ah, andy brock
[05:22:52] <shawwwn> okay good, I'm not that far gone at least.
[05:23:39] <shawwwn> oh, his github is ajbrock, twitter is ajmooch.
[05:23:54] <shawwwn> (context was https://twitter.com/theshawwn/status/1424896719557365764)
[05:23:59] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 00:53): ‚ÄòI'm a big fan of unofficial repros *as long as they're faithful reimplementations*. @ajmooch's BigGAN was extremely helpful for our work. https://github.com/ajbrock/BigGAN-PyTorch‚Äô
[05:24:30] <shawwwn> oh yes, and: thank you.
[05:26:17] <Obormot\Arcturus> rmmh: Can't just be ingredients; it's also prep methods, proportions, etc.
[05:27:01] <rmmh> How different are recipes with identical ingredients? 
[05:27:16] <rmmh> *stares at egg dishes grumpily*
[05:28:33] <rmmh> *glares at sugar*
[05:29:15] <shawwwn> starting to be a fan of using a bedside table as a workdesk
[05:29:47] <shawwwn> first, your ass is on your bed. As someone whose ass hurts because chairs, this is no small benefit
[05:30:03] <shawwwn> if you use an actual table, it's also comfortable
[05:30:25] <shawwwn> and since you're already next to it, you can just sit up whenever you feel like doing something.
[05:30:38] <shawwwn> my sole problem with it is that there's no back support. sadly this may be a dealbreaker
[05:30:57] <shawwwn> really I just want a chair that feels like a bed
[05:34:39] <Obormot\Arcturus> quanticle: Also. I bought some basmati rice to try it out... but did you know there's also such a thing as ... jasmati rice? https://en.wikipedia.org/wiki/Jasmati
[05:34:40] <Robomot> Jasmati - Wikipedia (Jasmati Rice is a conventionally bred, inbred line (variety) of long grain of rice whose name is derived from jasmine rice and basmati. Jasmati is neither "genetically engineered" (as in transgenic) nor is it an F1 hybrid. ‚Ä¶)
[05:34:55] <Obormot\Arcturus> "It appears to have been first created in the United States. To what degree Jasmati is derived from its etymological parent grains is unknown and disputed.[1] [2] The patent for Jasmati, registered in 1993 by the Texas-based corporation, RiceTec, had legal implications for Thai and Indian farmers who rely heavily on the exports of the parent crops, and caused controversy. Presently the debate seems to have cooled down. As the
[05:34:55] <Obormot\Arcturus>  degree to which Jasmati draws from Jasmine cannot be ascertained,[3] the informed consumer should be aware that it is therefore a different grain and may or may not be a complete substitute for either Jasmine or basmati. This suggests that Jasmati may have its own unique merits as a cooking ingredient. "
[05:35:17] <Obormot\Arcturus> > registered in 1993 by the Texas-based corporation, RiceTec
[05:35:53] <Obormot\Arcturus> BUT! Complicating this further is that there is *another*, unrelated variety of rice called ... Texmati
[05:35:56] <Obormot\Arcturus> (Also from Texas)
[05:36:50] <Obormot\Arcturus> https://en.wikipedia.org/wiki/RiceTec
[05:36:50] *** Quits: Robomot (~Robomot@user/robomot) (Remote host closed the connection)
[05:37:02] <feepbot> RiceTec Inc. is a private American company based and headquartered in Alvin, Texas, that develops and produces hybrid rice seed for the American and various international markets.
[05:37:02] <shawwwn> welp, looks like this is what I'm doing today https://twitter.com/theprincessxena/status/1424898501700472834
[05:37:05] <|dbotdan> Xe (@theprincessxena, 2021-08-10 01:00): ‚ÄòIt is ford improved shorthand for my notes on a self-study Gregg Shorthand course I found on YouTube: https://www.youtube.com/playlist?list=PLq6d0iWrpP0sd9Pzg3dnzjxqtk19W9c6q‚Äô
[05:37:06] *** Joins: Robomot (~Robomot@user/robomot)
[05:37:06] *** ChanServ sets mode: +v Robomot
[05:37:17] <Obormot\Arcturus> "RiceTec previously owned[2] the consumer brand name RiceSelect which markets Texmati brand rice in grocery stores throughout North America.[3] The company was founded in 1990 as a foreign for profit corporation and is owned by the Prince of Liechtenstein Foundation.[4][5] "
[05:38:36] <Obormot\Arcturus> They were involved in a whole patent controversy over Basmati rice!
[05:39:54] <Obormot\Arcturus> Actually they were involved in TWO parent controversies, over basmati and separately over jasmine rice
[05:40:12] <Obormot\Arcturus> In which they opposed the countries of India and Thailand respectively
[05:41:10] <Obormot\Arcturus> "Witoon Lianchamroon, coordinator of the Thai Network on Community Rights and Biodiversity (BIOTHAI), says jasmine rice is the next target in line after another of Asia's aromatic rices -- basmati rice from India and Pakistan -- that RiceTec is encroaching on.
[05:41:10] <Obormot\Arcturus> ''RiceTec is attacking basmati. Now it will be very easy to attack jasmine,'' Witoon said. ''The company can patent it like what they did to basmati. If they find the gene for jasmine aroma, they can claim a monopoly on it just for identifying it.'' "
[05:41:44] <Obormot\Arcturus> This fucking company is like some kind of cyberpunk villain
[05:42:32] <gwern> I thought gene patents for identification got struck down?
[05:43:06] * Obormot\Arcturus shrug
[05:45:31] <gwern> https://en.wikipedia.org/wiki/Biological_patents_in_the_United_States#Myriad_Genetics_case yeah, it sounds like you couldn't just patent the gene for jasmine aroma. I assume those lawsuits were pre-2013?
[05:45:32] <Robomot> Biological patents in the United States - Wikipedia [Myriad Genetics case] (As with all utility patents in the United States, a biological patent provides the patent holder with the right to exclude others from making, using, selling, or importing the claimed invention or discovery in biology for a limited period of time - for patents filed after 1998, 20 years from the filing date.[1] ‚Ä¶)
[05:45:44] <Obormot\Arcturus> Yes they were
[05:46:11] <gwern> yeah, then myriad is probably why ricetec didn't go on to do just that. it wouldn't be hard to find the gene
[05:51:12] <feepbot> <gwern> https://arye.substack.com/p/io-mio-wo
[05:51:13] <feepbot> ·ó∫IO MI–Ø–ØO–Ø WO–Ø‚ÖÉ·ó° - by Arye - The Last Great Mystery (I, for one, welcome our new mirror image overlords)
[05:51:25] *** Joins: nshepperd24 (~nshepperd@li364-218.members.linode.com)
[05:53:02] *** Joins: galambo_ (galambo@user/galambo)
[05:55:08] *** Quits: Absalom (thelounge@envs.net) (Quit: Ping timeout (120 seconds))
[05:55:08] *** Quits: nshepperd2 (~nshepperd@li364-218.members.linode.com) (Quit: Ping timeout (120 seconds))
[05:55:08] *** Quits: Dyo (~Dyo@user/dyo) (Ping timeout: 272 seconds)
[05:55:08] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (Quit: Laa shay'a waqi'un moutlaq bale kouloun moumkine)
[05:55:08] *** Quits: nanotube (~nanotube@user/nanotube) (Ping timeout: 272 seconds)
[05:55:08] *** Quits: nshepperd (nshepperd@2600:3c03::f03c:92ff:fe28:92c9) (Ping timeout: 272 seconds)
[05:55:10] *** nshepperd24 is now known as nshepperd2
[05:55:17] *** Joins: nshepper1 (nshepperd@2600:3c03::f03c:92ff:fe28:92c9)
[05:55:20] *** Quits: galambo (galambo@user/galambo) (Ping timeout: 272 seconds)
[05:55:20] *** Joins: Lord_of_Life (~Lord@user/lord-of-life/x-2819915)
[05:55:22] *** Joins: nanotube (~nanotube@user/nanotube)
[05:55:48] *** Joins: Absalom (thelounge@envs.net)
[05:56:18] <feepbot> <gwern> uploads https://www.gwern.net/docs/philo/2021-turpin.pdf
[05:56:59] <shawwwn> nshepperd2: why were you nshepperd24 briefly?
[05:57:40] * shawwwn wonders how gwern comes across these obscure papers
[05:57:46] <shawwwn> hard to believe you read them all
[06:00:50] <shawwwn> that feeling when carmack sends you his secret AI talk notes and eleuther is mentioned explicitly
[06:08:10] <catern> "Regarded as a major cultural center, Detroit is known for its contributions to music and as a repository for art, architecture and design, along with its historical automotive background." https://en.wikipedia.org/wiki/Detroit ü§î
[06:08:11] <Robomot> Detroit - Wikipedia (Detroit (/d…™Ààtr…î…™t/, locally also /ÀàdiÀêtr…î…™t/) is the largest and most populous city in the U.S. state of Michigan, the largest U.S. city on the United States‚ÄìCanada border, as well as the seat of Wayne County. The municipality of Detroit had a 2019 estimated population of 670,031, making it the 24th-most populous city in the United States. ‚Ä¶)
[06:08:14] *** Quits: feepbot (~feepbot@ppp-93-104-163-43.dynamic.mnet-online.de) (Ping timeout: 252 seconds)
[06:08:27] *** Quits: feep (~feep@2001:a61:3530:9601:febc:d767:4eab:b638) (Ping timeout: 245 seconds)
[06:08:36] *** Joins: feep_ (~feep@2001:a61:2aa9:d601:bcb4:fa27:923c:ae6)
[06:09:20] *** Joins: feepbot (~feepbot@ppp-93-104-13-110.dynamic.mnet-online.de)
[06:11:42] *** Quits: ua_ (~ua@public-gprs232059.centertel.pl) (Read error: Connection reset by peer)
[06:13:57] <feepbot> <gwern> 'The Black Student Union led the call to remove the rock last summer. Crews began removing it just before 7 a.m. Friday, securing it with straps and lifting it with a crane before moving it to a flatbed truck. It cost an estimated $50,000, covered by private donations, to remove.'
[06:13:57] <feepbot> https://abc7chicago.com/racist-rock-university-of-wisconsin-chamberlin-removed-from-madison-campus/10939064/ even worse than I thought, $50k, not $10k
[06:13:57] <feepbot> University of Wisconsin moves Chamberlin Rock seen as symbol of racism - ABC7 Chicago (The boulder was referred to as a derogatory name for Black people in a Wisconsin State Journal story in 1925.)
[06:21:19] <quanticle> Obormot\Arcturus: Huh, this is the first I've heard of "jasmati" rice, but it does look interesting.
[06:28:27] <shawwwn> can someone remind me why backprop is preferred over forward autodiff?
[06:28:55] <gwern> what's the computational complexity of forward autodiff?
[06:29:15] <shawwwn> seems the same as an inference pass
[06:29:47] <gwern> that sounds wrong... is that for a *single* parameter or something?
[06:30:29] <shawwwn> the memory requirements are 2x, because you need a number and its dual for every parameter
[06:30:41] <shawwwn> the dual is simply the gradient
[06:31:26] <shawwwn> forward autodiff is "computing the gradient at every step" -- e.g. if you compute  x^2, it also computes 2x for the dual
[06:31:50] <shawwwn> so there's no memory requirement of storing tensors for the backwards pass. In fact, there is no backwards pass
[06:32:48] <shawwwn> it's remarkably hard to find simple information about this topic. https://towardsdatascience.com/forward-mode-automatic-differentiation-dual-numbers-8f47351064bf is overcomplicated, but sort of gets the idea across.
[06:33:00] <feepbot> Forward Mode Automatic Differentiation (AD) is one of the driving forces behind the success story of Deep Learning. It allows us to efficiently calculate gradient evaluations for our favorite composed functions‚Ä¶
[06:33:38] <gwern> 'There are 2 modes for automatic differentiation for computing MG and DG: 1. Reverse more accumulation: During each iteration of SGD, evaluate MG first. Compute the loss and then compute the DG on the way back to compute the gradient using the loss. Then the weights are adjusted and the next iteration is performed. Computing DG on the way back is exactly the backpropagation. 2. Forward mode...
[06:33:44] <gwern> ...accumulation: Evaluate DG along with MG in each iteration. At the end of the forward step, DG is also available. The gradient can then be applied and the next iteration can be started. Do note that with either reverse mode or forward mode, the end result is the same ‚Äî the value of the gradient computed is the same. But the difference is the cost of computation. Why so? Matrix-matrix...
[06:33:50] <gwern> ...operations in the forward accumulation mode are more expensive than matrix-vector operations in the reverse accumulation mode (O(n¬≥) vs O(n¬≤) respectively.). As a rule of thumb, if n > m (i.e, inputs are more than outputs), then backpropagation is more efficient. Otherwise, forward mode accumulation would be better. Therefore, while designing your neural net, you must carefully choose...
[06:33:52] <shawwwn> looks like https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html has some details about the tradeoffs: 
[06:33:52] <shawwwn> > These two functions compute the same values (up to machine numerics), but differ in their implementation: jacfwd uses forward-mode automatic differentiation, which is more efficient for ‚Äútall‚Äù Jacobian matrices, while jacrev uses reverse-mode, which is more efficient for ‚Äúwide‚Äù Jacobian matrices. For matrices that are near-square, jacfwd probably has an edge over jacrev.
[06:33:56] <gwern> ...your gradient computation algorithm that is most efficient for your application. In many popular neural nets today, the outputs are far less than the inputs, and hence backpropagation is more popular.' https://medium.com/deep-dives-into-computer-science/like-backpropagation-is-there-forward-propagation-as-well-fedb22828b36 ah
[06:33:56] <Robomot> The role of automatic differentiation in machine learning | by Kiran Achyutuni | Deep Dives into Computer Science | Medium (Backpropagation is fundamental to machine learning. It is a technique for computing the gradient of a function. Given there is a technique called backpropagation, one wonders if there is a technique‚Ä¶)
[06:33:57] <shawwwn> PASTE BATTLE
[06:34:09] <gwern> so if you are feeding in, say, 1024^3 inputs to get out 1 output, you want backwards mode
[06:34:44] <gwern> hm... when in DL do we feed in fewer inputs than outputs?
[06:34:49] <shawwwn> I was just wondering that.
[06:35:07] <shawwwn> "outputs" quickly becomes huge, especially if it's an image.
[06:35:19] <gwern> generative models, potentially? like, a G in a GAN? the output image is bigger than the _z_
[06:36:18] <shawwwn> even if it wasn't, it seems like you could compress pretty much anything into a smaller _z_
[06:36:19] <gwern> they compute the same gradient, so if you did forward mode on the G, then it'd still be compatible with backwards mode on the D, assuming the library can handle mixed models
[06:36:30] <nshepperd2> the outputs when training an image is the loss function
[06:36:35] <nshepperd2> image generator
[06:36:38] <nshepperd2> that's a scalar
[06:36:56] <gwern> oh. because the D collapses the 'output' back down into 1 scalar
[06:39:02] <shawwwn> hm? no
[06:39:08] <shawwwn> G's output is clearly the full image
[06:39:11] <shawwwn> D's output is a scalar, yeah
[06:39:27] <nshepperd2> you don't minimize G's output, you minimize the loss function
[06:40:23] <nshepperd2> if for some reason you wanted to compute that actual jacobian of G, then yeah, you'd want forward mode. but nobody ever needs to do that
[06:40:32] <shawwwn> hm.
[06:41:02] <gwern> there are GAN formulations which do the loss function on each pixel. would that work?
[06:41:04] <nshepperd2> (except when you're training a normalizing flow probability model, in which case the output of G has to be the same size as the input, so it wouldn't matter whether you use forward or backward mode)
[06:41:09] <shawwwn> that still feels wrong. I'm probably mistaken, but if that were true, it seems like you could use forward mode for the G part and backwards mode for the D part
[06:41:51] <gwern> yeah, that's what I said, but I'd bet that the libraries just don't support something weird like that
[06:41:53] <shawwwn> which would be a net savings
[06:42:03] <nshepperd2> you could do that but it would be way more expensive and pointless
[06:42:07] <shawwwn> (JAX does :) jaxfwd)
[06:42:21] <nshepperd2> using forward mode on G yields the entire jacobian of G
[06:42:21] <shawwwn> why would it be more expensive? that's the part I don't understand
[06:42:26] <nshepperd2> which you don't need
[06:42:45] <shawwwn> you don't need the jacobian of G to backprop through G?
[06:42:55] <nshepperd2> for minimizing the loss function, you only need the jacobian-vector product J(G) x grad(D)
[06:43:27] <nshepperd2> which is way more efficiently computed directly with backward mode
[06:43:29] <shawwwn> but G needs to be trained too, not just D.
[06:43:36] <nshepperd2> without ever materializing the jacobian
[06:44:09] <shawwwn> so of course you'd need to compute the gradient of each parameter of G with respect to the loss function, too
[06:44:19] <nshepperd2> yes, of course
[06:44:31] <shawwwn> then why don't you need G's jacobian?
[06:44:48] <nshepperd2> because you don't
[06:44:48] <shawwwn> in fact, I don't even remember what a jacobian is. That's probably step one.
[06:44:57] <shawwwn> fine
[06:45:01] <shawwwn> leave the jacobian behind. see if it cares
[06:45:47] * shawwwn re-studies https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html
[06:45:48] <nshepperd2> gwern: even with individual pixel loss, you just add all the losses together
[06:45:55] <nshepperd2> so the result is still a scalar
[06:45:58] <feepbot> The Autodiff Cookbook ‚Äî JAX  documentation
[06:48:52] <shawwwn> https://twitter.com/sea_snell/status/1424917008127254534/photo/1 says something similar 
[06:49:00] <shawwwn> and I can't quote it since it's a screenshot :)
[06:49:02] <|dbotdan> Charlie Snell (@sea_snell, 2021-08-10 02:14) Images: https://nitter.fdn.fr/pic/media%2FE8ZSdpPWEAINjrp.jpg%3Fname%3Dorig (description: graphical user interface, text, application, chat or text message; confidence: 0.99)
[06:49:26] <nshepperd2> RiversHaveWings: the documentation for jax.lax.gather just says, basically, "gather is complicated. go read the XLA docs lol". so... i don't know xD
[06:51:07] <shawwwn> nshepperd2: what did RHW ask about jax?
[06:51:17] * shawwwn didn't see anything in the last hour
[06:52:20] <shawwwn> MAYBE I KNOW
[06:52:23] <shawwwn> MAYBE I COULD ANSWER
[06:52:23] <nshepperd2> that was uh, 8 hours ago
[06:52:27] <shawwwn> MAYBE I COULD CONTRIBUTE
[06:52:47] <shawwwn> well, what was the question? obscure jax/xla stuff is my jams
[06:52:49] <nshepperd2> she was just responding to my shitpost :)
[06:52:59] <shawwwn> ah.
[06:53:02] <nshepperd2> about me not being able to understand jax.lax.gather
[06:53:20] <shawwwn> well, what would you like to know about jax.lax.gather?
[06:53:54] <nshepperd2> how to use it lol
[06:56:05] <shawwwn> you don't use it directly. Just index into a tensor
[06:56:16] <shawwwn> like you would in numpy
[06:56:49] <shawwwn> lax is the lower layer -- it's almost a 1 to 1 correspondence between lax and xla ops
[06:57:10] <shawwwn> jax is a high level interface over lax. so think of lax as ASM and jax as python (because, incidentally, it is)
[06:57:47] <shawwwn> also see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#in-place-updates for more gather-scatter related questions
[06:57:54] <gwern> xla=lax naruhodo
[06:57:58] <feepbot> üî™ JAX - The Sharp Bits üî™ ‚Äî JAX  documentation
[06:58:24] <shawwwn> omg
[06:58:44] <shawwwn> I can't believe gwern spotted that before me
[06:58:46] * shawwwn hangs up his hat
[06:59:03] <nshepperd2> i spotted that last night. it came to me in a dream
[06:59:32] <nshepperd2> also, how does indexing into tensors with tensors work
[06:59:44] <shawwwn> as far as answering your question directly ... yeah, the xla docs are ... hard to read. https://www.tensorflow.org/xla/operation_semantics#gather is the one you're looking for
[06:59:55] <feepbot> Operation Semantics  |  XLA  |  TensorFlow
[07:00:15] <shawwwn> I've read it a few times, just to try to osmose the knowledge. So far, none of it has stuck. But it's becoming slightly less alien
[07:00:49] <shawwwn> the real truth is that anything you're curious about, you should look at the jaxpr dumps for. It's jax.make_jaxpr(fn)(args go here) I believe
[07:01:01] <shawwwn> and if you're really curious, you should look at the HLO dumps, which I forget how to dump off hand
[07:01:11] <shawwwn> you'll have to run that on an actual TPU though to see the optimized dumps
[07:01:16] *** Joins: galambo__ (~galambo@user/galambo)
[07:01:17] <quanticle> Obormot\Arcturus: re: the Peter Watts short story, I'd read it before. It's all right.
[07:02:10] <quanticle> If you want a better story in the same setting, I recommend "Cyclopterus". It was (re)printed in an anthology of short stories called _Mission Critical_, edited by Jonathan Strahan.
[07:03:05] <quanticle> https://www.worldcat.org/title/mission-critical/oclc/1099591747&referer=brief_results
[07:03:05] <Robomot> Mission Critical. (Book, 2019) [WorldCat.org] (Get this from a library! Mission Critical.. [Jonathan Strahan])
[07:04:10] <nshepperd2> oh cool, i can probably reverse engineer what gather means with make_jaxpr
[07:04:24] <Obormot\Arcturus> quanticle: Interesting, I'll check it out
[07:04:41] *** Quits: galambo_ (galambo@user/galambo) (Ping timeout: 248 seconds)
[07:08:26] <shawwwn> nshepperd2: a few other things to note
[07:09:56] *** feep_ is now known as feep
[07:10:36] <nshepperd2> tbh i really wish there was an einsum like thing where i could just go tensor.gather("output[i, j, k] = input[i, index[j,k]]", index=index) or something
[07:10:55] <shawwwn> run with XLA_FLAGS="--xla_dump_hlo_as_text --xla_dump_to=/tmp/generated" to see the HLO outputs
[07:11:18] <shawwwn> there is, sort of: like numpy, there's jax index_tricks or something. I'll look it up
[07:15:08] <quanticle> "Peter Watts is actually a lot more cheerful than you might expect."
[07:15:11] <quanticle> https://rifters.com/real/author.htm
[07:15:11] <Robomot> Rifters.com‚ÄîAuthor (A webpage for new Peter Watts' novel.)
[07:15:13] <quanticle> lol
[07:15:31] <shawwwn> it's jax.s_, and I know I've found it useful in the past
[07:15:35] <shawwwn> but right now I can't remember why
[07:15:55] <nshepperd2> ah yes, my favourite function, s_
[07:15:56] <quanticle> From the top of that page: "Whenever I find my will to live becoming too strong, I read Peter Watts."
[07:16:30] <shawwwn> it's not even a function, lol. you index into it
[07:16:51] <shawwwn> yeah it just returns slices
[07:16:57] <nshepperd2> :monocle: 
[07:17:22] <shawwwn> basically, it returns an expression that you can then pass into foo[<result goes here>]
[07:17:36] <shawwwn> which in the case of python is ... just slices.
[07:17:52] <shawwwn> maybe this isn't as useful as I was thinking it might be for you.
[07:18:02] <shawwwn> what were you hoping to do, which regular indexing into tensors doesn't do?
[07:18:26] <nshepperd2> uh, all sorts of shit
[07:19:08] <nshepperd2> what if I have an tensor of indices with some batch dimensions which match the values tensor and some which don't
[07:19:43] <shawwwn> if it doesn't match, how would you use it to index into the tensor?
[07:19:55] <nshepperd2> as far is I can tell, tensor[x, y, other_tensor] only lets you do extremely basic shit
[07:20:32] <nshepperd2> shawwwn: you would return a result with an additional dimension
[07:22:27] <shawwwn> what's an example of some non-basic shit you were trying to do?
[07:22:36] <shawwwn> my non-basic shit is unfortunately too basic, since it does work:
[07:22:39] <shawwwn> https://www.irccloud.com/pastebin/r70Dof4I/
[07:22:50] <feepbot> Snippet | IRCCloud
[07:23:08] <nshepperd2> that's extremely basic
[07:23:17] <shawwwn> what can I say, it's how I roll
[07:23:19] <shawwwn> basically
[07:23:34] <nshepperd2> what about output[i, j] = input[i, index[i, j, 0], index[i, j, 1]]
[07:23:52] <nshepperd2> can't remember the actual thing i wanted to do
[07:25:05] <Obormot\Arcturus> gwern: Why does default.html talk about a zero width space inside nav links when instead what we have is <svg> elements
[07:25:16] <shawwwn> well, right away, since the outer "i"'s match, it sounds like you want jax.lax.scan https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html
[07:25:16] <shawwwn> The docs are stupid, but basically it does something for every slice along the leading axis of a tensor
[07:25:27] <shawwwn> as for this specific thing... *me thinks*
[07:25:28] <feepbot> jax.lax.scan ‚Äî JAX  documentation
[07:25:45] <Obormot\Arcturus> gwern: That whole comment seems wildly outdated
[07:26:01] <nshepperd2> or, say, implementing any of the equations in https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather
[07:26:01] <Robomot> torch.gather ‚Äî PyTorch 1.9.0 documentation
[07:31:26] <gwern> Obormot\Arcturus: I hadn't noticed that you changed the navbar completely. I don't know if that's outdated or not: I still see <a> elements in there, and I never understood why it was screwed up to begin with
[07:33:13] <Obormot\Arcturus> It is outdated, because they can't possibly be self-closing anymore since *they have stuff inside them*
[07:33:35] <Obormot\Arcturus> Anyway I'm fixing it
[07:35:47] <Obormot\Arcturus> gwern: https://github.com/gwern/gwern.net/commit/43d91fb2eecbc7ac56bca4b869a9d9236216b0a6 what the heck is this
[07:35:48] <Robomot> links.css: haskell.org/*.hs clash: override in favor of source code icon ¬∑ gwern/gwern.net@43d91fb ¬∑ GitHub (Site infrastructure for gwern.net (CSS/JS/HS/images/icons) - links.css: haskell.org/*.hs clash: override in favor of source code icon ¬∑ gwern/gwern.net@43d91fb)
[07:35:49] * gwern shrugs. if it breaks again, tidy should tell him immediately, and at least he knows wtf is going on
[07:35:57] <Obormot\Arcturus> That's not even valid css :\
[07:36:37] <gwern> it's not? hm. wonder why the w3c css validator didn't flag it, or if it did and just has so many false positives I didn't notice
[07:36:54] <Obormot\Arcturus> Yeah that doesn't do or mean anything
[07:37:07] <Obormot\Arcturus> What is it supposed to be?
[07:37:28] <gwern> Obormot\Arcturus: anyway, the intent of that was to make 'haskell.org/*.hs' be link-iconed as '*.hs' ie with a source code icon, instead of being link-iconed as 'haskell.org/*', with the lambda icon
[07:37:29] <Robomot> 404 Not Found
[07:37:29] <Robomot> 404 Not Found
[07:37:45] <Obormot\Arcturus> Hmm
[07:38:17] <Obormot\Arcturus> So that line is intended to say "haskell.org unless it ends with .hs"?
[07:38:50] <gwern> yeah
[07:38:59] <shawwwn> nshepperd2: I'm a little shocked that I don't immediately know the answer to this
[07:39:03] <shawwwn> working on it.
[07:40:05] <Obormot\Arcturus> Oh you later changed that line to
[07:40:06] <Obormot\Arcturus> a[href*="haskell.org"][href$=".hs"]::after
[07:40:12] <Obormot\Arcturus> ... which also doesn't do what you want
[07:40:25] <Obormot\Arcturus> Indeed it does the exact opposite of what you want...
[07:41:36] <Obormot\Arcturus> Hmm wait
[07:43:08] <shawwwn> nshepperd2: so, for
[07:43:08] <shawwwn> >>> t = torch.tensor([[1, 2], [3, 4]])
[07:43:08] <shawwwn> >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))
[07:43:08] <shawwwn> tensor([[ 1,  1],
[07:43:08] <shawwwn>         [ 4,  3]])
[07:43:09] <shawwwn> The JAX equivalent is:
[07:43:09] <shawwwn> >>> t = jnp.array([[1, 2], [3, 4]])
[07:43:10] <shawwwn> >>> t[((0,0), (1,1)), ((0,0), (1,0))]
[07:43:10] <shawwwn> DeviceArray([[1, 1],
[07:43:11] <shawwwn>              [4, 3]], dtype=int32)
[07:44:04] <shawwwn> even I am shaking my head at how complicated it seems vs the pytorch gather
[07:44:22] <nshepperd2> gosh
[07:44:29] <shawwwn> but, basically
[07:44:47] <nshepperd2> having to repeat the indices array is pretty bad
[07:45:03] <nshepperd2> or whatever the fuck you did to it there
[07:45:24] <shawwwn> >>> i1 = jnp.array(((0,0), (1,1)))
[07:45:24] <shawwwn> >>> i2 = jnp.array(((0,0), (1,0)))
[07:45:24] <shawwwn> >>> t[i1, i2]
[07:45:24] <shawwwn> DeviceArray([[1, 1],
[07:45:24] <shawwwn>              [4, 3]], dtype=int32)
[07:45:44] <shawwwn> so think of each axis slice separately
[07:46:22] <nshepperd2> ah, concatenating it with an enumeration of the batch dimensions
[07:47:16] <shawwwn> if it makes sense to you, then you're ahead of me.
[07:47:32] <shawwwn> the only thing I keep telling myself is "at least I haven't had to do this yet; maybe this is just a contrived example"
[07:47:47] <Obormot\Arcturus> gwern: So yeah, you actually got it right in one place and totally wrong in another
[07:47:49] <shawwwn> but at least there does exist a mapping from pytorch gather to jax indexing, even if it's ... obscure
[07:47:58] <shawwwn> I'll have to sit down and write a converter
[07:48:09] <Obormot\Arcturus> gwern: ` a[href*="haskell.org"]:not([href$='.hs'])::after` is the form you want (replacing the plain `a[href*="haskell.org"]`)
[07:48:32] * Obormot\Arcturus fixes
[07:48:57] <shawwwn> nshepperd2: okay so, now that we have this magic figured out, I guess we can open it up to see how it works, with jax.make_jaxpr
[07:49:32] <shawwwn> https://www.irccloud.com/pastebin/tAugR054/
[07:49:43] <feepbot> Snippet | IRCCloud
[07:50:23] <Obormot\Arcturus> (also it needs the "case-insensitive" flag, of course)
[07:50:32] <nshepperd2> i'd rather a converter from equations. there's a lot that pytorch gather can't do either
[07:50:57] <shawwwn> nshepperd2: that's interesting. Can you give me a concrete example?
[07:51:45] <shawwwn> e.g. output[i, j] = input[i, index[i, j, 0], index[i, j, 1]] ?
[07:51:55] <nshepperd2> that would be an example
[07:53:02] <shawwwn> .. can you humor my lack of intelligence and expand it out into a concrete "here's a result; here's an input" with actual numbers?
[07:53:12] <shawwwn> (and here's an index array)
[07:54:35] * shawwwn just writes it in python
[07:58:56] <nshepperd2> input = [[[1,2],[3,4]],[[5,6],[7,8]]]; input.shape = [2,2,2]; index = [ [[0,0]], [[1,1]] ]; output = [[1], [8]]
[07:59:01] <shawwwn> apparently this https://www.irccloud.com/pastebin/k5gNjn4s/
[07:59:12] <feepbot> Snippet | IRCCloud
[08:00:46] <shawwwn> hmm
[08:01:05] <shawwwn> so it seems like one way a converter could work, would be to write it in python (how I did in that snippet above), and then track each array index access
[08:01:30] <shawwwn> and at the end, try to convert those index accesses / array writes into a single (or small number of) indexes and updates
[08:02:14] <shawwwn> that last step is the hard part.
[08:03:45] <nshepperd2> I was hoping that if I figured how that whole GatherDimensionNumbers thing works, i could write a converter by directly translating the expression for each dimension index in the inputs array
[08:04:08] <nshepperd2> but uh
[08:04:42] <shawwwn> well, I'd like to help figure it out with you
[08:04:52] <shawwwn> this seems like one of those problems that's at the edge of my abilities, which is a good place to be
[08:05:48] <shawwwn> if you want to experiment with XLA, https://jax.readthedocs.io/en/latest/notebooks/XLA_in_Python.html is a pretty nice start
[08:05:59] <feepbot> XLA in Python ‚Äî JAX  documentation
[08:06:24] <shawwwn> oh, actually
[08:06:43] <shawwwn> git clone https://github.com/google/jax ; cd jax ; rg '\.Gather'
[08:06:54] <feepbot> GitHub - google/jax: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more - GitHub - google/jax: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more
[08:06:58] <shawwwn> there's a crapload of hits for lax.GatherDimensionNumbers
[08:07:04] <shawwwn> (this is my favorite way to learn the internals)
[08:07:39] <shawwwn> do yourself a favor and open it in Pycharm though
[08:08:07] <shawwwn> (command-B / ctrl-B for goto definition is extremely helpful, as is ctrl-shift-F for global find-all)
[08:10:28] <nshepperd2> RiversHaveWings: is your roommate at all punk? I dreamed that we met and for some reason i remember that your roomate had a crazy bright red mohawk
[08:10:34] <nshepperd2> pycharm, eh
[08:11:00] <nshepperd2> ah, a python IDE
[08:13:52] *** Quits: nullcone (uid11626@id-11626.brockwell.irccloud.com) (Quit: Connection closed for inactivity)
[08:14:17] <shawwwn> gwern: https://twitter.com/marksaroufim/status/1424920417320411142
[08:14:26] <|dbotdan> Mark Saroufim (@marksaroufim, 2021-08-10 02:27): ‚ÄòForward time complexity = O(#inputs) | Forward memory complexity = O(constant) | Reverse  time complexity = O(#outputs) | Reverse memory complexity = O(#operations) | https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6‚Äô
[08:17:02] <shawwwn> nshepperd2: if you do install pycharm, open a project and then do tools -> create command line launcher. It lets you run `charm .` in any python project
[08:17:06] <shawwwn> to open it immediately
[08:17:10] <gwern> shawwwn: I see. but the real difficulty there seems to be understanding what is even meant by 'outputs'...
[08:17:17] <shawwwn> gwern: yes
[08:17:26] <SDr> okay, write up a piece: https://pastebin.com/DSvDdQK7   <- email to apple.  would you mind double-checking it to see if all points come across?
[08:17:27] <Robomot> Dear Apple,Many times Apple customer here. I'm writing to you in response to - Pastebin.com (Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.)
[08:17:56] <Obormot\Arcturus> Well, "many times Apple customer here" is not idiomatic English at all...
[08:18:28] <Obormot\Arcturus> "contest with maximum prejudice" is also a phrase that doesn't make sense
[08:18:28] <shawwwn> SDr: ah. I'm sorry for giving you a hard time earlier. I didn't realize your protest was on moral grounds
[08:18:37] <shawwwn> it would help to mention such things
[08:19:28] <Obormot\Arcturus> "There is a very bright line, a Chesterton's Fence that you've crossed here" is mixing metaphors; one does not *cross* Chesterton's fences, one *knocks them down* (or, leaves them standing, I guess)
[08:19:43] <rmmh> don't knock down the rubicon unless you've tried it
[08:19:45] <Obormot\Arcturus> What the heck is a "kill chain"
[08:20:03] <gwern> rmmh: look, the rubycon dice has been cast, and now all that's left is to see if apple rolled a nat 1 or 20
[08:20:22] <SDr> https://en.wikipedia.org/wiki/Kill_chain
[08:20:22] *** Quits: Robomot (~Robomot@user/robomot) (Remote host closed the connection)
[08:20:41] <Obormot\Gaia> rmmh: SDr's use of idioms here reminds me of the old pub owner dude in Boondock Saints
[08:20:50] <shawwwn> SDr: looks like line 12 is malformed?
[08:21:06] <Obormot\Gaia> "We gotta buy you like a book of proverbs or something... this mix and match shit has gotta go"
[08:21:09] * shawwwn actually respects that Obormot switched computers mid-read
[08:21:17] <nshepperd2> the spirit of christ condemns you in the strongest possible terms
[08:21:54] <shawwwn> "-may you never be on the "social distancing" side of the conversation before 2020 March" I found this very confusing. Maybe cut it
[08:22:14] <SDr> shawwwn, what's an example where small group of people later turned out to be "right"
[08:22:16] *** Quits: feepbot (~feepbot@ppp-93-104-13-110.dynamic.mnet-online.de) (Ping timeout: 272 seconds)
[08:22:19] <SDr> gay rights?
[08:22:44] <SDr> anyways, cut
[08:22:46] <rmmh> trans fats
[08:23:01] *** Joins: feep_ (~feep@2001:a61:3474:4a01:43f0:91af:ee19:5551)
[08:23:09] *** Joins: feepbot (~feepbot@ppp-93-104-182-211.dynamic.mnet-online.de)
[08:23:32] *** Quits: feep (~feep@2001:a61:2aa9:d601:bcb4:fa27:923c:ae6) (Ping timeout: 272 seconds)
[08:23:44] <shawwwn> it's worth thinking carefully of a solid example.
[08:24:40] <shawwwn> your letter also needs to hone in on the distinction between scanning locally, and scanning locally photos *which are then uploaded to icloud*
[08:24:52] <shawwwn> the former is a privacy intrusion. The latter is not. And Apple is doing the latter.
[08:25:18] *** Joins: Robomot (~Robomot@user/robomot)
[08:25:18] *** ChanServ sets mode: +v Robomot
[08:25:54] <shawwwn> though I see your last few paragraphs do contain some quotes that do make that distinction
[08:26:29] <nshepperd2> i'd wanna mention that it's actually illegal to install malware on peoples' phones
[08:26:39] <shawwwn> Oh, interesting. line 12 is actually a full quote from apple
[08:26:42] <Obormot\Arcturus> SDr: Ok, well, my comment on this letter is that the highly technical and *obscure* jargon is going to pretty much eliminate any chance of anyone taking this missive seriously
[08:26:59] <rmmh> how long until apple labels all detractors pedos
[08:26:59] <Obormot\Arcturus> (Also, typo: substitue)
[08:27:19] <shawwwn> I didn't find the jargon particularly unclear, for what it's worth. Nor particularly technical
[08:27:30] <gwern> if you were going to bother writing it at all, it should be an open letter. let's face it, there is precisely 0% chance of apple doing anything in response to this letter, as they will have considered all this well in advance and chose to do it anyway
[08:28:25] <shawwwn> I don't think gwern's point is true. Tim Cook has been known to reply to letters like these.
[08:28:29] <Obormot\Arcturus> That too
[08:28:30] <shawwwn> the chance is near zero, but it's not precisely zero
[08:29:07] <shawwwn> and "they've considered this, so don't send it" is stupid. paying customers should say why they're no longer paying customers. Gwern himself has done this
[08:29:50] <gwern> shawwwn: and it works even better if they know other customers are reading it
[08:30:12] <shawwwn> I might be willing to sign an open letter, fwiw. But I'd have to be persuaded first
[08:30:21] <shawwwn> my hangup is, every argument here could equally apply to photos in the cloud
[08:30:30] <rmmh> they'll probably put out a letter in a month about how they'll make extra certain that the code is locked to only scan icloud uploads, they promise
[08:30:37] <shawwwn> I don't understand why apple chose to do it device-local, but it seems to me that they could've equally done it cloud-remote.
[08:31:02] <shawwwn> after all, it's hash-matching.
[08:31:04] <Obormot\Arcturus> So what explains the apparent increase in hysteria about pedophilia and "child sexual abuse" and so on in recent years?
[08:31:07] <rmmh> shawwwn: icloud doesn't (generally) have your crypto keys
[08:31:12] <rmmh> it's not hash matching, either
[08:31:15] <Obormot\Arcturus> This shit seems to be getting more and more insane all the time - why?
[08:31:15] <nshepperd2> if they do it cloud remote it makes a mockery of the claim that they are encrypted an inaccessible
[08:31:33] <rmmh> oh wait they gave up on E2E icloud crypto
[08:31:47] <shawwwn> (they could do it with fully homomorphic encryption)
[08:31:55] <nshepperd2> no they couldn't
[08:31:59] <rmmh> The encryption keys are stored on Apple‚Äôs own servers in Cupertino. Without these encryption keys, no one can decrypt anything. The thing is, the encryption keys are readily accessible if one has access to the user‚Äôs Apple ID account (as in knowing the login and password and being able to pass two-factor authentication). If a third party gains control over the user‚Äôs Apple ID/iCloud
[08:32:01] <rmmh> account, they can download and decrypt information.
[08:32:18] <Robomot> [Less Wrong [frontpage]] A Qualitative and Intuitive Explanation of Expected Value by adamzerner - https://www.greaterwrong.com/posts/LJArjH2h4TACfksaT/a-qualitative-and-intuitive-explanation-of-expected-value
[08:32:26] <rmmh> homomorphic encryption schemes are currently like "using a mere 10MB of data, we have successfully computed the fifth fibonacci number"
[08:32:32] <gwern> Obormot\Arcturus: my cynical explanation is that with the collapse of ISIS and islamic terrorism, and the public turning on the war on drugs, CP is the most solid remaining horseman
[08:32:42] <SDr> v2 https://pastebin.com/pgTwwWVe   final
[08:32:43] <Robomot> Subject: Betraying user trustDear Apple,Many times Apple customer here. - Pastebin.com (Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.)
[08:32:45] <shawwwn> (I meant "could" theoretically, not practically. :)
[08:32:58] <nshepperd2> they couldn't do it theoretically either
[08:33:06] <gwern> rmmh: not quite that bad! you can actually do a surprising amount these days. like you can run whole real neural networks like CNNs
[08:33:14] <Obormot\Arcturus> SDr: "longtime" is the word you want, as in, "longtime Apple customer"
[08:33:19] <shawwwn> cut the comma after "FAQ" on line 12
[08:33:23] <nshepperd2> homomorphic encryption would yield an encrypted result which is useless
[08:33:28] <Obormot\Arcturus> (Or is it hyphenated? I always forget)
[08:33:31] <gwern> nshepperd2: not to the customer
[08:33:42] <Obormot\Arcturus> https://www.merriam-webster.com/dictionary/longtime nope I was right the first time
[08:33:43] <Robomot> Longtime | Definition of Longtime by Merriam-Webster (Longtime definition is - having been so for a long time : long-standing. How to use longtime in a sentence.)
[08:34:30] <nshepperd2> gwern: indeed, but the customer is hardly gonna feel like reporting themselves to NCMEC
[08:34:56] <shawwwn> > Furthermore, by opening up Pandora's box of client-side checking, you have signalled for other companies to follow suit.
[08:34:56] <shawwwn> 1. "signalled" is a typo; it's "signaled"
[08:34:56] <shawwwn> 2. don't other companies already do this?
[08:35:53] <gwern> nshepperd2: you could have the customer's device upload the executable to run remotely, and ship the result back, and then the device tattles
[08:36:33] <shawwwn> eh. that's basically the same thing.
[08:36:48] <nshepperd2> yeah you could do that. seems like pointless complication though
[08:36:55] <nshepperd2> if you're already install malware
[08:36:57] <gwern> nshepperd2: ie my iphone uploads a cryptoblob to apple's icloud, which runs the cryptoblob on my cryptodata, sends back a mystery 0/1, and my iphone decrypts it, and tattles if 'CP==TRUE'. my data, despite being in the cloud, is always 100% encrypted, and nothing *can* be leaked other than the presence/absence of CP
[08:36:58] <shawwwn> "local content matching by any other name smells just as rancid" and all that.
[08:37:09] <gwern> nshepperd2: and this can run on more data than even fits on my iphone
[08:37:28] <shawwwn> gwern: people's whole problem here is that your phone tattles
[08:37:30] <shawwwn> rather than the cloud
[08:37:40] <Obormot\Arcturus> "In a company-wide internal letter to Apple employees in response, NCMEC‚Äôs executive director of strategic partnerships Marita Rodriguez described the criticism as the ‚Äúscreeching voices of the minority.‚Äù[39]" ... nice, nice
[08:37:56] <gwern> shawwwn: well sure, but it does at least respect your icloud privacy more than "we'll just store it all encrypted with our own keys, trust us"
[08:38:01] <nshepperd2> gwern: yeah, i guess this would enable you to screw over the customer even more by not even telling them which picture allegedly matches
[08:38:23] <galambo__> after reading this letter, I wonder if I should switch from windows to linux
[08:38:47] <gwern> nshepperd2: sure. if you wanted to, I assume you could ship back per file bits, or maybe a tree
[08:38:56] *** galambo__ is now known as galambo
[08:39:01] <shawwwn> look
[08:39:15] <shawwwn> the iCloud choice isn't merely a policy decision
[08:39:19] <shawwwn> you've opted to surrender your photos to iCloud
[08:39:29] <shawwwn> the fact that your phone does the legwork of the matching changes nothing about your decision there.
[08:39:54] <Obormot\Arcturus> "Apple‚Äôs idealized outcome solves a lot of seemingly intractable problems. On one hand, CSAM is horrific and Apple hasn‚Äôt been doing anything about it" ... why should Apple do anything about "it"?? It's not Apple's job! Even the critics are conceding the most absurd points
[08:40:13] <shawwwn> it's completely equivalent to deciding to develop your photos at a local film store, and then the film store bringing a person to your house to look through the negatives with a little scope before carrying the photos off to the store
[08:40:34] <shawwwn> yes, it happens in your house. But no, that's not an invasion of privacy, because you've *opted into it*
[08:40:51] <shawwwn> that's why I don't get this.
[08:41:19] <shawwwn> the idea that we can't trust apple when they say "the scanning is off when iCloud sync is off" seems ludicrous. If that were true, then we can't trust them when they say anything whatsoever
[08:41:20] <feep_> no you're right, it's a concession of privacy
[08:41:45] *** feep_ is now known as feep
[08:41:55] <Obormot\Arcturus> (Even the problem is defined in a totally Orwellian manner: "child sexual abuse material" is vague enough that it can be pretty much *anything*, and the notion that the *existence* of "child sexual abuse material" is itself, directly, a problem, is completely insane given that vagueness)
[08:42:09] <shawwwn> it means when children get raped, obormot
[08:42:10] <nshepperd2> i think it's just morally wrong to install code on someone's device whose only purpose is to harm the user
[08:42:14] <feep> that said, yes, you can't trust them when they say anything whatsoever
[08:42:19] <feep> this is news?
[08:42:26] <rmmh> CSAM is now the preferred term to CP
[08:42:42] <feep> rmmh: I don't understand why, CSAM is more restrictive than CP
[08:42:50] <shawwwn> it's probably not
[08:42:53] <Obormot\Arcturus> "Second, though, legitimate societal concerns about the needs of law enforcement and the radicalizing nature of the Internet should be taken seriously." ... no they shouldn't!
[08:43:21] <feep> I mean, to be clear, nitpicking aside, I'm in full agreement with obormot :p
[08:43:28] <Obormot\Arcturus> feep: Only if you interpret it in a sane way, which is not the only interpretion of which the term admits
[08:43:37] <feep> Obormot\Arcturus: that is fair
[08:43:43] <shawwwn> nshepperd2: do you feel it's morally wrong to install such code in a cloud service? I'm having trouble coming to terms with the distinction
[08:44:10] <Obormot\Arcturus> (And that ambiguity is very much the point)
[08:44:16] <shawwwn> because, again, even if it is on your device, it hinges on the extremely-important policy decision of you opting into a cloud service
[08:44:22] <nshepperd2> at least the other antifeatures iphones come with have some sort of rationalization about being necessary for security or something
[08:44:47] <Obormot\Arcturus> "Taking the slightly easier route, though ‚Äî iCloud backups, Facebook messaging, etc. ‚Äî means some degree of vulnerability that, let‚Äôs not forget, is sometimes justifiably leveraged. Law enforcement can get a warrant for those backups or chat logs, just as they can install a wire tap." wrong
[08:44:57] <Obormot\Arcturus> Police shouldn't be able to tap your phone either
[08:45:04] <Obormot\Arcturus> Or any of that other shit
[08:45:05] <feep> https://www.youtube.com/watch?v=WIPdJ8i-ya4 video on topic
[08:45:06] <Robomot> Dog Responds to Secret Code Word - YouTube (http://www.videobash.comI guess you could call him a PETophile.)
[08:47:40] <nshepperd2> shawwwn: no because the company owns those cloud servers
[08:48:02] <shawwwn> that's an interesting point.
[08:48:43] <shawwwn> but unfortunately it changes nothing about the moral implications, because apple is running nothing on your device *unless* you opt into those cloud servers
[08:48:45] <nshepperd2> nominally you own your device. stallman would never install malware on someone else's computer
[08:49:24] <feep> yes, using a cloud service clearly means you consent to having your device scanned, that is how consent works
[08:49:25] <shawwwn> the phone has to run software to upload the photos to the servers too. at that point, the ethics don't change whether the hash matching happens on the phone or on their servers
[08:49:35] <nshepperd2> i don't think it's acceptable for call of duty to install malware either, even if you "opt into" it
[08:49:36] <feep> same as how having sex with someone means consenting to having them come into your house with no warning
[08:49:57] <shawwwn> if you don't consent to it, then don't consent to it. Right?
[08:50:05] <feep> right, just don't have sex with them
[08:50:24] <shawwwn> yes, that's generally the solution when someone is crossing boundaries, along with potentially removing them from your life
[08:50:29] <shawwwn> as SDr is doing.
[08:50:35] <feep> I support this conclusion. don't use apple lol
[08:53:26] <feep> basically, this is xkcd whatshisface with "they
[08:53:30] <feep> 're doing bad things!"
[08:54:25] <shawwwn> .note gwern some conversation about "what's an output?" https://twitter.com/theshawwn/status/1424949317530439688
[08:54:27] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 04:22): ‚ÄòThe reason that was confusing is because, in order to train G, you're still taking the derivative of each parameter of G w.r.t. the loss function. | But I guess that the image output is an intermediate step, so it does make sense. It'll just take some time to think that way.‚Äô
[09:02:24] <shawwwn> nshepperd2: https://jakevdp.github.io/PythonDataScienceHandbook/02.07-fancy-indexing.html makes a pretty convincing case that the basic examples you were objecting to earlier get you pretty darn far
[09:02:26] <nshepperd2> shawwwn: to some extent the issue with local scanning is that this is just more of apple doubling down on being maximally anti-stallman. obviously, removing the malware the notifies NCMEC must never be permitted, which instantly provides justification of all sort of draconian anti-user-freedom measures
[09:02:36] <feepbot> Fancy Indexing | Python Data Science Handbook
[09:02:55] <shawwwn> can you think of a reason you'd want something like output[i,j] = input[i, index[i, j, 0], index[i, j, 1]] ? that's my only hangup with it
[09:02:58] <nshepperd2> maybe you can now accuse jailbreakers of being pedos and get the police to break their doors down
[09:03:03] <shawwwn> it seems dangerous to spend a lot of time on hypothetical desires
[09:04:33] <shawwwn> nshepperd2: that's another good point that I hadn't considered. But I'm not sure it follows that jailbreaking could become illegal
[09:04:58] <shawwwn> I was about to agree with you, but then I realized that circumventing apple's software is a ToS violation, not a law violation
[09:05:02] <nshepperd2> anything can follow with enough hysteria
[09:08:14] <Obormot\Arcturus> Hmm
[09:08:29] <Obormot\Arcturus> Modus delens is meta- modus tollens
[09:08:42] <Obormot\Arcturus> (Meta- modus ponens is still modus ponens)
[09:11:17] <nshepperd2> meta-modus-tollens? so... ¬¨¬¨P -> ¬¨¬¨Q?
[09:11:22] <Obormot\Arcturus> no
[09:12:07] <nshepperd2> ohh, rejecting the implication, i see
[09:12:09] <Obormot\Arcturus> yes
[09:12:26] <shawwwn> .note gwern apparently every (or almost every?) loss function has one output, so therefore there are always more inputs than outputs, and backprop always wins...? https://twitter.com/theshawwn/status/1424953693502328852 On the other hand, "Some RL algorithms will do things like output an action and its value https://twitter.com/marksaroufim/status/1424954006380498947 "
[09:12:33] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 04:40): ‚Äòlmao. | Thank you for walking me through this. I appreciate it greatly. | That was f'in hilarious though. "Oh yeah, all this stuff everyone's been talking about? Just set n=1 and you'll be right 99.9% of the time." And n=1 is far simpler, since it means backprop always wins :)‚Äô
[09:12:33] <|dbotdan> Mark Saroufim (@marksaroufim, 2021-08-10 04:41): ‚ÄòSome RL algorithms will do things like output an action and its value‚Äô
[09:13:48] *** Quits: galambo (~galambo@user/galambo) (Ping timeout: 268 seconds)
[09:15:23] <Obormot\Arcturus> (P->Q & P -> Q = m.p.; P->Q & ¬¨Q -> ¬¨P = m.t.; P & ¬¨Q -> ¬¨(P->Q) = m.d. ... for anyone still wondering what I meant there)
[09:18:04] *** Joins: galambo (~galambo@user/galambo)
[09:25:38] <Obormot\Gaia> (And the latter is also/actually: [(P->Q) -> ¬¨(P & ¬¨Q)] & (P & ¬¨Q) -> ¬¨(P->Q), hence m.m.t.)
[09:26:47] <Obormot\Gaia> (Symmetrically with [(P->Q) & P -> Q] & P -> Q; cf. Achilles & Tortoise)
[09:27:23] *** Joins: Gurkenglas (~Gurkengla@dslb-002-203-144-156.002.203.pools.vodafone-ip.de)
[09:27:39] <shawwwn> if anything, this conversation was much more confusing than just asking nshepperd2 https://twitter.com/theshawwn/status/1424957724740177950
[09:27:42] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 04:56): ‚ÄòWhy would it be exponential? | For every operation, you also run an operation to compute its gradient. | It seems like the time complexity is equal to the forward pass time complexity, multiplied by a constant factor (which is 2, because dual).‚Äô
[09:28:05] <shawwwn> no one seems clear on what the time complexity is
[09:28:52] *** Joins: ua_ (~ua@public-gprs232059.centertel.pl)
[09:29:33] *** Quits: sm2n (~sm2n@user/sm2n) (Read error: Connection reset by peer)
[09:37:01] <nshepperd2> feep: https://irc.zlkj.in/uploads/b48fd0c58398416a/20210809165431_0.png my favourite of the "consciousness fourier transform"s :) 
[09:37:02] <Robomot> image/png (256x256; 203 KB)
[09:41:36] *** Joins: sm2n (~sm2n@user/sm2n)
[09:44:48] <nshepperd2> shawwwn: i can explain. but i gotta go out to the store first. also you will need to know what a jacobian is
[09:45:03] <shawwwn> nshepperd2: <3. Luckily, a jacobian is slightly less mysterious than it was a few months ago
[09:45:19] <shawwwn> we're currently at https://twitter.com/theshawwn/status/1424961972123095081 :D
[09:45:25] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 05:13): ‚ÄòIt would mean that if you double the parameter count, the time complexity would remain the same. So it would take the same amount of computation to train 100M params as 1B params. | But that can't be true.‚Äô
[09:45:27] <shawwwn> enjoy store
[09:54:56] *** Quits: ua_ (~ua@public-gprs232059.centertel.pl) (Excess Flood)
[09:56:03] *** Joins: ua_ (~ua@public-gprs232059.centertel.pl)
[10:00:20] *** Quits: ua_ (~ua@public-gprs232059.centertel.pl) (Excess Flood)
[10:10:55] *** Joins: feep[work] (~mathis@217.64.163.97)
[10:12:04] <shawwwn> nshepperd2: some feedback from bradbury:
[10:12:04] <shawwwn> "the easiest way to do weird indexing/gather things is to use indexing plus vmap. you should play around with that; it can feel a little magical"
[10:13:00] *** Quits: Fusxfaranto (~Fusxfaran@cpe-75-85-179-208.san.res.rr.com) (Ping timeout: 268 seconds)
[10:16:56] *** Joins: galambo_ (galambo@user/galambo)
[10:20:42] *** Quits: galambo (~galambo@user/galambo) (Ping timeout: 272 seconds)
[10:29:37] <rsaarelm> https://enkiv2.medium.com/anime-dissecting-a-vibe-abebf9b14f2c
[10:29:38] <Robomot> Anime: Dissecting a Vibe. A response to iniksbane on what‚Ä¶ | by John Ohno | Aug, 2021 | Medium (Another blogger, iniksbane, has responded to my piece ‚ÄòWhat is Anime‚Äô. Their response is worth reading in full, and I think we are more closely in agreement than that essay makes out. Where I‚Ä¶)
[10:30:38] <shawwwn> .note gwern lmao. the answer is, forward autodiff requires N forward passes, where N is total parameter count. Sorry for the notespam <3 https://twitter.com/theshawwn/status/1424973488130072578
[10:30:51] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 05:58): ‚Äò@austinvhuang was correct: forward autodiff requires one forward pass *per parameter*. (Thanks to @jekbradbury for also explaining this to me.) | Thank you also to @sea_snell @marksaroufim @sai_prasanna for the wonderful conversations! | ‚Äô
[10:34:29] *** Joins: ua_ (~ua@public-gprs232059.centertel.pl)
[10:35:09] <ggreer> https://en.wikipedia.org/wiki/Member_states_of_the_United_Nations only 9 countries went away in 70 years? ya know I think that happened a lot more often in the past
[10:35:10] <Robomot> Member states of the United Nations - Wikipedia (The United Nations member states are the 193 sovereign states that are members of the United Nations (UN) and have equal representation in the UN General Assembly.[1] The UN is the world's largest intergovernmental organization. ‚Ä¶)
[10:39:23] <shawwwn> feep: feature request: .clearnotes <user> to wipe all pending notes for <user> from me. Tomorrow gwern will wake up to 3, two mistaken ones followed by a correct answer.
[10:39:44] <shawwwn> I'd like to .clearnotes gwern then .note gwern the last note.
[10:42:20] <nshepperd2> don't worry, he's already been highlighted in the notes in the channel. you can't hide your shame
[10:42:49] <mst> shame? shawwwn? bitch please
[10:43:03] <mst> he's about as capable of shame as I am :D
[10:43:15] * shawwwn has his laptop plugged into his guitar amp, and that irccloud bloop scared the shit out of him
[10:43:18] <shawwwn> (well done)
[10:43:24] <shawwwn> (and that's true)
[10:45:29] <shawwwn> wtf.
[10:45:55] <shawwwn> I may actually give up on this whole scientific computing thing. t is a 2x2 matrix, and I'm shocked that t * t is a 2x2x2x2 matrix
[10:46:03] <shawwwn> I'm just trying to take component-wise multiplication
[10:46:20] <shawwwn> oh wait
[10:46:25] <shawwwn> the *derivative* is 2x2x2x2
[10:46:28] * shawwwn was playing with jacfwd
[10:46:44] <shawwwn> but ... the derivative is 2x2x2x2? what?
[10:47:03] <shawwwn> >>> jax.jacrev(lambda t: t * t)(t.astype('f')).shape
[10:47:03] <shawwwn> (2, 2, 2, 2)
[10:47:35] <shawwwn> oh, because there are np.prod(t.shape) outputs
[10:47:43] <shawwwn> >>> jax.jacrev(lambda t: (2 * t).sum())(t.astype('f')).shape
[10:47:44] <shawwwn> (2, 2)
[10:53:10] <shawwwn> okay, nshepperd2, help. My brain isn't working. How do you write a loss function that makes "all model parameters equal"? By that I mean, if you start with model params [1,2,3], you should eventually get [2,2,2] or something
[10:54:19] <nshepperd2> lambda t: (t - t.mean()).square()?
[10:54:34] <nshepperd2> err, lambda t: (t - t.mean()).square().mean()
[10:55:05] <shawwwn> more than that, this is genuinely surprising: https://www.irccloud.com/pastebin/xJ8K5FmW/
[10:55:17] <feepbot> Snippet | IRCCloud
[10:55:29] <shawwwn> apparently taking a matmul of two inputs and summing the result, the derivative is just ... one of the inputs?
[10:55:53] <shawwwn> repeated N times, where N is the size of the other input along one axis? well, so much for trying to derive gradients from just looking at outputs
[10:57:37] <shawwwn> that loss function works great. Thank you
[10:57:42] <nshepperd2> feels bad that jax tensors don't have .square() or .sqrt()
[10:58:33] <shawwwn> heh. numpy for ya.
[10:58:39] *** Joins: Lord_of_Life_ (~Lord@user/lord-of-life/x-2819915)
[10:59:09] <shawwwn> though you can extend jaxlib.xla_extension.DeviceArrayBase with it, perhaps
[10:59:24] <shawwwn> but you'd have to extend the jax Tracer base class too
[10:59:35] * shawwwn still hasn't looked into monkeypatching extra functionality onto jax tensors
[10:59:58] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (Ping timeout: 272 seconds)
[11:00:29] <shawwwn> yolo
[11:00:29] <shawwwn> >>> jaxlib.xla_extension.DeviceArrayBase.square 
[11:00:29] <shawwwn> >>> z.square()
[11:00:29] <shawwwn> DeviceArray([[6.249976 , 6.249981 ],
[11:00:29] <shawwwn>              [6.2499847, 6.2499895]], dtype=float32)
[11:00:41] <shawwwn> er, I cut too much.
[11:00:44] <shawwwn> jaxlib.xla_extension.DeviceArrayBase.square = lambda self: (self ** 2)
[11:01:18] *** Lord_of_Life_ is now known as Lord_of_Life
[11:02:44] <shawwwn> ayyy it worked https://www.irccloud.com/pastebin/xg0VSZCN/
[11:02:56] <feepbot> Snippet | IRCCloud
[11:03:13] <nshepperd2> jax.core.Tracer i guess
[11:03:31] <nshepperd2> cool
[11:03:32] <shawwwn> so yes, just bolt on whatever functionality you want onto jax.core.Tracer and jaxlib.xla_extension.DeviceArrayBase
[11:03:51] <shawwwn> you need jaxlib.xla_extension.DeviceArrayBase for the actual arrays jax returns, and jax.core.Tracer for all the intermediate tracing stuff it does for jit / grad
[11:04:07] <nshepperd2> i'll add that to my jaxtorch library
[11:04:11] <shawwwn> nice
[11:04:21] <shawwwn> jaxtorch is a good idea
[11:04:45] <shawwwn> one problem is that jax random functions require a random key
[11:05:00] <shawwwn> whereas torch functions don't. (I've been thinking about a jaxtorch type lib for a little while)
[11:05:32] <shawwwn> but it seems best to keep the API jax compatible, followed by torchlike (rather than reverse priority)
[11:06:17] <nshepperd2> i solve that by including the prng state in the context object which gets passed to the forward() function of every module
[11:06:41] <shawwwn> mm.
[11:07:23] <nshepperd2> my way seems to be way more jax compatible than whatever insane global mutability stuff flax/haiku do
[11:07:44] <shawwwn> that still implies that you need to pass "something" into each rng function, because otherwise you could make the context object global (haiku style) or just pass the key (jax style)
[11:07:51] <shawwwn> only global haiku-style would give you torch API
[11:08:09] <shawwwn> unless the context object is the thing that you call to get random tensors instead of torch.rand, which is still an API break
[11:08:46] <shawwwn> whereas there's nothing to pass into torch: torch.rand(2,2)
[11:08:48] <nshepperd2> i'm not really set on making the api 100% word for word compatible
[11:09:03] <nshepperd2> invoking cx.randn(2,2) is fine by me
[11:09:05] <shawwwn> nah, don't go for compatibility. go for simplicity
[11:09:13] <shawwwn> ah.
[11:10:30] <shawwwn> how do you handle parameter init? cx.get_variable("foo", init: cx.zeros)?
[11:10:48] <shawwwn> er, wow, my brain must really be mush tonight
[11:11:02] <shawwwn> self.foo = cx.get_variable("foo", init=lambda: cx.zeros(shape))
[11:11:20] <shawwwn> and then make cx inherit the model's scope
[11:11:38] <shawwwn> I guess it could be an actual Parameter object
[11:11:54] <nshepperd2> def __init__(self): self.weight = jaxtorch.init.normal(c1, c2)
[11:11:57] <shawwwn> but it'd still need access to the scope in order to properly name the var
[11:12:13] <shawwwn> what does jaxtorch.init.normal(c1, c2) return?
[11:12:18] <nshepperd2> self.weight is a Param object which just a. identifies the parameter b. says how to initialize it
[11:12:32] <shawwwn> yes, but how is the identification resolved?
[11:12:38] <shawwwn> that was the tricky part for me with tftorch
[11:12:54] <shawwwn> does model.__setattr__ set it?
[11:13:14] <nshepperd2> literally just using the object's id
[11:13:28] <shawwwn> lolwhat, no you can't. Ok I'll suspend my disbelief
[11:13:32] <shawwwn> but the names need to be torch-style
[11:13:49] <shawwwn> .get_state() or whatever would return dotted names
[11:14:12] <nshepperd2> yeah, every module has a named_parameters() function which returns a list of (recursively) all the params and their dotted names
[11:14:13] <shawwwn> "model.0.linear.b" or whatever
[11:15:01] <shawwwn> I had the same thing. But, what if someone wants to do computation in the constructor? There's no context object yet
[11:15:14] <shawwwn> in fact, jaxtorch.init.normal doesn't have a context object either
[11:15:21] <shawwwn> that was the problem I ran into
[11:15:41] <shawwwn> the Params object would need to support all computation, including the ability to wipe and set its values
[11:15:46] <shawwwn> modules can (and do!) do that
[11:15:52] <shawwwn> in the constructor, before the forward pass I mean
[11:16:05] <nshepperd2> the Param object contains a function which returns the initial value
[11:16:20] <nshepperd2> the function takes a rng key in case it needs it
[11:16:24] <shawwwn> is the context object passed to the model's constructor?
[11:16:28] <nshepperd2> no
[11:16:54] <shawwwn> so if there's no context object in the constructor, and the constructor requires the initial value, and randomness requires a random key, and the context object contains the random key, where does the random key come from?
[11:17:10] <shawwwn> (this isn't a contrived example; it's the usual)
[11:17:15] <nshepperd2> actual initialization, creating all the tensors, happens after you make the Module
[11:17:49] <shawwwn> I mean... if you want to concede not making it like torch, then sure, that works. But in torch, people initialize their tensors in the constructor, with randomness
[11:18:38] <shawwwn> and since initialization can be arbitrary, that means it needs access to the random key
[11:18:49] <nshepperd2> that barely makes any difference
[11:19:01] <shawwwn> oh?
[11:19:23] <nshepperd2> self.weight = torch.random.normal(4, 5) vs self.weight = jaxtorch.init.normal(4,5)
[11:19:40] <nshepperd2> you just defer the initialization by putting it in a lambda
[11:19:49] <shawwwn> self.weight = jaxtorch.init.normal(4,5) * 2.0
[11:19:52] <shawwwn> does this work?
[11:20:16] <nshepperd2> self.weight = Param(lambda key: jax.random.normal(4,5, key=key) * 2.0)
[11:21:00] <shawwwn> that runs into a different problem: what is self.weight after that expression?
[11:21:02] <shawwwn> er
[11:21:03] <shawwwn> self.weight.shape
[11:21:15] <shawwwn> (I ran into that one too.)
[11:21:48] <shawwwn> because code needs to know self.weight.shape in the constructor, before the forward pass (unfortunately)
[11:22:12] <nshepperd2> oyeah, self.weight = Param(lambda key: jax.random.normal(4,5, key=key) * 2.0, shape=[4,5])
[11:22:46] <nshepperd2> actually, I think it's currently, self.weight = Param(lambda shape, key: jax.random.normal(*shape, key=key) * 2.0, shape=[4,5]) but I'm not sure if I like it that way
[11:23:02] <shawwwn> yeah, it quickly gets ugly.
[11:23:19] <shawwwn> I think the right solution is for Param to be a jax Tracer, which supports all computation
[11:23:27] <nshepperd2> you could also just do Param(np.randn(4,5) * 2.0)
[11:23:28] <shawwwn> but that's harder and you punting is probably better.
[11:23:45] <shawwwn> you can't tho
[11:24:01] <shawwwn> because for huge GPT models, that will require huge amounts of system memory just to initialize it
[11:24:06] <shawwwn> I guess you'd need that anyway if you want to load from checkpoint
[11:24:24] <shawwwn> but then you'd need 2x that memory after initializing the model, during checkpoint loading
[11:24:31] <shawwwn> unless you only load one tensor from disk at a time
[11:24:37] <nshepperd2> huge amounts of system memory are way more readily available than gpu memory, lol
[11:25:01] <shawwwn> yes, but remember people will want to use this on their laptops. and 2x might be the difference between being able to load and inference vs crashing
[11:25:31] <nshepperd2> you can delete the initializers after using them
[11:25:32] <shawwwn> also, you have to be able to control the randomness using keys, for repeatable initializations across tpu pods
[11:25:59] <nshepperd2> do you really
[11:26:09] <shawwwn> probably not. that was the only contrived example I've used :(
[11:26:31] <nshepperd2> anyway, what you'd actually do is self.weight = jaxtorch.init.normal(4,5, stddev=2.0)
[11:26:35] <shawwwn> but sharding is still a bitch
[11:26:46] <shawwwn> yeah yeah, but that just raises the question of how jaxtorch.init.normal is implemented
[11:26:51] <shawwwn> it means that computation in the constructor is a requirement
[11:26:59] <shawwwn> what if the init depends on a different tensor's value? etc
[11:27:01] <nshepperd2> nope
[11:30:37] <nshepperd2> but yeah i guess if for some reason you ever want to do complicated init where multiple params depend on each other, you could pass a rng key to the constructor
[11:31:28] <nshepperd2> or better, have a separate init function which lets you modify the tensors after creation
[11:31:36] <nshepperd2> via the context object
[11:32:00] <Obormot\Gaia> quanticle: I read "Cyclopterus" ... man, Watts is really on a doomer kick, isn't he
[11:42:58] <shawwwn> nshepperd2: the more I think about it, the more I think computation should be supported in the constructor
[11:44:21] <shawwwn> and the more I think about it, the more it seems like the context object needs to be global
[11:44:35] <shawwwn> otherwise in the forward function, you'll have to pass it to every submodule
[11:44:50] <shawwwn> I think it should be passed to the constructor and stored in the model
[11:45:42] <shawwwn> that would solve both controllable random init, and the problem of having to pass it to every submodule in the forward function
[11:46:16] <shawwwn> your technique would look like self.linear(cx, self.linear2(cx, x)) instead of self.linear(self.linear2(x))
[11:46:52] <shawwwn> but if the context object is available in the constructor, then not only do you not need any Params object (because you can just create jax tensors immediately), but you can also pass it to submodules as you construct them
[11:47:46] <shawwwn> I still tend to favor the global approach though.
[11:48:35] <shawwwn> in main():
[11:48:35] <shawwwn> with jaxtorch.Context():
[11:48:35] <shawwwn>   mdl = MyModel()
[11:49:03] <shawwwn> you can even have a default Context() object, the way that tensorflow has a default Graph() object
[11:49:42] <shawwwn> I really think that controllable randomness is one of jax's strengths, and it may or may not be a bad idea to force everyone not to use it
[11:52:50] <kuudes> re that apple cp scanner
[11:52:57] <kuudes> a bit of background, if you guys will
[11:53:16] <kuudes> the issue is that in eu ePrivacy directive has expanded
[11:53:34] <kuudes> and now includes also cloud storage, I think
[11:54:01] <kuudes> ePrivacy bans eavesdropping any private comms without due legal court order etc
[11:54:29] <kuudes> so isps and telcos are not allowed to process call or sms or email or ip data or metadata but for transmitting the communication
[11:54:57] <kuudes> so this bans also current system where facebook and apple and google scan comms that go through them for cp
[11:55:10] <kuudes> and also where in the same point, likely nsa copies the comms as whole
[11:55:21] <kuudes> this alteration will likely at least hamper nsa much
[11:55:37] <kuudes> but think of the children, so cp should be still battled
[11:55:53] <kuudes> so currently the strongest solution proposition is this device side scanning
[11:56:51] <kuudes> as this would comply with eprivacy and gdpr, anything user does not consent is not stored on user system, and from user system anything is not output but in the case where there is proper cause / due suspicion
[11:57:29] <kuudes> so it is better than current, but still bad insofar that comms get scanned
[11:57:35] <nshepperd2> shawwwn: passing it to every submodule is preferable to it being global imo
[11:57:45] <kuudes> but at least it closes 1 large nsa loophole
[11:58:19] <kuudes> I think a major thing would be that eu customer icloud would be restricted to be hosted within eu.
[11:59:11] <nshepperd2> shawwwn: the point is to do this with a minimum of magic
[11:59:29] <shawwwn> nshepperd2: it's a mistake to do it with a maximum of code
[12:00:15] <shawwwn> there's a reason people like torch.rand(3,3) and can remember it, vs ???.rand(3,3) or jaxtorch.rand(ctx, 3, 3)
[12:00:31] <nshepperd2> nah
[12:00:42] <shawwwn> mm.
[12:08:29] <shawwwn> nshepperd2: in that case, I think you might need to pass the context object into both the constructor and the forward function (and each submodule individually in the forward function). Because the constructor would need controlled randomness (requiring an rng key) and arbitrary computation, and the forward function would also need randomness (e.g. dropout). I'm not sure the forward function can use the key state from construction 
[12:08:29] <shawwwn> time.
[12:09:12] <shawwwn> I'm not sure it can't, either. But storing the context object is stateful, which seems less good if you prefer passing the context object around
[12:09:52] <shawwwn> I really hate repetition, but if you don't mind it then you might want to hate statefulness. And storing the context in the constructor to use during the forward pass is pretty stateful
[12:10:43] <shawwwn> one case you should try to support, if you can: some easy way of storing tensors for use later. E.g. GPT K and V caching during sampling
[12:10:50] <shawwwn> it would be neat if the context object could somehow take care of that bookkeeping
[12:11:29] <adiabatic> I was kind of wondering why nobody said anything while I was around about the new Apple child-porn scanner but maybe that's because only two of us here have iPhones (or at least talk about them)
[12:13:06] *** Joins: CryptoDavid (uid14990@id-14990.highgate.irccloud.com)
[12:13:43] <shawwwn> I have an iPhone too. nshepperd almost convinced me it's immoral, but I still cling to the fact that you can turn off the malware by turning off icloud sync
[12:14:18] *** Quits: srijan (sid19575@tinside.irccloud.com) (Ping timeout: 240 seconds)
[12:14:25] <adiabatic> part of apple's defense against the US twisting their arm is "we are incapable of doing that"
[12:14:33] *** Quits: kaizen_ (sid60510@tinside.irccloud.com) (Read error: Connection reset by peer)
[12:14:51] *** Quits: cannedprimates_ (sid16585@tinside.irccloud.com) (Read error: Connection reset by peer)
[12:15:00] *** Joins: srijan (sid19575@id-19575.tinside.irccloud.com)
[12:15:01] *** Joins: kaizen_ (sid60510@id-60510.tinside.irccloud.com)
[12:15:05] *** Joins: cannedprimates_ (sid16585@id-16585.tinside.irccloud.com)
[12:15:11] <adiabatic> now that they have what they claim is a kiddie-porn scanner built into the phone
[12:15:19] <adiabatic> and they pinky swear it'll only be used for kiddie porn
[12:15:41] <adiabatic> I had no idea tim cook was a power-bottom rice queen
[12:16:02] <adiabatic> because this is basically him bending over for the likes of xi
[12:16:27] <adiabatic> what's appalling is that Apple seems genuinely surprised by the backlash
[12:17:48] <adiabatic> meanwhile the reddit and HN midwits can't say "slippery slope" without typing "fallacy" after it
[12:18:09] <adiabatic> s/midwits/zoomers/
[12:22:29] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Remote host closed the connection)
[12:23:42] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[12:28:38] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 272 seconds)
[12:33:08] <kuudes> imo the main important thing would be that each phone user would be only in their own country's jurisdiction
[12:33:17] <kuudes> usa should not be able to snoop european comms
[12:33:27] <kuudes> but this is idealistic position :-p
[12:33:41] <shawwwn> nshepperd2: do you still feel like walking me through the jacobian stuff, or nah?
[12:33:58] <kuudes> eu court would sort of be ok with nsa snooping, iff usa made a law allowing nsa snooping, which then could be sued in usa
[12:37:29] *** Quits: Tene (~tene@mail.digitalkingdom.org) (Ping timeout: 248 seconds)
[12:43:14] *** Quits: momus (~momus@ec2-18-195-22-48.eu-central-1.compute.amazonaws.com) (Ping timeout: 258 seconds)
[12:43:24] *** Joins: momus (~momus@ec2-18-195-22-48.eu-central-1.compute.amazonaws.com)
[12:44:10] *** Joins: Tene (~tene@mail.digitalkingdom.org)
[12:44:34] <nshepperd2> shawwwn: sure. so consider a function f : R^m -> R^n
[12:44:43] <shawwwn> ok
[12:45:21] <nshepperd2> in the case of training a neural network f would generally be your loss function, and m = number of parameters and n = 1
[12:48:04] <nshepperd2> the jacobian of f is like the "full" derivative of f, and it's a matrix containing the derivative of each element of the output wrt each element of the input. if you name the input and output at a specific point with y = f(x), the jacobian is J(f)_ij = d(y_i)/d(x_j)
[12:49:40] <nshepperd2> the jacobian is a matrix shaped like n √ó m
[12:49:55] <shawwwn> is n=1 because loss generally returns a scalar?
[12:50:11] <nshepperd2> yes, the loss is always a scalar
[12:50:25] <feep[work]> I want to write a blog post entitled "'XY Problem' Considered Harmful"
[12:50:27] <shawwwn> (not true for RL, since each action apparently has its own loss value; TIL)
[12:50:38] <feep[work]> it's ruining stackoverflow
[12:50:49] <nshepperd2> even for RL, you average together all the loss values
[12:50:53] <nshepperd2> and end up with a scalar
[12:50:55] <feep[work]> everyone is trying to guess what the original asker actually wants, but I'm not the original asker and I have an unrelated reason
[12:51:36] <shawwwn> nshepperd2: source on the claim was https://twitter.com/marksaroufim/status/1424954006380498947
[12:51:49] <shawwwn> and also https://twitter.com/marksaroufim/status/1424956694035976192
[12:51:52] <|dbotdan> Mark Saroufim (@marksaroufim, 2021-08-10 04:41): ‚ÄòSome RL algorithms will do things like output an action and its value‚Äô
[12:51:53] <|dbotdan> Mark Saroufim (@marksaroufim, 2021-08-10 04:52): ‚ÄòI believe they exist in scientific computing where for e.g you're trying to figure out the state of a whole system given a few initial conditions and trying to do it in a differentiable way | https://arxiv.org/pdf/1607.07892.pdf‚Äô
[12:52:57] <nshepperd2> yeah sometimes for things other than loss minimization you need jacobians of things where n > 1
[12:53:11] <nshepperd2> or otherwise derivetives of them
[12:53:16] <shawwwn> so in general though, the jacobian is a flat, 1 by m matrix, where m is the number of parameters
[12:53:18] <nshepperd2> anyway
[12:53:39] <nshepperd2> well, we'll keep it general and say n x m
[12:53:50] <nshepperd2> for our f : m -> n function
[12:54:10] <shawwwn> "why use n when 1 will do?" giggled the mad hatter, twirling in his seat
[12:54:22] <nshepperd2> because it's important to explain how n and m relate to the computation cost
[12:54:26] <shawwwn> ah
[12:54:42] <shawwwn> my hangup was, for forward and backward, n seems to be 1. Is that correct?
[12:54:46] *** Joins: thoros (~thoros@80-121-140-225.adsl.highway.telekom.at)
[12:55:03] <shawwwn> I guess the question reveals that I'm not understanding where the complexity is being introduced though.
[12:55:25] <shawwwn> anyway, yes, with you so far
[12:55:25] <nshepperd2> for *neural network applications*, n is often 1
[12:55:34] <nshepperd2> this is a property of the function you're looking at
[12:55:47] <nshepperd2> anyway, we sometimes want to compute the jacobian
[12:56:07] *** Quits: Obormot\Sirius (~obormot@user/obormot) (Read error: Connection reset by peer)
[12:56:22] *** Joins: Obormot\Sirius (~obormot@user/obormot)
[12:56:36] <nshepperd2> in the case of a neural network loss function, n = 1, which means the jacobian is actually a 1 x m row vector, it's the gradient
[12:57:12] <nshepperd2> and the gradient, which is the same as the jacobian for a function with scalar output, is what we need for gradient descent
[12:57:49] <nshepperd2> so the two things we have at our disposal for computing the jacobian are forward and backward autodiff
[12:59:12] <nshepperd2> by the way, there's a chain rule for jacobians. J(f . g) = J(f) J(g)
[12:59:55] <nshepperd2> which is sort of what both autodiff modes are based on
[13:00:51] <nshepperd2> anyway, what forward-mode autodiff actually does is computes a jacobian-vector product for us. it lets you compute J(f) v for some v
[13:02:05] <nshepperd2> that v is the dual part of the dual vector you pass in as input: f(x + vŒµ)
[13:03:10] <nshepperd2> if you set your v to, say, [1,0,0,0,0....], then this basically computes the first column of the jacobian for you
[13:03:43] <nshepperd2> and that is why you need to do m passes with forward-mode to compute the jacobian
[13:04:02] <nshepperd2> one for each column
[13:05:33] <nshepperd2> similarly, backward-mode autodiff actually computes a vector-jacobian product for you. it lets you compute v J(f) for some v
[13:06:43] <nshepperd2> and again you can do multiple passes, one for each n, setting v  to [1,0,0,0...], [0,1,0,0...], ..., to compute all rows of the J(f)
[13:07:05] <nshepperd2> so  that requires n passes with backward-mode to compute the jacobian
[13:07:16] <shawwwn> I just don't quite see the leap from "if you set your v to, say, [1,0,0,0,0....], then this basically computes the first column of the jacobian for you" to "and that is why you need to do m passes with forward-mode to compute the jacobian"
[13:07:28] <nshepperd2> but! luckily in neural networks, n=1, so you only need one pass
[13:08:42] <nshepperd2> shawwwn, you do forward with v=[1,0,0,0...], then with v=[0,1,0,0,...], then with v=[0,0,1,0...], etc
[13:08:56] <nshepperd2> each time you get a different column of the jacobian
[13:09:03] <nshepperd2> of which there are a total of m
[13:18:09] <shawwwn> ah.
[13:18:15] <shawwwn> this was a lightbulb moment. Thank you
[13:18:26] <shawwwn> it clarifies the difference between jvp and vjp
[13:19:53] <nshepperd2> cool
[13:19:59] <shawwwn> If vjp is that conceptually trivial, though, why is it such a pain in the ass to scale memory usage? What I mean is, it almost seems trivial to evaluate vjp in two steps rather than one: the left half, then the right half
[13:20:06] <shawwwn> sure, it'd take two passes. But it'd be half the memory usage
[13:20:08] <shawwwn> or would it?
[13:21:52] <nshepperd2> mmm, i'm not sure what you mean by that
[13:21:58] <shawwwn> I guess not necessarily, since what you really need are the activations from the forward pass during the backwards pass
[13:22:09] <shawwwn> basically, I was trying to think of some convenient way to trade time for memory
[13:22:21] <shawwwn> you're in a situation where the TPU crashes due to oom. batch size is 1. now what?
[13:22:29] <shawwwn> (during training)
[13:22:51] <shawwwn> I was hoping that you could "just compute the gradient of one parameter at a time" for a factor of M memory usage savings
[13:24:36] <shawwwn> for example...
[13:24:36] <shawwwn> > and again you can do multiple passes, one for each n, setting v  to [1,0,0,0...], [0,1,0,0...], ..., to compute all rows of the J(f)
[13:24:36] <shawwwn> Since v is [1], that means we do [1] x J(f), which computes the entire first row of J(f). But I'm interested in understanding where, precisely, the memory requirements come into play
[13:24:39] <nshepperd2> gradient checkpointing is the closest thing to that i think
[13:25:16] <shawwwn> why though? how could it be true that computing the derivative of one teeny tiny parameter would require the same memory usage as computing the derivative of all parameters?
[13:25:22] <shawwwn> (without checkpointing)
[13:26:29] <nshepperd2> because a change in one parameter has ripple effects that affect pretty much all activations after that parameter is used
[13:27:29] <nshepperd2> like if you tweak one parameter in the 'key' weights of an attention block, it's gonna affect everything
[13:27:32] <shawwwn> can you think of a simple example to illustrate that? it would go along way to solidifying my theoretical understanding
[13:27:46] <nshepperd2> because the whole attention matrix is gonna be slightly different
[13:28:00] *** Joins: galambo__ (galambo@user/galambo)
[13:28:17] * shawwwn sighs... he doesn't even see that much
[13:28:29] <shawwwn> if you tweak the key, why does the entire activation change?
[13:28:41] <shawwwn> using https://github.com/shawwn/openai-server/blob/jax-wip/openai_server/gpt/jax/model.py#L237-L257 for example
[13:28:54] <feepbot> openai-server/model.py at jax-wip ¬∑ shawwn/openai-server ¬∑ GitHub (OpenAI API webserver. Contribute to shawwn/openai-server development by creating an account on GitHub.)
[13:29:07] <shawwwn> you seem to know these things at a visceral level
[13:30:00] *** Quits: Urchin[emacs] (~user@user/urchin) (Ping timeout: 258 seconds)
[13:30:08] <shawwwn> to guess wildly, does changing the key affect all the softmax outputs?
[13:30:15] <nshepperd2> yes, exactly
[13:31:27] <shawwwn> so, tying it all together
[13:31:48] <nshepperd2> K_bthr is gonna have at least one difference for each t, which means W_bhtt is gonna be slightly different for every t, which means the softmax outputs will be different which means the resulting A_bts will be slightly different in every value
[13:31:58] <nshepperd2> and the effects cascade
[13:32:12] *** Quits: galambo_ (galambo@user/galambo) (Ping timeout: 268 seconds)
[13:32:19] <shawwwn> why is the entire softmax activation during the forward pass necessary to compute the jacobian of just a single float of K, for example?
[13:32:50] <nshepperd2> so you're going to have to hold onto all the activations after that black to track all the effects on loss back to that one parameter
[13:32:56] <shawwwn> hm.
[13:33:00] <shawwwn> I see.
[13:33:08] <shawwwn> so it's the depth of the network that determines the memory requirements
[13:33:24] <nshepperd2> yes
[13:33:25] <shawwwn> therefore, it must be a little more efficient to only train the last few layers
[13:33:33] <shawwwn> well, a lot more efficient
[13:34:01] <shawwwn> so if one was looking to train a mega-model even if you're ooming, one could do it by freezing the first K layers
[13:34:15] <shawwwn> rather than freezing the parameters layer-wise
[13:34:17] *** Joins: Dyo (~Dyo@host.sedf.de)
[13:34:18] <nshepperd2> yeah, if you want to, you can train extremely deep networks by just continuously adding new layers and training them to convergence
[13:34:28] <nshepperd2> at the end of the network
[13:34:36] <nshepperd2> and have basically O(1) memory usage
[13:34:37] <shawwwn> heh.
[13:35:01] <shawwwn> what about in a GPT, where the stupid vocab embedding shows up at the start and the end?
[13:35:15] <shawwwn> guess you can try training the vocab embedding to convergence first, and then freeze it
[13:35:34] <nshepperd2> just use a different embedding for the start and end, lol
[13:36:02] <nshepperd2> minGPT does it that way, nothing stopping you
[13:36:05] <shawwwn> you could, but... that'd require another 50257x1600, which is a hefty boi
[13:36:25] <shawwwn> it always seemed stupid as hell that the vocab embedding dim needed to match the model embedding dim
[13:36:38] <shawwwn> seems like it could be 50257x128, then 128x1600
[13:37:21] <shawwwn> which would only need 8% as many parameters
[13:38:10] <shawwwn> so, the only remaining blind spot in my understanding (ha, I wish) is why the activation from the forward pass is necessary to compute the gradient during the backwards pass
[13:38:13] <nshepperd2> that would make the logits rank constrained. which is theoretically a bottleneck for prediction quality but in practice idk if anyone knows how much of a bottleneck it actually is
[13:39:00] <shawwwn> oh, it's because activations aren't reversible
[13:39:12] <shawwwn> if they were, then you wouldn't need to store the forward activations
[13:39:19] <shawwwn> hence reversible layers
[13:39:51] <shawwwn> obviously for x * y + z, you can't recover x unless you know z and y in advance
[13:40:01] <nshepperd2> yeah hence all those complicated papers on making reversible resnets and stuff so that you can recompute the activations in reverse order for the backward pass
[13:40:03] <shawwwn> though I'm not quite sure why you need to recover x in order to compute its gradient...
[13:40:46] <nshepperd2> d(x * y + z)/dx = y
[13:41:00] <nshepperd2> d(x * y + z)/dy = x
[13:41:21] <shawwwn> oh
[13:41:34] <shawwwn> and because chain rule, you need those in order to compute the actual gradient of model params
[13:41:50] <shawwwn> (assuming that x * y + z is an intermediate computation during the forward pass or something)
[13:42:04] <nshepperd2> that's pretty much it. pretty much every nn layer has its formula for the gradient which depends on the inputs in some way
[13:42:16] <shawwwn> I see
[13:42:39] <shawwwn> so what you're saying is, I need to implement the reversible computation theorized in https://theswissbay.ch/pdf/Gentoomen%20Library/Extra/Richard_P._Feynman-Feynman_Lectures_on_Computation__-Addison-Wesley%281996%29.pdf in JAX
[13:42:49] <shawwwn> that way there's no need for reversible layers
[13:43:49] <nshepperd2> heh
[13:45:38] <shawwwn> thanks for walking me through this
[13:46:07] <feep[work]> > The Princicitates Discordia states that the Princici Discordia "is a work of fiction, a fantasy, a parody, a mockery, a lie, a fraud, a hoax, a farce, a gibe, a joke, a trick, a jape, a tale, a riddle, a puzzle, a myth, a legend, a fable, a parable, an allegory, a manual, a treatise, a guide, a tract, a poem, a collection, a book, a novel, a play, a film, a documentary, a biography, a script, a 
[13:46:13] <feep[work]> song, a piece of music, a choreography, a drama, a comedy, a historical piece, a political pamphlet, a philosophical treatise or an aphorism."
[13:46:16] <feep[work]> what I find remarkable about that is that gpt-2 never repeated itself once.
[13:46:23] <feep[work]> (also it is totally on brand.)
[14:13:06] *** Joins: bjelleklang (~c@wikipedia/Bjelleklang)
[14:21:28] *** Quits: CryptoDavid (uid14990@id-14990.highgate.irccloud.com) (Quit: Connection closed for inactivity)
[14:24:34] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[14:28:57] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 248 seconds)
[14:43:03] <PapuaHardyNet> https://gist.github.com/kinoc/dca36b12b5e956688a9b92a87ba7c52c
[14:43:04] <Robomot> So now you want to finetune that GPT-J-6B on a 3090/TITAN GPU ... okay, using HF and DeepSpeed too ¬∑ GitHub (So now you want to finetune that GPT-J-6B on a 3090/TITAN GPU ... okay, using HF and DeepSpeed too - j6b_train_hf_ds.py)
[14:44:47] <PapuaHardyNet> oh I missed a good jax discussion.
[14:45:55] <dbohdan> feep[work]: That's a pretty good list
[14:45:57] <Gurkenglas> feep[work], you could look at the probability of it continuing to produce no repetition varying with how long it's been not repeating itself
[14:55:40] <feep[work]> shawwwn: hey, I'm wondering
[14:56:21] <feep[work]> even giving up on gpt-j on gpu, is it possible to CPU it on a system with a mere 16GB free? or would it just instantly crater into swap
[14:56:42] <feep[work]> my problem is that I have a p powerful 16gb system (16 cores) and a horribly weak 32gb system (my nas)
[14:56:50] <feep[work]> 16 threads*
[14:57:14] <feep[work]> and is there a docker setup for it?
[14:58:04] <feep[work]> somebody seems to have done it on 16GB but "there seems to be some performance loss"
[14:58:31] *** Quits: bjelleklang (~c@wikipedia/Bjelleklang) (Quit: Leaving)
[15:00:21] <feep[work]> looks like running on pure cpu would need up to 64GB :(
[15:05:11] <nshepperd2> download more ram
[15:06:07] <nshepperd2> maybe i should finetune a gpt-j on irc logs again
[15:06:21] <nshepperd2> see if there's substantial improvement from gpt-2
[15:16:28] <shawwwn> feep[work]: I think you need 1-by-50257 for the current token, and n-by-50257-by-n_heads-by-n_embd//n_heads for each K, V activation. Then you can swap each model param in from disk, if you want
[15:17:17] <shawwwn> so the theoretical minimum memory requirement is probably <100MB depending on how crazy you are
[15:18:09] <shawwwn> to actually *do* that, though, you'd need to get jax working on your system and start figuring out how to load tensors on demand during the forward pass
[15:18:44] <shawwwn> and by that I mean, simply "run jax", since that's already a description of how virtual memory works :P
[15:20:57] <shawwwn> nshepperd2: I'm writing up a thread on what you taught me. I was about to write "the theoretical minimum memory requirement for forward mode is sizeof(float32)*2, since you're computing each parameter one by one, and you only need memory for the parameter and its dual." But that can't be right. I need to account for the size of each activation, don'tI?
[15:22:02] <shawwwn> would it be true to say that the memory requirement is the maximum activation size during the forward pass? e.g. the activation with the biggest shape, times sizeof(float32), times 2 (because dual)
[15:22:04] <nshepperd2> it's more like sizeof(float32)*2*(the width of the most thicc layer)
[15:22:31] <shawwwn> hm.. why width, instead of np.prod(shape)?
[15:22:43] <nshepperd2> yes that's what i meant
[15:22:48] <shawwwn> ah
[15:23:39] <shawwwn> but the activation eventually collapses down to a shape of 1 (the loss output) right?
[15:23:55] <nshepperd2> yes
[15:24:06] <shawwwn> and at that point, the dual contains the gradient of the param wrt loss
[15:24:38] <shawwwn> does that mean each activation also has to have a dual, though?
[15:24:44] <nshepperd2> with forward mode, no, it wouldn't be the gradient
[15:24:56] <shawwwn> hm
[15:25:08] <nshepperd2> it would be a single element of the gradient
[15:25:38] <nshepperd2> corresponding to whichever dual vector you original initialized to 1 (with the rest of them set to 0)
[15:25:38] <shawwwn> oh, terminoogy fail. So the gradient is "the derivative of each parameter with respect to the loss"
[15:25:55] <shawwwn> I meant "gradient" as "the gradient of the current parameter wrt loss"
[15:26:08] <shawwwn> oh
[15:26:12] <nshepperd2> gradient = "the vector of derivatives of the loss with respect to each parameter"
[15:26:12] <PapuaHardyNet> shawwwn: I just want to confirm, did you claim that jax can run GPT-J on <100MB RAM theoretically?
[15:26:40] <shawwwn> PapuaHardyNet: if you're willing to swap parameters in from disk on demand, sure
[15:26:52] <PapuaHardyNet> huh, isn't that the same as using swap
[15:27:03] <PapuaHardyNet> except you are handling it instead of the OS
[15:27:09] <shawwwn> yes :)
[15:27:36] <nshepperd2> yes that's "it can run on <100MB" roughly in the same sense that "you can swim across the ocean"
[15:27:56] <shawwwn> well, for 16 tokens of output it shouldn't be too bad
[15:28:09] <kuudes> ocean is not very deep when near shore :D
[15:28:25] <shawwwn> or maybe it might be, since that's swapping 24GB per forward pass
[15:29:11] <shawwwn> (you can do a bit better by loading the bf16 weights and decoding them to fp32 once they're in memory, so, 12GB per forward pass)
[15:29:36] <nshepperd2> i suspect you might be better off swapping things to the NAS's memory over the network than to disk
[15:30:20] <kuudes> hmm, are there cheap usb ramdisks?
[15:31:43] <shawwwn> nshepperd2: does every operation with a dual result in a dual?  so therefore every activation would need 2*sizeof(float32), not just sizeof(float32). I was trying to think of some clever way of eliminating most of the dual overhead, since only one of the duals is initialized to 1
[15:31:57] <shawwwn> but I guess that's impossible
[15:32:45] <nshepperd2> yes, every operation involving a dual will result in a dual
[15:33:49] <nshepperd2> and just like my example with the attention block in backward-mode, all activations will be duals very quickly since they are affected by whichever parameter it is
[15:34:28] *** nshepper1 is now known as nshepperd
[15:35:13] <shawwwn> and makes no sense anyhow, since the dual part certainly doesn't stay 1 throughout the computation. never mind. (that's also a nice proof of why you need m passes though -- each activation is a dual wrt a *single* parameter, so if each block has a 1924x1600 activation, it needs those 1024x1600 duals in order to represent the derivative of a *single* parameter. So it needs m of those to do all the parameters, or 1 for one parameter 
[15:35:13] <shawwwn> at a time)
[15:35:22] <shawwwn> yeah. cool
[15:40:24] <nshepperd2> although be aware that the minimum memory will actually be more than the size of the biggest 'activation' if your network has a more complex topology than just a bunch of layers stacked on top of each other. like a resnet layer, resnet(x) = x + convolutions(x) or whateve
[15:41:13] <nshepperd2> the resnet layer needs to keep x in memory plus whatever the memory usage is for the calculation of the convolutions, obviously
[16:14:14] *** Quits: gwern (~gwern@user/gwern) (Ping timeout: 252 seconds)
[16:16:26] *** Quits: superz (~superegg@user/superegg) (Ping timeout: 252 seconds)
[16:16:26] *** Quits: koisoke (xef4@epilogue.org) (Ping timeout: 252 seconds)
[16:21:56] *** Quits: Dyo (~Dyo@host.sedf.de) (*.net *.split)
[16:21:56] *** Quits: Tene (~tene@mail.digitalkingdom.org) (*.net *.split)
[16:21:56] *** Quits: momus (~momus@ec2-18-195-22-48.eu-central-1.compute.amazonaws.com) (*.net *.split)
[16:21:56] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (*.net *.split)
[16:21:56] *** Quits: ua_ (~ua@public-gprs232059.centertel.pl) (*.net *.split)
[16:21:56] *** Quits: sm2n (~sm2n@user/sm2n) (*.net *.split)
[16:21:56] *** Quits: Robomot (~Robomot@user/robomot) (*.net *.split)
[16:21:56] *** Quits: nanotube (~nanotube@user/nanotube) (*.net *.split)
[16:21:56] *** Quits: nshepperd2 (~nshepperd@li364-218.members.linode.com) (*.net *.split)
[16:21:56] *** Quits: darklambda (~whateverg@111.65.56.56) (*.net *.split)
[16:21:56] *** Quits: niko (~niko@libera/staff/niko) (*.net *.split)
[16:21:56] *** Quits: vrs (~vrs@user/vrs) (*.net *.split)
[16:21:56] *** Quits: kuudes (~kuudes@user/kuudes) (*.net *.split)
[16:21:56] *** Quits: dbohdan2 (~dbohdan@user/dbohdan) (*.net *.split)
[16:21:57] *** Quits: espes (~espes@183.106.189.35.bc.googleusercontent.com) (*.net *.split)
[16:21:57] *** Quits: thoros (~thoros@80-121-140-225.adsl.highway.telekom.at) (*.net *.split)
[16:21:57] *** Quits: kaizen_ (sid60510@id-60510.tinside.irccloud.com) (*.net *.split)
[16:21:57] *** Quits: srijan (sid19575@id-19575.tinside.irccloud.com) (*.net *.split)
[16:21:57] *** Quits: Absalom (thelounge@envs.net) (*.net *.split)
[16:21:58] *** Quits: capisce (srodal@user/capisce) (*.net *.split)
[16:21:58] *** Quits: Guest9286 (~lucentmat@2001:470:69fc:105::dec) (*.net *.split)
[16:21:58] *** Quits: rxcomm (~rxcomm@user/rxcomm) (*.net *.split)
[16:21:59] *** Quits: soapes (~soapes@46-126-108-131.dynamic.hispeed.ch) (*.net *.split)
[16:21:59] *** Quits: |dbotdan (~dbotdan@user/dbotdan) (*.net *.split)
[16:21:59] *** Quits: tributarian_ (sid381145@id-381145.brockwell.irccloud.com) (*.net *.split)
[16:21:59] *** Quits: Asterisk_ (~asterisk@69.195.134.172) (*.net *.split)
[16:21:59] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (*.net *.split)
[16:21:59] *** Quits: SomeBrashAtom_ (sid391184@id-391184.stonehaven.irccloud.com) (*.net *.split)
[16:21:59] *** Quits: milanandreew (uid500468@user/milanandreew) (*.net *.split)
[16:21:59] *** Quits: RiversHaveWings (sid16604@user/rivershavewings) (*.net *.split)
[16:21:59] *** Quits: Limbo (~limbo@user/limbo) (*.net *.split)
[16:21:59] *** Quits: Santzes (~Santzes@2400:8901::f03c:92ff:fe49:47a6) (*.net *.split)
[16:22:00] *** Quits: hellleshin (~talinck@108-225-123-172.lightspeed.cntmoh.sbcglobal.net) (*.net *.split)
[16:22:00] *** Quits: Lambda_86 (~Lambda_86@30.95.231.35.bc.googleusercontent.com) (*.net *.split)
[16:22:00] *** Quits: edwardk (sid47016@haskell/developer/edwardk) (*.net *.split)
[16:22:00] *** Quits: quanticle (~quanticle@user/quanticle) (*.net *.split)
[16:22:00] *** Quits: dbohdan (~dbohdan@user/dbohdan) (*.net *.split)
[16:22:00] *** Quits: martin02 (silas@hund.fs.lmu.de) (*.net *.split)
[16:22:00] *** Quits: rugan (~mich@srv1.demonlabs.be) (*.net *.split)
[16:22:00] *** Quits: luna-is-here (~quassel@2a02:908:f761:2a3c::a) (*.net *.split)
[16:22:00] *** Quits: Obormot\Arcturus (~obormot@user/obormot) (*.net *.split)
[16:22:01] *** Quits: Betawolf (~matthew@xn--bta-yla.net) (*.net *.split)
[16:22:01] *** Quits: SDr (~SDr@user/sdr) (*.net *.split)
[16:22:01] *** Quits: Obormot\Gaia (~obormot@user/obormot) (*.net *.split)
[16:22:01] *** Quits: dove (~jordan@li1158-85.members.linode.com) (*.net *.split)
[16:22:01] *** Quits: lool0 (34a62ba551@user/lool0) (*.net *.split)
[16:22:01] *** Quits: s0ph1a (sid246387@id-246387.brockwell.irccloud.com) (*.net *.split)
[16:22:01] *** Quits: d6e (~d6e@2604:880:a:6::a95) (*.net *.split)
[16:22:01] *** Quits: dTal (quassel@dragonbox/wizzard/dtal) (*.net *.split)
[16:22:02] *** Quits: energizer (~energizer@user/energizer) (*.net *.split)
[16:22:02] *** Quits: anderson (~ande@134.209.221.71) (*.net *.split)
[16:22:02] *** Quits: jrayhawk (~jrayhawk@user/jrayhawk) (*.net *.split)
[16:22:02] *** Quits: lizzie (~alex@meowface.org) (*.net *.split)
[16:22:02] *** Quits: catern (~sbaugh@2604:2000:8fc0:b:a9c7:866a:bf36:3407) (*.net *.split)
[16:22:03] *** Quits: galambo__ (galambo@user/galambo) (*.net *.split)
[16:22:03] *** Quits: Obormot\Sirius (~obormot@user/obormot) (*.net *.split)
[16:22:03] *** Quits: feep[work] (~mathis@217.64.163.97) (*.net *.split)
[16:22:03] *** Quits: Gurkenglas (~Gurkengla@dslb-002-203-144-156.002.203.pools.vodafone-ip.de) (*.net *.split)
[16:22:03] *** Quits: feepbot (~feepbot@ppp-93-104-182-211.dynamic.mnet-online.de) (*.net *.split)
[16:22:03] *** Quits: srhm (~ascii@user/srhm) (*.net *.split)
[16:22:03] *** Quits: brand0 (~brandon@user/brand0) (*.net *.split)
[16:22:03] *** Quits: topdownjimmy (~topdownji@user/topdownjimmy) (*.net *.split)
[16:22:03] *** Quits: cannedprimates_ (sid16585@id-16585.tinside.irccloud.com) (*.net *.split)
[16:22:03] *** Quits: feep (~feep@2001:a61:3474:4a01:43f0:91af:ee19:5551) (*.net *.split)
[16:22:03] *** Quits: mgxm (~mgxm@user/mgxm) (*.net *.split)
[16:22:03] *** Quits: edf0_ (edef@panther.nathan7.eu) (*.net *.split)
[16:22:03] *** Quits: mst (~matthewt@vps-6c71e929.vps.ovh.net) (*.net *.split)
[16:22:03] *** Quits: CoJaBo (~aztec@user/CoJaBo) (*.net *.split)
[16:22:04] *** Quits: Pent (sid313808@id-313808.tooting.irccloud.com) (*.net *.split)
[16:22:04] *** Quits: PlanckWalk (~Planck@user/planckwalk) (*.net *.split)
[16:22:04] *** Quits: otoburb (~otoburb@user/otoburb) (*.net *.split)
[16:22:04] *** Quits: gry (~quassel@botters/gry) (*.net *.split)
[16:22:04] *** Quits: GvP (~GvP@ip70-162-85-176.ph.ph.cox.net) (*.net *.split)
[16:22:04] *** Quits: namespace_ (~op@2601:601:8180:5440:6e01:22f9:a0ba:536b) (*.net *.split)
[16:22:04] *** Quits: StathisA (~StathisA@80.107.84.195) (*.net *.split)
[16:22:04] *** Quits: flgr (sid200704@tinside.irccloud.com) (*.net *.split)
[16:22:04] *** Quits: dv^_^ (~cat@80-42-10-216.dynamic.dsl.as9105.com) (*.net *.split)
[16:22:04] *** Quits: V (~v@anomalous.eu) (*.net *.split)
[16:22:04] *** Quits: Mateon1 (~Thunderbi@user/mateon1) (*.net *.split)
[16:22:04] *** Quits: Nivim (~Nivim@174-24-72-253.clsp.qwest.net) (*.net *.split)
[16:22:04] *** Quits: koolazer (~koo@user/koolazer) (*.net *.split)
[16:22:04] *** Quits: synthmeat (~synthmeat@user/synthmeat) (*.net *.split)
[16:22:04] *** Quits: boxscape (~boxscape@user/boxscape) (*.net *.split)
[16:22:05] *** Quits: kanzure (~kanzure@user/kanzure) (*.net *.split)
[16:22:05] *** Quits: mortehu_ (mortehu@178.79.163.96) (*.net *.split)
[16:22:05] *** Quits: UniquelyGeneric (~karma@cpe-68-173-154-253.nyc.res.rr.com) (*.net *.split)
[16:22:05] *** Quits: chromis (sid150893@id-150893.tinside.irccloud.com) (*.net *.split)
[16:22:05] *** Quits: meowcat (~flynn@135-180-44-47.fiber.dynamic.sonic.net) (*.net *.split)
[16:22:05] *** Quits: pie_ (~pie_bnc@user/pie/x-2818909) (*.net *.split)
[16:22:05] *** Quits: xkapastel (~user@li1019-193.members.linode.com) (*.net *.split)
[16:22:05] *** Quits: saturn2 (~visitant@user/clone-of-saturn/x-1551297) (*.net *.split)
[16:22:05] *** Quits: linear (~line@autogen.ai) (*.net *.split)
[16:22:05] *** Quits: kiboneu (~kiboneu@ec2-3-87-189-227.compute-1.amazonaws.com) (*.net *.split)
[16:22:05] *** Quits: MatrixTravelerbo (~voyagert2@2001:470:69fc:105::22) (*.net *.split)
[16:22:06] *** Quits: Ralith (~ralith@2001:470:69fc:105::776) (*.net *.split)
[16:22:06] *** Quits: apoc (~apoc@user/apoc) (*.net *.split)
[16:22:06] *** Quits: ToyKeeper (~ToyKeeper@user/toykeeper) (*.net *.split)
[16:22:06] *** Quits: Kol (~Kol@d75-157-122-186.bchsia.telus.net) (*.net *.split)
[16:22:06] *** Quits: corby (~corby@c0rby.de) (*.net *.split)
[16:22:06] *** Quits: vorpalhex (sid421573@id-421573.tooting.irccloud.com) (*.net *.split)
[16:22:06] *** Quits: rmmh (~none@user/rmmh) (*.net *.split)
[16:22:06] *** Quits: nshepperd (nshepperd@2600:3c03::f03c:92ff:fe28:92c9) (*.net *.split)
[16:22:06] *** Quits: wilrnh (~wilrnh@235.83.74.34.bc.googleusercontent.com) (*.net *.split)
[16:22:06] *** Quits: PapuaHardyNet (apotheosis@tilde.institute) (*.net *.split)
[16:22:07] *** Quits: enterprisey (~enterpris@wikipedia/enterprisey) (*.net *.split)
[16:22:07] *** Quits: ivan (~ivan@user/ivan) (*.net *.split)
[16:22:07] *** Quits: Snuupy (~Snuupy@user/snuupy) (*.net *.split)
[16:22:07] *** Quits: cacheyourdreams (~cache@user/cacheyourdreams) (*.net *.split)
[16:22:07] *** Quits: boubacious (~boubaciou@h-213-164-204-45.NA.cust.bahnhof.se) (*.net *.split)
[16:22:07] *** Quits: ptrcmd (~ptrcmd@user/ptrcmd) (*.net *.split)
[16:22:07] *** Quits: inara (~inara@static.38.6.217.95.clients.your-server.de) (*.net *.split)
[16:22:07] *** Quits: TheWhisper (~TheWhispe@user/thewhisper) (*.net *.split)
[16:22:07] *** Quits: Jach (~Jach@2601:600:8600:307:7e31:e7a7:f1f3:a96b) (*.net *.split)
[16:22:08] *** Quits: arfa (~root@static.50.58.9.5.clients.your-server.de) (*.net *.split)
[16:22:08] *** Quits: pompolic (~bruh@91-137-144-5.opticon.hu) (*.net *.split)
[16:22:08] *** Quits: Mithaldu (sid27181@stonehaven.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: acertain (sid470584@stonehaven.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: Baughn (sid153359@tinside.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: bildramer (~bildramer@2a02:587:6240:ea00:4d63:2874:babb:4f13) (*.net *.split)
[16:22:08] *** Quits: edef (~quassel@user/edef) (*.net *.split)
[16:22:08] *** Quits: travankor (travankor@user/travankor) (*.net *.split)
[16:22:08] *** Quits: dutchie (~dutchie@user/dutchie) (*.net *.split)
[16:22:08] *** Quits: cyberjunkie (~cyberjunk@wireguard/tunneler/cyberjunkie) (*.net *.split)
[16:22:08] *** Quits: redlizard (~redlizard@a111159.upc-a.chello.nl) (*.net *.split)
[16:22:08] *** Quits: esprimo_ (~a@108.61.211.108) (*.net *.split)
[16:22:08] *** Quits: aweinstock (~aweinstoc@cpe-67-248-65-250.nycap.res.rr.com) (*.net *.split)
[16:22:08] *** Quits: rsaarelm (rsaarelm@hilla.kapsi.fi) (*.net *.split)
[16:22:08] *** Quits: gbear605 (~gbear605@user/gbear605) (*.net *.split)
[16:22:08] *** Quits: foamy (sid25727@id-25727.tooting.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: moobar (sid171730@id-171730.charlton.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: potatope (sid139423@id-139423.tooting.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: ggreer (sid252791@id-252791.brockwell.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: pdg (sid395042@id-395042.tooting.irccloud.com) (*.net *.split)
[16:22:08] *** Quits: ecx (~user@user/ecx) (*.net *.split)
[16:25:47] *** Quits: izica[m] (~izicamatr@2001:470:69fc:105::ce9c) (Ping timeout: 252 seconds)
[16:27:43] *** Joins: arfa (~root@static.50.58.9.5.clients.your-server.de)
[16:27:43] *** Joins: Jach (~Jach@2601:600:8600:307:7e31:e7a7:f1f3:a96b)
[16:27:43] *** Joins: TheWhisper (~TheWhispe@user/thewhisper)
[16:27:43] *** Joins: inara (~inara@static.38.6.217.95.clients.your-server.de)
[16:27:43] *** Joins: ptrcmd (~ptrcmd@user/ptrcmd)
[16:27:43] *** Joins: boubacious (~boubaciou@h-213-164-204-45.NA.cust.bahnhof.se)
[16:27:43] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[16:27:43] *** Joins: gwern (~gwern@user/gwern)
[16:27:43] *** Joins: Santzes (~Santzes@2400:8901::f03c:92ff:fe49:47a6)
[16:27:43] *** Joins: RiversHaveWings (sid16604@user/rivershavewings)
[16:27:43] *** Joins: milanandreew (uid500468@user/milanandreew)
[16:27:43] *** Joins: SomeBrashAtom_ (sid391184@id-391184.stonehaven.irccloud.com)
[16:27:43] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[16:27:43] *** Joins: tributarian_ (sid381145@id-381145.brockwell.irccloud.com)
[16:27:43] *** Joins: Asterisk_ (~asterisk@69.195.134.172)
[16:27:43] *** Joins: |dbotdan (~dbotdan@user/dbotdan)
[16:27:43] *** Joins: soapes (~soapes@46-126-108-131.dynamic.hispeed.ch)
[16:27:43] *** Joins: rxcomm (~rxcomm@user/rxcomm)
[16:27:43] *** Joins: Guest9286 (~lucentmat@2001:470:69fc:105::dec)
[16:27:43] *** Joins: superz_ (~superegg@user/superegg)
[16:27:43] *** Joins: alexbobp_ (~alex@meowface.org)
[16:27:43] *** Joins: koisoke_ (xef4@epilogue.org)
[16:27:43] *** Joins: TC (~talinck@108-225-123-172.lightspeed.cntmoh.sbcglobal.net)
[16:27:43] *** Joins: Dyo (~Dyo@host.sedf.de)
[16:27:43] *** Joins: galambo__ (galambo@user/galambo)
[16:27:43] *** Joins: Obormot\Sirius (~obormot@user/obormot)
[16:27:43] *** Joins: thoros (~thoros@80-121-140-225.adsl.highway.telekom.at)
[16:27:43] *** Joins: Tene (~tene@mail.digitalkingdom.org)
[16:27:43] *** Joins: momus (~momus@ec2-18-195-22-48.eu-central-1.compute.amazonaws.com)
[16:27:43] *** Joins: cannedprimates_ (sid16585@id-16585.tinside.irccloud.com)
[16:27:43] *** Joins: kaizen_ (sid60510@id-60510.tinside.irccloud.com)
[16:27:43] *** Joins: srijan (sid19575@id-19575.tinside.irccloud.com)
[16:27:43] *** Joins: Lord_of_Life (~Lord@user/lord-of-life/x-2819915)
[16:27:43] *** Joins: ua_ (~ua@public-gprs232059.centertel.pl)
[16:27:43] *** Joins: feep[work] (~mathis@217.64.163.97)
[16:27:43] *** Joins: sm2n (~sm2n@user/sm2n)
[16:27:43] *** Joins: Gurkenglas (~Gurkengla@dslb-002-203-144-156.002.203.pools.vodafone-ip.de)
[16:27:43] *** Joins: Robomot (~Robomot@user/robomot)
[16:27:43] *** Joins: feepbot (~feepbot@ppp-93-104-182-211.dynamic.mnet-online.de)
[16:27:43] *** Joins: feep (~feep@2001:a61:3474:4a01:43f0:91af:ee19:5551)
[16:27:43] *** Joins: Absalom (thelounge@envs.net)
[16:27:43] *** Joins: nanotube (~nanotube@user/nanotube)
[16:27:43] *** Joins: nshepperd (nshepperd@2600:3c03::f03c:92ff:fe28:92c9)
[16:27:43] *** Joins: nshepperd2 (~nshepperd@li364-218.members.linode.com)
[16:27:43] *** Joins: srhm (~ascii@user/srhm)
[16:27:43] *** Joins: luna-is-here (~quassel@2a02:908:f761:2a3c::a)
[16:27:43] *** iridium.libera.chat sets mode: +v Robomot
[16:27:43] *** Joins: brand0 (~brandon@user/brand0)
[16:27:43] *** Joins: topdownjimmy (~topdownji@user/topdownjimmy)
[16:27:43] *** Joins: wilrnh (~wilrnh@235.83.74.34.bc.googleusercontent.com)
[16:27:43] *** Joins: darklambda (~whateverg@111.65.56.56)
[16:27:43] *** Joins: PapuaHardyNet (apotheosis@tilde.institute)
[16:27:43] *** Joins: Obormot\Arcturus (~obormot@user/obormot)
[16:27:43] *** Joins: Ralith (~ralith@2001:470:69fc:105::776)
[16:27:43] *** Joins: niko (~niko@libera/staff/niko)
[16:27:43] *** Joins: vrs (~vrs@user/vrs)
[16:27:43] *** Joins: kuudes (~kuudes@user/kuudes)
[16:27:43] *** Joins: dbohdan2 (~dbohdan@user/dbohdan)
[16:27:43] *** Joins: kanzure (~kanzure@user/kanzure)
[16:27:43] *** Joins: espes (~espes@183.106.189.35.bc.googleusercontent.com)
[16:27:43] *** Joins: mortehu_ (mortehu@178.79.163.96)
[16:27:43] *** Joins: mgxm (~mgxm@user/mgxm)
[16:27:43] *** Joins: apoc (~apoc@user/apoc)
[16:27:43] *** Joins: ToyKeeper (~ToyKeeper@user/toykeeper)
[16:27:43] *** Joins: edf0_ (edef@panther.nathan7.eu)
[16:27:43] *** Joins: CoJaBo (~aztec@user/CoJaBo)
[16:27:43] *** Joins: mst (~matthewt@vps-6c71e929.vps.ovh.net)
[16:27:43] *** Joins: capisce (srodal@user/capisce)
[16:27:43] *** Joins: pompolic (~bruh@91-137-144-5.opticon.hu)
[16:27:43] *** Joins: Mithaldu (sid27181@stonehaven.irccloud.com)
[16:27:43] *** Joins: acertain (sid470584@stonehaven.irccloud.com)
[16:27:43] *** Joins: Betawolf (~matthew@xn--bta-yla.net)
[16:27:43] *** Joins: Pent (sid313808@id-313808.tooting.irccloud.com)
[16:27:43] *** Joins: PlanckWalk (~Planck@user/planckwalk)
[16:27:43] *** Joins: ivan (~ivan@user/ivan)
[16:27:43] *** Joins: UniquelyGeneric (~karma@cpe-68-173-154-253.nyc.res.rr.com)
[16:27:43] *** Joins: otoburb (~otoburb@user/otoburb)
[16:27:43] *** Joins: chromis (sid150893@id-150893.tinside.irccloud.com)
[16:27:43] *** Joins: Snuupy (~Snuupy@user/snuupy)
[16:27:43] *** Joins: gry (~quassel@botters/gry)
[16:27:43] *** Joins: GvP (~GvP@ip70-162-85-176.ph.ph.cox.net)
[16:27:43] *** Joins: namespace_ (~op@2601:601:8180:5440:6e01:22f9:a0ba:536b)
[16:27:43] *** Joins: StathisA (~StathisA@80.107.84.195)
[16:27:43] *** Joins: Baughn (sid153359@tinside.irccloud.com)
[16:27:43] *** Joins: flgr (sid200704@tinside.irccloud.com)
[16:27:43] *** Joins: meowcat (~flynn@135-180-44-47.fiber.dynamic.sonic.net)
[16:27:43] *** Joins: bildramer (~bildramer@2a02:587:6240:ea00:4d63:2874:babb:4f13)
[16:27:43] *** Joins: dv^_^ (~cat@80-42-10-216.dynamic.dsl.as9105.com)
[16:27:43] *** Joins: edef (~quassel@user/edef)
[16:27:43] *** Joins: V (~v@anomalous.eu)
[16:27:43] *** Joins: Mateon1 (~Thunderbi@user/mateon1)
[16:27:43] *** Joins: Nivim (~Nivim@174-24-72-253.clsp.qwest.net)
[16:27:43] *** Joins: pie_ (~pie_bnc@user/pie/x-2818909)
[16:27:43] *** Joins: SDr (~SDr@user/sdr)
[16:27:43] *** Joins: xkapastel (~user@li1019-193.members.linode.com)
[16:27:43] *** Joins: travankor (travankor@user/travankor)
[16:27:43] *** Joins: Kol (~Kol@d75-157-122-186.bchsia.telus.net)
[16:27:43] *** Joins: Obormot\Gaia (~obormot@user/obormot)
[16:27:43] *** Joins: dutchie (~dutchie@user/dutchie)
[16:27:43] *** Joins: cyberjunkie (~cyberjunk@wireguard/tunneler/cyberjunkie)
[16:27:43] *** Joins: redlizard (~redlizard@a111159.upc-a.chello.nl)
[16:27:43] *** Joins: esprimo_ (~a@108.61.211.108)
[16:27:43] *** Joins: dove (~jordan@li1158-85.members.linode.com)
[16:27:43] *** Joins: koolazer (~koo@user/koolazer)
[16:27:43] *** Joins: synthmeat (~synthmeat@user/synthmeat)
[16:27:43] *** Joins: saturn2 (~visitant@user/clone-of-saturn/x-1551297)
[16:27:43] *** Joins: aweinstock (~aweinstoc@cpe-67-248-65-250.nycap.res.rr.com)
[16:27:43] *** Joins: cacheyourdreams (~cache@user/cacheyourdreams)
[16:27:43] *** Joins: linear (~line@autogen.ai)
[16:27:43] *** Joins: lool0 (34a62ba551@user/lool0)
[16:27:43] *** Joins: ecx (~user@user/ecx)
[16:27:43] *** Joins: pdg (sid395042@id-395042.tooting.irccloud.com)
[16:27:43] *** Joins: ggreer (sid252791@id-252791.brockwell.irccloud.com)
[16:27:43] *** Joins: potatope (sid139423@id-139423.tooting.irccloud.com)
[16:27:43] *** Joins: moobar (sid171730@id-171730.charlton.irccloud.com)
[16:27:43] *** Joins: kiboneu (~kiboneu@ec2-3-87-189-227.compute-1.amazonaws.com)
[16:27:43] *** Joins: rmmh (~none@user/rmmh)
[16:27:43] *** Joins: vorpalhex (sid421573@id-421573.tooting.irccloud.com)
[16:27:43] *** Joins: corby (~corby@c0rby.de)
[16:27:43] *** Joins: foamy (sid25727@id-25727.tooting.irccloud.com)
[16:27:43] *** Joins: gbear605 (~gbear605@user/gbear605)
[16:27:43] *** Joins: rsaarelm (rsaarelm@hilla.kapsi.fi)
[16:27:43] *** Joins: dTal (quassel@dragonbox/wizzard/dtal)
[16:27:43] *** Joins: d6e (~d6e@2604:880:a:6::a95)
[16:27:43] *** Joins: s0ph1a (sid246387@id-246387.brockwell.irccloud.com)
[16:27:50] *** Joins: dbohdan (~dbohdan@user/dbohdan)
[16:27:50] *** Joins: Lambda_86 (~Lambda_86@30.95.231.35.bc.googleusercontent.com)
[16:27:50] *** Joins: edwardk (sid47016@haskell/developer/edwardk)
[16:27:50] *** Joins: quanticle (~quanticle@user/quanticle)
[16:27:50] *** Joins: martin02 (silas@hund.fs.lmu.de)
[16:27:50] *** Joins: rugan (~mich@srv1.demonlabs.be)
[16:27:59] *** Joins: energizer (~energizer@user/energizer)
[16:27:59] *** Joins: anderson (~ande@134.209.221.71)
[16:27:59] *** Joins: jrayhawk (~jrayhawk@user/jrayhawk)
[16:27:59] *** Joins: catern (~sbaugh@2604:2000:8fc0:b:a9c7:866a:bf36:3407)
[16:29:48] *** Quits: galambo__ (galambo@user/galambo) (Ping timeout: 268 seconds)
[16:30:01] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 248 seconds)
[16:30:57] *** Quits: Guest9286 (~lucentmat@2001:470:69fc:105::dec) (Ping timeout: 245 seconds)
[16:36:42] *** Joins: izica[m] (~izicamatr@2001:470:69fc:105::ce9c)
[16:45:39] *** Joins: enterprisey (~enterpris@wikipedia/enterprisey)
[17:04:42] *** Joins: Guest9286 (~lucentmat@2001:470:69fc:105::dec)
[17:13:12] <RiversHaveWings> nshepperd: not at all eheh
[17:17:35] <nshepperd> my dream lied to me~ hehe
[17:19:46] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[17:21:44] <nshepperd2> https://irc.zlkj.in/uploads/6a63f0c66a95af25/20210810123212_0.png "the portal to hell was discovered on a siberian plateau | oil painting"
[17:21:45] <Robomot> image/png (256x256; 189 KB)
[17:22:24] <nshepperd2> these samples are going to be good, i can tell
[17:23:27] <nshepperd2> https://irc.zlkj.in/uploads/bd0ceab2de8ce11a/20210810124913_0.png second one
[17:23:27] <Robomot> image/png (256x256; 206 KB)
[17:24:29] <feep[work]> nice :o
[17:25:24] <RiversHaveWings> nshepperd2: ooh!
[17:26:25] <nshepperd2> :)
[17:37:50] *** Joins: boxscape (~boxscape@user/boxscape)
[17:38:30] *** Quits: milanandreew (uid500468@user/milanandreew) (Quit: Connection closed for inactivity)
[17:54:55] *** Joins: MatrixTravelerbo (~voyagert2@2001:470:69fc:105::22)
[18:15:32] <shawwwn> nshepperd2: I've been trying to prove why dual numbers work. Assuming i^2 = 0, does this look correct? https://www.irccloud.com/pastebin/LQEH3pQ3/
[18:15:44] <feepbot> Snippet | IRCCloud
[18:17:22] <shawwwn> I think that shows that duals hold the result f(a) and its derivative f'(a) for all f
[18:17:29] <nshepperd2> yep, that's basically it
[18:17:35] <shawwwn> waow
[18:17:41] <shawwwn> look ma, I'm a mathematician
[18:17:48] <shawwwn> took a couple hours
[18:18:05] <shawwwn> thanks!
[18:20:05] *** Joins: milanandreew (uid500468@user/milanandreew)
[18:26:13] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[18:28:52] <[itchyjunk]> Hm
[18:29:35] <[itchyjunk]> So i watched 2 bots. 1 was cleaning with a brush tool and one with a mop tool. both seemed to be using infrared to detect objects and generally travelling in a straight line
[18:29:48] <[itchyjunk]> wouldn't it be better if they tried to map their environment ?
[18:30:06] <[itchyjunk]> Every time you run them, they do the whole path finding stuff
[18:30:32] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 258 seconds)
[18:30:50] <feep[work]> pathfinding is cheap
[18:31:13] <feep[work]> list of seasons according to gpt-2: summer, autumn, winter, spring, the middle age, fall, dark ages, aftermath, awakening
[18:33:38] <[itchyjunk]> hmmm
[18:34:03] <[itchyjunk]> cheap as in they dont need additional hardware other than the infrared ?
[18:34:26] <[itchyjunk]> Can't they store the info from last clean cycle and use that as map?
[18:35:00] <[itchyjunk]> the little bots work, they go around cleaning. was just surprised that it didn't seem to map its local area
[18:36:09] <feep[work]> I mean, the map doesn't really benefit if you're running continuously
[18:36:23] <feep[work]> though some cleaning bots do keep a map
[18:36:27] <feep[work]> my neato does~
[18:36:29] <[itchyjunk]> hmm
[18:36:34] <[itchyjunk]> ah i see
[18:36:48] <[itchyjunk]> i guess it would be more expensive bots that do that maybe
[18:36:48] <feep[work]> but that makes sense cause it *needs* a persistent map, cause I can define exclusion zones in it.
[18:37:08] <[itchyjunk]> right, the exclusion zones is exactly why i thought maps would be nice
[18:37:23] <feep[work]> I think it depends on how you built your software to start.
[18:37:38] <feep[work]> if you started out like "we don't need maps, we can just drive random" I can imagine it being hard to retrofit them in
[18:38:09] <[itchyjunk]> that makes sense
[18:38:34] <[itchyjunk]> will it head more towards maps in future? Or there is no obvious gain from maps that you wouldnt get from random algos?
[18:39:00] <[itchyjunk]> although! it was supposidly supposed to find it's way back to its charging place
[18:39:10] <[itchyjunk]> but when i was running it, it would just die at some point
[18:39:14] <feep[work]> XD
[18:39:20] <feep[work]> don't look at me, mine works like that :p
[18:49:30] *** Quits: bildramer (~bildramer@2a02:587:6240:ea00:4d63:2874:babb:4f13) (Ping timeout: 240 seconds)
[18:58:31] <gwern> shawwwn: so I was right in the first place, forward mode *is* the one that requires a pass per parameter
[18:58:32] <feepbot> gwern: shawwwn left a note 10 hours, 4 minutes ago: some conversation about "what's an output?" https://twitter.com/theshawwn/status/1424949317530439688
[18:58:32] <feepbot> gwern: shawwwn left a note 9 hours, 46 minutes ago: apparently every (or almost every?) loss function has one output, so therefore there are always more inputs than outputs, and backprop always wins...? https://twitter.com/theshawwn/status/1424953693502328852 On the other hand, "Some RL algorithms will do things like output an action and its value
[18:58:32] <feepbot> https://twitter.com/marksaroufim/status/1424954006380498947 "
[18:58:32] <feepbot> gwern: shawwwn left a note 8 hours, 27 minutes ago: lmao. the answer is, forward autodiff requires N forward passes, where N is total parameter count. Sorry for the notespam <3 https://twitter.com/theshawwn/status/1424973488130072578
[18:58:42] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 04:22): ‚ÄòThe reason that was confusing is because, in order to train G, you're still taking the derivative of each parameter of G w.r.t. the loss function. | But I guess that the image output is an intermediate step, so it does make sense. It'll just take some time to think that way.‚Äô
[18:58:44] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 04:40): ‚Äòlmao. | Thank you for walking me through this. I appreciate it greatly. | That was f'in hilarious though. "Oh yeah, all this stuff everyone's been talking about? Just set n=1 and you'll be right 99.9% of the time." And n=1 is far simpler, since it means backprop always wins :)‚Äô
[18:58:45] <|dbotdan> Mark Saroufim (@marksaroufim, 2021-08-10 04:41): ‚ÄòSome RL algorithms will do things like output an action and its value‚Äô
[18:58:45] <|dbotdan> Shawn Presser (@theshawwn, 2021-08-10 05:58): ‚Äò@austinvhuang was correct: forward autodiff requires one forward pass *per parameter*. (Thanks to @jekbradbury for also explaining this to me.) | Thank you also to @sea_snell @marksaroufim @sai_prasanna for the wonderful conversations!‚Äô
[19:02:16] <RiversHaveWings> "In a GAN, for example, I thought forward might be superior to backprop, because G outputs an image. But someone pointed out the loss function is w.r.t. the discriminator, which is a scalar." Could you even evaluate the Jacobian of G? Wouldn't it just be too huge to evaluate or do anything with?
[19:05:55] <nshepperd2> https://irc.zlkj.in/uploads/a1aecfd9f85d7989/20210810132316_0.png  https://irc.zlkj.in/uploads/1922c6c3dd979de7/20210810134017_0.png  https://irc.zlkj.in/uploads/042263459479c619/20210810141421_0.png more siberian hell portals
[19:05:56] <Robomot> image/png (256x256; 173 KB)
[19:05:56] <Robomot> image/png (256x256; 151 KB)
[19:05:56] <Robomot> image/png (256x256; 192 KB)
[19:09:51] <Obormot\Arcturus> Wow I just saw a tiny (less than 1cm) centipede in my room
[19:10:06] <Obormot\Arcturus> I didn't know they even came in sizes that small! So cute
[19:10:29] <feep[work]> is .. isn't that a millipede
[19:10:59] <Obormot\Arcturus> No a millipede is a totally unrelated creature
[19:11:11] <feep[work]> that makes sense I guess
[19:11:32] <Obormot\Arcturus> I would be very surprised to see a millipede in my room, they live in, like, forest floors and stuff
[19:12:09] <Obormot\Arcturus> BUT they are also very cute https://en.wikipedia.org/wiki/Pill_millipede
[19:12:10] <Robomot> Pill millipede - Wikipedia (Pill millipedes are any members of two living (and one extinct) orders of millipedes, often grouped together into a single superorder, Oniscomorpha. The name Oniscomorpha refers to the millipedes' resemblance to certain woodlice (Oniscidea), also called pillbugs or "roly-polies". ‚Ä¶)
[19:12:24] <feep[work]> ^^
[19:13:05] <Obormot\Arcturus> https://www.youtube.com/watch?v=TjRg-MuaATY look at how adorable
[19:13:06] <nshepperd2> https://irc.zlkj.in/uploads/9381246cb6a6d80e/20210810144029_0_suddenly.png "suddenly"
[19:13:06] <Robomot> MEET THE GIANT PILL MILLIPEDE, DOUG. - YouTube (Meet the Giant Pill Millipede, Doug. Often mistaken for a pill bug or roly poly, Doug is actually a type of millipede. Come along to learn a little about thi...)
[19:13:06] <Robomot> image/png (256x256; 184 KB)
[19:16:09] <Obormot\Arcturus> And some of them are fuzzy: https://bugguide.net/images/raw/4HD/HXH/4HDHXH1HEHLR9HJHXLAZGLAZUHHRNHAZIL8ZMLRR7LVZ8L1Z9HHR8LJHMHBH0LGZKLVHMHYHWHTHHL.jpg
[19:16:46] *** Joins: two2thehead (~AVX0@124.195.207.172)
[19:17:55] <nshepperd2> gosh. that is quite shiny too
[19:19:23] <two2thehead> kuudes, s0ph1a feep : how to visualize high drag coefficient 101 https://hw-videos.worldstarhiphop.com/u/vid/2019/10/pD6zxyQRfd3N.mp4
[19:24:19] *** Joins: Limbo (~limbo@user/limbo)
[19:26:32] <Obormot\Arcturus> Apples and cheese make a surprisingly good flavor combination. Right now I am having bread with butter and cheese (aged Havarti) on it, with applesauce. And I have also heard from multiple sources of people putting cheese (usually cheddar iirc) on their apple pies. Any other examples?
[19:26:37] *** Joins: galambo__ (galambo@user/galambo)
[19:27:39] <RiversHaveWings> https://twitter.com/Dominic2306/status/1425030946617806864
[19:27:44] <|dbotdan> Dominic Cummings (@Dominic2306, 2021-08-10 09:47): ‚ÄòAlso NB how irrelevant EU is. Viz state of the art in AI/data science, southern England & a few other European pockets contribute a bit, Eurozone is ~irrelevant. It's US & China dominated while Europe burbles about leading on 'ethics' & 'privacy' like total losers‚Äô Images: https://bird.trom.tf/pic/media%2FE8aM1qqXMA8ESv-.jpg%3Fname%3Dorig
[19:27:44] <|dbotdan> (description: chart, scatter chart; confidence: 0.92)
[19:29:22] <gwern> RiversHaveWings: it's particularly impressive when you consider schmidhuber
[19:29:39] <RiversHaveWings> Yes
[19:31:03] * nshepperd2 quickly makes a parody of that plot where every circle is just "Schmidhuber et al"
[19:31:50] <feep[work]> https://github.com/FeepingCreature/ddate/commit/a29cbc5ed0133ea3bf882ce05621725c4dab2658
[19:31:51] <Robomot> Last, last, last, last, last... ¬∑ FeepingCreature/ddate@a29cbc5 ¬∑ GitHub (The ddate source ripped out of util-linux. Contribute to FeepingCreature/ddate development by creating an account on GitHub.)
[19:32:01] <feep[work]> found a better format for gpt prompting: new season is up!
[19:32:58] <Obormot\Arcturus> "There is another reason why the future might be nothing like what we imagine‚Äîwho exactly is the ‚Äúwe‚Äù that is doing the imagining? Nerds."
[19:33:11] <nshepperd2> feep[work]: gosh, the winter solstice is here so soon
[19:33:16] <feep[work]> ^^
[19:38:59] <galambo__> is dominic cummings worth subscribing to?
[19:39:07] *** Quits: feep[work] (~mathis@217.64.163.97) (Ping timeout: 268 seconds)
[19:39:45] <two2thehead> galambo__, he falls under the Moldbug category for me
[19:40:08] <two2thehead> sharp systems guy, terrible at implementing policy
[19:42:12] <galambo__> two2thehead, he still seems much better at sorting things out than the average bureaucrat 
[19:42:20] <galambo__> how are the maldives?
[19:42:43] <two2thehead> it's alright
[19:43:07] <two2thehead> I think it's selection bias. Obviously he and moldbug aren't exact duplicates personality-wise, etc
[19:46:57] *** Joins: bildramer (~bildramer@2a02:587:623e:6900:2763:3c4c:f733:b6f6)
[19:49:05] *** Joins: galambo_ (~galambo@user/galambo)
[19:50:25] *** Joins: galambo (galambo@user/galambo)
[19:52:41] *** Quits: galambo__ (galambo@user/galambo) (Ping timeout: 268 seconds)
[19:53:55] *** Quits: galambo_ (~galambo@user/galambo) (Ping timeout: 268 seconds)
[19:54:32] *** Joins: nullcone (uid11626@id-11626.brockwell.irccloud.com)
[19:59:44] *** Joins: son0p (~ff@181.136.122.143)
[20:27:24] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[20:31:20] <Robomot> [Overcoming Bias] Seeking Status Fashion Stats - https://www.overcomingbias.com/2021/08/seeking-status-fashion-stats.html
[20:31:40] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 258 seconds)
[20:33:02] <Obormot\Arcturus> "And yet sometimes scifi does genuinely get out ahead of us. For another funny example, Reg Barclay looking up terrifying diseases in the Starfleet medical database because of his hypochondria. That was written in the days before webmd! And I'm sure that'll happen in whatever far future where medical databases, and baseline humans, exist."
[20:33:07] <Obormot\Arcturus> ""In the future, everyone will be like Reg Barclay" is the true dystopia that we were not prepared for  :("
[20:40:10] *** Quits: brand0 (~brandon@user/brand0) (Ping timeout: 268 seconds)
[21:02:32] *** Joins: brand0 (~brandon@user/brand0)
[21:07:51] <Obormot\Arcturus> Yet another poster on DSL offhandedly comments that Moldbug's writing is confusing... I have never understood people who say that
[21:08:37] <Obormot\Arcturus> Long-winded, yeah. Ornate, flowery, fanciful, totally. But *confusing*? Like, dude would routinely say the same thing like three or four times, in different ways, just to hammer it in for even the most dense reader
[21:08:50] <PapuaHardyNet> totally agree
[21:14:23] *** Joins: otthorn (otthorn@zamok.crans.org)
[21:16:57] <ggreer> ug see many words. ug feel confused
[21:19:21] <shawwwn> openai demo in 11 minutes: https://www.twitch.tv/openai cc: nshepperd2 RiversHaveWings  feep dbohdan kuudes PapuaHardyNet  but not gwern
[21:19:32] <feepbot> Twitch (OpenAI Codex (code-writing AI) Live Demo: 10am PT, Tues Aug 10th)
[21:20:00] <dbohdan> shawwwn: I'll check it out
[21:23:05] <feep> "but not gwern"? lol
[21:23:15] <feep> also thanks yay
[21:24:55] <dbohdan> shawwwn: You should have written "but not gw*rn" to avoid notifying him
[21:25:25] <shawwwn> ('tis the joke)
[21:25:31] <dbohdan> (I know)
[21:25:46] <shawwwn> <3
[21:25:56] <shawwwn> https://www.youtube.com/watch?v=Bd8_vO5zrjo&ab_channel=Montemayor
[21:26:07] <feepbot> The Battle of Midway 1942: Told from the Japanese Perspective (1/3) - YouTube
[21:30:30] <shawwwn> lol
[21:30:58] <kuudes> thanks shawwwn
[21:32:08] <two2thehead> danke shawwwn 
[21:32:21] <dbohdan> Nice
[21:33:00] <PapuaHardyNet> sweeet
[21:33:41] <dbohdan> (That "nice" was re: two2thehead's pun)
[21:34:20] * shawwwn signed up for the beta https://openai.com/join
[21:34:53] <dbohdan> Obormot\Arcturus: Surely not what that person meant, but I think it is fair to call Moldbug's oeuvre as a whole confusing.  Over time his positive, prescriptive views seem to shift away from SovCorps to a much less cool traditional monarchy, and there is not a clear dividing line.  I don't remember him going "yeah, I've changed my mind" anywhere on UR.
[21:35:41] <two2thehead> dbohdan, ?
[21:35:57] <two2thehead> these guys look like vat grown twins
[21:36:15] <PapuaHardyNet> openai grows their own developers
[21:36:28] <dbohdan> two2thehead: "danke shawwwn" was a play on "danke sch√∂n", no?
[21:36:30] <PapuaHardyNet> way better than using leetcode for ensuring quality
[21:36:44] <two2thehead> no, but if anyone asks, YES, YES IT WAS! :V
[21:36:46] <shawwwn> what's a schon?
[21:37:00] <dbohdan> two2thehead: Heh
[21:37:08] <dbohdan> Obormot\Arcturus: Land made this point in a tweet recently: https://twitter.com/Outsideness/status/1413008247603163141
[21:37:13] <two2thehead> "In German, the phrase Danke sch√∂n is equivalent to the English expressions 'thank you very much' or 'thank you kindly'."
[21:37:19] <two2thehead> Knew that sounded familiar
[21:37:23] <|dbotdan> Outsideness (@Outsideness, 2021-07-08 05:33): ‚ÄòThere's a Yarvin puzzle I can't click together whose central feature is Urbit being the practical demolition of the goofball monarchism at Gray Mirror.‚Äô
[21:38:48] <shawwwn> ha
[21:38:53] <Obormot\Arcturus> dbohdan: That's just him simplifying for the Substack audience
[21:40:13] <Obormot\Arcturus> dbohdan: As for Land's thing, well, that question would be a lot more pressing if Urbit, you know ... worked
[21:40:48] <dbohdan> Obormot\Arcturus: Maybe it's the target audience.  Land's explanation is more fun, though: https://twitter.com/Outsideness/status/1413039512268075011
[21:40:50] <|dbotdan> Outsideness (@Outsideness, 2021-07-08 07:37): ‚ÄòMy (very charitable) take is that he's stripping all the decentralism out of GM in order not to cast shade on Urbit.‚Äô
[21:41:28] <Obormot\Arcturus> Heh
[21:41:48] <dbohdan> Someone may have to remake Urbit with Scheme or Wasm in place of Nock for it succeed
[21:42:35] <PapuaHardyNet> this is amazing
[21:43:04] <shawwwn> he wrote "the current Bicoin", not "the current Bitcoin price"
[21:43:33] <two2thehead> should be interesting to see if it compensates
[21:43:42] <dbohdan> Bicoin is just Hetcoin + Homcoin
[21:44:10] <two2thehead> feep, I'll give my left nut if this can make the adder software we discussed
[21:44:22] <two2thehead> adders software framework/algorithm/whatever
[21:45:16] <feep> two2thehead: I forgot what we talked about
[21:45:23] <two2thehead> >_<
[21:45:32] <two2thehead> We'll talk about it later :V
[21:46:18] *** Quits: Mithaldu (sid27181@stonehaven.irccloud.com) (Ping timeout: 240 seconds)
[21:46:18] *** Quits: acertain (sid470584@stonehaven.irccloud.com) (Ping timeout: 240 seconds)
[21:46:28] * dbohdan . o O ( Make it query its own API!  Make it query its own API! )
[21:47:12] <PapuaHardyNet> dbohdan: you want to start the basilisk early, eh?
[21:47:27] <dbohdan> No point in waiting, really
[21:48:06] <two2thehead> okay, that's impressive
[21:48:23] <feep> yeah
[21:48:25] <kuudes> wow
[21:48:32] <feep> js is well documented and incremental, I think that's why this works
[21:48:36] <two2thehead> I'm reminded of how a story-boarder and an animator working together
[21:48:48] <dbohdan> feep: Imagine this with RPG Maker
[21:49:05] <two2thehead> dbohdan, exactly!
[21:49:08] *** Joins: oxide (~lambda@user/oxide)
[21:49:10] <feep> dbohdan: yess!
[21:49:18] <feep> codex-j when
[21:49:31] <kuudes> ask it to make a tetris
[21:49:33] <PapuaHardyNet> I wonder what hardware they are using in their backend
[21:52:46] * two2thehead claps
[21:53:05] <feep> yeah this is genuinely impressive
[21:53:16] <two2thehead> executable pseudocode
[21:53:36] <dbohdan> Oooh
[21:53:45] <dbohdan> "You can do it!"
[21:54:16] *** Joins: acertain (sid470584@stonehaven.irccloud.com)
[21:54:35] *** Quits: meowcat (~flynn@135-180-44-47.fiber.dynamic.sonic.net) (Quit: Lost terminal)
[21:55:35] *** Joins: Fusxfaranto (~Fusxfaran@cpe-75-85-179-208.san.res.rr.com)
[21:57:30] <PapuaHardyNet> what is monkaS exacctly
[21:57:41] <PapuaHardyNet> I keep reading it on the twitch chat
[21:58:25] <two2thehead> hmm
[21:58:29] <feep> literally google monkaS smiley
[21:58:36] <feep> it's people doing twitch wrong, possibly ironically
[21:59:04] <two2thehead> So code-x can scale on a digital system, while gpt3 only works on analogue hardware ie the mind. 
[21:59:48] <PapuaHardyNet> that's a really interesting use of Codex. Imagine if a Desktop Environment had a clear ApI
[22:00:06] <PapuaHardyNet> voice recognition + codex + DE API gives us a really nice interface
[22:00:23] <dbohdan> Cool demo!  Thanks for alerting me, shawwwn.
[22:00:27] <two2thehead> same
[22:00:56] <feep> same
[22:01:34] <PapuaHardyNet> yes, was really nice
[22:01:40] <kuudes> world's greatest unboxing video
[22:02:27] <PapuaHardyNet> imagine a demo like this is happening live when the world's first AGI decides that it is time to stop answering to humans
[22:02:53] <kuudes> hmm... wait, why did it hang
[22:02:55] <kuudes> :D
[22:04:48] <kuudes> the second demoed thing was emailing viewer provided addresses
[22:04:58] <kuudes> so yes, a great unboxing video
[22:05:01] *** Joins: Mithaldu (sid27181@stonehaven.irccloud.com)
[22:07:29] *** Joins: meowcat (~flynn@135-180-44-47.fiber.dynamic.sonic.net)
[22:17:15] *** Joins: Skarn (~skarn@user/skarn)
[22:18:06] <Obormot\Arcturus> gwern: https://www.gwern.net/Design#syntax-highlighting-algol stray newline here
[22:18:07] <Robomot> Design Of This Website ¬∑ Gwern.net [] (Meta page describing Gwern.net site implementation and experiments for better ‚Äòstructural reading‚Äô of hypertext; technical decisions using Markdown and static hosting.)
[22:27:35] <RiversHaveWings> https://twitter.com/RiversHaveWings/status/1425153609985978373
[22:27:42] <|dbotdan> Rivers Have Wings (@RiversHaveWings, 2021-08-10 17:54): ‚ÄòNew Colab notebook: CLIP Guided Diffusion HQ 512x512 | https://colab.research.google.com/drive/1V66mUeJbXrTuQITvJunvnWVn96FEbSI3‚Äô
[22:27:52] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[22:32:25] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 258 seconds)
[22:35:48] <RiversHaveWings> feep: ~
[22:36:04] <RiversHaveWings> This version still fits into 16GB GPUs
[22:36:33] <RiversHaveWings> I turned FP16 on for the 512x512 model
[22:36:43] <RiversHaveWings> It still seems to work
[22:42:51] <feep> sweet
[22:42:54] <feep> propose a prompt?
[22:44:03] <RiversHaveWings> "a clockwork angel, trending on ArtStation"
[22:44:26] <feep> alright
[22:49:54] <feep> RiversHaveWings: this run is gonna spend most of its time downloading the pt lol
[22:49:58] <RiversHaveWings> oh no
[22:50:08] <feep> 180KB/s thank you microsoft for that high throughput
[22:50:13] <RiversHaveWings> Wow
[22:50:22] <feep> that is not my connection, btw. that is microsoft.
[22:50:34] <RiversHaveWings> ...Did their server fall over
[22:50:44] <RiversHaveWings> It was reasonable for me when testing it repeatedly on Colab
[22:50:59] <feep> they probably have An Agreement
[22:51:21] <feep> anyway I'll just let it go
[22:51:26] <RiversHaveWings> :)
[22:51:26] <feep> and slep~
[23:01:22] *** Quits: brand0 (~brandon@user/brand0) (Ping timeout: 268 seconds)
[23:12:09] *** Quits: galambo (galambo@user/galambo) (Ping timeout: 248 seconds)
[23:15:16] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[23:23:45] *** Joins: mattil (~mattilinn@87-92-9-185.bb.dnainternet.fi)
[23:24:40] *** Joins: galambo (galambo@user/galambo)
[23:26:33] *** Quits: mattil (~mattilinn@87-92-9-185.bb.dnainternet.fi) (Quit: Leaving)
[23:27:52] *** Joins: Mithaldu_ (sid27181@id-27181.stonehaven.irccloud.com)
[23:28:03] *** Joins: acertain_ (sid470584@id-470584.stonehaven.irccloud.com)
[23:28:09] *** Quits: Mithaldu (sid27181@stonehaven.irccloud.com) (Ping timeout: 248 seconds)
[23:28:09] *** Mithaldu_ is now known as Mithaldu
[23:28:16] *** Joins: voltage_ (voltage@user/voltage)
[23:28:31] *** Quits: acertain (sid470584@stonehaven.irccloud.com) (Ping timeout: 268 seconds)
[23:28:31] *** acertain_ is now known as acertain
[23:33:30] *** Quits: thoros (~thoros@80-121-140-225.adsl.highway.telekom.at) (Quit: WeeChat 3.0.1)
[23:47:48] *** Quits: two2thehead (~AVX0@124.195.207.172) (Quit: Leaving)
[23:58:41] <shawwwn> dbohdan: <3
