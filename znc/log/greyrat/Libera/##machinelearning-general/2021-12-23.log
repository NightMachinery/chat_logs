[00:25:37] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 240 seconds)
[00:26:47] *** Joins: shoky (uuuggg@141.226.193.67)
[00:29:17] *** Quits: sinaowolabi_ (~SinaOwola@160.152.16.116) (Ping timeout: 240 seconds)
[00:32:14] <Klinda> thank you hodapp
[00:32:47] <Klinda> I mean for generative algorithms is inutitive to pick the one with most probability
[00:41:04] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[00:54:01] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[00:56:22] <Klinda> what are the basis to have to take a deep learning course ?
[00:59:25] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[01:10:27] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Ping timeout: 256 seconds)
[01:12:25] *** Joins: sinaowolabi_ (~SinaOwola@41.58.61.48)
[01:22:37] *** Joins: Donitz (~Donitz@88-115-149-65.elisa-laajakaista.fi)
[01:32:33] *** Quits: sinaowolabi_ (~SinaOwola@41.58.61.48) (Ping timeout: 256 seconds)
[01:41:21] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[01:45:24] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[01:49:17] *** Quits: Donitz (~Donitz@88-115-149-65.elisa-laajakaista.fi) (Ping timeout: 252 seconds)
[01:50:41] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[01:52:37] *** Quits: Gimble (~G1mble2@2a01:4b00:8e07:7900:e79c:6460:44ed:8423) (Quit: Leaving)
[01:52:43] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[02:08:33] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:09:26] *** Joins: maplefinger (~maplefing@user/lindis)
[02:09:56] *** Parts: maplefinger (~maplefing@user/lindis) ()
[02:11:24] *** Joins: black_13 (~jjosb@2600:1700:1100:61c0:d8a6:dd31:b400:d92)
[02:12:29] <black_13> would it be possible to use the or a character training set to just identify the border of a scanned character not the value of the character
[02:14:34] <hodapp> what sort of character training set?
[02:14:51] <hodapp> just cropped images of each character?
[02:15:29] <black_13> i have a scanned dictionary i want to try text segmentation
[02:16:26] <black_13> its a 19th century book
[02:16:41] <hodapp> are you wanting to use this for the training data?
[02:17:13] <black_13> you know i dont know exactly what i want to do
[02:17:39] <black_13> if the book was in better shape it would be easier to segment
[02:18:00] <black_13> some of the lines of text overlap
[02:18:08] <black_13> or just touch
[02:18:24] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[02:21:22] <hodapp> to be able to train over it you'd need the characters annotated
[02:33:50] <black_13> annotated?
[02:34:05] <black_13> you mean the characters in individual pieces
[02:34:13] <black_13> not words but chars
[02:36:03] <hodapp> if you're wanting to train it to recognize the borders of characters, then yes, you'd need those borders annotated
[02:36:20] <black_13> thanks
[02:47:43] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[03:08:37] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Ping timeout: 240 seconds)
[03:11:40] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:18:32] <dostoyevsky2> black_13: in OCR it's common to create a UNET model for figuring out where the characters are... e.g. mask-RCNN.  But you'd need to have at least all the characters available so you can generate random pages where you know where the characters are, as you've put them there with an algorithm
[03:24:59] *** Quits: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de) (Ping timeout: 252 seconds)
[04:03:36] <black_13> dostoyevsky2, https://ibb.co/JBFWsFm
[04:03:36] <SigmoidFroid> ⇒  heathsgermanengl00breu-0017 — ImgBB
[04:04:05] *** Quits: Guest4472 (~somewhere@122-199-43-68.ip4.superloop.com) (Quit: No Ping reply in 180 seconds.)
[04:04:16] <black_13> you can see how some of the characters touch
[04:05:19] *** Joins: etolier (~somewhere@122-199-43-68.ip4.superloop.com)
[04:05:43] *** etolier is now known as Guest6406
[04:08:49] <dostoyevsky2> black_13: you mean the ligatures?  They are actually one dedicated character, combining two characters
[04:09:25] <black_13> that is true its one touching down to the next line
[04:13:54] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[04:14:10] <black_13> if you look in the upper left corner you can see this
[04:25:29] *** Parts: Malvolio (~Malvolio@user/malvolio) ()
[04:32:17] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 240 seconds)
[04:34:22] *** Joins: shoky (uuuggg@141.226.193.67)
[04:40:59] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[05:03:24] *** Joins: Malvolio (~Malvolio@user/malvolio)
[05:10:44] *** Quits: Colt (~Colt@user/colt) (Remote host closed the connection)
[05:11:16] *** Joins: Colt (~Colt@user/colt)
[05:40:58] *** Quits: black_13 (~jjosb@2600:1700:1100:61c0:d8a6:dd31:b400:d92) (Quit: Leaving)
[07:25:11] *** Joins: miique (~miique@181.46.139.166)
[09:05:38] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[09:15:17] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 240 seconds)
[09:17:00] *** Joins: shoky (uuuggg@141.226.193.67)
[09:29:30] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[09:49:46] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Read error: No route to host)
[10:18:56] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[10:46:14] *** Quits: Colt (~Colt@user/colt) (Ping timeout: 260 seconds)
[10:46:41] *** Joins: Colt (~Colt@user/colt)
[10:50:43] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Ping timeout: 256 seconds)
[11:25:30] *** Joins: palasso (~palasso@user/palasso)
[11:55:53] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[12:03:13] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[12:26:13] *** Joins: Klinda (~superleag@user/klinda)
[12:32:15] *** Joins: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de)
[13:02:10] *** Joins: manti7 (~manti7@176.10.104.94)
[13:27:24] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:46:07] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-162.dhcp.inet.fi)
[13:46:57] *** Quits: rvalue (~rvalue@user/rvalue) (Ping timeout: 256 seconds)
[13:48:46] *** Joins: rvalue (~rvalue@user/rvalue)
[13:50:37] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Ping timeout: 240 seconds)
[13:52:23] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[14:46:52] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[15:16:17] *** Quits: miique (~miique@181.46.139.166) (Ping timeout: 240 seconds)
[15:31:36] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[16:12:51] *** Joins: `Tim (~zenguin@user/zenguin)
[16:23:02] *** Quits: sinaowolabi (~SinaOwola@41.58.61.48) (Ping timeout: 240 seconds)
[16:40:57] *** Joins: sinaowolabi (~SinaOwola@41.58.249.239)
[16:50:25] *** Joins: sinaowolabi_ (~SinaOwola@160.152.16.116)
[17:34:14] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: >_<!!! U_UzzZ)
[17:47:34] *** Joins: Malvolio (~Malvolio@user/malvolio)
[18:05:42] <Klinda> so in general in machine learning the y of one example just come from a probability distribution, the second example to another probability distribution? or all the y are related to the same distribution (bernoulli, poisson, gaussian) etc ?
[18:06:16] <Klinda> like one example (x,y), y come to a bernoulli
[18:06:31] <Klinda> secondo example (x,y) come to "another" bernoulli
[18:06:33] <Klinda> ?
[18:07:35] <Klinda> I think it's in this way because we assume that the examples are independent
[18:20:13] <mefistofeles> Klinda: not really sure what's making you think about these things that way, but ideally the distribution is the same
[18:20:58] <mefistofeles> in real-life it might change, technically, because you know how sampling/measuring things can actually affect the sample, but that's probably not what you are thinking about
[18:38:02] <Klinda> in generalized linear models, you get the parameter of the distribution each time you generate one y
[18:38:23] <Klinda> by doing theta transpose * x
[18:38:36] <Klinda> then learn theta
[18:38:37] <Klinda> output it
[18:38:45] <Klinda> and give that parameter to the distrubtion
[18:38:57] <Klinda> so the distribution change eveytime.
[18:39:05] <Klinda> that's what i think
[18:41:00] <Klinda> I mean changin the paramter of the distribution doesn't change the "distribution" ?
[18:42:10] <Klinda> you give one example, you calculate theta^T x, you learn theta with gradient descent or something, than that value is the parameter of a "common" distribution, like posson, bernoulli etc
[18:42:33] <Klinda> and there you output the mean of that distribution, in the end
[18:42:46] <Klinda> so that's your y
[18:43:18] <mefistofeles> Klinda: well, if the distribution changes, then yes, it's a different distribution
[18:43:22] <mefistofeles> trivially
[18:43:23] <mefistofeles> :P
[18:43:48] <Klinda> if the paramter change you mean?
[18:44:01] <mefistofeles> yes, but it seems that this is just semantics xD
[18:44:32] <Klinda> well I mean I should study the distributions in general, like a probability exam
[18:44:41] <Klinda> maybe I understand more
[19:02:06] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[19:10:35] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-162.dhcp.inet.fi) (Read error: Connection reset by peer)
[19:25:16] *** Joins: arjun (~arjun@user/arjun)
[19:33:52] *** Quits: arjun (~arjun@user/arjun) (Remote host closed the connection)
[19:46:11] <hodapp> Klinda: almost everything in machine learning and statistical learning is making the assumption that you are drawing from the same distribution
[19:46:45] <hodapp> if you are training on one distribution and testing on another (while assuming it's the same), that's covariate shift and it's a source of error unto itself
[19:46:51] *** Joins: georgios (~georgios@user/georgios)
[19:47:21] <hodapp> if the distributions are explicltly different, you're talking about things like transfer learning or domain adaptation, as sometimes this is the problem you have to solve
[19:49:58] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Ping timeout: 268 seconds)
[19:59:53] <Klinda> hodapp: you didn't understand well, I said it's the same distribution, but for each example it's always another instance, like one example Bernoulli 1, example 2 Bernoulli 2 ? or all refers to Bernoulli ?
[20:01:55] <Klinda> if you fit the particular parameter for each example, to the bernoulli distribution
[20:02:04] <Klinda> you are creeate everytime a new distribution?
[20:04:35] <HuntsMan> Klinda: sampling from a distribution creates new samples, not new distributions, the distribution is the same
[20:05:31] <mefistofeles> yes, what HuntsMan said, but also, of course, you CAN change the distributions (like do some backpropagation and stuff)
[20:05:55] <mefistofeles> so it really depends on your specific case. You don't *need* to do that, though.
[20:06:13] <HuntsMan> mefistofeles: I think the confusion comes from sampling, not from changing distributions, so better not mention that
[20:07:06] <mefistofeles> HuntsMan: ok
[20:14:10] *** Quits: sinaowolabi_ (~SinaOwola@160.152.16.116) (Remote host closed the connection)
[20:45:43] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 256 seconds)
[20:58:50] <Klinda> yes I understand, but I need to understand more HuntsMan
[21:00:12] <Klinda> because I think in the end we do the inverse problem, we have the samples, we just learn the best bernoulli by learning the parameter of it
[21:02:03] <Klinda> so everytime the parameter change, for each sample
[21:02:52] <HuntsMan> Klinda: you need to define the learning problem first
[21:03:03] <Klinda> we learn theta right?
[21:03:08] <Klinda> theta^T * x
[21:03:14] <HuntsMan> for which learning algorithm?
[21:03:27] <Klinda> generlized linear models, so every linear function
[21:03:32] <Klinda> logistic regression
[21:03:37] <Klinda> linear regression
[21:03:40] <HuntsMan> yeah you learn theta
[21:04:37] <Klinda> yes for each example you learn the theta, fit to a distribution, for example for binary classification you have bernoulli, for regression problems you have the gaussian
[21:04:39] <Klinda> right?
[21:05:18] <HuntsMan> I think you are confusing this with bayesian methods
[21:05:28] <HuntsMan> but I am not sure, you jump from one method to a different one
[21:05:38] <HuntsMan> in logistic regression, you do not learn a distribution over the parameters
[21:05:45] <Klinda> do you know how generlized linear models ?
[21:05:53] <Klinda> work?
[21:06:27] <Klinda> maybe I can give you a picture
[21:06:45] <HuntsMan> sure, but there the distributions are fixed
[21:07:13] <Klinda> yes by theta
[21:07:16] <Klinda> right?
[21:07:37] <HuntsMan> those can be distribution parameters
[21:07:51] <Klinda> you don't learn the parameters of the distribution
[21:07:54] <Klinda> only theta
[21:08:05] <HuntsMan> those can be distribution parameters <-
[21:08:38] <HuntsMan> then what distributions are you talking about?
[21:08:57] <Klinda> well if you have a classification problem, you have a bernoulli
[21:09:00] <HuntsMan> "we just learn the best bernoulli"? See? you just said you don't learn the parameters of the distribution
[21:09:05] <HuntsMan> a binary classification problem only
[21:09:13] <HuntsMan> these details matter a lot
[21:09:24] <Klinda> yes sorry to not mention that
[21:09:27] <Klinda> btw
[21:09:36] <Klinda> for each example
[21:09:46] <Klinda> you learn a different theta vector, right?
[21:09:59] <Klinda> no
[21:10:00] <Klinda> wait
[21:10:02] <Klinda> xD
[21:10:14] <Klinda> for each exapmple, you have the same theta vector
[21:10:20] <HuntsMan> no
[21:10:32] <HuntsMan> that is a fundamental misunderstanding
[21:11:21] *** Quits: sinaowolabi (~SinaOwola@41.58.249.239) (Ping timeout: 268 seconds)
[21:11:21] <Klinda> what?
[21:11:33] <Klinda> theta vector is the same, for every example
[21:11:52] <Klinda> you optimize it with gradient descent
[21:12:30] <Klinda> O^T * x^(i)
[21:12:33] <HuntsMan> for each example, you learn a different theta vector, right? <- NO!
[21:12:45] <Klinda> yes I corrected myself
[21:13:05] <Klinda> but for each example, you have a different parameter because of x^(i)
[21:13:53] <HuntsMan> no, its the same parameter vector
[21:14:27] <HuntsMan> the predictions are different given different x^(i)
[21:15:23] <Klinda> so I mean what you plung in the distributions ?
[21:15:27] <Klinda> *plug
[21:15:40] <HuntsMan> what distributions?
[21:15:42] <HuntsMan> see? you are confusing concepts
[21:15:48] <Klinda> I think you plug a different parameter for every example
[21:16:00] <Klinda> distributions = Poisson, Bernoulli, Gaussian...
[21:16:05] <HuntsMan> but for what?
[21:16:07] <HuntsMan> again, confusing concepts
[21:16:08] <Klinda> it doesn't matter
[21:16:11] <HuntsMan> it does
[21:16:16] <HuntsMan> because now you are making no sense
[21:16:49] <Klinda> I mean I am not understandng the concept in general if everytime is used a new instance of distrubtion or not
[21:17:09] <HuntsMan> distribution for what? are you talking about the distribution of the labels?
[21:17:35] <Klinda> because to predict y you take the mean of the distrubtion
[21:17:52] <HuntsMan> no, you don't have to
[21:18:01] <HuntsMan> that's not how you make predictions with logistic regression
[21:18:15] <Klinda> I am talking about genralized linear models
[21:18:26] <Klinda> so these are new models?
[21:18:50] <HuntsMan> you said logistic regression, which is a GLM
[21:19:33] <HuntsMan> what you can usually do is to output the PARAMETERS of the bernoulli distribution
[21:19:40] <Klinda> yes so why don't you agree that y is taken by E [ y ; O^T x^(i) ] ?
[21:19:57] <HuntsMan> because that makes no sense
[21:20:11] <Klinda> so the teacher of stanford is mad?
[21:20:25] <HuntsMan> ah, now magically some Stanford teacher comes into play
[21:20:33] <HuntsMan> should I guess what are you talking about?
[21:21:10] <HuntsMan> you keep giving your own interpretations of some lecture which might not be correct
[21:21:20] <Klinda> I just learn that from him
[21:22:17] <HuntsMan> sure, whatever
[21:22:55] <HuntsMan> you did not even link to this lecture
[21:22:57] <Klinda> yes maybe I didn't understand, but that's what I can say to you
[21:23:15] <Klinda> do you have time to read a 2 hour lesson?
[21:23:40] <HuntsMan> no, but I have time to look at a single slide
[21:24:32] <Klinda> https://www.youtube.com/watch?v=sj0iPn03i7Q&list=RDCMUCBa5G_ESCn8Yd4vw5U-gIcg&start_radio=1&rv=sj0iPn03i7Q&t=3168
[21:24:33] <SigmoidFroid> ⇒  Stanford CS229: Machine Learning | Summer 2019 | Lecture 6 - Exponential Family & GLM - YouTube, 01:54:49
[21:24:37] <Klinda> go to 51:00
[21:25:13] <Klinda> or aroud that
[21:25:15] <Klinda> xD
[21:25:45] <HuntsMan> by the way, claiming something to be correct because it is from Stanford is a fallacy
[21:26:12] <HuntsMan> this is exponential families, a slightly different topic
[21:28:03] <HuntsMan> here you are actually predicting a distribution, this is usually done by having a model that predicts the parameters of the distribution
[21:29:40] <Klinda> so in this model each example as its bernoulli ?
[21:30:00] <HuntsMan> what do you mean example here?
[21:30:21] <Klinda> at testing time
[21:30:35] <Klinda> one feature vector
[21:30:45] <HuntsMan> so it is the input x?
[21:31:01] <Klinda> yes
[21:31:15] <HuntsMan> then no
[21:31:22] <HuntsMan> the output of the model is a bernoulli distribution
[21:31:29] <HuntsMan> what you call example is the input of the model
[21:31:35] <Klinda> yes sorry
[21:31:53] <Klinda> so in the y is generated by a bernoulli... but how?
[21:32:02] <HuntsMan> I already said it
[21:32:09] <HuntsMan> you predict the parameters of the bernoulli distribution
[21:32:35] <Klinda> yes but in practice I don't know
[21:32:37] <Klinda> what happens
[21:32:57] <HuntsMan> it is just what I said
[21:33:10] <HuntsMan> probability distributions are abstract mathematical objects
[21:33:24] <HuntsMan> so I can understand they are mysterious to many
[21:33:32] <Klinda> but so for each example you have a different distribution ?
[21:33:42] <HuntsMan> no, its the same distribution, but different parameters
[21:33:50] <Klinda> and what does it means?
[21:33:54] <HuntsMan> do you know the parameters of the bernoulli distribution?
[21:34:23] <Klinda> yes is bernoulli(phi)
[21:34:34] <HuntsMan> and what is the parameter?
[21:34:44] <Klinda> phi
[21:35:04] <HuntsMan> yes but what does it mean? It has an interpretation
[21:35:16] <Klinda> is a probability of an event
[21:35:19] <Klinda> that is equal to
[21:35:26] <HuntsMan> p(y=1)
[21:36:00] <Klinda> P(y; phi) = phy ^y * (1-phi)^(1-y)
[21:36:13] <HuntsMan> that's the PDF
[21:36:21] <Klinda> yes
[21:37:13] <Klinda> so if is y = 0 you have the second, if y = 1 you have the first
[21:37:49] <Klinda> so in the what happens?
[21:38:00] <Klinda> you learn phi
[21:38:05] <HuntsMan> phi is the probability of y = 1
[21:38:32] <Klinda> ok
[21:38:41] <Klinda> so you learn the probability of y = 1
[21:39:00] <HuntsMan> yes
[21:39:17] <Klinda> so phi^1
[21:39:21] <Klinda> so that number
[21:39:43] <Klinda> and why is that my y ?
[21:39:45] <Klinda> idk
[21:39:56] <Klinda> it's not true too
[21:40:08] <Klinda> ah no wait
[21:40:12] <Klinda> you do
[21:40:40] <HuntsMan> nobody said that is your y
[21:40:42] <HuntsMan> you misunderstood
[21:40:53] <Klinda> so then what happens?
[21:41:28] <HuntsMan> nothing?
[21:41:32] <HuntsMan> I am not sure what you mean
[21:41:42] <Klinda> to output y
[21:41:49] <Klinda> you have to learn the parameter of a Bernoulli
[21:42:05] <Klinda> that is phi
[21:42:27] <Klinda> phi is our theta^T x^(i)
[21:42:34] <Klinda> optimized
[21:42:53] <HuntsMan> that is your output, the probability of y=1
[21:43:05] <HuntsMan> because you are outputting the parameters of the bernoulli distribution
[21:46:29] <Klinda> no I need to learn more
[21:46:32] <Klinda> I don't understnad
[21:46:35] <HuntsMan> yep
[21:46:40] <HuntsMan> about probability distributions
[21:46:56] *** Joins: `Tim (~zenguin@user/zenguin)
[21:48:05] <Klinda> I know for example how a bernoulli works, like if you have acoin that output HCHHCH
[21:48:16] <Klinda> and all that stuffs, but here we are predicting y
[21:48:18] <Klinda> xD
[21:50:01] <dostoyevsky2> Isn't y what you are training your prediction with and y_hat what the model inferred for your x?
[21:54:57] <Klinda> I usually see y hat in what is the final answer of an algorithm, probably yes
[21:56:23] <HuntsMan> AKA the prediction (which is a distribution here)
[21:58:38] <Klinda> I need to study math realated to machine learning to understand all better
[22:08:50] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[22:25:44] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[22:40:45] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Ping timeout: 256 seconds)
[22:41:01] <Klinda> HuntsMan: https://www.youtube.com/watch?v=sj0iPn03i7Q&list=RDCMUCBa5G_ESCn8Yd4vw5U-gIcg&start_radio=1&t=3168s look at that
[22:41:02] <SigmoidFroid> ⇒  Stanford CS229: Machine Learning | Summer 2019 | Lecture 6 - Exponential Family & GLM - YouTube, 01:54:49
[22:41:18] <Klinda> he's using for each example a gaussian distribution
[22:42:22] *** Joins: Donitz (~Donitz@88-115-149-215.elisa-laajakaista.fi)
[22:43:55] <Klinda> at 1:02
[22:49:03] *** Joins: sinaowolabi (~SinaOwola@41.58.53.5)
[22:49:08] <HuntsMan> Klinda: you can provide a link to the exact time
[22:50:08] <Klinda> https://youtu.be/sj0iPn03i7Q?t=3667
[22:50:09] <SigmoidFroid> ⇒  Stanford CS229: Machine Learning | Summer 2019 | Lecture 6 - Exponential Family & GLM - YouTube, 01:54:49
[22:50:11] <Klinda> this
[22:52:07] <HuntsMan> Klinda: so, what about this? we already discussed
[22:52:17] <Klinda> each time there is a gaussian
[22:52:23] <Klinda> for each example
[22:52:27] <Klinda> so different gaussians
[22:52:31] <Klinda> for every example
[22:52:40] <Klinda> so how y is inventend in this way
[22:52:55] <HuntsMan> what does "different gaussians" mean actually?
[22:53:08] <Klinda> you only compute the values of the line to get the y
[22:53:14] <HuntsMan> so?
[22:53:22] <Klinda> why there is a gaussian
[22:53:24] <HuntsMan> a gaussian distribution has two parameters, mean and variance
[22:53:25] <Klinda> to get the y
[22:53:28] <HuntsMan> let me say it again
[22:53:38] <HuntsMan> because you PREDICT THE PARAMETERS of the distribution (a gaussian in this case)
[22:54:06] <HuntsMan> but I think your question is more about why we use a gaussian or other distributions?
[22:54:11] <HuntsMan> simple, to model uncertainty :D
[22:54:11] <Klinda> and why do I care about a distributions?
[22:54:21] <Klinda> distribution?
[22:54:33] <HuntsMan> how would you model uncertainty in the output?
[22:54:35] <Klinda> why do I need to know the parameters of a distribution
[22:54:47] <HuntsMan> I think you should take a course in probability distributions and basic statistics
[22:55:06] <Klinda> yes
[22:56:13] <Klinda> I mean if the y is based to a gaussian
[22:56:26] <HuntsMan> because I am not going to explain you the whole why of distributions, you need to learn that yourself
[22:56:54] <Klinda> so is important to know about the distributions and the parameters?
[22:56:57] <Klinda> in general?
[22:57:14] <HuntsMan> yeah
[22:57:16] <Klinda> than I understand that?
[22:57:16] <HuntsMan> the basic ones
[23:01:33] <HuntsMan> Klinda: do you know the central limit theorem?
[23:18:13] *** Joins: SiegeLord (~sl@user/siegelord)
[23:32:53] *** Quits: Klinda (~superleag@user/klinda) (Ping timeout: 256 seconds)
[23:52:39] *** Joins: Klinda (~superleag@user/klinda)
[23:52:44] <Klinda> sorry HuntsMan
[23:52:49] <Klinda> I was having dinner
[23:52:58] <Klinda> no btw
[23:53:18] <Klinda> but the stanford teacher say in one lesson one time... I don't remember when
[23:54:51] <HuntsMan> that is really one basic thing you should know
[23:55:50] <Klinda> ok thank you
[23:55:52] <Klinda> and then?
[23:56:01] <Klinda> where do you think is easy to understand that?
[23:58:34] <HuntsMan> I can only say it takes time and knowledge
[23:59:24] <Klinda> should I see an university course that explain it?
[23:59:51] <HuntsMan> no, some basic statistics courses
