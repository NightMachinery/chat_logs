[00:05:39] *** Joins: marcello42 (~mp@2001:1a81:1373:1000:b615:e09c:7d18:7b3f)
[00:14:19] *** Joins: jlrnick (~josephler@2a01cb040a15940045b1aaa583125d83.ipv6.abo.wanadoo.fr)
[00:19:57] *** flooded is now known as _flood
[00:21:12] *** Quits: maxyz (~maxy@80.254.172.91) (Ping timeout: 250 seconds)
[00:43:21] *** Quits: clime (~clime@37-48-61-114.nat.epc.tmcz.cz) (Ping timeout: 256 seconds)
[00:49:40] *** Joins: etolier_ (~somewhere@122-199-44-127.ip4.superloop.com)
[00:51:41] *** Quits: etolier (~somewhere@202-65-84-142.ip4.superloop.com) (Ping timeout: 256 seconds)
[01:19:16] *** Quits: marcello42 (~mp@2001:1a81:1373:1000:b615:e09c:7d18:7b3f) (Ping timeout: 250 seconds)
[01:20:45] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[01:31:59] <Jong> Could I get opinions? How many years until a super human AGI ?
[01:32:36] <mefistofeles> Jong: tough question :P
[01:32:46] <Jong> guess?
[01:32:55] <Jong> please? I want to hear people's thoughts
[01:33:13] <mefistofeles> I really don't have a good guess, and I'm fairly sure the first declared AGI will not be truly one
[01:33:59] <Jong> a more basic question is, do you believe a super human AGI will ever come to exist?
[01:35:02] <mefistofeles> the wikipedia article actually has interesting relevant information
[01:35:14] <mefistofeles> you should check the references and estimates from the experts
[01:36:05] <Jong> Good point. But there's something about realtime human-to-human interaction I like
[01:36:22] <mefistofeles> yeah, I understand
[01:36:32] <mefistofeles> I just really don't have a good guess, tbh
[01:37:00] <mefistofeles> I can only say that in terms of information theory a super human AGI should be theoretically impossible
[01:37:16] <Jong> I think  AGI will basically be reinforcement learning on steroids
[01:37:45] <Jong> with pleasure and pain as the loss function
[01:37:49] <mefistofeles> https://en.wikipedia.org/wiki/Artificial_general_intelligence#/media/File:Estimations_of_Human_Brain_Emulation_Required_Performance.svg this is an interesting figure
[01:37:50] <SigmoidFroid> ⇒  Artificial general intelligence - Wikipedia
[01:39:02] <mefistofeles> in a few decades we should have enough computing power in our personal computers to do mind uploading
[01:39:09] <mefistofeles> according to Kurzweil, that is
[01:39:21] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) ()
[01:39:58] <Jong> woah I just thought of an idea! Combine RL with NLP!  If the net outputs a respectful kind sentence, it gets rewarded. If it outputs a disrespectful evil sentence it gets punished
[01:41:20] <mefistofeles> I'm pretty sure they do things like that, they probably ban many words
[01:41:27] <mefistofeles> like github's/MS copilot
[01:47:20] <Jong> mind uploading would be awesome
[01:47:36] <Jong> Oh well, enough speculating for the day :-)
[02:03:41] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:06:26] *** Joins: brand0 (~brandon@user/brand0)
[02:34:05] *** Quits: causative_ (~halberd@98.118.7.200) (Changing host)
[02:34:05] *** Joins: causative_ (~halberd@user/causative)
[02:48:32] *** Quits: jlrnick (~josephler@2a01cb040a15940045b1aaa583125d83.ipv6.abo.wanadoo.fr) (Ping timeout: 250 seconds)
[03:43:35] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[04:02:35] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[04:14:50] *** Joins: georgios (~georgios@user/georgios)
[05:02:27] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[06:01:41] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: Strength!)
[06:08:23] *** Joins: Malvolio (~Malvolio@user/malvolio)
[06:59:35] *** Joins: _ohm (~research@user/ohm/x-5690770)
[07:27:53] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[07:29:30] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[07:49:00] *** Quits: rvalue (~rvalue@user/rvalue) (Quit: ZNC - https://znc.in)
[07:53:48] *** Joins: rvalue (~rvalue@user/rvalue)
[08:36:37] *** Quits: Scarecr0w (scarecr0w@user/scarecr0w) (Ping timeout: 240 seconds)
[08:38:04] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[08:39:54] *** Joins: Scarecr0w (scarecr0w@user/scarecr0w)
[08:41:27] *** Quits: rvalue (~rvalue@user/rvalue) (Ping timeout: 256 seconds)
[08:52:42] *** Joins: rvalue (~rvalue@user/rvalue)
[08:59:01] *** Quits: rvalue (~rvalue@user/rvalue) (Ping timeout: 256 seconds)
[09:00:07] *** Joins: rvalue (~rvalue@user/rvalue)
[09:11:04] *** Joins: `Tim (~zenguin@user/zenguin)
[09:22:55] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[09:25:49] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 256 seconds)
[09:29:23] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[09:45:30] *** Quits: brand0 (~brandon@user/brand0) (Read error: Connection reset by peer)
[09:50:45] *** Joins: brand0 (~brandon@user/brand0)
[10:04:24] *** Joins: jlrnick (~josephler@2a01cb040a15940045b1aaa583125d83.ipv6.abo.wanadoo.fr)
[10:09:14] *** Quits: jlrnick (~josephler@2a01cb040a15940045b1aaa583125d83.ipv6.abo.wanadoo.fr) (Ping timeout: 252 seconds)
[10:16:21] *** Joins: maxyz (~maxy@80.254.172.91)
[10:36:34] *** Quits: [_] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[10:47:15] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 256 seconds)
[11:08:27] *** Joins: `Tim (~zenguin@user/zenguin)
[11:19:52] *** Joins: manti7 (~manti7@176.10.104.94)
[11:28:13] *** Joins: palasso (~palasso@user/palasso)
[12:30:52] *** Quits: Codaraxis_ (~Codaraxis@user/codaraxis) (Remote host closed the connection)
[12:31:12] *** Joins: Codaraxis_ (~Codaraxis@user/codaraxis)
[12:45:46] <lericson> can somebody help me clarify the terminology transformer encoder and decoder?
[12:47:32] <lericson> i understand them when you have a pair of networks, but that is often not the case
[12:57:50] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0d8-5.dhcp.inet.fi)
[13:46:30] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[13:48:17] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0d8-5.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:49:42] *** Joins: DnzAtWrk (~DnzAtWrk@1408.pc.puv.fi)
[13:57:49] *** Quits: DnzAtWrk (~DnzAtWrk@1408.pc.puv.fi) (Ping timeout: 256 seconds)
[14:01:18] *** Joins: jlrnick (~josephler@2a01cb040a15940045b1aaa583125d83.ipv6.abo.wanadoo.fr)
[14:24:55] *** Joins: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d)
[14:29:30] *** Quits: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d) (Client Quit)
[14:38:39] *** Quits: `Tim (~zenguin@user/zenguin) (Remote host closed the connection)
[14:38:57] *** Joins: `Tim (~zenguin@user/zenguin)
[14:50:49] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[15:22:51] <Jong> lericson  what do you mean a pair of networks?
[15:29:31] <Jong> lericson think of encoder and decoder as compression and decompression.   The idea comes from autoencoders.   Once the encoder and decoder are trained, they can be used separately. Watch this video for more: https://youtu.be/r0Ogt-q956I?t=9315
[15:29:33] <SigmoidFroid> ⇒  Deep Learning: A Crash Course - YouTube
[15:30:45] <Jong> lericson  another way to look at is it dimensionality reduction and dimensionality expansion
[15:31:03] *** Quits: palasso (~palasso@user/palasso) (Read error: Connection reset by peer)
[15:31:10] *** Joins: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d)
[15:32:40] *** Quits: Codaraxis_ (~Codaraxis@user/codaraxis) (Remote host closed the connection)
[15:32:54] *** Joins: palasso (~palasso@user/palasso)
[15:33:00] *** Joins: Codaraxis_ (~Codaraxis@user/codaraxis)
[15:33:35] <lericson> i am aware of all of these things, and that is precisely the pair of networks i was referring to -- but transformers are not necessarily used in that way, yet i see people call them transformer decoders
[15:34:27] <lericson> i think the terminology only makes sense when you actually do encode and decode
[15:37:57] *** Quits: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d) (Quit: WeeChat 3.4)
[15:38:08] *** Joins: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d)
[15:38:34] *** Quits: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d) (Client Quit)
[15:38:45] *** Joins: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d)
[15:41:08] *** Parts: marcello42 (~mp@2001:1a81:1389:1500:4832:8ed6:7825:272d) ()
[15:41:51] <Jong> lericson  I've observed encoder and decoder used incorrectly by people with subpar understanding of neural networks.  For example,  I see encoder and decoder used to describe a U-net, but a U-net is not an encoder-decoder because of the skip connections, and because fundamentally it's not encoding and decoding but rather combining context with detail together.
[15:43:42] <Jong> lericson  search for the word "decoder" in https://arxiv.org/pdf/1706.03762.pdf
[15:43:47] <SigmoidFroid> ⇒  [1706.03762] Attention Is All You Need  | UToron/Google -> In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention
[15:44:02] <Jong> You'll see the paper calls other nets encoder-decoder but not itself.
[15:44:40] <lericson> it definitely does
[15:44:46] <lericson> 3.1 encoder and decoder stacks
[15:44:48] <Jong> whoops
[15:46:20] <Jong> "the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations"
[15:46:57] <Jong> encoder = token to continuous, and decoder = continuous back to token?
[15:50:44] <Jong> lericson  yeah I see what you mean.
[15:51:09] <Jong> I mean, I see why your question is a good question
[15:52:17] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 256 seconds)
[15:53:16] <Jong> 'In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models'
[15:54:05] *** Joins: manti7 (~manti7@176.10.104.94)
[15:54:18] <Jong> The fact that they put quotes around encoder-decoder is almost like they're saying "we don't know whether we would've called it that, but others are and since our ideas are building upon theirs, we'll use the terms they used"
[16:10:49] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 256 seconds)
[16:11:07] *** Joins: manti7 (~manti7@176.10.104.94)
[16:32:13] <lericson> the term comes from the bidirectional LSTM thing
[16:32:17] <lericson> in this context
[16:32:37] <lericson> i think
[16:32:42] <lericson> ah whatever
[16:36:54] <lericson> the idea was that one network goes over a sequence and codifies it, then another network passes over it and decodes it
[16:37:09] <lericson> so for example first your english understander munges it up, then the german producer translates to german
[16:37:35] <lericson> i guess switch encoder and decoder in that explanation
[16:38:10] <lericson> in my mind the encoder goes from actual data → latent variable model, but here it is different
[16:39:01] <lericson> the reason i fail to explain this is also the reason i ask the question lol :p
[17:18:51] *** Quits: jlrnick (~josephler@2a01cb040a15940045b1aaa583125d83.ipv6.abo.wanadoo.fr) (Ping timeout: 250 seconds)
[17:39:02] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 250 seconds)
[17:59:16] *** Parts: mefistofeles (~mefistofe@user/mefistofeles) ()
[18:00:24] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[18:12:06] *** Joins: `Tim (~zenguin@user/zenguin)
[18:46:29] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[19:19:14] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[19:25:47] *** Joins: jlrnick (~josephler@2a01cb040a159400c46f749ba8bd31ea.ipv6.abo.wanadoo.fr)
[19:52:23] *** Quits: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com) (Ping timeout: 256 seconds)
[20:01:57] *** Joins: shoky (uuuggg@141.226.193.67)
[20:02:37] *** Quits: jlrnick (~josephler@2a01cb040a159400c46f749ba8bd31ea.ipv6.abo.wanadoo.fr) (Ping timeout: 240 seconds)
[20:04:27] *** Quits: shoky_ (uuuggg@141.226.193.67) (Ping timeout: 256 seconds)
[20:06:36] *** Joins: lntl (~joey@24.136.23.254)
[20:06:44] *** Parts: lntl (~joey@24.136.23.254) ()
[20:09:48] *** Quits: jerome- (~jerome@88.173.24.196) (Remote host closed the connection)
[20:11:38] *** Joins: jerome- (~jerome@88.173.24.196)
[20:15:42] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[20:26:11] *** Joins: Hunts (~hunts@p5494d423.dip0.t-ipconnect.de)
[20:26:13] *** Quits: HuntsMan (~hunts@p200300c1ff03c00032e171fffec8909c.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[20:36:47] *** Joins: markcerv (~markcerv3@52-119-125-204.PUBLIC.monkeybrains.net)
[21:01:06] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[21:06:31] *** Quits: markcerv (~markcerv3@52-119-125-204.PUBLIC.monkeybrains.net) (Quit: Leaving)
[21:06:48] *** Joins: markcerv (~markcerv3@52-119-125-204.PUBLIC.monkeybrains.net)
[21:52:25] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[22:33:39] *** Quits: markcerv (~markcerv3@52-119-125-204.PUBLIC.monkeybrains.net) (Read error: Connection reset by peer)
[22:35:26] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[22:48:08] *** Quits: Donitz (~Donitz@88-115-149-215.elisa-laajakaista.fi) (Read error: No route to host)
[22:54:34] *** Joins: Donitz (~Donitz@88-115-149-215.elisa-laajakaista.fi)
[23:27:34] *** Joins: markcerv (~markcerv3@52-119-125-204.PUBLIC.monkeybrains.net)
[23:54:50] *** Joins: spaceseller (~spacesell@31.147.205.13)
[23:57:12] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit:  !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~)
[23:59:19] <spaceseller> Simple questions about Transformers (pytorch or keras). Is there anyone who knows something about the subject?
