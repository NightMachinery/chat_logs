[00:01:22] <tabakhase> orzel so write it down once in a file called ".env" next to the compose-file containing "My_DB=1.2.3.4"- and put "$My_DB" in the compose file itself
[00:03:13] <tabakhase> the syntax ${APP_ENV:-prod} allows for "inline defaults" as well - for someone who hasnt set it / didnt create the ".env"
[00:04:34] <tabakhase> and this kinda is "entirely seperate from whats thrown in environment:" - but you can totally have a "environment: - DB_HOST=$My_DB"
[00:05:25] <tabakhase> (also try the  "docker-compose config" command for easy debugging of this stuff :P - that prints what it read&parsed to terminal)
[00:07:45] <Ryu_> If a server has to LAN IPs ( from being manually assgined), is that compatible with docker?  Can you have a docker container on each LAN IP?
[00:07:53] <Ryu_> I mean *two*
[00:08:09] <tabakhase> Ryu_ sure that
[00:08:42] <tabakhase> -p 1.2.3.4:80:80 && -p 9.7.6.5:80:80 will work jut fine
[00:09:24] <Ryu_> tabakhase: Any idea why deploying NGINX on both 1.2.3.4 and 1.2.3.5 will cause the first one to crash and the second one to not work?
[00:09:56] <Ryu_> First one only starts to work when I delete the second one.
[00:10:06] <tabakhase> uh? i feel youll have to prepare a nopaste for this :D
[00:11:00] <tabakhase> also "work" - does only that forward break, or anything more? (containers networking within themself&to host remains all good?)
[00:11:36] <tabakhase> and what happens when you attach both to the same? (be it with both -pÂ´s or as -p 80:80)
[00:13:38] <Ryu_> tabakhase: I am using different IPs since they both need the same ports.   (80 and 443).   By broken I mean the container itself crashes and keeps on crashing.
[00:14:25] <Lutin> Bossi so I'm still not sure what I shall do :)
[00:14:41] <Ryu_> Thus, nothing it working since it is a crashing container.  It stops crashing when I delete the new container.
[00:16:01] <Ryu_> That is why I was wondering if there is some limitation with Docker
[00:16:48] <Ryu_> I noticed the app containers that connect to NGINX were not crashing.  Only the NGINX container.  I am not sure if the Letsencrypt container crashed.
[00:18:01] <Ryu_> tabakhase: My planned setup is to have each LAN IP with its own NGINX container, Letsencrypt container and associated app containers.  Every LAN IP will be a different app.
[00:18:37] <Ryu_> I have one app deployed on one LAN IP so far.
[00:19:04] <tabakhase> as said "crashing" doesnt mean much... who even knows what you consider "crashing" ;-) --- so nopaste, a) your network config (ip addr, feel free to redact) b) docker commands / compose files youre running & respective dockerfiles if anythign is custom
[00:21:14] <tabakhase> oh and obv any output youre getting / docker logs calls from those / any actual error messages you see if any...
[00:30:42] *** Quits: CodeSpelunker (~CodeSpelu@user/codespelunker) (Remote host closed the connection)
[00:31:57] <Ryu_> tabakhase: Here is the state after running the setup.  https://pastebin.com/eufpUAcf
[00:33:04] <Ryu_> I noticed it did not crash my containers deployed on my first IP this time.
[00:36:43] <Ryu_> tabakhase: Do those logs mean anything to you or anyone?
[00:37:57] <orzel> tabakhase: i understand that. But as said, this is not what i want. I dont want to 'configure' or whatever. I want a path to be used in different place. This shouldn't be in a separate file. There's not 'env', unversioned env file or whatever. Just plain ol'constant.
[00:37:59] <tabakhase> Ryu_ i dont see anything "crashing" there
[00:38:10] <orzel> it seems impossible actually. not impressed by dockercompose files :/
[00:38:10] <tabakhase> just a container that ends instantly as it errors out uh?
[00:38:31] *** Joins: thiras (~thiras@user/thiras)
[00:39:00] <tabakhase> orzel see the "x- values" and yaml anchors in the example link
[00:40:00] <tabakhase> the "&default-app" and " <<: *default-app " parts specifically
[00:40:54] <orzel> yes, i'm still reading this. (thx for the link!)
[00:41:52] <tabakhase> Ryu_ compose file of that nginx? cause "req: Can't open "/etc/nginx/certs/default.key.new" for writing, Read-only file syste" seems sus..
[00:42:52] <Ryu_> tabakhase: what is strange is the exact setup worked for the first application
[00:43:31] <tabakhase> are you bind mounting stuff in there to hold the certs or so? different chmods on that? different user container is running at or any of that?
[00:44:46] <tabakhase> but ye, none of that should have anything todo with the ip/published ports =)
[00:45:31] <orzel> tabakhase: that's more about factorizing same args for services. Nice, but not what i want. Typically the env var will be different on each docker service  for the common path/ipaddress 
[00:45:47] <orzel> there's definitely a lack of constant i think
[00:47:11] <tabakhase> orzel anchros work at all sorta levels and deepmerge - so unless the name fo the actual var you want to set is different you can still use it -- if now you actually have "BLA=blub" in one, and "MOE=blub" in the other, this "blub" is what you want to group?
[00:48:01] <Ryu_> tabakhase: orzel Here is the docker compose file https://pastebin.com/j4NJ8K34
[00:50:10] <tabakhase> Ryu_ line 21...?
[00:51:22] <Ryu_> tabakhase: Only letsencrypt module is suppose to be able to write to it.   the NGINX proxy should only be able to read it.  It was recommended by the creator of the image.
[00:51:35] <tabakhase> and did you pull that from somewhere else? a volume on /usr/share/nginx/html has a bad smell....
[00:51:37] <Ryu_> It worked fine for a different deployment
[00:52:27] <Ryu_> tabakhase: I took this off of a NextCloud deployment.  I wasn't sure what was needed and what was not.  I left everything I saw for NGINX
[00:52:44] <orzel> tabakhase: yes for 'blub'
[00:52:44] <Ryu_> That was the first application deployed
[00:53:00] <Ryu_> I am using this on a homeassist deployment
[00:54:33] <Ryu_> tabakhase: Is there something I should be aware of for /usr/share/nginx/html?
[00:54:52] <tabakhase> orzel i think then you are actually out of luck unless youre okay with creating a .env file (generally shouldnt be in git, but maybe youre the one exception/month...)
[00:55:47] <tabakhase> or just whack a "run `echo 'path=/bla/blub' > .env` before first boot" into the readme :P
[00:56:58] <tabakhase> Ryu_ well without knwoing the container/app - how does it update? by pulling new images? - great, now your OLD volume with OLD code will be reused ontop of the new image :D
[00:58:20] <tabakhase> ((some containers expect that, and have mechanics to "share the frontend" files or such ((ive seen entrypoints do cp/rsync from one path to the shared-space-path)) -)
[00:58:40] <Ryu_> tabakhase: Your saying I should ask the creator of that NGINX/letsencrypt combo and ask about it
[00:59:05] <tabakhase> more the haas image - and in doubt, dont :P
[00:59:43] <tabakhase> uhm actually scrolling further in your file - you dont even use it?
[00:59:59] <Ryu_> tabakhase: the has image has nothing to do with that volume that.  That is something only used by the letsencrypt/NGINX containers
[01:00:21] <Ryu_> it is used in NGINX/letsencrypt.  It is only for that I believe
[01:00:33] <tabakhase> Ryu_ yea so lemme correct myself there - as volume for that use (.well-known) thats just fine
[01:00:57] <tabakhase> your name "homeassist_html" completly threw it in before even reading the haas container itself m)
[01:01:19] *** Quits: Brainium (~brainium@user/brainium) (Ping timeout: 265 seconds)
[01:01:56] <Ryu_> I have it named to show the project it is associated with
[01:02:29] <Ryu_> Since I will have a nextcloud NGINX/letsencrypt contaienr and a homeassist NGINX/letsencrypt contaienr
[01:02:47] <tabakhase> compose already does this tho :P -- defaults to "parent folder" so haas/docker-compose.yaml becomes hass_nginx and so on :P
[01:02:58] <tabakhase> happens to services, but also volumes & networks
[01:03:42] <Ryu_> Well I have a naming convention I already started :)
[01:04:07] <Ryu_> Any idea what could be wrong though?  The container just restarts
[01:04:17] <tabakhase> so anyhow, from the logs whoever is trying to write the cert cant, and then nginx goes poof cause it cant find a cert on disk
[01:05:25] <tabakhase> dunno if that nginx tries to generate some selfsigned staging-cert or so (cause on first boot you obvs. dont have valid LE-certs yet) so nginx has to start either "without any https configured" or with a dummy
[01:06:19] <Ryu_> tabakhase: I noticed the first time I ran this, it activated my letsencrypt container on my nextcloud setup.   I wonder if there is an issue with having more then one letsencrypt containers on different LAN IPs.
[01:06:35] <tabakhase> ((on a offtopic, while im a nginx fan - as ingress for docker i had better results with traefik or caddy where docker is a bit more "first party"...))
[01:07:07] <tabakhase> Ryu_ "container_name: homeassist-letsencrypt" his bypasses the per-project naming
[01:08:04] <tabakhase> so yae, that may have fired your other container (saw it doesnt match, killed & made a new one) if compose thinks it belongs in the same stack... - otherwise it should have yelled about duplicate container name
[01:08:38] <tabakhase> tho your nextcloud le shouldnt have a haas name uh... im confsed :D
[01:09:01] <Ryu_> One lets encrypt container is name nextcloud-letsencrypt and is on LAN IP 1.2.3.4  .  The other is homeassist-letsencrypt and is on 1.2.3.5
[01:09:23] <tabakhase> ((there is no point to have a container_name in compose at all 99% of the time, only makes issues...))
[01:11:26] <Ryu_> tabakhase: I ran both projects like this but gave each one its own name.   docker-compose -p projectname up
[01:12:35] <Ryu_> tabakhase: What are you suggesting I do to fix this?
[01:13:29] *** Joins: Brainium (~brainium@user/brainium)
[01:14:39] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Quit: WeeChat 3.2)
[01:15:42] <tabakhase> pull things straight (naming), observe both stacks (logs/undetached) for crosstalk, then deeper on > [22:35:26] <tabakhase> dunno if that nginx tries to generate some selfsigned staging-cert or so (cause on first boot you obvs. dont have valid LE-certs yet) so nginx has to start either "without any https configured" or with a dummy
[01:17:30] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[01:17:51] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[01:23:20] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[01:31:52] *** Joins: tyson2 (~user@70.54.112.49)
[01:35:15] <Ryu_> tabakhase: thankyou, I will have to work on this some more.  Bye
[01:35:21] *** Quits: Ryu_ (~Ryu945@154.3.251.91) (Quit: Leaving)
[01:36:10] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 268 seconds)
[01:39:08] *** Quits: m0nkey_ (~quassel@user/m0nkey/x-3352227) (Quit: Quit)
[01:41:46] *** Joins: m0nkey_ (~quassel@user/m0nkey/x-3352227)
[01:49:05] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[01:54:03] *** Quits: chasmo77 (~chas77@71.63.241.168) (Ping timeout: 268 seconds)
[01:57:05] <Lutin> Bossi still around ?
[01:57:18] <Bossi> yes
[01:58:21] <Lutin> nice! ... so what about a container, that should work as well... issue I have or think to have is that the switches need to be redundant or when they are on host OS.. then actually not
[01:58:50] *** Quits: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963) (Ping timeout: 265 seconds)
[01:59:30] <Bossi> i use the construct to use a failover-bridge configuration to connect to physical network adapters, everything else has just single instances
[02:00:11] <Lutin> Bossi so it can't go down actually ?
[02:00:36] <Bossi> when one external link goes down the bridge will switch to the alternate port
[02:01:37] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[02:02:13] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[02:03:30] <Lutin> Bossi oh yeah you have 2 interfaces on your server you mean ?
[02:04:20] <Lutin> Bossi if you have a config example would be nice... these servers are VPS-es on redundant servers so...
[02:04:38] <Bossi> so my current server has 10 native ethernet interfaces eth0 - eth9
[02:05:18] <Lutin> physical ones ?
[02:05:38] <Bossi> yes, physical ones :D, i got plenty
[02:05:53] <Bossi> 2 onboard and 2 quad
[02:06:05] <Bossi> all for redundancy
[02:06:33] <Bossi> bond0 runs in Bonding Mode: fault-tolerance (active-backup), and bond members are eth0 and eth1
[02:06:49] <Lutin> yeah I knwo... you can go crazy that way :)
[02:07:00] <Lutin> but do you have an example config for vswitch ?
[02:07:09] <Bossi> bond0 is solely used for the host os without any further tricks
[02:07:50] <Bossi> bond1 is also running in Bonding Mode: fault-tolerance (active-backup) and members are eth2 and eth6
[02:08:24] <Bossi> while bond0 has an ip address assigned, bond1 is only "up" with no layer 3
[02:08:38] <Lutin> or do you think I can do it with a container ? I think I can... but if that one goes down... meh
[02:09:13] <Lutin> Bossi that kind of setups with those bonds is what I did on physical servers/switches
[02:09:27] <Bossi> sure, i run my ovs on the bare metal host
[02:09:40] <Bossi> but connect the containers through t
[02:09:43] <Bossi> *it*
[02:10:39] <Bossi> in my use case i needed to have native layer 2 communication inside the containers
[02:11:32] <Bossi> when you want to do this in a container how will another container get access to your network stack?
[02:12:05] <Bossi> it may be feasible when you run another docker instance in a container, with containers inside as well
[02:13:07] <Bossi> the problem is how to get your "external" 802.1q trunk into the machine/container/vm ...
[02:13:54] *** Joins: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963)
[02:14:29] <Lutin> sec
[02:25:56] *** Joins: thiras (~thiras@user/thiras)
[02:30:02] *** Quits: Sven_vB (~sven@user/sven-vb/x-2094958) (Remote host closed the connection)
[02:30:29] *** Joins: Sven_vB (~sven@user/sven-vb/x-2094958)
[02:32:41] *** Quits: CombatVet (~c4@user/combatvet) (Ping timeout: 244 seconds)
[02:33:13] *** Joins: CombatVet (~c4@user/combatvet)
[02:41:30] *** Quits: notsponsible (~notsponsi@45.144.113.76) (Ping timeout: 250 seconds)
[02:47:44] *** Joins: pvdp (~Pieter@58.105.183.94)
[02:50:00] *** Quits: Steeve (~steve@user/steeve) (Quit: end)
[02:58:49] *** Quits: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net) (Quit: My MacBook has gone to sleep. ZZZzzzâ¦)
[03:24:40] <Lutin> Bossi ok that was more then a sec, sorry :)
[03:24:55] <Bossi> np, still awake
[03:25:49] <Bossi> where are you located if i may ask
[03:27:41] <Lutin> Bossi me as wellneeded to help a friend stay focussed. I'm in France :)
[03:28:05] <Bossi> so we are almost neighbours, i live in Germany
[03:28:37] <Lutin> Oh nice! Yes I'm actually Dutch... so once I was even closer :D
[03:28:49] <Lutin> Germans are cool people, I like them
[03:29:53] <Bossi> so, i am wondering what you really want to achieve running a vswitch in a container, and how to get it connected in the first place to some "outside" entity to get some VLAN-trunk
[03:30:53] <Bossi> and the second part is, how to connect something to said vswitch outside of the container, or will there be a docker instance in the same container as the vswitch?
[03:32:03] <Lutin> yeah good question. I want to VPN into my docker "network"
[03:32:21] <Lutin> so I don't need any trunk to the outside world... but maybe later on...
[03:32:30] <Bossi> Dutch are also cool people, it mostly does not rely on your origin
[03:32:31] <Lutin> I mean... swarm fixes that
[03:32:50] <Lutin> Dutch people are selfish these days... that is why I left
[03:33:00] <Lutin> the country is too small
[03:34:45] *** Quits: tang^ (~doofus@2604:3d09:47c:f970:adf6:40a4:648f:c446) (Quit: Bye)
[03:37:35] <Lutin> Bossi vswitch can do L3 /
[03:37:36] <Lutin> ?
[03:37:43] <Lutin> I mean... you let it route ?
[03:40:41] <Bossi> no, i just use it for L2 stuff, handling VLAN connections to containers
[03:41:01] <Lutin> ah ok... I need some routing because of VPN
[03:41:12] <Lutin> so, my idea was VyOS
[03:44:35] <Bossi> this something i still have on my imaginary Todo-List
[03:45:36] <Lutin> Bossi the VPN or the VyOS ?
[03:45:43] <Lutin> Maybe we should teamup ?
[03:45:53] *** Joins: sleym (~sleym@185.124.240.166)
[03:46:02] <Bossi> different sorts of VPNs, currently i tunnel everything through ssh
[03:46:27] <Lutin> yeah nog doable for me
[03:46:31] <Lutin> and also not very internal
[03:46:33] <Bossi> and i always procrastinate it till i end up with some WLAN of Hell
[03:46:40] <Lutin> hehe
[03:46:56] <Lutin> I will be around in the weekend
[03:47:15] <Lutin> I'm not sure what todo with the switch... swarming it is not possible
[03:47:20] <Lutin> one per node could be
[03:50:11] <Lutin> Bossi this seems to be your kind of way https://medium.com/@joatmon08/making-it-easier-docker-containers-on-open-vswitch-4ed757619af9
[03:51:58] <Bossi> currently i am very busy with a project, not much time left for anything else. After that want to restructure my server environment at home including VMs and containers, making everything redundant, upgrading to 10GBit
[03:52:14] <Lutin> or this one https://www.tutorialspoint.com/how-to-use-an-ovs-bridge-for-networking-on-docker 
[03:52:36] <Lutin> yeah we are deploying new services for a project, this came to mind
[03:52:44] <Lutin> so I'm gonna test it anyways
[03:53:09] <Bossi> yes, native support for ovs is imho still missing in docker, so i start container without network at all and connect them with ovs-docker to the switch
[03:53:36] <Lutin> yap
[03:53:46] <Lutin> so you don't use swarm as wel I guess ?
[03:53:50] <Bossi> sometimes docker has strange ideas when you start containers with more than one network
[03:53:52] <Lutin> *well
[03:54:02] <Lutin> indeed, doesn't add up
[03:54:32] <Bossi> no, atm most of the containers should not float around, this is something i want to change in the future
[03:55:54] <Bossi> and the other project i mentioned where i relied on specific VLANs connected directly to containers running honeypots for each network segment, there were no redundant docker hosts
[03:56:22] <Bossi> i even worked with docker on Raspberry PIs for that project to deploy those to small subsidaries
[03:57:28] <Lutin> ah kewl!
[03:57:57] <Lutin> yeah I wanted to use rancher for another hugeass project, I already tested that, but I don't like the overhead compared to docker
[03:58:23] <Lutin> but those l8s fanboys always say... is that stil a thing, docker/swarm ?
[03:58:31] <Lutin> k8s
[03:58:48] <Bossi> everything i found on the net regarding honeypots uses almost the same blueprint, but all of them lacked the large scale approach while i wanted to be able to deploy them worldwide with several hundreds of nodes
[03:59:09] <Lutin> yeah same thing here
[03:59:22] <Lutin> but what kind of services are you doing ? IoT ?
[04:00:00] <Bossi> you need to think about the whole life cycle, installation, deployment, running, fixing broken devices, ....
[04:00:23] <Lutin> the issue with vswitch/Vyos as container is that it's a SPOF... it's less SPOF when it runs on the host itself
[04:00:35] <Lutin> Bossi sound cool
[04:00:40] <Bossi> no it was in a huge enterprise having offices all over the world for placing those in internal networks to detect malicous activities
[04:01:31] <Bossi> when i start such a honeypot on a host at some hoster, within 15 minutes you get "visitors" deploying wannacry for instance
[04:02:14] <Bossi> and then you get sometimes nice e-mail from those hosters that you are vulnerable ...
[04:02:50] <Lutin> hehe
[04:03:10] *** Quits: orzel (~orzel@2a01:e0a:df:6ba0:1cfe:b463:3789:a442) (Quit: Konversation terminated!)
[04:03:40] <Bossi> it never made it to production beside the local datacenter since first my boss left the company who triggered the project and then i decided to leave as well
[04:03:57] <Lutin> Hehe... always go with the good ones :D
[04:04:39] *** Joins: noarb- (~noarb@user/noarb)
[04:05:04] <Lutin> I'm going to sleep, I have some pasta I want to finish... we need to stay in touch!
[04:05:07] <Bossi> but i implemented all those nice stuff, separated the docker hosts from the internet, using a seperate build host for building customized images, ran an internal registry for deploying those things
[04:05:18] <Lutin> yeah cool!
[04:05:24] *** Quits: noarb (~noarb@user/noarb) (Ping timeout: 268 seconds)
[04:05:32] <Lutin> I also run my own registry... I even created my own docker image builder :D
[04:05:40] <Lutin> which is awesome!
[04:06:01] <Bossi> for real production you should stage your stuff, from dev -> test -> prod
[04:07:28] <Bossi> ofc, i should be around somehow, just ping me, sometimes it may take some time, but mentioning my nick will trigger highlighting
[04:07:56] <Bossi> irc is sometimes not a real time communication
[04:08:16] <Lutin> same with me :) I don't lurk, I think you don't as wel
[04:08:19] <Lutin> well
[04:08:25] <Lutin> my laptop sleeps next to me so haha!
[04:08:43] <Lutin> but next to my bed... my dog has privileges to be in my bed :)
[04:10:24] *** Joins: yamchah2 (~yamchah2@user/yamchah2)
[04:11:55] <Lutin> Bossi sleep well! we have something nice to talk about now and then :)
[04:12:01] <Lutin> nice meeting you!
[04:12:15] <Bossi> sure, ty, sleep well as well
[04:12:22] <Lutin> thanks!
[04:15:39] <Lutin> Bossi but about the docker/k8s overhead idea... waht do you think about it ?
[04:17:55] *** Quits: thc202 (~thc202@user/thc202) (Ping timeout: 244 seconds)
[04:18:48] <Bossi> i must admit that also procrastinate the subject of orchestrating since quite some time, but also due to the fact that there is not really much to be orchestrated. While i saw a lot of changes in the last few years regarding orchestration and configuration management is quite the same.
[04:19:29] <Bossi> some linux distros doesnt even ship docker-compose any longer, they try to push you directly to use k8s
[04:19:32] *** Quits: SomeWeirdAnon (~shwn@2a02:8109:abf:ffb4:38d2:141e:ecd6:356b) ()
[04:20:18] *** Quits: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net) (Ping timeout: 272 seconds)
[04:22:09] <Bossi> which imho is a lot of efford when you just want to run one docker host, so stick with docker run or get docker-compose elsewhere. On the other hand you should skip using the docker version at all from some distros because it is outdated even when the distro is brand new
[04:33:04] *** Quits: CombatVet (~c4@user/combatvet) (Ping timeout: 244 seconds)
[04:35:07] *** Quits: Sasazuka (~Sasazuka@user/sasazuka) (Quit: My MacBook has gone to sleep. ZZZzzzâ¦)
[04:38:26] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[04:38:49] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[04:40:38] <Lutin> Bossi yeah issue...
[04:40:48] <Lutin> I like docker even for clusters
[04:41:10] *** Joins: IonTau (~IonTau@106-69-214-62.dyn.iinet.net.au)
[04:42:47] *** Joins: CombatVet (~c4@user/combatvet)
[04:42:49] <Bossi> to play with cluster features i need at least 2 or more "servers" to build some, it would be nice to have a swarm-cluster and other clusters orchestrated differently at the same time, to see what the pros and cons are
[04:44:08] <Bossi> and you need to be able just to "pull the plug" to see if your idea of failover or high availability is working, i saw a lot of installations where ppl just looked sad after some problem hit them
[04:44:34] <Bossi> it just worked in their powerpoint slides
[04:47:16] <Lutin> yeah, I'm a HA guy so... I'm not afraid :)
[04:47:23] <Bossi> i used to design, setup and run complete infrastructure services providing DHCP, DNS, LDAP and other stuff using VMware and LinuxVMs, using LVS for loadbalancing, nowadays i would think about a setup using containers for several aspects
[04:48:00] <Lutin> we need some webservice we replicate over 3 DC's... we do DNS failover with LUA... so... if a server fails... you don't get the record anymore because of... looooooooowwwwwwww TTL
[04:48:36] <Lutin> yeah I did oVirt/Foreman/etc for a couple of years on a 26 server cluster with netapp storage
[04:48:45] <Lutin> that set was masive
[04:48:53] <Lutin> 1TB of mem back the days :D
[04:49:06] <Lutin> 40TB storage
[04:49:55] <Bossi> zpool list:
[04:50:15] <Lutin> yeah, I do guster now
[04:50:17] <Bossi> big   87.2T  79.1T  8.12T        -         -    59%    90%  1.00x  ONLINE  -
[04:50:19] <Lutin> gluster
[04:50:25] <Lutin> nice
[04:50:35] <Lutin> yeah I did that 40TB already 8 years ago :D
[04:50:52] <Lutin> or no, 9!
[04:50:53] <Bossi> 128TB in total disks for that zpool
[04:50:59] <Lutin> :)
[04:51:01] <Lutin> nice
[04:51:14] <Lutin> not realy slow with that much parity ?
[04:51:20] <Lutin> *really
[04:51:26] <Bossi> it is sufficient for my use case
[04:51:30] <Bossi> i need space
[04:52:10] <Bossi> 8x12TB disks, nowadays i would use 16TB or 18TB disks
[04:52:42] *** Quits: CombatVet (~c4@user/combatvet) (Ping timeout: 244 seconds)
[04:52:58] <Bossi> i use zfs on linux since 2009, started with zfs-fuse
[04:53:52] <Bossi> also thought about scaling up things using ceph and instead of cepf-fs put some zfs on top
[04:54:11] <Lutin> nah... ceph has it's issues 
[04:54:22] <Bossi> everything has its "issues" :D
[04:55:06] *** Joins: CombatVet (~c4@user/combatvet)
[04:59:45] *** Joins: mihael (~mihael@2001:4454:2ac:7100:b581:df8c:9254:aaac)
[04:59:45] <rawtaz> Bossi: about running ZFS on top of Ceph, what would you say is a proper recordsize and ashift?
[05:00:49] <Bossi> we didnt implement such thing yet, just discussed this with a friend as a potential scale up, since he is expierenced using ceph already
[05:01:52] <rawtaz> ok
[05:02:15] <rawtaz> i use zfs on ceph in a VPS or two
[05:02:21] <Lutin> hehe yeah ceph can be nice
[05:02:33] <Lutin> I think ZFS would help you as good as well too
[05:02:41] <Bossi> i use snapshots very intensivly, while ceph-fs has very limited capacity for those, and ceph-fs should also not used as a multi-node setup, so using zfs has the advantage of having the nice snapshot features, and extensibility will be better
[05:03:37] <rawtaz> its nice alright
[05:04:00] <Bossi> when i currently try to expand a zpool i need at least spend a zmirror, or a new zraid2 set, while using ceph, i just provide a blockstorage device and zfs does not need to replicate data, this is handled by the ceph layer
[05:04:51] <rawtaz> yeah exactly
[05:05:07] <Bossi> u just need enough bandwidth between the storage nodes
[05:05:42] <Bossi> nice to meet someone which already does what we thought about years ago :D
[05:06:09] <rawtaz> oh its on a small scale, but i agree its nice :D
[05:07:20] <Bossi> thats also the reason why i already bought 10GBit switches to have some decent bandwidth around
[05:07:33] <rawtaz> what do you store on this stuff?
[05:08:05] <Bossi> different kind of data, almost 100TB of public media servers, backups, ....
[05:08:18] <rawtaz> ok
[05:10:25] <Bossi> just checked with lsblk how much disk i have on one server 308.5TB
[05:12:19] <Lutin> yap... but that bandwidth can be the issue later on
[05:12:32] <Bossi> but its meanwhile a lot of disks connected by USB3
[05:12:42] <rawtaz> that doesnt sound right
[05:13:08] <Lutin> yeah ? cool... does that work out well ?
[05:13:12] <Bossi> therefore the idea to scale it with ceph
[05:15:04] <Lutin> I love the idea of USB
[05:15:12] <Lutin> but, what kind of USB disks ?
[05:16:08] <Bossi> for "low speed" i use disk cabinets with 8 slots and one USB3 connection to get them connected at least
[05:16:25] <Lutin> the issue with USB is that you don't know which disk is connected where most of the time
[05:16:37] <Lutin> oka
[05:16:42] <Lutin> which cabinets ?
[05:16:48] <Bossi> i use labels to mount them, regardless where they appear
[05:16:58] <Lutin> yeah sure but ports... 
[05:17:01] <Lutin> always an issue
[05:17:17] <Lutin> but I think data transfer isn't that slow 
[05:17:23] <Bossi> had no issues so far, even when power down
[05:17:44] <Lutin> :)
[05:17:45] <Bossi> i can saturate my GBit Internet link while writing to the disks :D
[05:17:53] <Lutin> hehe
[05:18:01] <Lutin> that's easy
[05:18:12] <Lutin> I could even do that with 6GB bonds :D
[05:18:35] <Bossi> the older 2TB disks were really slow ...
[05:18:42] <Lutin> anyways, I need to sleep now, it's for sure :D
[05:18:48] <Bossi> gn8 Lutin
[05:18:54] <Lutin> thanks1
[05:18:57] <Lutin> sleep well
[05:19:07] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[05:22:21] *** Joins: CodeSpelunker (~CodeSpelu@user/codespelunker)
[05:24:00] *** Quits: Zelec941 (~Zelec@135-23-82-85.cpe.pppoe.ca) (Quit: The Lounge - https://thelounge.chat)
[05:24:22] *** Joins: Zelec9410 (~Zelec@135-23-82-85.cpe.pppoe.ca)
[05:25:21] <rawtaz> USB just doesnt strike me as something i'd trust for some serious storage
[05:25:53] <rawtaz> what cabinets do you use for the disks, i.e. what is the actual controller talking USB with the host?
[05:26:55] *** Quits: Zelec9410 (~Zelec@135-23-82-85.cpe.pppoe.ca) (Client Quit)
[05:30:25] *** Quits: vlm (~vlm@user/vlm) (Quit: "")
[05:31:29] *** Quits: mihael (~mihael@2001:4454:2ac:7100:b581:df8c:9254:aaac) (Quit: Client closed)
[05:33:25] *** Joins: trevors (~trevors@c-65-96-171-157.hsd1.ma.comcast.net)
[05:34:44] *** Joins: vlm (~vlm@user/vlm)
[05:48:36] *** Quits: sleym (~sleym@185.124.240.166) (Read error: Connection reset by peer)
[06:01:04] *** Joins: pete443_ (~pete@user/pete443)
[06:03:00] *** Quits: pete443 (~pete@user/pete443) (Ping timeout: 250 seconds)
[06:05:12] *** Quits: jpmh (uid445439@id-445439.stonehaven.irccloud.com) (Quit: Connection closed for inactivity)
[06:11:16] *** Joins: mihael (~mihael@2001:4454:2ac:7100:b581:df8c:9254:aaac)
[06:11:27] *** Quits: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net) (Quit: Textual IRC Client: www.textualapp.com)
[06:26:58] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Quit: Atum_)
[06:27:54] *** Quits: tyson2 (~user@70.54.112.49) (Remote host closed the connection)
[06:49:23] *** Joins: notsponsible (~notsponsi@62.182.99.49)
[06:50:14] *** Quits: IonTau (~IonTau@106-69-214-62.dyn.iinet.net.au) (Ping timeout: 250 seconds)
[06:56:33] *** Joins: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net)
[06:59:38] *** Quits: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi) (Ping timeout: 244 seconds)
[07:00:06] <lilgopher> I'm using docker compose to start a service that uses elastic search, but elastic search takes longer to start. Is there a way in docker to make my service to start after elastic search is ready?
[07:00:48] *** Joins: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi)
[07:01:28] *** Quits: mihael (~mihael@2001:4454:2ac:7100:b581:df8c:9254:aaac) (Quit: Client closed)
[07:01:47] *** Quits: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net) (Read error: Connection reset by peer)
[07:04:24] *** Joins: rewrit3 (~rewrit3@user/rewrit3)
[07:05:20] *** Joins: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net)
[07:06:01] *** Quits: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi) (Ping timeout: 258 seconds)
[07:13:15] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[07:13:27] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[07:25:58] *** Quits: russjr08 (~russjr08@fw.internal.russ.network) (Quit: Till next time!)
[07:29:49] *** Joins: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi)
[07:32:19] *** Quits: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi) (Client Quit)
[07:32:35] *** Joins: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi)
[07:35:34] *** Joins: sed (~sed@pool-100-7-38-116.rcmdva.fios.verizon.net)
[07:38:02] *** Joins: russjr08 (~russjr08@fw.internal.russ.network)
[07:56:54] *** Quits: bancroft (~bancroft@bras-base-mtrlpq02hsy-grc-09-76-68-189-147.dsl.bell.ca) (Ping timeout: 272 seconds)
[08:09:24] *** Quits: Brainium (~brainium@user/brainium) (Quit: Konversation terminated!)
[08:10:18] *** Quits: CodeSpelunker (~CodeSpelu@user/codespelunker) (Quit: CodeSpelunker)
[08:18:57] *** Joins: rageshkrishna (~rageshkri@136.185.187.57)
[08:22:30] *** Quits: rageshkrishna (~rageshkri@136.185.187.57) (Remote host closed the connection)
[08:23:34] *** Joins: rageshkrishna (~rageshkri@136.185.187.57)
[08:30:52] *** Joins: IonTau (~IonTau@106-69-214-62.dyn.iinet.net.au)
[08:32:47] *** Joins: night_wulfe_ (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[08:34:34] *** Joins: doc|work (~doc@user/doc)
[08:35:21] <doc|work> is there a way to have docker-compose start a container only on certain conditions? (environment variable is what I want to use)
[08:35:39] *** Quits: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 265 seconds)
[08:40:38] *** Quits: ana1ogv0id (marc@52-119-113-34.PUBLIC.monkeybrains.net) (Remote host closed the connection)
[08:40:59] *** Joins: ana1ogv0id (marc@52-119-113-34.PUBLIC.monkeybrains.net)
[08:50:56] *** Joins: Crassus (~Crassus@user/crassus)
[08:58:02] *** Quits: Crassus (~Crassus@user/crassus) (Quit: Textual IRC Client: www.textualapp.com)
[08:58:21] *** Joins: mihael (~mihael@2001:4454:2ac:7100:b581:df8c:9254:aaac)
[09:31:32] *** Quits: mihael (~mihael@2001:4454:2ac:7100:b581:df8c:9254:aaac) (Quit: Client closed)
[09:34:10] *** night_wulfe_ is now known as night_wulfe
[09:34:49] *** Quits: sed (~sed@pool-100-7-38-116.rcmdva.fios.verizon.net) (Ping timeout: 246 seconds)
[09:37:56] *** Quits: IonTau (~IonTau@106-69-214-62.dyn.iinet.net.au) (Ping timeout: 250 seconds)
[09:39:04] <tabakhase> doc|home you can abuse merging, COMPOSE_FILES env or "-f 1.yml -f 2.yml" 
[09:39:26] <tabakhase> ("docker-compose config" for easy debugging)
[09:40:59] <tabakhase> say, "docker-compose -f docker-compose.yml -f docker-compose.tunnel.yml up -d" where the tunnel file has some extra service ((and/or overwrites/merges values in existing services from the other files))
[09:51:05] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[09:51:32] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[10:00:43] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Quit: WeeChat 3.2)
[10:06:54] <doc|work> tabakhase, thanks for the suggestion, that seems a bit tedious all the same :/ 
[10:08:02] <doc|work> maybe better to just have different compose files per environment
[10:11:59] *** Joins: onizu (uid373383@id-373383.highgate.irccloud.com)
[10:15:10] <tabakhase> doc|home what part of it ? the -fÂ´s? sure, :D but setting COMPOSE_FILES env. var is just fine - and can even be done in the ".env" file right in this folder so you dont need to pollute your shell
[10:21:33] <doc|work> tabakhase,  oh, right! COMPOSE_FILES might not be so bad though. Messy in the directory but I can live with that. Thanks!
[10:31:43] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[11:21:22] *** Joins: Crassus (~Crassus@user/crassus)
[11:35:20] *** Joins: coax (sid481900@id-481900.brockwell.irccloud.com)
[11:40:46] *** Quits: OPK (~OPK@user/opk) (Ping timeout: 265 seconds)
[11:49:31] *** Joins: etiennem- (~etienne@176-149-215-214.abo.bbox.fr)
[11:50:58] *** Quits: etienneme (~etienne@176-149-215-214.abo.bbox.fr) (Ping timeout: 252 seconds)
[12:03:43] *** Joins: chasmo77 (~chas77@71.63.241.168)
[12:05:33] *** Quits: Crassus (~Crassus@user/crassus) (Quit: Textual IRC Client: www.textualapp.com)
[12:18:41] *** Quits: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi) (Quit: test)
[12:18:59] *** Joins: artok (~azo@mobile-access-5672e1-99.dhcp.inet.fi)
[12:23:18] *** Quits: doc|work (~doc@user/doc) (Ping timeout: 265 seconds)
[12:27:04] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[12:27:40] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[12:32:26] *** Joins: rgl (~rgl@bl12-47-147.dsl.telepac.pt)
[12:41:52] *** Quits: PaulFranz (~pfranz@c-73-187-178-34.hsd1.pa.comcast.net) (Ping timeout: 244 seconds)
[12:44:22] *** Joins: PaulFranz (~pfranz@c-73-187-178-34.hsd1.pa.comcast.net)
[12:45:18] *** Joins: Lutin (~Lutin@user/lutin)
[13:24:18] *** Joins: fabienwang (~fabienwan@user/fabienwang)
[13:31:46] *** Joins: Crassus (~Crassus@user/crassus)
[13:35:55] *** Quits: jazzy (~jaziz@2600:380:8753:52bc:dd15:cdbe:7359:7673) (Ping timeout: 244 seconds)
[14:01:15] *** Quits: incognito (~relativit@user/incognito) (Remote host closed the connection)
[14:01:25] *** Joins: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net)
[14:08:11] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 265 seconds)
[14:11:32] *** Quits: cmc (~methos@gateway/tor-sasl/cmc) (Remote host closed the connection)
[14:12:33] *** Joins: cmc (~methos@gateway/tor-sasl/cmc)
[14:16:52] <Lutin> Bossi yoyo! I just though... damn... routing will be still an issue...
[14:24:59] *** Joins: fabienwang_ (~fabienwan@user/fabienwang)
[14:27:32] *** Quits: fabienwang (~fabienwan@user/fabienwang) (Ping timeout: 252 seconds)
[14:34:07] *** Joins: dolor_avis (~dolor_avi@user-5-173-12-65.play-internet.pl)
[14:42:32] *** Quits: fabienwang_ (~fabienwan@user/fabienwang) (Quit: fabienwang_)
[14:45:24] *** Quits: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net) (Ping timeout: 272 seconds)
[14:54:52] *** Joins: bancroft (~bancroft@bras-base-mtrlpq02hsy-grc-09-76-68-189-147.dsl.bell.ca)
[14:55:07] *** Joins: scaleww (~Cmaj6@77-41-20-31.ftth.glasoperator.nl)
[15:02:16] *** Joins: Pingerfowder (~Pingerfow@45.124.53.132)
[15:06:10] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[15:06:18] *** Joins: gschanuel7 (~gschanuel@user/gschanuel)
[15:06:39] <aab_> new version of docker available and nginx proxy manager for you folkes running em
[15:12:05] *** Joins: thiras (~thiras@user/thiras)
[15:19:10] *** Joins: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net)
[15:29:08] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 252 seconds)
[15:33:33] *** Quits: PaulFranz (~pfranz@c-73-187-178-34.hsd1.pa.comcast.net) (Ping timeout: 258 seconds)
[15:35:32] *** Joins: PaulFranz (~pfranz@c-73-187-178-34.hsd1.pa.comcast.net)
[15:38:37] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-10-70-54-112-49.dsl.bell.ca)
[16:00:19] *** Joins: bluehoney (~bluehoney@rrcs-70-61-49-98.central.biz.rr.com)
[16:07:17] *** Joins: iomari891 (~iomari891@105.112.138.38)
[16:27:15] *** Quits: rgl (~rgl@bl12-47-147.dsl.telepac.pt) (Ping timeout: 268 seconds)
[16:36:36] *** Quits: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net) (Ping timeout: 252 seconds)
[16:37:33] *** Quits: bluehoney (~bluehoney@rrcs-70-61-49-98.central.biz.rr.com) (Remote host closed the connection)
[16:42:02] *** Joins: Cleverness (~clevernes@pool-173-52-60-189.nycmny.fios.verizon.net)
[16:44:28] *** Quits: Cleverness (~clevernes@pool-173-52-60-189.nycmny.fios.verizon.net) (Client Quit)
[17:04:49] *** Joins: rgl (~rgl@bl12-47-147.dsl.telepac.pt)
[17:27:38] *** Joins: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net)
[17:41:05] *** Joins: bluehoney (~bluehoney@rrcs-70-61-49-98.central.biz.rr.com)
[17:41:59] *** Quits: gschanuel7 (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[17:42:25] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[17:43:43] *** Quits: zakame (~zakame@user/zakame) (Ping timeout: 268 seconds)
[17:44:23] *** Quits: setesat4 (~setesat@054340f9.skybroadband.com) (Quit: The Lounge - https://thelounge.chat)
[17:47:38] *** Joins: setesat4 (~setesat@054340f9.skybroadband.com)
[17:49:17] *** Joins: zakame (~zakame@user/zakame)
[17:55:05] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[17:57:46] *** Quits: rageshkrishna (~rageshkri@136.185.187.57) (Ping timeout: 265 seconds)
[17:58:04] *** Quits: SirScott (~SirScott@c-67-176-100-163.hsd1.co.comcast.net) (Ping timeout: 258 seconds)
[18:03:49] *** Joins: SirScott (~SirScott@c-67-176-100-163.hsd1.co.comcast.net)
[18:07:42] *** Joins: goddard (~goddard@user/goddard)
[18:08:01] <goddard> what cloud service lets you deplloy easily with docker compose?
[18:09:39] *** Quits: pvdp (~Pieter@58.105.183.94) (Remote host closed the connection)
[18:29:36] <Lutin> goddard aws I think or no azure has something
[18:32:11] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Quit: Atum_)
[18:33:32] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[18:42:32] <rawtaz> i happened to notice that azure has support for uploading a docker-compose file. but i'd be surprised if the others doenst have it as well
[18:43:04] <rawtaz> that said, "easily deploy with docker compose" to me would be equal to "easily setting up a VPS" which is pretty much any cloud service provider. i'd recommend hetzner.cloud
[18:43:11] <rawtaz> they have the best control panel :)
[18:44:56] *** Quits: bluehoney (~bluehoney@rrcs-70-61-49-98.central.biz.rr.com) (Ping timeout: 252 seconds)
[18:55:07] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Quit: Atum_)
[18:58:13] *** Joins: scadman (~scadman@cpc145252-maid8-2-0-cust473.20-1.cable.virginm.net)
[19:03:28] *** Joins: night_wulfe_ (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[19:03:38] *** Quits: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 252 seconds)
[19:04:16] *** Quits: Crassus (~Crassus@user/crassus) (Quit: My MacBook has gone to sleep. ZZZzzzâ¦)
[19:05:43] *** night_wulfe_ is now known as night_wulfe
[19:06:29] *** Quits: flommi_ (~flommi@puck942.startdedicated.de) (Quit: Quit)
[19:07:05] *** Joins: flommi (~flommi@puck942.startdedicated.de)
[19:14:00] *** Joins: flommi_ (~flommi@puck942.startdedicated.de)
[19:14:42] *** Quits: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net) (Read error: Connection reset by peer)
[19:14:59] *** Quits: flommi (~flommi@puck942.startdedicated.de) (Ping timeout: 268 seconds)
[19:17:42] *** Quits: flommi_ (~flommi@puck942.startdedicated.de) (Client Quit)
[19:17:54] *** Joins: flommi (~flommi@puck942.startdedicated.de)
[19:23:11] *** Quits: scaleww (~Cmaj6@77-41-20-31.ftth.glasoperator.nl) (Quit: Leaving)
[19:23:16] <artok> anything that has good cli
[19:28:22] <Lutin> artok I heard your cli is perfect
[19:31:33] <s17> I don't use -compose.  I build and then push to ECR and use terraform to start it up
[19:34:37] *** Joins: GeorgeK (~GeorgeK@cpe-70-92-5-228.wi.res.rr.com)
[19:35:36] *** Quits: scadman (~scadman@cpc145252-maid8-2-0-cust473.20-1.cable.virginm.net) (Quit: Konversation terminated!)
[19:37:35] *** Parts: FreeBDSM (~uzer@user/freebdsm) ()
[19:37:53] *** Quits: dolor_avis (~dolor_avi@user-5-173-12-65.play-internet.pl) (Quit: dolor_avis)
[19:46:37] *** Quits: tyson2 (~user@bras-base-toroon0624w-grc-10-70-54-112-49.dsl.bell.ca) (Remote host closed the connection)
[19:47:43] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-10-70-54-112-49.dsl.bell.ca)
[19:48:40] *** Joins: cliluw (~cliluw@47.147.80.149)
[19:51:00] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Quit: rewrit3)
[19:53:22] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[19:53:59] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[19:57:25] *** Joins: lemonzest (~lemonzest@user/lemonzest)
[19:57:27] <rawtaz> that doesnt always make sense, and in those cases compose is awesome :)
[19:57:30] * rawtaz &
[20:08:50] <Lutin> s17 just everything by hand on baremetals is future :P
[20:17:59] *** Quits: tyson2 (~user@bras-base-toroon0624w-grc-10-70-54-112-49.dsl.bell.ca) (Ping timeout: 258 seconds)
[20:37:42] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[20:45:13] *** Quits: cliluw (~cliluw@47.147.80.149) (Remote host closed the connection)
[20:46:23] *** Joins: cliluw (~cliluw@47.147.80.149)
[20:46:32] *** Joins: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com)
[20:48:21] *** Joins: sesquisentient (uid38151@id-38151.stonehaven.irccloud.com)
[20:48:46] *** sesquisentient is now known as ses
[20:50:42] *** Quits: cliluw (~cliluw@47.147.80.149) (Ping timeout: 252 seconds)
[20:51:13] <artok> includeos
[20:53:25] <Lutin> artok looks nice... gonna look that one up as well :)
[20:53:26] <Lutin> bbl!
[20:53:31] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[20:55:54] <s17> yeah, I need to learn more about compose.  I learned Terraform and then Docker and only a tiny bit about Compose
[20:56:07] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Ping timeout: 244 seconds)
[20:59:05] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[21:02:30] *** Joins: SomeWeirdAnon (~shwn@2a02:8109:abf:ffb4:9505:5c9e:7f24:4f04)
[21:03:08] *** Joins: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net)
[21:06:55] *** Joins: Atum__ (~IRC@user/atum/x-2392232)
[21:09:50] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Ping timeout: 272 seconds)
[21:10:23] *** Quits: foka (~foka@162.208.172.172) (Read error: Connection reset by peer)
[21:12:30] *** Joins: foka (~foka@162.208.172.172)
[21:15:15] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[21:43:40] *** Joins: doc|work (~doc@user/doc)
[21:43:50] *** Quits: sebastianos (~sebastian@user/sebastianos) (Read error: Connection reset by peer)
[21:46:34] *** Quits: bancroft (~bancroft@bras-base-mtrlpq02hsy-grc-09-76-68-189-147.dsl.bell.ca) (Ping timeout: 272 seconds)
[21:47:23] *** Joins: sebastianos (~sebastian@user/sebastianos)
[21:59:14] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[21:59:35] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[22:00:06] *** Quits: Atum__ (~IRC@user/atum/x-2392232) (Quit: Atum__)
[22:00:29] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-10-70-54-112-49.dsl.bell.ca)
[22:08:57] *** Quits: tyson2 (~user@bras-base-toroon0624w-grc-10-70-54-112-49.dsl.bell.ca) (Remote host closed the connection)
[22:09:54] *** Quits: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net) (Ping timeout: 252 seconds)
[22:11:53] *** Quits: cmc (~methos@gateway/tor-sasl/cmc) (Remote host closed the connection)
[22:12:53] *** Joins: cmc (~methos@gateway/tor-sasl/cmc)
[22:13:21] *** Quits: factor (~factor@c-66-30-67-217.hsd1.ma.comcast.net) (Read error: Connection reset by peer)
[22:14:31] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-10-70-54-112-49.dsl.bell.ca)
[22:15:24] *** Joins: jkwnki (~jkwnki@p4fedb926.dip0.t-ipconnect.de)
[22:21:16] *** Joins: factor (~factor@c-66-30-67-217.hsd1.ma.comcast.net)
[22:21:57] *** Joins: SmokenatorZ (~Smokenato@187.10.51.101)
[22:22:23] *** Quits: factor (~factor@c-66-30-67-217.hsd1.ma.comcast.net) (Read error: Connection reset by peer)
[22:23:02] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[22:28:27] *** Joins: jimmyb (~jimmyb@user/jimmyb)
[22:29:53] *** Joins: jazzy (~jaziz@2600:380:8753:52bc:b84e:2ddb:61ff:c252)
[22:31:32] *** Joins: cliluw (~cliluw@47.147.80.149)
[22:38:29] *** Quits: ada_ (~ada@user/ada/x-9065485) (Read error: Connection reset by peer)
[22:48:26] *** Quits: jayray (~jayray@user/jayray) (Ping timeout: 244 seconds)
[22:49:18] *** Quits: foka (~foka@162.208.172.172) (Read error: Connection reset by peer)
[22:49:38] *** Joins: jayray (~jayray@user/jayray)
[22:49:50] *** Joins: foka (~foka@162.208.172.172)
[22:54:24] *** Quits: foka (~foka@162.208.172.172) (Ping timeout: 250 seconds)
[22:55:59] *** Quits: jayray (~jayray@user/jayray) (Ping timeout: 265 seconds)
[23:01:48] *** Quits: thanas (~thanas@user/thanas) (Quit: ZNC - https://znc.in)
[23:02:22] *** Joins: Atum__ (~IRC@user/atum/x-2392232)
[23:02:50] *** Joins: Lutin (~Lutin@user/lutin)
[23:03:38] *** Joins: incognito (~relativit@user/incognito)
[23:03:46] *** Joins: thanas (~thanas@user/thanas)
[23:05:21] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Ping timeout: 272 seconds)
[23:06:27] *** Joins: Brainium (~brainium@user/brainium)
[23:10:23] *** Joins: jayray (~jayray@user/jayray)
[23:11:12] *** Quits: lemonzest (~lemonzest@user/lemonzest) (Quit: Quitting)
[23:15:14] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Read error: Connection reset by peer)
[23:17:07] *** Joins: factor (~factor@c-66-30-67-217.hsd1.ma.comcast.net)
[23:17:44] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[23:27:51] *** Quits: ses (uid38151@id-38151.stonehaven.irccloud.com) (Quit: Connection closed for inactivity)
[23:39:09] *** Quits: Atum__ (~IRC@user/atum/x-2392232) (Quit: Atum__)
[23:42:34] <Lutin> artok how is your cli ?
[23:44:42] *** Joins: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net)
[23:46:15] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[23:47:21] <Lutin> artok that includeos feels weird
[23:50:42] *** Joins: Ryu_ (~Ryu945@154.3.251.91)
[23:53:24] *** Joins: foka (~foka@162.208.172.172)
