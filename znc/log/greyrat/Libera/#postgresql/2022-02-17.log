[00:02:20] *** Joins: rodo (~rodo@pop.92-184-117-117.mobile.abo.orange.fr)
[00:05:19] *** Quits: PCatinean (~pcatinean@188.26.231.238) (Quit: Leaving)
[00:06:18] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Remote host closed the connection)
[00:06:50] *** Joins: sliss (~sliss@109.136.165.60)
[00:08:12] *** Quits: `2jt (~jtomas@130.red-88-22-46.staticip.rima-tde.net) (Ping timeout: 240 seconds)
[00:10:55] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[00:11:15] *** Joins: ur5us_ (~ur5us@203.86.198.200)
[00:14:10] *** Joins: ur5us (~ur5us@2406:e002:6804:8a01:fe29:d3cc:a0c3:c22)
[00:14:23] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[00:15:07] *** Joins: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[00:15:08] *** Quits: ur5us_ (~ur5us@203.86.198.200) (Read error: Connection reset by peer)
[00:15:48] *** Quits: otisolsen70 (~otisolsen@xd4ed80b5.cust.hiper.dk) (Quit: Leaving)
[00:16:10] *** Quits: sliss (~sliss@109.136.165.60) (Quit: sliss)
[00:16:42] *** Joins: rednul (~rednul@mc.emailserverz.com)
[00:16:46] *** Quits: mizi (~mizi@user/mizi) (Ping timeout: 272 seconds)
[00:18:39] *** Joins: finsternis (~X@23.226.237.192)
[00:19:19] *** Quits: rodo (~rodo@pop.92-184-117-117.mobile.abo.orange.fr) (Read error: Connection reset by peer)
[00:19:56] *** Quits: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Ping timeout: 272 seconds)
[00:20:42] *** Joins: dre (~dre@101.191.49.59)
[00:21:19] *** Joins: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[00:22:18] *** Quits: r2ndur (~r2ndur@46-39-145-84.telset.ee) (Remote host closed the connection)
[00:22:26] <StuckMojo> ??synchronous_commit
[00:22:27] <pg_docbot> https://www.postgresql.org/docs/current/static/runtime-config-wal.html#GUC-SYNCHRONOUS-COMMIT
[00:27:03] *** midipix_ is now known as midipix
[00:30:52] *** Quits: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Remote host closed the connection)
[00:33:56] <ario> StuckMojo: you know how aurora readers do a round robin? wouldn't that hurt the host caching of queries?
[00:34:03] <Fairy> I want to drop a SCHEMA with 56k tables, as I understood it, with max_locks_per_transaction set to 1024 and max_connections set to 64, then 64 * 1024 = 65k, which is more than 56k; am I missing something?
[00:34:04] <ario> hot*
[00:34:25] *** Joins: NCS_One (~NCS_One@bl11-90-133.dsl.telepac.pt)
[00:34:33] *** Joins: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[00:35:46] *** Quits: rvalue (~rvalue@user/rvalue) (Read error: Connection reset by peer)
[00:35:54] <Fairy> (to clarify, I'm hitting the "out of shared memory" ERROR)
[00:35:59] *** Joins: rvalue (~rvalue@user/rvalue)
[00:36:29] <StuckMojo> ??mvcc
[00:36:29] <pg_docbot> http://en.wikipedia.org/wiki/Multiversion_concurrency_control :: http://devcenter.heroku.com/articles/postgresql-concurrency
[00:36:29] <pg_docbot> http://momjian.us/main/presentations/internals.html#mvcc :: https://brandur.org/postgres-atomicity
[00:36:29] <pg_docbot> https://www.postgresql.org/docs/current/static/mvcc.html
[00:36:55] *** Quits: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon) (Ping timeout: 240 seconds)
[00:38:57] *** Joins: ben__ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[00:40:23] <StuckMojo> ??wraparound
[00:40:23] <pg_docbot> https://www.postgresql.org/docs/current/static/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND
[00:40:52] *** Joins: andrew_dryga (uid297611@id-297611.helmsley.irccloud.com)
[00:41:28] *** Quits: furrymcgee (~devuan@cgn-89-1-211-93.nc.de) (Ping timeout: 272 seconds)
[00:41:45] <peerce> how did you end up with 56000 different relations ?!?
[00:41:48] *** Quits: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Ping timeout: 240 seconds)
[00:41:59] <peerce> that sounds out of control, or a case of excess partitioning
[00:42:13] <Berge> Fairy: Didn't we talk about this the other day? (-:
[00:42:38] <Fairy> this is cleanup ;-)
[00:42:58] <andrew_dryga> Hey guys, can someone give me a hint why an index is not used in this query? I tried to run vacuum analyze but it doesn't help and the query is fairly slow.
[00:42:58] <andrew_dryga> Query:
[00:42:58] <andrew_dryga> ```
[00:42:58] <andrew_dryga> SELECT DISTINCT (normalized_properties->>'make'), count(normalized_properties->>'make')
[00:42:58] *** Quits: andrew_dryga (uid297611@id-297611.helmsley.irccloud.com) (Excess Flood)
[00:43:06] *** Joins: andrew_dryga (uid297611@id-297611.helmsley.irccloud.com)
[00:43:10] <Fairy> it's great not doing something, but gotta make sure to destroy the evidence of the first attempt! :)
[00:43:12] <andrew_dryga> Maybe I'm blind and missing something, dunno
[00:43:41] <Fairy> Wouldn't want anyone seeing how little I know about what I'm doing ;)
[00:43:48] *** Quits: ur5us (~ur5us@2406:e002:6804:8a01:fe29:d3cc:a0c3:c22) (Ping timeout: 240 seconds)
[00:44:33] <andrew_dryga> Here is the explain:
[00:44:33] <andrew_dryga> ```
[00:44:33] <andrew_dryga> HashAggregate  (cost=158657.97..158705.66 rows=3815 width=40) (actual time=725.263..725.358 rows=259 loops=1)
[00:44:33] <andrew_dryga>   Group Key: (normalized_properties ->> 'make'::text)
[00:44:33] <andrew_dryga>   ->  Index Scan using peach_main_index on vehicles  (cost=0.42..158224.09 rows=57851 width=69) (actual time=0.050..645.461 rows=247315 loops=1)
[00:44:34] <andrew_dryga>         Filter: (((normalized_properties -> 'make'::text) IS NOT NULL) AND ((normalized_properties -> 'make'::text) <> 'null'::jsonb))
[00:44:34] <andrew_dryga>         Rows Removed by Filter: 34151
[00:44:35] <andrew_dryga> Planning Time: 0.443 ms
[00:44:35] <andrew_dryga> Execution Time: 725.468 ms
[00:44:36] <andrew_dryga> ```
[00:46:43] *** Joins: Klinda (~superleag@user/klinda)
[00:47:35] <Berge> Fairy: Didn't we suggest dropping the tables in the schema in batches?
[00:51:25] <Fairy> you did, but as a lazy developer, I opted for the nuclear option of resetting the work
[00:52:09] <Berge> It's unclear to me if you're just trying to solve the problem at hand (removing this schema as a one-off), or actually trying to solve something else
[00:52:45] *** Joins: ur5us (~ur5us@2406:e002:6804:8a01:fe29:d3cc:a0c3:c22)
[00:55:12] <Fairy> no, in this case, it was easier for me to drop the schemas (yes, there's "a few" of them *hides*)
[00:55:35] <Fairy> (well, was supposed to be easier, but then I'm back here again, so, you know)
[00:55:46] <Berge> Of course, that would've been easier if you didn't hit the lock limit
[00:55:48] <Berge> But as you did…
[00:56:12] *** Quits: evdubs_ (~evdubs@user/evdubs) (Ping timeout: 240 seconds)
[00:56:31] <Berge> So, you can either just increase the limit to something very, very high for this specific purpose (and decrease it afterwards), or generate some DDL to remove tables in batche
[00:57:57] *** Joins: pvn1 (~Adium@147.87.131.200)
[00:59:03] <Fairy> my impression was (if I understood the above rules right), I temporarily increased the limit for this purpose now, but I still hit the issue with dropping 52k+table schemas
[00:59:09] <Berge> Fairy: Something like: SELECT FORMAT('DROP TABLE "%s";', table_name) FROM information_schema.tables WHERE table_schema = 'myschema';
[00:59:14] <Berge> And save that output to a file, and just \i that file
[00:59:37] <pvn1> How do you name your tables? Singular (i.e. [c,C]ustomer) or plural (i.e. [c,C]ustomers)?
[00:59:40] <johto> %I
[00:59:43] <Berge> pvn1: Sigular
[00:59:43] <johto> instead of "%s"
[00:59:46] <Berge> johto: ah, yes
[00:59:52] <Berge> I didn't actually test the query either
[01:00:26] <pvn1> Berge: capitalized (CamelCase)?
[01:00:37] <Berge> pvn1: Oh, no, absolutely not
[01:00:46] *** Joins: Auron (Auron956@user/auron)
[01:00:48] <Berge> In postgres, you have to quote such identifiers
[01:00:49] *** Joins: KombuchaKip (~kip@192.252.230.5)
[01:01:18] <Berge> Or, to be specific: Unqouted identifiers (such as table names or column names) are always folded to lower acse
[01:01:20] <Berge> case
[01:02:14] <pvn1> and snake_case then
[01:02:15] <pvn1> ?
[01:02:25] <Berge> yes, I prefer that
[01:02:52] <Berge> I know some people here prefer plural names, so don't take my opinion as more than a random person on the Internet's opinion (-:
[01:03:33] <pvn1> Berge: appreciate your comment, nontheless ;)
[01:03:42] <pvn1> *nonetheless
[01:03:52] <pvn1> THX!
[01:03:53] <Berge> I think singular makes for slightly more readable queries
[01:04:19] <pvn1> and it makes more sense: each row is an instance of the `table`
[01:04:34] <pvn1> (in OOP parlance)
[01:04:44] <Berge> Exactly
[01:05:02] <Berge> But you could also argue that a table is a collection of multiple rows
[01:05:03] <pvn1> !!Gracias!
[01:05:48] *** Quits: ur5us (~ur5us@2406:e002:6804:8a01:fe29:d3cc:a0c3:c22) (Ping timeout: 240 seconds)
[01:05:57] <pvn1> well, an array then. But an array should also be singular, IMHO
[01:06:34] <Berge> It's not a hill I'm willing to die on, though (-:
[01:06:58] *** Joins: maret (~maret@nat-88-212-37-89.antik.sk)
[01:08:29] *** Quits: magla (~gelignite@55d40b59.access.ecotel.net) (Quit: Stay safe!)
[01:08:57] <pvn1> ;0)
[01:09:00] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Client Quit)
[01:14:39] *** Joins: pvn (~Adium@85-195-230-32.fiber7.init7.net)
[01:15:27] *** Joins: aremaref (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net)
[01:17:34] *** Quits: pvn1 (~Adium@147.87.131.200) (Ping timeout: 272 seconds)
[01:18:57] *** Quits: Elodin (~elodin@user/elodin) (Read error: Connection reset by peer)
[01:19:49] <bmomjian1> I know I can read/write data in another cluster using fdws, and read it using logical replication, but how can I call a function on another server from SQL?
[01:20:08] *** Joins: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26)
[01:20:11] <peerce> same way, via FDW
[01:20:24] <peerce> oh, remote function, wait.  hmmmm.
[01:23:12] *** Joins: aremaref_ (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net)
[01:24:04] <ktosiek> dblink?
[01:24:26] <ktosiek> But that will not play nice with the planner
[01:24:35] <aremaref_> When doing an ORDER BY, if there are two rows that have the same value, how does postgres decide which row comes first? Which one it gets first?
[01:24:53] *** Joins: Elodin (~elodin@user/elodin)
[01:25:57] *** Joins: evdubs (~evdubs@user/evdubs)
[01:27:23] <ktosiek> aremaref_: whatever happens to be more convenient at the moment, it depends heavily on the plan.
[01:28:32] <bmomjian1> I am working on a microservices talk and realized a 'setter' function can't be easily called remotely.
[01:29:34] <peerce> i suppose you could use a plperlu or plpython or something function to open a connection to the remote server, and issue your query, but that would be pretty much the same as using dblink
[01:29:46] <bmomjian1> Ewe, yeah, that would work.
[01:30:15] <bmomjian1> I am thinking for the FDW I can just call the setter locally --- it is logical replication that has me hung up on the call.
[01:30:34] *** Quits: ekathva (~ekathva@n3k02urgmuqvuf9nu-1.v6.elisa-mobile.fi) (Remote host closed the connection)
[01:30:37] <bmomjian1> I am trying to show microservice data encapsulation.
[01:32:58] *** Quits: ben__ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Remote host closed the connection)
[01:34:21] *** Joins: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[01:35:04] *** Joins: schinckel (uid38120@id-38120.ilkley.irccloud.com)
[01:36:43] *** Quits: uncleyear (~ian@pppoe.178-66-157-18.dynamic.avangarddsl.ru) (Ping timeout: 256 seconds)
[01:37:58] <ktosiek> bmomjian1: That sounds interesting, you want to do RPC between services hosted in different postgres clusters?
[01:41:44] <bmomjian1> Yes.
[01:43:30] <StuckMojo> there's some gotchas around large objects and backups?
[01:43:35] <StuckMojo> iSTR...
[01:43:59] *** Joins: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon)
[01:44:59] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[01:45:00] *** Quits: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26) (Ping timeout: 240 seconds)
[01:45:05] *** Quits: Auron (Auron956@user/auron) (Remote host closed the connection)
[01:48:55] *** Joins: bpmedley (~bpmedley@2600:1700:e2c:8410:5052:1a9e:4853:6fe1)
[01:49:55] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[01:50:38] <ktosiek> bmomjian1: for that you'll need to use dblink or (as peerce mentioned) some language that can open connections. Somehow RPC *from* a database feels a bit strange. I'd expect calls to the DB to be (almost) final - like "this value has changed, user told us so, save that". I would only expect Event Sourcing-style communication from one DB to another - think one DB saying "we've recorded a change to this fact", and another going "oh! In that case
[01:50:38] <ktosiek> I'll update some part of my reports"
[01:50:45] <mbanck> bmomjian1: dunno, did you look at postgrest, the RESTful API thingy? Maybe that is useful here
[01:52:40] *** Quits: pedja (~pedja@user/deus-ex/x-7934090) (Quit: Leaving)
[01:54:08] *** Quits: Rashad (~textual@2a01:9700:1a7c:8900:658f:58fa:f5cc:30bb) (Quit: My MacBook Air has gone to sleep. ZZZzzz…)
[01:54:46] *** Quits: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net) (Read error: Connection reset by peer)
[01:55:08] *** Quits: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Remote host closed the connection)
[01:55:12] <ktosiek> StuckMojo: yes
[01:55:25] *** Quits: wolfshappen (~waff@irc.furworks.de) (Ping timeout: 256 seconds)
[01:56:04] <ktosiek> oh wait, maybe I'm out of date
[01:57:12] <ktosiek> what kind of backups are we talking? I think even pg_dump handles them for years now
[01:57:39] *** Quits: NCS_One (~NCS_One@bl11-90-133.dsl.telepac.pt) (Quit: Lost terminal)
[01:57:40] *** Joins: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net)
[01:58:05] <StuckMojo> it's someone on slack having issues restoring a 9.6 dump (taken with v13) which has like 30M LOs in it
[01:58:44] *** Quits: fcr (~fran@r167-60-22-5.dialup.adsl.anteldata.net.uy) (Ping timeout: 272 seconds)
[01:59:22] *** Quits: pvn (~Adium@85-195-230-32.fiber7.init7.net) (Quit: Leaving.)
[01:59:48] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[02:00:12] *** Quits: Komzzpa (~kom@2a02:220f:2000:2c00:86eb:38e3:744:b14c) (Ping timeout: 252 seconds)
[02:00:18] <ktosiek> I'm lost - 9.6 taken with v13? Do you mean PostgreSQL server 9.6 dumped with pg_dump from 13.x?
[02:01:30] <StuckMojo> pg_dump v13, as is best practice
[02:01:43] <StuckMojo> he's upgrading from 9.6 to 13
[02:02:22] <StuckMojo> he got it fixed, by raising shared_buffers and mainenance_work_mem (i suspect it was the latter that did the trick)
[02:02:37] *** Joins: Atque (~Atque@user/atque)
[02:03:11] *** Quits: Elodin (~elodin@user/elodin) (Read error: Connection reset by peer)
[02:03:39] *** Joins: funhouse (~funhouse@user/funhouse)
[02:04:01] *** Joins: fcr (~fran@r186-48-162-7.dialup.adsl.anteldata.net.uy)
[02:05:10] <ktosiek> I've just looked at Slack: there's also fsync = off involved, which probably helped too
[02:05:29] <StuckMojo>  he already had that off
[02:07:42] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[02:09:07] *** Joins: Elodin (~elodin@user/elodin)
[02:11:16] *** Quits: rendar (~rendar@user/rendar) (Quit: Leaving)
[02:11:42] <ktosiek> read through the thread (and your answers), and it's way out of my league :-). Hopefully the restore is fast enough now.
[02:12:32] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Read error: Connection reset by peer)
[02:12:36] *** Quits: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net) (Ping timeout: 240 seconds)
[02:14:12] *** Quits: Nekomander (~BadAdvice@user/badadvicecat) (Ping timeout: 240 seconds)
[02:14:14] *** Joins: k8yun (~k8yun@user/k8yun)
[02:14:36] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[02:16:36] *** Quits: john_johnk (~Thunderbi@232.58.140.77.rev.sfr.net) (Ping timeout: 240 seconds)
[02:20:12] *** Quits: cliluw (~cliluw@47.147.73.223) (Ping timeout: 240 seconds)
[02:24:59] *** Joins: Nekomander (~BadAdvice@user/badadvicecat)
[02:25:21] *** Quits: michalz (~michalz@185.246.204.89) (Remote host closed the connection)
[02:25:48] <ario> what should a max number of connection pools ought to be?
[02:25:58] <ario> based on the number of commands per second?
[02:29:21] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Remote host closed the connection)
[02:29:53] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[02:30:02] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[02:30:06] <RLa> ario, more like number of concurrent commands per second
[02:31:46] <ario> ah yeah that makes more sense
[02:33:50] *** Joins: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net)
[02:34:50] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[02:35:53] <hoppity> Hi guys, can Postgres store a datetime with timezone?
[02:36:07] <kjetilho> yes
[02:36:40] <davidfetter> um
[02:38:39] <davidfetter> timestamp with time zone is a little wonky, but it's still by far the best thing to use. What it'll do is convert the input time to UTC and store that. It doesn't store the input time zone, or even the UTC offset of the input time zone.
[02:39:55] *** Joins: maret (~maret@nat-88-212-37-89.antik.sk)
[02:40:11] <davidfetter> tl;dr: use timestamptz and remember that it throws away the input time zone after it's done computing UTC. This is way better than simply ignoring the input time zone, as timestamp without time zone would do.
[02:43:59] *** Joins: cliluw (~cliluw@47.147.77.43)
[02:44:43] *** Quits: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net) (Ping timeout: 256 seconds)
[02:44:52] *** Quits: analogsalad (~analogsal@user/analogsalad) (Quit: bye)
[02:45:10] *** Joins: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net)
[02:48:09] <hoppity> davidfetter: the thing is that I will have some users enter a datetime with a given timezone but then I need to show to a different user the "time left" in his own timezone.
[02:48:09] *** Quits: rufito (~phil@186-79-29-179.baf.movistar.cl) (Quit: Leaving)
[02:48:12] <hoppity> if that makes sense
[02:51:06] <davidfetter> hoppity, it does, and that's why you should use timestamptz. when you want to convert, you can use the timezone client setting or AT TIME ZONE in sql.
[02:51:20] <hoppity> I see thank you
[02:53:15] *** Quits: RLa (~RLa@82.131.24.208.cable.starman.ee) (Quit: Client closed)
[02:54:24] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[02:55:32] *** Joins: ba|ch` (~user@p200300f3a700c4a06e44f7dadbf988c3.dip0.t-ipconnect.de)
[02:56:37] *** Quits: ba|ch (~user@p200300f3a700c4500cbe5164bd1b841a.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[02:59:22] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Quit: maret)
[03:03:48] *** Quits: shka (~herr@109.231.0.226) (Ping timeout: 240 seconds)
[03:03:58] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[03:05:02] *** Quits: braxas (sid508886@id-508886.lymington.irccloud.com) (Ping timeout: 240 seconds)
[03:05:02] *** Quits: Cromulent (sid301841@id-301841.helmsley.irccloud.com) (Ping timeout: 240 seconds)
[03:05:09] *** Quits: thana (uid398790@id-398790.ilkley.irccloud.com) (Read error: Connection reset by peer)
[03:05:14] *** Quits: billputer (sid35666@id-35666.lymington.irccloud.com) (Read error: Connection reset by peer)
[03:05:15] *** Quits: liquid-silence (sid522629@id-522629.uxbridge.irccloud.com) (Read error: Connection reset by peer)
[03:05:15] *** Quits: mihait (sid29790@id-29790.ilkley.irccloud.com) (Read error: Connection reset by peer)
[03:05:15] *** Quits: Skaag (sid164385@id-164385.hampstead.irccloud.com) (Read error: Connection reset by peer)
[03:05:18] *** Joins: thana (uid398790@id-398790.ilkley.irccloud.com)
[03:05:21] *** Quits: PyHedgehog (uid146498@id-146498.ilkley.irccloud.com) (Read error: Connection reset by peer)
[03:05:21] *** Quits: tobias1 (sid15532@django/member/tobias1) (Ping timeout: 245 seconds)
[03:05:21] *** Quits: sa (sid1055@id-1055.tinside.irccloud.com) (Ping timeout: 250 seconds)
[03:05:24] *** Joins: mihait (sid29790@id-29790.ilkley.irccloud.com)
[03:05:24] *** Joins: billputer (sid35666@id-35666.lymington.irccloud.com)
[03:05:25] *** Joins: liquid-silence (sid522629@id-522629.uxbridge.irccloud.com)
[03:05:30] *** Joins: PyHedgehog (uid146498@id-146498.ilkley.irccloud.com)
[03:05:32] *** Quits: geoffeg (sid5159@id-5159.ilkley.irccloud.com) (Ping timeout: 240 seconds)
[03:05:38] *** Quits: hoek (sid223452@id-223452.lymington.irccloud.com) (Read error: Connection reset by peer)
[03:05:38] *** Quits: hide (sid299131@id-299131.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:38] *** Quits: leont (sid489095@id-489095.uxbridge.irccloud.com) (Read error: Connection reset by peer)
[03:05:38] *** Quits: r0bby (r0bby@user/r0bby) (Ping timeout: 260 seconds)
[03:05:38] *** Quits: jposer (sid132110@id-132110.uxbridge.irccloud.com) (Ping timeout: 260 seconds)
[03:05:39] *** Joins: Cromulent (sid301841@id-301841.helmsley.irccloud.com)
[03:05:39] *** Quits: schinckel (uid38120@id-38120.ilkley.irccloud.com) (Read error: Connection reset by peer)
[03:05:40] *** Quits: scav (sid309693@id-309693.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:41] *** Quits: joenoon (sid411732@id-411732.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:41] *** Quits: ysh (sid6017@id-6017.ilkley.irccloud.com) (Read error: Connection reset by peer)
[03:05:42] *** Quits: ehamberg (sid18208@id-18208.hampstead.irccloud.com) (Read error: Connection reset by peer)
[03:05:42] *** Quits: OliverMT (sid16701@id-16701.hampstead.irccloud.com) (Read error: Connection reset by peer)
[03:05:42] *** Quits: keyvan (sid7672@id-7672.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:43] *** Quits: dsal (sid13060@id-13060.lymington.irccloud.com) (Read error: Connection reset by peer)
[03:05:45] *** Quits: kevinsjoberg (sid499516@id-499516.lymington.irccloud.com) (Read error: Connection reset by peer)
[03:05:46] *** Joins: hoek (sid223452@id-223452.lymington.irccloud.com)
[03:05:46] *** Quits: elsmorian (sid307171@id-307171.lymington.irccloud.com) (Ping timeout: 245 seconds)
[03:05:46] *** Quits: dba (uid533975@id-533975.hampstead.irccloud.com) (Ping timeout: 245 seconds)
[03:05:47] *** Quits: wooster_ (sid316977@id-316977.helmsley.irccloud.com) (Ping timeout: 250 seconds)
[03:05:47] *** Quits: jabashque (uid340452@id-340452.ilkley.irccloud.com) (Ping timeout: 250 seconds)
[03:05:48] *** Joins: hide (sid299131@id-299131.helmsley.irccloud.com)
[03:05:48] *** Quits: pacninja (uid471347@id-471347.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:49] *** Quits: nightstrike (uid487@id-487.uxbridge.irccloud.com) (Ping timeout: 240 seconds)
[03:05:50] *** Joins: geoffeg (sid5159@id-5159.ilkley.irccloud.com)
[03:05:50] *** Quits: Lvl4Sword (sid483043@user/lvl4sword) (Read error: Connection reset by peer)
[03:05:50] *** Quits: agronholm (sid403424@id-403424.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:51] *** Quits: kraih (sid17075@mojo/destroy-all-humans/kraih) (Read error: Connection reset by peer)
[03:05:52] *** Joins: keyvan (sid7672@id-7672.helmsley.irccloud.com)
[03:05:52] *** Joins: joenoon (sid411732@id-411732.helmsley.irccloud.com)
[03:05:53] *** Joins: ysh (sid6017@id-6017.ilkley.irccloud.com)
[03:05:54] *** Quits: shawnd (sid96041@id-96041.ilkley.irccloud.com) (Read error: Connection reset by peer)
[03:05:54] *** Quits: coaxmetal_ (sid177413@id-177413.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:55] *** Joins: dsal (sid13060@id-13060.lymington.irccloud.com)
[03:05:55] *** Joins: kevinsjoberg (sid499516@id-499516.lymington.irccloud.com)
[03:05:56] *** Quits: nickb (sid293439@id-293439.uxbridge.irccloud.com) (Read error: Connection reset by peer)
[03:05:58] *** Quits: rubin55 (sid175221@id-175221.hampstead.irccloud.com) (Read error: Connection reset by peer)
[03:05:59] *** Quits: Knyght (sid365271@id-365271.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:05:59] *** Joins: pacninja (sid471347@id-471347.helmsley.irccloud.com)
[03:05:59] *** Joins: leont (sid489095@id-489095.uxbridge.irccloud.com)
[03:05:59] *** Quits: gajus (sid202456@id-202456.tinside.irccloud.com) (Read error: Connection reset by peer)
[03:05:59] *** Quits: pitastrudl (sid332046@user/pitastrudl) (Read error: Connection reset by peer)
[03:05:59] *** Quits: KnownSyntax (sid233169@user/knownsyntax) (Read error: Connection reset by peer)
[03:06:00] *** Quits: MrNaz (sid133418@id-133418.uxbridge.irccloud.com) (Read error: Connection reset by peer)
[03:06:00] *** Joins: jabashque (uid340452@id-340452.ilkley.irccloud.com)
[03:06:00] *** Quits: Liothen (sid23291@id-23291.helmsley.irccloud.com) (Read error: Connection reset by peer)
[03:06:02] *** Quits: jakesyl__ (sid56879@id-56879.hampstead.irccloud.com) (Read error: Connection reset by peer)
[03:06:03] *** Quits: Vierdo (uid133359@id-133359.tinside.irccloud.com) (Read error: Connection reset by peer)
[03:06:04] *** Joins: agronholm (sid403424@id-403424.helmsley.irccloud.com)
[03:06:04] *** Joins: OliverMT (sid16701@id-16701.hampstead.irccloud.com)
[03:06:09] *** Joins: Lvl4Sword (sid483043@user/lvl4sword)
[03:06:10] *** Joins: wooster_ (sid316977@id-316977.helmsley.irccloud.com)
[03:06:16] *** Joins: shawnd (sid96041@id-96041.ilkley.irccloud.com)
[03:06:19] *** Joins: Skaag (sid164385@id-164385.hampstead.irccloud.com)
[03:06:20] *** Joins: nickb (sid293439@id-293439.uxbridge.irccloud.com)
[03:06:23] *** Joins: MrNaz (sid133418@id-133418.uxbridge.irccloud.com)
[03:06:24] *** Joins: dba (sid533975@id-533975.hampstead.irccloud.com)
[03:06:26] *** Joins: gajus (sid202456@id-202456.tinside.irccloud.com)
[03:06:31] *** Joins: rubin55 (sid175221@id-175221.hampstead.irccloud.com)
[03:06:32] *** Joins: KnownSyntax (sid233169@user/knownsyntax)
[03:06:33] *** Joins: ehamberg (sid18208@id-18208.hampstead.irccloud.com)
[03:06:35] *** Joins: Liothen (sid23291@id-23291.helmsley.irccloud.com)
[03:06:38] *** Joins: schinckel (uid38120@id-38120.ilkley.irccloud.com)
[03:06:40] *** Joins: Vierdo (sid133359@2a03:5180:f::2:8ef)
[03:06:40] *** Joins: braxas (sid508886@id-508886.lymington.irccloud.com)
[03:06:41] *** Joins: kraih (sid17075@mojo/destroy-all-humans/kraih)
[03:06:48] *** Joins: coaxmetal_ (sid177413@2a03:5180:f:1::2:b505)
[03:06:53] *** Joins: jposer (sid132110@2a03:5180:f:5::2:40e)
[03:06:53] *** Joins: scav (sid309693@id-309693.helmsley.irccloud.com)
[03:06:54] *** Joins: elsmorian (sid307171@id-307171.lymington.irccloud.com)
[03:06:55] *** Joins: jakesyl__ (sid56879@2a03:5180:f:4::de2f)
[03:06:56] *** Joins: pitastrudl (sid332046@user/pitastrudl)
[03:07:00] *** Joins: Knyght (sid365271@2a03:5180:f:1::5:92d7)
[03:07:02] *** Joins: sa (sid1055@2a03:5180:f::41f)
[03:07:04] *** Joins: tobias1 (sid15532@django/member/tobias1)
[03:07:05] *** Joins: nightstrike (uid487@2a03:5180:f:5::1e7)
[03:07:14] *** Joins: r0bby (r0bby@user/r0bby)
[03:09:02] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[03:09:56] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:13:51] *** Quits: funhouse (~funhouse@user/funhouse) (Quit: Client closed)
[03:16:35] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[03:17:48] *** Quits: finsternis (~X@23.226.237.192) (Ping timeout: 240 seconds)
[03:22:43] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[03:27:13] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[03:27:38] *** Joins: Guest1107 (~Thunderbi@p10605177-ipngn25001marunouchi.tokyo.ocn.ne.jp)
[03:30:32] *** Joins: Guest79 (~Guest79@7.sub-174-197-195.myvzw.com)
[03:30:36] *** Quits: andrew_dryga (uid297611@id-297611.helmsley.irccloud.com) (Quit: Connection closed for inactivity)
[03:31:37] <Guest79> How should I index for    where x = any(array(<insert 1000's of id's>)) and y = 'example';?
[03:32:08] <Guest79> using btree(x, y) where y = 'example'; isn't cutting it
[03:32:19] *** Quits: Guest1107 (~Thunderbi@p10605177-ipngn25001marunouchi.tokyo.ocn.ne.jp) (Ping timeout: 256 seconds)
[03:32:28] <Guest79> or do I need to batch it out somehow?
[03:37:37] *** Quits: jdavfsxd (~rvgzuuqp@gateway/tor-sasl/rvgzuuqp) (Quit: jdavfsxd)
[03:38:24] *** Quits: aditsu (~aditsu@pcd605164.netvigator.com) (Remote host closed the connection)
[03:38:28] *** Quits: chadea (~a@pool-96-241-113-90.washdc.fios.verizon.net) (Read error: Connection reset by peer)
[03:38:44] *** Joins: aditsu (~aditsu@pcd605164.netvigator.com)
[03:38:46] *** Quits: pmcnabb (~pmcnabb@user/pmcnabb) (Quit: Ping timeout (120 seconds))
[03:38:48] *** Quits: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net) (Ping timeout: 272 seconds)
[03:38:51] *** Joins: chadea (~a@pool-96-241-113-90.washdc.fios.verizon.net)
[03:39:17] *** Joins: pmcnabb (~pmcnabb@user/pmcnabb)
[03:39:57] *** Joins: lucerne0 (~lucerne@ip202.ip-51-178-215.eu)
[03:39:58] *** Joins: shavefan9 (~ShaveFan@shavefan.com)
[03:40:33] *** Quits: belst (~belst@bel.st) (Ping timeout: 256 seconds)
[03:40:35] *** Quits: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon) (Ping timeout: 240 seconds)
[03:40:39] *** Quits: hoppity (~hoppity@user/hoppity) (Remote host closed the connection)
[03:40:43] *** Joins: Bitflux (~byte@byteflux.net)
[03:40:49] *** Joins: belst (~belst@94.23.7.172)
[03:41:07] *** Quits: calebj_ (~me@egb.calebj.io) (Ping timeout: 256 seconds)
[03:41:07] *** Quits: MapMan (mapman@rick.hlds.pl) (Ping timeout: 256 seconds)
[03:41:07] *** Quits: dka (~code-is-a@ns3059207.ip-193-70-33.eu) (Ping timeout: 256 seconds)
[03:41:07] *** Quits: tolecnal (tolecnal@login.xiro.net) (Ping timeout: 256 seconds)
[03:41:34] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Ping timeout: 250 seconds)
[03:41:35] *** lucerne0 is now known as lucerne
[03:41:41] *** Quits: shavefan (~ShaveFan@shavefan.com) (Ping timeout: 256 seconds)
[03:41:41] *** Quits: Nyakajima (~Hayate@lunaluna.nyakajima.net) (Ping timeout: 256 seconds)
[03:41:41] *** Quits: Byteflux (~byte@byteflux.net) (Ping timeout: 256 seconds)
[03:41:41] *** shavefan9 is now known as shavefan
[03:41:59] *** Quits: op2 (~op2@user/op2) (Ping timeout: 250 seconds)
[03:42:39] *** Joins: calebj (~me@egb.calebj.io)
[03:42:49] *** Joins: tolecnal (tolecnal@login.xiro.net)
[03:43:00] *** Joins: MapMan (~mapman@rick.hlds.pl)
[03:43:25] *** Joins: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon)
[03:44:35] *** Joins: belst_ (~belst@94.23.7.172)
[03:44:44] *** Quits: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon) (Remote host closed the connection)
[03:44:53] *** Joins: Nyakajima (~Hayate@lunaluna.nyakajima.net)
[03:44:55] *** Joins: dka (~code-is-a@ns3059207.ip-193-70-33.eu)
[03:45:00] *** Quits: belst (~belst@94.23.7.172) (Ping timeout: 240 seconds)
[03:45:00] *** Quits: rdrg109 (~rdrg109@user/rdrg109) (Ping timeout: 240 seconds)
[03:45:07] *** Joins: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon)
[03:45:24] *** Quits: immibis (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de) (Ping timeout: 240 seconds)
[03:45:25] *** Joins: rdrg109 (~rdrg109@user/rdrg109)
[03:48:32] *** Joins: immibis (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de)
[03:48:46] <StuckMojo> Guest79: where y = ... isn't using (x,y) because y is not the first column. you need (y,x)
[03:48:53] <StuckMojo> or just (y)
[03:49:50] <Guest79> StuckMojo apologies that was an error of translation.
[03:50:02] <Guest79> where y = 'example' and x = any(array(<insert 1000's of id's>));
[03:52:00] *** Quits: k8yun (~k8yun@user/k8yun) (Quit: Leaving)
[03:52:04] <Guest79> explain analyze shows index scan
[03:52:09] <Guest79> i need it to be faster
[03:52:29] *** Joins: k8yun (~k8yun@user/k8yun)
[03:52:59] <StuckMojo> try (y,x)
[03:53:24] *** Quits: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net) (Ping timeout: 240 seconds)
[03:53:28] <Guest79> the index is in fact (y,x) already, again I apologize
[03:54:12] *** Quits: EvanCarroll (~ecarroll@68-78-105-35.lightspeed.hstntx.sbcglobal.net) (Ping timeout: 240 seconds)
[03:54:56] <StuckMojo> then try (x,y)
[03:55:23] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[03:55:35] *** Quits: bindu (~bindu@user/bindu) (Ping timeout: 240 seconds)
[03:55:36] <Guest79> lol yes I've tried both
[03:55:44] *** Joins: bindu_ (~bindu@user/bindu)
[03:56:15] *** Quits: Reiner_Unsinn (~quassel@p579d7a94.dip0.t-ipconnect.de) (Ping timeout: 252 seconds)
[03:57:08] *** Joins: fstd (~fstd@xdsl-81-173-174-103.nc.de)
[03:57:08] *** Joins: Xgc_ (~Xgc@user/xgc)
[03:57:17] <Guest79> where y = 'example' and x = any(array(<insert 10000000's of id's>));    index using btree(y, x) where y='example';    comes out at 1.25 minutes... how can I speed this up? do I have to batch it out and merge together? or is there a faster way pls?
[03:57:32] <StuckMojo> the only way an index scan will be faster is to use a partial index
[03:57:32] *** Joins: immibis_ (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de)
[03:57:42] *** bindu_ is now known as bindu
[03:57:52] *** Joins: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net)
[03:57:53] *** Quits: immibis (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de) (Remote host closed the connection)
[03:57:53] *** Quits: chadea (~a@pool-96-241-113-90.washdc.fios.verizon.net) (Read error: Connection reset by peer)
[03:57:54] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Quit: Ping timeout (120 seconds))
[03:58:01] *** Quits: fstd_ (~fstd@xdsl-81-173-174-103.nc.de) (Read error: Connection reset by peer)
[03:58:11] *** Joins: chadea (~a@pool-96-241-113-90.washdc.fios.verizon.net)
[03:58:24] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[03:58:30] *** Quits: mowcat (~mowcat@2a00:23c5:d190:1901:f22f:74ff:fe77:1e1c) (Remote host closed the connection)
[03:58:46] *** Quits: zlinux[] (~zlinux@149.109.18.164) (Read error: Connection reset by peer)
[03:59:14] *** Joins: zlinux[] (~zlinux@149.109.18.164)
[03:59:32] <StuckMojo> you could partition the table
[04:00:08] <Guest79> that might work ty
[04:00:20] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[04:00:21] *** Quits: ufk (~textual@bzq-84-108-89-219.cablep.bezeqint.net) (Ping timeout: 272 seconds)
[04:00:21] *** Quits: gp5st (~gp5st@pool-72-77-44-213.pitbpa.fios.verizon.net) (Ping timeout: 272 seconds)
[04:00:21] *** Quits: Xgc (~Xgc@user/xgc) (Ping timeout: 272 seconds)
[04:00:34] <StuckMojo> you could try to induce a parallel index scan using a UNIION ALL where the array is split into chunks
[04:01:12] <StuckMojo> x = any(1-10,000) union all x = any(10,0001 - 20,000) UNION ALL ... etc
[04:01:22] <StuckMojo> parallel index scan / parallel query
[04:01:31] <StuckMojo> ??parllel
[04:01:31] <pg_docbot> Nothing found
[04:01:35] <StuckMojo> ??parallel
[04:01:35] <pg_docbot> http://tweakers.net/reviews/638/2 :: http://sourceforge.net/projects/gridsql/
[04:01:35] <pg_docbot> http://database-explorer.blogspot.com/2010/02/parallel-query-1.html :: http://rhaas.blogspot.com.tr/2015/11/parallel-sequential-scan-is-committed.html?m=1
[04:01:35] <pg_docbot> https://github.com/gbb/par_psql/ :: https://www.postgresql.org/docs/current/parallel-query.html
[04:01:50] <StuckMojo> last link
[04:02:51] <StuckMojo> clarification: by 1-10,000 i mean values #1 to #10,000 in your list of values
[04:03:50] <StuckMojo> the idea being each of those queries would fork off as a parallel query process and execute concurrently, then the result sets get merged together
[04:04:09] <StuckMojo> a simple OR might do it: (y =
[04:04:39] <StuckMojo> a simple OR might do it: (y = 'example' and x = any(1-10)) or (y = 'ex' and x = any(11-20)) or ...
[04:05:02] <StuckMojo> you'll probably have to fiddle with the parallel query settings in postgresql.conf
[04:05:45] *** Joins: trafficjam (~qubes@2001:8004:2758:10cc:664e:3885:de18:6a4c)
[04:09:07] *** Joins: hoppity (~hoppity@user/hoppity)
[04:10:47] *** Quits: fordfrog (~fordfrog@gentoo/developer/fordfrog) (Quit: Leaving)
[04:11:40] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[04:12:41] *** Joins: gp5st (~gp5st@pool-72-77-44-213.pitbpa.fios.verizon.net)
[04:13:24] *** Quits: hoppity (~hoppity@user/hoppity) (Ping timeout: 240 seconds)
[04:15:57] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[04:17:43] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[04:27:10] *** Joins: zachary (~zachary@124.133.18.218)
[04:27:37] *** Joins: zachxz (~zachary@124.133.18.218)
[04:27:53] *** Quits: eroux (~eroux@102-65-81-186.ftth.web.africa) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[04:31:24] *** Quits: zachary (~zachary@124.133.18.218) (Ping timeout: 240 seconds)
[04:34:13] *** Joins: travaldo (~travaldo@159.203.88.148)
[04:37:18] *** Xgc_ is now known as Xgc
[04:40:42] *** Joins: hoppity (~hoppity@S0106b4fbe4e5da7b.cg.shawcable.net)
[04:40:42] *** Quits: hoppity (~hoppity@S0106b4fbe4e5da7b.cg.shawcable.net) (Changing host)
[04:40:42] *** Joins: hoppity (~hoppity@user/hoppity)
[04:40:58] *** Quits: hoppity (~hoppity@user/hoppity) (Client Quit)
[04:42:35] *** Quits: xinming (~xinming@115.221.14.75) (Ping timeout: 256 seconds)
[04:43:20] *** Joins: xinming (~xinming@115.221.11.48)
[04:44:05] *** Joins: masber (~masber@213.55.224.10)
[04:45:01] *** Joins: tmunro`` (~user@freefall.freebsd.org)
[04:45:25] *** Quits: tmunro` (~user@freefall.freebsd.org) (Remote host closed the connection)
[04:52:55] *** Joins: pagnol (~user@014198154145.ctinets.com)
[04:53:33] *** Quits: k8yun (~k8yun@user/k8yun) (Read error: Connection reset by peer)
[04:53:59] *** Joins: k8yun (~k8yun@user/k8yun)
[04:54:48] *** Quits: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net) (Ping timeout: 272 seconds)
[05:01:09] *** Quits: cliluw (~cliluw@47.147.77.43) (Ping timeout: 252 seconds)
[05:01:12] *** Quits: trafficjam (~qubes@2001:8004:2758:10cc:664e:3885:de18:6a4c) (Read error: Connection reset by peer)
[05:01:25] *** Joins: cliluw (~cliluw@47.147.77.43)
[05:02:36] *** Quits: Nekomander (~BadAdvice@user/badadvicecat) (Ping timeout: 240 seconds)
[05:02:36] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[05:02:40] *** Joins: BadAdviceCat (~BadAdvice@user/badadvicecat)
[05:02:53] <StuckMojo> ??scram
[05:02:53] <pg_docbot> https://www.postgresql.org/docs/current/auth-password.html :: https://www.postgresql.org/docs/current/sasl-authentication.html
[05:02:59] *** Quits: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net) (Read error: Connection reset by peer)
[05:03:51] *** Joins: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net)
[05:06:10] *** Quits: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net) (Read error: Connection reset by peer)
[05:06:58] *** Joins: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net)
[05:07:16] *** Joins: trafficjam (~qubes@2001:8004:1500:3f08:d68e:9278:5e4d:cc71)
[05:07:28] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[05:08:11] *** Quits: immibis_ (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de) (Remote host closed the connection)
[05:08:38] *** Joins: immibis_ (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de)
[05:15:41] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[05:20:36] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[05:22:40] *** Quits: k8yun (~k8yun@user/k8yun) (Ping timeout: 272 seconds)
[05:24:15] *** Quits: haniaF (~haniaF@83.24.223.148.ipv4.supernova.orange.pl) (Ping timeout: 256 seconds)
[05:24:27] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[05:24:56] *** Joins: haniaF (~haniaF@79.191.92.1.ipv4.supernova.orange.pl)
[05:28:35] *** Quits: Xof (~Xof@157-131-136-66.dedicated.static.sonic.net) (Quit: Bye.)
[05:28:59] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[05:30:16] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[05:31:12] *** Joins: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net)
[05:31:38] *** Joins: k8yun (~k8yun@user/k8yun)
[05:33:02] *** Quits: unyu (~pyon@user/pyon) (Quit: Reboot.)
[05:33:23] *** Joins: k8yun_ (~k8yun@user/k8yun)
[05:34:36] *** Quits: haniaF (~haniaF@79.191.92.1.ipv4.supernova.orange.pl) (Ping timeout: 240 seconds)
[05:35:03] *** Joins: haniaF (~haniaF@83.24.216.182.ipv4.supernova.orange.pl)
[05:35:39] *** Quits: Kyros (~kyros@user/kyros) (Quit: ZNC 1.8.2 - https://znc.in)
[05:35:52] *** Joins: NotKyros (~kyros@user/kyros)
[05:36:16] *** NotKyros is now known as Kyros
[05:37:52] *** Quits: k8yun (~k8yun@user/k8yun) (Ping timeout: 272 seconds)
[05:43:00] *** Quits: f3f3lix (~weechat@55d48350.access.ecotel.net) (Ping timeout: 240 seconds)
[05:45:16] *** Joins: f3f3lix (~weechat@55d48b13.access.ecotel.net)
[05:46:18] <b0nn> hi all, if I want to query a table of Foo, finding all Foo that have Nominal ID as a ParentID, and then find all of the Foo that have ParentID that are in that set, and so on recursively until I have all the descendants of a Foo with ID of <bar> ; how do I detect that I have reached the end, assuming the possibility that there might be <gasp> circular 'references'
[05:46:18] *** Quits: masber (~masber@213.55.224.10) (Quit: Client closed)
[05:47:27] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Remote host closed the connection)
[05:48:04] <StuckMojo> b0nn: keep more n-2 ids, compare to see if you're in a circle?
[05:48:12] <StuckMojo> s/more//
[05:48:29] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[05:48:53] <StuckMojo> maybe a window function inside the CTE?
[05:49:08] <StuckMojo> i'm guessing here. you might be screwed with the circular refs
[05:49:47] <b0nn> Yeah - I guess the solution is to keep running the query until a set of results stops growing
[05:50:21] *** Joins: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net)
[05:50:32] <b0nn> that is, even though the query might return results, just stop when the storage set stops growing (because it should only hold one instance of each row)
[05:53:42] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Ping timeout: 272 seconds)
[05:54:24] <StuckMojo> b0nn: stick around, others here are a lot more adept at WITH RECURSIVE, such as davidfetter
[05:54:36] *** Quits: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net) (Ping timeout: 240 seconds)
[05:54:58] *** Quits: Su-Shee (~Susanne@p5089429c.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[05:55:04] <StuckMojo> he has a recursive cte example that draws fractal. no joke
[05:55:11] *** Quits: acidsys (~LSD@2a03:4000:55:d20::3) (Excess Flood)
[05:55:20] <StuckMojo> s/draws/draws a /
[05:56:00] *** Joins: Su-Shee (~Susanne@p4ffc7bc5.dip0.t-ipconnect.de)
[05:57:40] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[05:58:15] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[05:58:17] *** Joins: acidsys (~LSD@2a03:4000:55:d20::3)
[06:02:13] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[06:02:34] *** Quits: Guest79 (~Guest79@7.sub-174-197-195.myvzw.com) (Quit: Client closed)
[06:03:33] *** Quits: justGhost (~justache@user/justache) (Remote host closed the connection)
[06:03:40] *** Joins: zer0bitz_ (~zer0bitz@2001:2003:f74d:b800:3511:c1b2:b4e7:7f1c)
[06:04:19] *** Joins: Nekomander (~BadAdvice@user/badadvicecat)
[06:04:23] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[06:04:28] *** Quits: BadAdviceCat (~BadAdvice@user/badadvicecat) (Ping timeout: 272 seconds)
[06:04:43] *** Joins: justGhost (~justache@user/justache)
[06:07:35] *** Quits: zer0bitz (~zer0bitz@2001:2003:f74d:b800:c9dd:301:29f7:88f5) (Ping timeout: 250 seconds)
[06:10:23] <peerce> best solution is to not allow circular references.       parentID must be < id, assuming id is a monotonicly increasing series such as is generated by a sequence
[06:14:49] <b0nn> heh, I'd love if that were the case, but I can only play the cards I'm dealt
[06:20:17] *** Joins: unyu (~pyon@user/pyon)
[06:24:46] *** Quits: kanin (~kanin@112.41.64.39) (Read error: Connection reset by peer)
[06:34:24] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[06:36:07] *** Joins: n0fun_ (~jack@i577BC0C9.versanet.de)
[06:36:29] *** Quits: n0fun (~jack@i577BC06E.versanet.de) (Ping timeout: 256 seconds)
[06:41:39] <trafficjam> guys is it worth using aws RDS or Aurora DB?
[06:42:04] <trafficjam> any of you used them before?
[06:42:50] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Remote host closed the connection)
[06:43:25] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[06:44:44] *** Quits: trafficjam (~qubes@2001:8004:1500:3f08:d68e:9278:5e4d:cc71) (Quit: trafficjam)
[06:47:02] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Remote host closed the connection)
[06:47:16] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[06:47:38] *** Quits: TomTom (uid45892@id-45892.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[06:48:35] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Remote host closed the connection)
[06:49:29] *** Quits: sebastorama (~sebastora@104.131.87.249) (Ping timeout: 252 seconds)
[06:51:45] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[06:52:02] <ario> can i delete rows and on contraint error do nothing?
[06:52:17] <ario> i don't want it to throw if there is a foreign key contraint error
[06:52:45] <schinckel> I got surprised by generate_series (yearly interval) and starting on a leap day.
[06:52:58] <schinckel> Only the first one is on the 29th, subsequent all revert to 28th.
[06:53:26] <schinckel> Which probably makes sense if you think of each row as being “one year greater than the previous”, but it caught me out.
[06:53:52] <schinckel> Because I was thinking as “row_number() years greater than the start date"
[06:55:45] <schinckel> b0nn: You might be able to stop whenever you have a current id that is also in an array of parents. I think that’s how I’ve done it in the past.
[06:56:03] <schinckel> (Or more specifically, I’ve used that trick in a constraint to _prevent_ circular references)
[06:56:31] <schinckel> ario: It doesn’t make sense to - then you’d have inconsistent data.
[06:57:02] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Ping timeout: 272 seconds)
[07:00:24] *** Joins: EvanCarroll (~ecarroll@67-200-246-226.static.logixcom.net)
[07:01:22] <b0nn> schinckel: That sounds like a plan, and it will keep the detection of the circular with the db
[07:01:52] *** Joins: ponsfrilus1 (~Thunderbi@adsl-178-39-226-100.adslplus.ch)
[07:03:41] *** Quits: ponsfrilus (~Thunderbi@vpn-254-092.epfl.ch) (Ping timeout: 256 seconds)
[07:03:41] *** ponsfrilus1 is now known as ponsfrilus
[07:05:27] *** Quits: Vacuity (~Vacuity@user/vovo) (Ping timeout: 252 seconds)
[07:05:44] <riceandbeans> Would you rather lots of unused RAM or sufficient RAM but more and more powerful CPUs on your DB server?
[07:06:00] <b0nn> 3
[07:07:05] *** Joins: Azem (~haise01@user/haise01)
[07:07:19] *** Joins: Vacuity (~Vacuity@user/vovo)
[07:10:24] *** Quits: Emet-Selch (~haise01@user/haise01) (Ping timeout: 252 seconds)
[07:11:55] *** Quits: dante443 (~dante443@99-88-166-99.lightspeed.milwwi.sbcglobal.net) (Ping timeout: 256 seconds)
[07:12:41] <b0nn> riceandbeans: "It depends" - does having more CPU increase the amount of RAM required because there's more data being processed
[07:12:58] <b0nn> Is the process CPU bound
[07:14:17] <b0nn> I mean - looking at https://www.youtube.com/watch?v=wlvKAT7SZIQ - he noted that JSON processing was CPU bound, so, if there is more CPU available, then more data processing can take place which means more RAM will be used
[07:15:54] *** Quits: mrgz (~mrgz@201-42-0-191.dsl.telesp.net.br) (Ping timeout: 252 seconds)
[07:16:03] *** Quits: travaldo (~travaldo@159.203.88.148) (Quit: travaldo)
[07:17:43] <riceandbeans> Ok for VMs in something like AWS, would you prioritize compute power or memory size
[07:18:04] <b0nn> That's what I'm trying to tell you - it depends
[07:18:29] <b0nn> How much work is being done, how much data should be sitting there, and so on
[07:21:00] <b0nn> What you need to do is work out what you expect the VM to be doing, where you expect the bottlenecks to be, and then work out what's best for your usecase
[07:21:35] <b0nn> The /neat/ thing about AWS, tho, is that you're not married to the decision, you can change things as you learn more
[07:21:48] *** Quits: ash_worksi (~ash_m@user/ash-m/x-3292451) (Ping timeout: 240 seconds)
[07:22:27] <b0nn> "Oh I had way too much RAM on this VM this month - I'm not expecting the load to change - I'll dial that back a smidge"
[07:23:02] <b0nn> or "Hang an alert that I'm not processing enough queries, I better add CPU/add another VM" and so on
[07:23:41] *** Joins: tozhu (~tozhu@218.89.244.95)
[07:30:12] *** Quits: zer0bitz_ (~zer0bitz@2001:2003:f74d:b800:3511:c1b2:b4e7:7f1c) (Ping timeout: 240 seconds)
[07:30:15] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[07:33:07] *** Joins: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net)
[07:34:43] *** Joins: k8yun__ (~k8yun@user/k8yun)
[07:38:12] *** Quits: k8yun_ (~k8yun@user/k8yun) (Ping timeout: 240 seconds)
[07:39:12] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[07:41:41] *** Quits: k8yun__ (~k8yun@user/k8yun) (Quit: Leaving)
[07:47:42] *** Quits: sreve_ (~quassel@p549d71aa.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[07:47:57] *** Joins: sreve (~quassel@p4ff4449e.dip0.t-ipconnect.de)
[07:50:52] *** Quits: n0fun_ (~jack@i577BC0C9.versanet.de) (Ping timeout: 272 seconds)
[07:51:35] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[08:08:36] *** Quits: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net) (Ping timeout: 272 seconds)
[08:09:49] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[08:11:22] *** Joins: Junxter (~Junxter@222.95.164.193)
[08:12:06] <sobel> it really depends on your workload, but most databases are IO bound first, then memory bound, then CPU bound last
[08:12:25] <sobel> if your traffic is read (SELECT) heavy then RAM will probably benefit you the most
[08:12:47] <sobel> if it is INSERT heavy then IO will probably benefit you the most
[08:13:04] <sobel> hope that helps
[08:18:49] *** Quits: dsrt^ (~dsrt@50.235.176.163) (Remote host closed the connection)
[08:19:41] *** Joins: shiranaihito (~textual@123-192-192-149.dynamic.kbronet.com.tw)
[08:22:15] *** Joins: pvn (~Adium@85-195-230-32.fiber7.init7.net)
[08:22:26] *** Quits: pvn (~Adium@85-195-230-32.fiber7.init7.net) (Client Quit)
[08:26:47] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Remote host closed the connection)
[08:27:33] *** Quits: JordiGH (~jordi@user/jordigh) (Ping timeout: 250 seconds)
[08:29:16] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[08:32:53] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Remote host closed the connection)
[08:33:48] *** Quits: aremaref (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net) (Ping timeout: 240 seconds)
[08:33:56] *** Quits: aremaref_ (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net) (Ping timeout: 272 seconds)
[08:35:27] *** Quits: schinckel (uid38120@id-38120.ilkley.irccloud.com) (Changing host)
[08:35:27] *** Joins: schinckel (uid38120@user/schinckel)
[08:36:28] *** Quits: yoshi_jms (~jmissao@user/yoshi-jms/x-9885530) (Remote host closed the connection)
[08:39:39] *** Joins: mattil_ (~mattil@helsinki.portalify.com)
[08:41:29] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[08:45:02] *** Joins: JordiGH (~jordi@fencepost.gnu.org)
[08:45:02] *** Quits: JordiGH (~jordi@fencepost.gnu.org) (Changing host)
[08:45:02] *** Joins: JordiGH (~jordi@user/jordigh)
[08:45:20] *** Quits: bmomjian1 (~bruce@momjian.us) (Ping timeout: 272 seconds)
[08:53:36] *** Quits: _mikey (~mikey@user/mikey/x-4335048) (Quit: WeeChat 3.4)
[09:02:12] *** Quits: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net) (Ping timeout: 240 seconds)
[09:06:07] *** Quits: JordiGH (~jordi@user/jordigh) (Ping timeout: 250 seconds)
[09:07:11] <riceandbeans> I'd say we're probably INSERT heavy
[09:07:56] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Remote host closed the connection)
[09:11:33] <ggb> riceandbeans: You should *know* this sort of thing. If you don't, before optimizing, you need to get a handle on characterizing your workload and nailing down your monitoring.
[09:12:42] <riceandbeans> Well, I know we have extreme reliance on the DB, everything goes through it, and everything is exceedingly slow, but we also have memory issues
[09:12:45] <ggb> It's not a moral judgment or a huge failing not to know. There are lots of orgs out there that don't, and a lot of those are some form of successful. But if you don't know your workload backward and forward, and you're responsible for any part of the infrastructure, the most important thing you can do is to come to know the workload.
[09:13:00] <riceandbeans> I don't own the DB, no one does
[09:13:21] <ggb> Who gets called if it goes down? That person or team owns it.
[09:13:26] <riceandbeans> First time I seen a place without a DBA
[09:13:48] <riceandbeans> Random people get called if they might have written code that caused problems recently
[09:13:58] <riceandbeans> No one owns the DB itself though
[09:16:10] <ggb> What you're describing is a lot of people owning it, not no one.
[09:16:31] <b0nn> heh, it's kinda the same thing
[09:16:41] <b0nn> "No he's looking after it, no he is..."
[09:16:56] *** Joins: eroux (~eroux@102-65-81-186.ftth.web.africa)
[09:17:06] <ggb> No one owns it == no one can get root on the thing. It's a black box sitting on the network that happens to respond to requests in a way that we find valuable to the business.
[09:17:11] *** ba|ch` is now known as ba|ch
[09:17:44] <ggb> Everyone owns it == anyone can annihilate the damn thing, but it'll be a bitch to find someone to take responsibility to improve it.
[09:17:59] <b0nn> "Cardinal rule: Don't share datastores - because then there's no clear ownership, and that means nobody owns it"
[09:18:28] <riceandbeans> I just want to make things better because they're getting worse day by day
[09:18:53] *** Joins: aremaref (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net)
[09:18:56] *** Joins: aremaref_ (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net)
[09:19:45] <ggb> You've got a political problem before technical. You can chase down performance, operational, and reliability concerns. But if everyone owns it, you'll just be wiping up water on the Titanic with paper towels.
[09:20:00] <b0nn> ^
[09:20:22] <riceandbeans> I can't solve political problems
[09:20:31] <b0nn> Whomever is in charge of the people needs to sort that out
[09:20:38] <riceandbeans> That will never happen
[09:21:13] <b0nn> Ok, speaking from experience, if you cannot get them onboard, and you take it on yourself, all you are buying is blame
[09:21:56] <ggb> You'll struggle to make things better, then. And any improvement you make is liable to be swept away by the next developer choosing their expedient path.
[09:22:19] <riceandbeans> So what you're saying is that I should f ind a new job
[09:22:42] <b0nn> or keep your head down
[09:22:44] <peerce> are you having fun where you are ?
[09:22:59] <ggb> I'm not telling you what to do. Just what will happen. (:
[09:23:12] <riceandbeans> No, I'm miserable.
[09:23:20] <peerce> then yes, you should find a new job.
[09:23:52] <ggb> If I were you, I'd look to get out, unless you're making enough money in the next 5 years to be able to retire or take 1+ years off.
[09:23:56] <b0nn> Luckily in most countries the market for devs is quite hot at the moment
[09:26:33] <riceandbeans> I could find work elsewhere but interviewing is a drag man
[09:31:11] *** Joins: emhwfhrom^ (~emhwfhrom@50.235.176.163)
[09:37:35] *** Quits: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon) (Ping timeout: 240 seconds)
[09:38:07] <peerce> is it more or less of a drag than your current job ?
[09:39:01] <peerce> my son has intervieweed (remotely) for a job here in California, and on site for a job in Switzerland.
[09:39:33] *** Joins: mexen (uid495612@user/mexen)
[09:39:42] <peerce> [he's in the UK right now]
[09:39:44] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[09:40:57] *** Quits: mattil_ (~mattil@helsinki.portalify.com) (Ping timeout: 256 seconds)
[09:43:09] *** Quits: Hecate (~mariposa@user/hecate) (Ping timeout: 250 seconds)
[09:47:19] *** Quits: ninjabanjo (~ninjabanj@107.150.45.163) (Quit: ZNC - https://znc.in)
[09:48:20] *** Joins: uncleyear (~ian@pppoe.178-66-157-18.dynamic.avangarddsl.ru)
[09:48:41] *** Joins: ninjabanjo (~ninjabanj@107.150.45.163)
[09:49:56] *** Quits: EvanCarroll (~ecarroll@67-200-246-226.static.logixcom.net) (Ping timeout: 272 seconds)
[09:52:16] *** Quits: uncleyear (~ian@pppoe.178-66-157-18.dynamic.avangarddsl.ru) (Client Quit)
[09:52:55] *** Quits: bindu (~bindu@user/bindu) (Ping timeout: 240 seconds)
[09:53:06] *** Quits: ninjabanjo (~ninjabanj@107.150.45.163) (Client Quit)
[09:53:18] *** Joins: uncleyear (~ian@pppoe.178-66-157-18.dynamic.avangarddsl.ru)
[09:54:35] *** Joins: ninjabanjo (~ninjabanj@107.150.45.163)
[09:57:38] *** Joins: funhouse (~funhouse@user/funhouse)
[10:00:42] *** Quits: dre (~dre@101.191.49.59) (Ping timeout: 272 seconds)
[10:05:57] *** Joins: zemis (~zemis@ip-89-176-21-138.net.upcbroadband.cz)
[10:06:40] *** Joins: EvanCarroll (~ecarroll@68-78-105-35.lightspeed.hstntx.sbcglobal.net)
[10:09:36] *** Quits: rvalue (~rvalue@user/rvalue) (Read error: Connection reset by peer)
[10:09:50] *** Joins: rvalue (~rvalue@user/rvalue)
[10:13:55] *** Quits: shiranaihito (~textual@123-192-192-149.dynamic.kbronet.com.tw) (Quit: My MacBook Air has gone to sleep. ZZZzzz…)
[10:16:43] *** Joins: _mikey (~mikey@user/mikey/x-4335048)
[10:19:42] *** Quits: aremaref_ (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net) (Ping timeout: 272 seconds)
[10:20:20] *** Quits: aremaref (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net) (Ping timeout: 272 seconds)
[10:21:27] *** Quits: zlinux[] (~zlinux@149.109.18.164) (Ping timeout: 256 seconds)
[10:21:50] *** Joins: TomTom (uid45892@id-45892.ilkley.irccloud.com)
[10:22:22] *** Joins: zlinux[] (~zlinux@149.109.18.164)
[10:23:42] <ario> anyone awake know how to restart a stalled replication slot?
[10:24:09] <ario> i deleted 12 million rows and my replication pub/sub died i think :)
[10:26:33] *** Joins: viaSanctus (~viaSanctu@user/viasanctus)
[10:26:35] *** Joins: ash_worksi (~ash_m@user/ash-m/x-3292451)
[10:30:59] *** Joins: the_lanetly_052_ (~the_lanet@194.135.169.19)
[10:33:15] *** Quits: randir (~randir@2.92.196.208) (Remote host closed the connection)
[10:33:48] *** Joins: randir (~randir@2.92.196.208)
[10:35:00] *** Joins: Reiner_Unsinn (~quassel@p579d7a94.dip0.t-ipconnect.de)
[10:35:29] *** Joins: chronon (~chronon@user/chronon)
[10:37:28] *** Joins: lxwulf (~lxwulf@user/lxwulf)
[10:37:54] *** Joins: manti7 (~manti7@176.10.104.94)
[10:38:23] *** Parts: lxwulf (~lxwulf@user/lxwulf) ()
[10:38:42] *** Quits: randir (~randir@2.92.196.208) (Ping timeout: 272 seconds)
[10:39:26] *** Joins: GNUmoon (~GNUmoon@gateway/tor-sasl/gnumoon)
[10:44:08] *** Quits: _mikey (~mikey@user/mikey/x-4335048) (Quit: WeeChat 3.4)
[10:44:15] *** Joins: maret (~maret@nat-88-212-37-89.antik.sk)
[10:45:10] *** Joins: randir (~randir@93.159.239.42)
[10:45:53] *** Joins: otisolsen70 (~otisolsen@xd4ed80b5.cust.hiper.dk)
[10:52:35] *** Joins: Komzzpa (~kom@2a02:220f:2000:2c00:8079:4cec:8bf1:2a0e)
[11:02:33] *** Quits: edgecase (~COPLANAR\@cpe00e0815f17ce-cme0dbd1400159.cpe.net.cable.rogers.com) (Ping timeout: 256 seconds)
[11:03:22] *** Joins: ravish0007_ (~quassel@ec2-65-0-45-121.ap-south-1.compute.amazonaws.com)
[11:03:24] *** Quits: bpmedley (~bpmedley@2600:1700:e2c:8410:5052:1a9e:4853:6fe1) (Ping timeout: 240 seconds)
[11:03:25] *** Quits: ravish0007 (~quassel@ec2-65-0-45-121.ap-south-1.compute.amazonaws.com) (Ping timeout: 240 seconds)
[11:05:09] *** Joins: JordiGH (~jordi@fencepost.gnu.org)
[11:05:09] *** Quits: JordiGH (~jordi@fencepost.gnu.org) (Changing host)
[11:05:09] *** Joins: JordiGH (~jordi@user/jordigh)
[11:07:27] *** Quits: funhouse (~funhouse@user/funhouse) (Quit: Client closed)
[11:11:11] *** Joins: ponsfrilus1 (~Thunderbi@vpn-253-062.epfl.ch)
[11:11:27] *** Joins: ur5us (~ur5us@2406:e002:6804:8a01:fe29:d3cc:a0c3:c22)
[11:11:47] *** Joins: darutoko (~darutoko@37.21.204.207)
[11:11:57] *** Joins: sliss (~sliss@109.136.165.60)
[11:13:10] *** Joins: finsternis (~X@23.226.237.192)
[11:13:32] *** Quits: ponsfrilus (~Thunderbi@adsl-178-39-226-100.adslplus.ch) (Ping timeout: 272 seconds)
[11:13:33] *** ponsfrilus1 is now known as ponsfrilus
[11:13:40] *** Joins: rodo (~rodo@pop.92-184-117-117.mobile.abo.orange.fr)
[11:14:16] *** Joins: Kohe (~Kohe@46.12.76.236.dsl.dyn.forthnet.gr)
[11:21:37] *** Quits: JordiGH (~jordi@user/jordigh) (Ping timeout: 240 seconds)
[11:24:36] *** Quits: rodo (~rodo@pop.92-184-117-117.mobile.abo.orange.fr) (Ping timeout: 240 seconds)
[11:31:20] *** Joins: palasso (~palasso@user/palasso)
[11:31:52] *** Joins: shka (~herr@109.231.3.55)
[11:32:58] *** Joins: michalz (~michalz@185.246.204.73)
[11:38:10] *** Joins: fordfrog (~fordfrog@gentoo/developer/fordfrog)
[11:38:17] *** Joins: `2jt (~jtomas@130.red-88-22-46.staticip.rima-tde.net)
[11:38:25] *** Joins: rodo (~rodo@pop.92-184-105-149.mobile.abo.orange.fr)
[11:39:23] <mcrane> ario: try restarting PostgreSQL on both servers might help
[11:41:00] *** Quits: ur5us (~ur5us@2406:e002:6804:8a01:fe29:d3cc:a0c3:c22) (Ping timeout: 240 seconds)
[11:43:58] <otisolsen70> Having set up streaming replication in PG 12.9, how do I trigger a failover - ie. make the current master fail and then let the slave become a new master (or standalone if no additional slave is available)?
[11:44:07] <otisolsen70> I have set up streaming replication on PG 12.9 as described in this howto: https://www.percona.com/blog/2019/10/11/how-to-set-up-streaming-replication-in-postgresql-12/ (hav NOT implemented the section "Enabling Archiving on Master and the Standby recovery using Archives")
[11:49:14] *** Joins: edgecase (~COPLANAR\@cpe00e0815f17ce-cme0dbd1400159.cpe.net.cable.rogers.com)
[11:49:35] *** Quits: x5c30 (~x5c30@user/x5c30) (Ping timeout: 256 seconds)
[11:50:54] *** Quits: gambl0re (~gambl0re@2607:fea8:a59f:c360::82dc) (Ping timeout: 252 seconds)
[11:51:41] <otisolsen70> As I see it, there are two scenarios: 1) controlled shutdown of master due to maintenance, etc. and then promotion of slave to master. and 2) uncontrolled failure of master in which the master suddenly disappears - e.g. the power cord or network cord is suddenly yanked and it is not coming back any time soon.
[11:53:20] *** Joins: rendar (~rendar@user/rendar)
[11:56:06] <otisolsen70> Is this the correct way of handling scenario 1: on the master run "service postgresql stop"; on the slave run pg_ctl promote -D /var/lib/postgresql/12/main ; reconfigure applications/pg_bouncer to connect to the new master   ?
[11:56:35] <otisolsen70> And in scenario 2, I guess it is the same except the first step of doing "service postgresql stop" on the master has already been done by the master being gone.
[11:59:10] *** Joins: wolfshappen (~waff@irc.furworks.de)
[11:59:52] *** Quits: wolfshappen (~waff@irc.furworks.de) (Client Quit)
[12:00:35] <peerce> otisolsen70; a robust high availability solution has hardware fencing, so it can prevent the former master from rebooting and thinking its still in charge
[12:00:50] <peerce> I've seen fencing done with power strips :D
[12:01:06] <peerce> also with storage and network switches
[12:01:21] <dminuoso> peerce: Mmm, fencing is a particularly interesting subject because of different failure modes.
[12:01:38] <dminuoso> Say if the network between two nodes is unreliable, you might not have a reliable fencing mechanism.
[12:01:46] <peerce> yeah, goal is no single points of failure, and that includes your HA management :-D
[12:01:53] <peerce> its not easy.
[12:01:59] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Quit: WeeChat 3.4)
[12:02:07] <otisolsen70> I dont have any special hardware fencing mechanism. I might get that at some point but not for now. So I will have to live with the risk of split brian.
[12:02:12] *** Joins: op2 (~op2@user/op2)
[12:02:30] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[12:02:52] <peerce> you can do it with a managed switch.   just put the former master on a different 'fenced' VLAN
[12:02:52] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Client Quit)
[12:03:01] <peerce> purgatory :)
[12:03:21] <dminuoso> A lot of formal research has gone into fault tolerance (see byzantine faults for example). It's quite a complicated design space
[12:03:28] *** Joins: dege (~dege@user/dege)
[12:03:42] <otisolsen70> peerce, sure, I will make sure that if I promote the slave, I will do something to ensure that the master not accidentially gets back online.
[12:03:57] <otisolsen70> It is just that I dont have a STONITH device or similar to do this for me
[12:04:27] <dminuoso> otisolsen70: Do the nodes run on metal or virtualized?
[12:04:38] <peerce> of course, switches are another single point of failure, building a truly redundant failsafe switching fabric is challenging
[12:04:52] <otisolsen70> dminuoso, one is virtual in Azure. The other is physical on premise
[12:05:00] <dminuoso> peerce: if you start building "redundant failsave switching fabric" you drive up complexity with additional failure modes.
[12:05:12] <dminuoso> Its best to just accept that "failsave" is impossible to attain
[12:05:21] <peerce> yeah, that too.
[12:05:27] <peerce> or just go cloudy-and-pray
[12:05:27] *** Quits: Nekomander (~BadAdvice@user/badadvicecat) (Ping timeout: 256 seconds)
[12:05:36] <peerce> 'eventual consistency most of the time'
[12:05:43] *** Joins: wolfshappen (~waff@irc.furworks.de)
[12:05:51] <dminuoso> eventual availability.
[12:06:01] <dminuoso> Entire AWS regions going down is a mark proposition.
[12:06:05] <dminuoso> *dark
[12:06:22] *** Joins: x5c30 (~x5c30@user/x5c30)
[12:06:30] <otisolsen70> But is my described procedure above the correct one?
[12:06:41] <otisolsen70> Obviously, some fencing, etc. would make things safer.
[12:09:07] *** Quits: ponsfrilus (~Thunderbi@vpn-253-062.epfl.ch) (Ping timeout: 256 seconds)
[12:09:29] *** Joins: ponsfrilus (~Thunderbi@adsl-178-39-226-100.adslplus.ch)
[12:09:53] <peerce> we did all our production redundancy with our own application messaging and management layers, failovers had to be invoked manually.
[12:11:15] <peerce> basically everything was on a publish/subscribe message bus, and the backup OLTP servers subscribed to the same subjects as the primaries and ran the same processes.   everything was event driven
[12:11:43] <peerce> but it ran in a sort of rapid repeat batch mode.  do a batch, do another batch, repeat forever.
[12:12:07] <peerce> on as many worker jobs as could efficiently run concurrently to maintain the best balance of overall throughput
[12:12:29] <peerce> we didn't need a connection pool, we had a worker pool.
[12:12:56] *** Quits: B-| (~Unknown@93.186.210.16) (Quit: leaving)
[12:14:56] *** Parts: ash_worksi (~ash_m@user/ash-m/x-3292451) ()
[12:17:46] *** Joins: vladoski (~vladoski@2001:b07:add:d406:907e:4f:b57f:11c2)
[12:20:27] *** Joins: merzo (~Thunderbi@95.133.42.21)
[12:22:49] *** Joins: Ergo^ (~ergo@91.238.59.144)
[12:24:28] *** Quits: Kohe (~Kohe@46.12.76.236.dsl.dyn.forthnet.gr) (Ping timeout: 272 seconds)
[12:28:16] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[12:28:22] *** Joins: vladoski_ (~foo@2001:b07:add:d406:45f4:7b92:7e74:f5c4)
[12:28:40] *** Joins: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[12:29:00] *** Joins: bindu (~bindu@user/bindu)
[12:29:39] *** Quits: vladoski_ (~foo@2001:b07:add:d406:45f4:7b92:7e74:f5c4) (Client Quit)
[12:31:36] *** Quits: bindu (~bindu@user/bindu) (Remote host closed the connection)
[12:32:16] <ben_> hello there. i'm storing a date range in 2 date fields at the moment, (although it could easily be migrated to a daterange). i am storing years, quarters and months in this range: [Date, Date). is there a way i can use an exclusion constraint on this, in such a way that prevents overlapping months quarters and months, but the years quarters and months CAN overlap. i.e [2022-01-01, 2023-01-01), [2022-01-01, 2023-03-3
[12:32:16] <ben_> 1), [2022-01-01, 2023-02-01) is allowed, but i couldn't add another overlapping month
[12:32:19] *** Joins: bindu (~bindu@user/bindu)
[12:32:36] *** Quits: jbg (~jbg@user/jbg) (Quit: Ping timeout (120 seconds))
[12:33:04] <ben_> that example should have been: [2022-01-01, 2023-01-01), [2022-01-01, 2022-03-3), [2022-01-01, 2022-02-01)
[12:33:04] *** Joins: jbg (~jbg@user/jbg)
[12:35:04] <ben_> 3rd try: [2022-01-01, 2023-01-01), [2022-01-01, 2022-03-31), [2022-01-01, 2022-02-01)
[12:35:45] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[12:37:45] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[12:38:48] <ben_> essentially this: https://dbfiddle.uk/?rdbms=postgres_14&fiddle=7df8bc43580bb377e6531839d0bff984
[12:38:50] *** Joins: manti7 (~manti7@176.10.104.94)
[12:39:08] *** Joins: shiranaihito (~textual@111-249-197-68.dynamic-ip.hinet.net)
[12:39:17] <otisolsen70> peerce, is it correctly understood that in PG12.x manual promotion is basically just to stop the master and then run "pg_ctl promote" on the slave? Or is anything else needed?
[12:46:36] *** Joins: csm3105 (~csm3105@143.red-83-48-84.staticip.rima-tde.net)
[12:50:26] *** Quits: fordfrog (~fordfrog@gentoo/developer/fordfrog) (Quit: Leaving)
[12:50:54] *** Joins: Kohe (~Kohe@46.12.76.236.dsl.dyn.forthnet.gr)
[12:57:40] *** Joins: lxwulf (~lxwulf@user/lxwulf)
[12:58:47] *** Joins: furrymcgee (~devuan@cgn-89-1-211-93.nc.de)
[13:03:23] <peerce> thats like the bare minimum, and is usually done within a much bigger framework of failover management.     there's tools like repmgr and such that simplify this process
[13:03:44] *** Quits: pagnol (~user@014198154145.ctinets.com) (Ping timeout: 272 seconds)
[13:03:58] <peerce> you probably want a wal archive, with basebackups
[13:04:06] <peerce> it all depends on your requirements
[13:04:24] <peerce> and clusters require monitoring and management.
[13:05:57] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[13:06:29] <peerce> Clusters are Complicated(tm)
[13:06:37] *** Joins: psoo (~psoo@dslb-090-186-134-090.090.186.pools.vodafone-ip.de)
[13:07:12] <otisolsen70> peerce, ok. Yeah, I will look into repmgr. But for now I just want the bare minimum.
[13:07:41] <peerce> reppmgr is pretty old, I can't remember what the new easy is.
[13:07:53] <peerce> hang on a few, i'll try and find something to jog my mem.
[13:08:41] <otisolsen70> peerce, for wal archive I need something like a shared storage to store them on, right? And this is only in case the slave gets too much behind on writes, right?
[13:09:59] <peerce> a seequence of basebackups and wal archive lets you do point in time recovery to any transaction before an event you want to recover from.
[13:10:18] <peerce> i usually put the walarchive on a nfs server
[13:10:53] <peerce> streaming replication clients will use it to recover from downtime to catch up from longer than the active server's wal supports
[13:10:57] *** Joins: mowcat (~mowcat@2a00:23c5:d190:1901:f22f:74ff:fe77:1e1c)
[13:14:40] *** Quits: mowcat (~mowcat@2a00:23c5:d190:1901:f22f:74ff:fe77:1e1c) (Remote host closed the connection)
[13:16:15] <otisolsen70> peerce, and if the wal archive is present and mentioned in the config, then that catch up by the slave happens automatically?
[13:16:19] * merpaderp shivers hearing nfs mentioned in context with database
[13:16:43] <otisolsen70> Instead of NFS this could be something like Azure Files or S3, right?
[13:17:27] <peerce> ugh, s3 is only good for nearline archival usage not online file usage.
[13:17:32] <peerce> copy from, copy to.
[13:17:42] *** Joins: mowcat (~mowcat@2a00:23c5:d190:1901:f22f:74ff:fe77:1e1c)
[13:17:43] <peerce> https://pgbackrest.org/
[13:18:01] <peerce> thats another tool that is newer, and does more than repmgr
[13:18:21] <peerce> but it can be used to manage streaming replicas, too.
[13:18:48] <otisolsen70> peerce, what do you mean by nearline?
[13:19:30] *** Joins: n0fun_ (~jack@i577BC0C9.versanet.de)
[13:20:11] <otisolsen70> peerce, pgbackrest docs seems to suggest using S3 buckets?
[13:21:14] <peerce> um, somewhat less than online (such as active database primary storage), and not quite as low as slow as offline where you have to physically fetch volumes from a vault.
[13:21:15] *** Quits: bindu (~bindu@user/bindu) (Ping timeout: 240 seconds)
[13:22:06] <otisolsen70> merpaderp, what do you suggest for wal archive instead of  NFS?
[13:22:30] <otisolsen70> peerce, so how do you use NFS then? Do you have a third server that is hosting the NFS share? Adn then both master and slave mount it from there?
[13:22:44] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[13:23:11] <otisolsen70> If so, what happens in the event that the NFS server fails? Or disk fills up? Or network is split between master and NFS server or slave and NFS server?
[13:23:20] <merpaderp> otisolsen70: I don't know, just watch your conversation closely, hopeing to learn something about postgres ha af
[13:23:24] *** Quits: zachxz (~zachary@124.133.18.218) (Ping timeout: 240 seconds)
[13:23:44] <merpaderp> I have had bad experience with nfs being stalled, but that can happen to whatever storage, so doesn't really matter.
[13:25:12] *** Quits: `2jt (~jtomas@130.red-88-22-46.staticip.rima-tde.net) (Remote host closed the connection)
[13:25:30] *** Joins: `2jt (~jtomas@130.red-88-22-46.staticip.rima-tde.net)
[13:25:35] *** Quits: shiranaihito (~textual@111-249-197-68.dynamic-ip.hinet.net) (Quit: My MacBook Air has gone to sleep. ZZZzzz…)
[13:25:44] <peerce> if you pay for netapp grade NFS storage, it can be fully redundant
[13:29:42] *** Quits: ponsfrilus (~Thunderbi@adsl-178-39-226-100.adslplus.ch) (Ping timeout: 272 seconds)
[13:29:51] <otisolsen70> peerce, ok. Is that what you use?
[13:29:53] *** Joins: ponsfrilus1 (~Thunderbi@vpn-253-161.epfl.ch)
[13:30:11] <peerce> me?  hahahaha, I'm retired.   thats what the production often used at my last $job.
[13:30:26] <otisolsen70> peerce, ok.
[13:30:27] *** Quits: Kohe (~Kohe@46.12.76.236.dsl.dyn.forthnet.gr) (Ping timeout: 252 seconds)
[13:30:53] <furrymcgee> you couls use a database to store your wal archives as well
[13:32:12] *** ponsfrilus1 is now known as ponsfrilus
[13:33:04] *** Joins: zauberfisch_ (~Zauberfis@cm147-42.liwest.at)
[13:34:17] *** Joins: tozhu (~tozhu@218.89.244.95)
[13:34:40] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[13:35:45] <peerce> nah, that would be really ineffcient
[13:38:01] *** Joins: fordfrog (~fordfrog@gentoo/developer/fordfrog)
[13:40:42] *** Quits: zauberfisch_ (~Zauberfis@cm147-42.liwest.at) (Read error: Connection reset by peer)
[13:40:59] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[13:46:12] *** Quits: jazzy (~jaziz@user/jaziz) (Ping timeout: 240 seconds)
[13:50:18] *** Joins: ben__ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[13:50:51] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[13:51:52] *** Joins: shiranaihito (~textual@220-133-46-51.hinet-ip.hinet.net)
[13:54:24] *** Quits: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Ping timeout: 272 seconds)
[13:55:00] *** Quits: evdubs (~evdubs@user/evdubs) (Ping timeout: 240 seconds)
[13:59:01] <ben__> reposting to the later crowd, is this possible with exclusion constraints? https://dbfiddle.uk/?rdbms=postgres_14&fiddle=a15d382bc3f2e4ee38664ab04f9f59cc
[13:59:48] <ilmari> ben__: if you use a daterange column instead of two separate ones, yes
[13:59:52] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[13:59:59] *** Joins: Vitto (~Vitto@se-14.nat.univ-paris4.fr)
[14:01:00] *** Quits: fcr (~fran@r186-48-162-7.dialup.adsl.anteldata.net.uy) (Ping timeout: 240 seconds)
[14:02:03] <ilmari> actually, you could do `exclude ( (daterange(start_date, end_date, '[)')) with &&)`
[14:02:36] *** Joins: evdubs (~evdubs@user/evdubs)
[14:02:38] <ilmari> ??exclusion constraint
[14:02:39] <pg_docbot> https://www.postgresql.org/docs/current/ddl-constraints.html
[14:04:02] <ben__> ilmari: yeah i realise that, but look again, the years/quarters/months should be allowed to overlap
[14:04:16] <ben__> then a second overlapping month should not be
[14:06:27] <ilmari> how about dates that don't start on a unit boundary?
[14:06:46] <ilmari> like the offset quarter ('2022-02-01', '2022-05-01')
[14:06:55] <ilmari> s/dates/ranges/
[14:07:55] <ben__> yeah well they are controlled in the app at the moment, we can assume they'll always be whole years/quarters/months
[14:08:01] *** Parts: serafeim (serafeim@2001:41d0:700:1ccb::10) (WeeChat 3.4)
[14:08:11] <ben__> always jan to march, april to june etc...
[14:08:25] *** Joins: fcr (~fran@r167-60-109-101.dialup.adsl.anteldata.net.uy)
[14:09:41] <ilmari> you culd add age(end_date, start_date) to the exclusion constraint
[14:09:49] <ben__> i mean i could redesign this and get rid of the years and quarters...
[14:09:53] <ilmari> that way only overlapping ranges of the same length get excluded
[14:10:01] <ben__> oh nice
[14:10:30] <ilmari> (and check constraint that the dates are only the 1st of the month, and only the allowed lengths (1 month, 3 months, 1 year)
[14:11:13] <ilmari> e.g. `exclude ( (daterange(start_date, end_date, '[)')) with &&, ( (age(end_date, start_date)) with = )`
[14:12:02] *** Joins: Atque (~Atque@user/atque)
[14:12:50] <ilmari> you need the btree_gist extension for the = operator
[14:13:00] <ilmari> s/for/to exclude using/
[14:14:13] <ben__> ilmari: almost https://dbfiddle.uk/?rdbms=postgres_14&fiddle=e5049c364da6ff2e3911a9f35e6218bd
[14:14:18] <ben__> "ERROR:  functions in index expression must be marked IMMUTABLE"
[14:14:47] <ilmari> oh, age() is only stable, not immutable
[14:15:08] <ben__> that's a concept i've not encountered before
[14:16:20] <ilmari> ah, age(timestamptz, timestamptz) is immutable, but age(timestamp,timestamp) is not
[14:16:34] <ilmari> and the date gets implicitly cast to timestamp, not timestamptz
[14:17:08] <ben__> casting to timestamptz doesn't seem to work either
[14:19:11] <ilmari> because the cast from date to timestamptz isn't immutable either
[14:21:13] *** Quits: smp (~smp@user/smp) (Ping timeout: 256 seconds)
[14:22:45] <sliss> aLeSD[m]: While thinking of your UPDATE problem I had to use it myself too and created a solution to do multiple changes on a column with only one UPDATE (thx to Erwin Brandstetter): https://stackoverflow.com/questions/71145242/do-multiple-regex-changes-in-one-update-with-the-help-of-plpgsql
[14:22:51] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Ping timeout: 256 seconds)
[14:24:03] <ben__> ilmari: not possible then?
[14:24:21] *** Quits: michalz (~michalz@185.246.204.73) (Ping timeout: 252 seconds)
[14:26:33] <ilmari> ben__: ah, extract(part from date) is immutable, so as long as you have the right constraints in place you could do extract(month from end_date) - extract(month from start_date)
[14:26:45] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[14:26:46] *** Joins: maret (~maret@nat-88-212-37-89.antik.sk)
[14:27:45] <ben__> nice
[14:27:47] <otisolsen70> peerce, in my described failover/promotion process above in which I shutdown the master first, then run pg_ctl promote on the slave. How do I bring the master back up? Do I delete everything on it and re-initialise it as slave using pg_basebackup?
[14:27:48] <ben__> thanks
[14:28:32] <otisolsen70> peerce, and also, in my described controlled  failover/promotion process, could I just skip the first step of stopping PG on the master? And just run pg_ctl promote on the slave? And will that inform the master that it should now be slave rather than master?
[14:29:10] <Myon> otisolsen70: it won't
[14:29:37] <dim> otisolsen70: have a look a pg_auto_failover maybe? see https://pg-auto-failover.readthedocs.io/en/master/?badge=master
[14:29:39] <Myon> otisolsen70: I recommend to shut down the old primary to make sure there's no split-brain
[14:30:09] <otisolsen70> Myon, ok. So there is no (built in) way to do a "live promotion"?
[14:30:26] <Myon> promote yes
[14:30:36] <Myon> failover including demotion no
[14:31:22] <otisolsen70> Myon, oh, ok. So doing pg_ctl promote will actually work in that slave will become master. But old master will also stay master. And if any application connects and writes to old master, I will have borked my data?
[14:32:03] <Myon> each instance is still consistent within itself, but overall it's bad, yes
[14:32:12] <otisolsen70> Myon, ok. Thanks for clarifying!
[14:32:15] <Myon> having two databases is often worse than having none :)
[14:32:35] <otisolsen70> Myon, so what is the recommended approach of getting the old master back online as a slave?
[14:32:54] <Myon> patroni, pg_auto_failover (or manual pg_basebackup)
[14:33:29] <otisolsen70> Myon, ok.
[14:34:37] <otisolsen70> On Debian/Ubuntu I also have a pg_ctlcluster that is similar to pg_ctl. Is it recommended to use that one instead when on debian?
[14:35:07] <Myon> it makes things easier, but you don't have to use it
[14:35:51] <otisolsen70> Myon, ok. I guess I could use that and just run pg_ctlcluster promote without specifying -D /path/to/datadir, right?
[14:35:59] <Myon> exactly
[14:40:19] <ioguix> wait, stopping the old primary, promoting the standby and restarting the old primary as standby is safe, as far as you check the very first step complete correctly on both side
[14:40:51] <ioguix> I mean, if the old primary has been stopped gracefully, if the standby receive the checkpoint shutdown, you're on the safe side
[14:41:17] <otisolsen70> ioguix, what to look for in the slave log to ensure first step was correctly completed?
[14:41:39] <Myon> true, but the footnotes on that procedure give room for errors
[14:44:57] <ioguix> otisolsen70: issue a checkpoint on the standby, then compare the "Latest checkpoint's REDO location" on both side
[14:45:34] <ioguix> otisolsen70: "pg_controldata $PGDATA | grep 'REDO location'"
[14:46:04] <otisolsen70> On both sides? But the master has been shut down, right?
[14:48:19] *** Joins: michalz (~michalz@185.246.204.89)
[14:49:48] <Myon> TBH, I wouldn't recommend that if you don't know exactly what that means
[14:51:18] *** Quits: xocolatl (~xocolatl@193.32.126.213) (Ping timeout: 252 seconds)
[14:51:54] <otisolsen70> I dont
[14:52:35] <mbanck> solutions like patroni or pg_auto_failover automate that
[14:54:28] *** Quits: ivii (~ivan@user/ivii) (Remote host closed the connection)
[14:54:56] <ioguix> (and the PAF resource agent of pacemaker)
[14:55:13] <mbanck> *nod*
[14:55:32] <otisolsen70> I have now stopped my old master. Then promoted my slave to master. Now I am trying to reinitialize my old master as a slave. I have done:  pg_basebackup -h 10.0.0.4 -U replicator -p 5432 -D /var/lib/postgresql/12/main -Fp -Xs -P -R
[14:55:46] <otisolsen70> On the old master. So now I think the old master is ready to be a slave...
[14:55:56] <otisolsen70> I then do: cat 12/main/postgresql.auto.conf
[14:55:57] <ioguix> otisolsen70: yes, on both side, pg_controldata doesn't need the instance to be up.
[14:56:33] <otisolsen70> on the old master. And I would expect that file to have exactly one "primary_conninfo" entry . But it has two. Is that normal?
[14:56:58] *** Joins: smp (~smp@user/smp)
[14:57:04] *** Joins: ivii (~ivan@user/ivii)
[14:57:20] <otisolsen70> Does this look right? https://paste.yt/p17779.html
[14:57:49] <ioguix> but I agree with Myon and mbank, if you can afford a rebuild, this is probably a safest procedure
[14:57:55] <otisolsen70> I mean, shouldnt there only be one entry in this? I know it says do not edit this file manually, so I wont. But I fear it is not right
[14:57:57] <mbanck> I'm not sure it's expected; pg_basebackup copies over the auto.conf from the (former) standby/new primary, then adds primary_conninfo
[14:58:22] <mbanck> I mean, I'm not sure whether pg_basebackup should remove the old line or whether it was said that just appending should be fine
[14:58:42] <otisolsen70> mbanck, ok, but this copy of auto.conf clearly still has the old reference to the old master itself (the one that is currently goign to be the slave)
[14:59:03] <mbanck> yes, but why shouldn't it? I guess you didn't remove it
[14:59:16] <mbanck> you can check on your new primary that it is still there
[14:59:17] <otisolsen70> mbanck, I did not remove it. Should I have?
[14:59:20] <mbanck> I don't think Postgres should remove it
[14:59:48] <otisolsen70> mbanck, yes, it is still there on the primary.
[15:00:01] <otisolsen70> mbanck, but are you saying that it is not a problem there are two entries?
[15:00:03] <mbanck> you shouldn't much with auto.conf, but I guess you could ALTER SYSTEM RESET primary_conninfo on the new primary before doing pg_basebackup
[15:00:16] <mbanck> AFAIK it's not a problem, Postgres will use the later one
[15:00:27] <otisolsen70> mbanck, so just psql -d template1 "ALTER SYSTEM RESET primary_conninfo;" ?
[15:00:32] <mbanck> well, you can try it, it should be obvious whether replication works or doesn't
[15:00:42] <mbanck> try it and see
[15:01:28] <otisolsen70> Forgot -c
[15:01:46] <otisolsen70> Now 12/main/postgresql.auto.conf is empty. (only comments in there)
[15:02:58] *** Joins: zlinux (~zlinux@149.109.20.220)
[15:03:22] <mbanck> on the primary
[15:03:36] <mbanck> if you re-run pg_basebackup, then you should have a single entry on the standby
[15:05:12] *** Quits: econo (uid147250@user/econo) (Quit: Connection closed for inactivity)
[15:05:24] *** Quits: zlinux[] (~zlinux@149.109.18.164) (Ping timeout: 240 seconds)
[15:05:29] <otisolsen70> Yes. It does.
[15:05:45] <otisolsen70> Now I should be able to start PG on the old master and it wil start up and become slave, right?
[15:05:57] *** Joins: jdavfsxd (~rvgzuuqp@gateway/tor-sasl/rvgzuuqp)
[15:06:26] <svip> Is it possible to do three inserts in a CTE?  e.g. with A as (insert into t1 returning id), B as (insert into t2 select a.id returning id) insert into t3 select b.id?
[15:07:00] <ilmari> yes
[15:07:05] <ilmari> what happened when you tried?
[15:07:56] <svip> 'missing FROM-clause entry for table "a"'
[15:08:01] <ilmari> but you need a from clause to reference the CTEs
[15:08:09] <svip> Ah!
[15:08:15] <ilmari> insert into t2 select id from a returning t2.id
[15:08:48] <svip> Thanks.
[15:13:07] *** Quits: smp (~smp@user/smp) (Quit: ZNC 1.8.2 - https://znc.in)
[15:14:50] *** Joins: smp (~smp@user/smp)
[15:19:19] *** Quits: Ergo^ (~ergo@91.238.59.144) (Read error: Connection reset by peer)
[15:20:57] *** Quits: michalz (~michalz@185.246.204.89) (Ping timeout: 250 seconds)
[15:20:58] *** Quits: rodo (~rodo@pop.92-184-105-149.mobile.abo.orange.fr) (Read error: Connection reset by peer)
[15:22:02] *** Joins: merzo1 (~Thunderbi@95.132.54.193)
[15:23:24] *** Quits: merzo (~Thunderbi@95.133.42.21) (Ping timeout: 240 seconds)
[15:23:24] *** merzo1 is now known as merzo
[15:23:50] *** Joins: Kohe (~Kohe@46.12.76.236.dsl.dyn.forthnet.gr)
[15:26:03] *** Joins: zauberfisch_ (~Zauberfis@cm147-42.liwest.at)
[15:29:30] *** Quits: sliss (~sliss@109.136.165.60) (Quit: sliss)
[15:31:24] *** Quits: nyov (~nyov@user/nyov) (Ping timeout: 240 seconds)
[15:33:32] *** Joins: nyov (~nyov@user/nyov)
[15:34:28] *** Quits: foul_owl (~kerry@23.82.193.104) (Ping timeout: 272 seconds)
[15:35:51] *** Quits: mowcat (~mowcat@2a00:23c5:d190:1901:f22f:74ff:fe77:1e1c) (Ping timeout: 252 seconds)
[15:40:07] *** Quits: pateh (~pateh@81-197-66-239.elisa-laajakaista.fi) (Remote host closed the connection)
[15:40:52] *** Joins: pateh (~pateh@81-197-66-239.elisa-laajakaista.fi)
[15:44:56] *** Joins: vit (~vit@chello085216193138.chello.sk)
[15:45:20] *** vit is now known as Guest9416
[15:46:30] *** Quits: lxwulf (~lxwulf@user/lxwulf) (Ping timeout: 272 seconds)
[15:49:57] *** Joins: foul_owl (~kerry@23.82.193.88)
[15:51:46] *** Joins: pagnol (~user@014198154145.ctinets.com)
[15:52:04] *** Joins: sliss (~sliss@109.136.165.60)
[15:52:14] *** Joins: pedja (~pedja@user/deus-ex/x-7934090)
[15:52:45] *** Guest9416 is now known as freeworld
[15:53:49] *** Joins: G_SabinoMullane (~G_SabinoM@pool-72-94-251-236.phlapa.fios.verizon.net)
[15:55:17] *** Joins: ksynwa (~ksynwa@5.45.111.57)
[15:56:02] <ksynwa> In our database, a select query is taking much longer inside a transaction than outside it. Anyone knows if there is a common reason for thi?
[15:56:05] <ksynwa> this*
[15:56:43] <Myon> look at EXPLAIN (ANALYZE, BUFFERS)
[15:57:08] <Myon> (that's not a common issue)
[15:57:56] <Zr40> also you can't run select queries outside of a transaction. (One is started for you if you don't)
[15:58:27] *** Joins: michalz (~michalz@185.246.204.57)
[15:58:42] *** Quits: zlinux (~zlinux@149.109.20.220) (Read error: Connection reset by peer)
[15:59:48] *** Joins: zlinux (~zlinux@149.109.2.149)
[16:02:51] *** Joins: Hecate (~mariposa@user/hecate)
[16:03:12] *** Joins: zeden (~zeden@user/zeden)
[16:04:38] *** Quits: zeden (~zeden@user/zeden) (Client Quit)
[16:05:14] *** Quits: zlinux (~zlinux@149.109.2.149) (Read error: Connection reset by peer)
[16:05:51] *** Joins: zeden (~zeden@user/zeden)
[16:06:41] *** Joins: zlinux (~zlinux@149.109.9.195)
[16:10:13] *** Joins: bindu (~bindu@user/bindu)
[16:12:13] *** Quits: pagnol (~user@014198154145.ctinets.com) (Ping timeout: 256 seconds)
[16:12:33] *** Joins: enoq (~enoq@2a05:1141:1f5:5600:b9c9:721a:599:bfe7)
[16:13:11] *** Quits: zauberfisch_ (~Zauberfis@cm147-42.liwest.at) ()
[16:14:54] *** Joins: FreEm1nD (~FreEm1nD@mail.guarapari.store)
[16:15:59] *** Quits: sliss (~sliss@109.136.165.60) (Remote host closed the connection)
[16:17:05] *** Joins: pagnol (~user@014198154145.ctinets.com)
[16:18:03] *** Joins: zauberfisch_ (~Zauberfis@cm147-42.liwest.at)
[16:18:57] *** Joins: Ergo^ (~ergo@91.238.59.144)
[16:19:33] *** Joins: john_johnk (~Thunderbi@232.58.140.77.rev.sfr.net)
[16:21:42] *** Quits: zlinux (~zlinux@149.109.9.195) (Read error: Connection reset by peer)
[16:22:04] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[16:23:03] *** Quits: Haudegen (~quassel@178.115.237.87.static.drei.at) (Quit: Bin weg.)
[16:23:51] *** Joins: zlinux (~zlinux@149.109.12.63)
[16:24:58] *** Quits: maxzor (~maxzor@2a01cb04054faa005e5492c0e79f264a.ipv6.abo.wanadoo.fr) (Quit: Leaving)
[16:27:15] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[16:31:11] *** zauberfisch_ is now known as zauberfisch
[16:34:52] *** Quits: schinckel (uid38120@user/schinckel) (Quit: Connection closed for inactivity)
[16:35:56] *** Joins: JordiGH (~jordi@fencepost.gnu.org)
[16:35:56] *** Quits: JordiGH (~jordi@fencepost.gnu.org) (Changing host)
[16:35:56] *** Joins: JordiGH (~jordi@user/jordigh)
[16:36:32] *** Quits: xinming (~xinming@115.221.11.48) (Ping timeout: 272 seconds)
[16:36:56] *** Joins: xinming (~xinming@115.221.11.48)
[16:39:48] *** Quits: immibis_ (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de) (Remote host closed the connection)
[16:41:19] *** Joins: immibis (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de)
[16:41:41] *** Quits: Kohe (~Kohe@46.12.76.236.dsl.dyn.forthnet.gr) (Ping timeout: 256 seconds)
[16:43:23] *** Parts: pagnol (~user@014198154145.ctinets.com) (ERC (IRC client for Emacs 25.2.2))
[16:44:59] *** Joins: rodo (~rodo@pop.92-184-105-149.mobile.abo.orange.fr)
[16:50:03] *** Joins: krismatrix (~krismatri@205.178.37.15)
[16:53:15] *** Joins: rufito (~phil@186-79-29-179.baf.movistar.cl)
[16:54:51] *** Joins: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net)
[16:56:15] *** Joins: rewrit3 (~rewrit3@user/rewrit3)
[16:56:49] <depesz> i have weird situation. restoring from FS-level snapshot, pg doesn't finish "recovery". it just hangs on last wal, and that's all.
[16:56:51] <depesz> it'
[16:57:48] <depesz> it's pg 12, on ubuntu bionic. last wal is in  "progress" for 5+ hours!
[16:58:00] <depesz> there is no real app traffic there, just some monitoring queries.
[16:58:10] <depesz> the recovery is not waiting on anything.
[16:58:36] <depesz> strace of startup process just shows "epoll_pwait(...)
[16:59:21] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[16:59:32] <depesz> tried pg_ctl promote - nothing happens. not even a message in logs.
[17:00:26] <depesz> DAMN. and just now, after 5+ hours it decided to finish recovery.
[17:00:33] <depesz> should have asked for help 4 hours ago :/
[17:06:51] *** Quits: dege (~dege@user/dege) (Quit: Textual IRC Client: www.textualapp.com)
[17:07:20] <Myon> lots of fsync?
[17:09:01] <depesz> no. the box was 100% idle
[17:09:27] <depesz> ok, 99.5%
[17:09:44] <depesz> anyway = it's a mystery for another day. now I'm glad I can start working on the real problem.
[17:10:22] <Myon> if the storage is very slow you wouldn't see that in CPU, not sure it would even show up in iostat
[17:10:47] <depesz> Myon: it's aws 16000 iops gp2 volume. VERY fast.
[17:10:53] <Myon> nod
[17:15:19] <ksynwa> Myon, Zr40: Thanks
[17:17:24] *** Quits: TheCoffeMaker (~TheCoffeM@user/thecoffemaker) (Ping timeout: 240 seconds)
[17:18:35] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[17:21:11] *** Joins: Haudegen (~quassel@91.114.49.10)
[17:21:15] *** Quits: todd (~todd@gateway/tor-sasl/toddf) (Ping timeout: 240 seconds)
[17:22:17] *** Joins: TheCoffeMaker (~TheCoffeM@user/thecoffemaker)
[17:23:23] *** Joins: todd (~todd@gateway/tor-sasl/toddf)
[17:29:19] *** Joins: ammer (uid541621@id-541621.helmsley.irccloud.com)
[17:30:00] *** Joins: pmcnabb1 (~pmcnabb@user/pmcnabb)
[17:30:12] *** Quits: pmcnabb (~pmcnabb@user/pmcnabb) (Ping timeout: 240 seconds)
[17:30:12] *** pmcnabb1 is now known as pmcnabb
[17:35:48] *** Quits: pmcnabb (~pmcnabb@user/pmcnabb) (Ping timeout: 240 seconds)
[17:39:59] *** Quits: FreEm1nD (~FreEm1nD@mail.guarapari.store) (Quit: Leaving)
[17:43:27] *** Quits: jnnnnnnnnn (~jnnnnnnnn@c-2172524e.016-77-73746f43.bbcust.telenor.se) (Ping timeout: 252 seconds)
[17:46:45] *** Joins: vbabiy (~vbabiy@24.88.40.231)
[17:46:57] *** Joins: FreeBDSM (~FreeBDSM@user/freebdsm)
[17:46:59] *** Joins: jnnnnnnnnn (~jnnnnnnnn@157.97.164.5)
[17:47:21] *** Parts: FreeBDSM (~FreeBDSM@user/freebdsm) ()
[17:47:27] <vbabiy> Is there a way to create a index that will ensure that a TZRange is exactly 20 seconds?
[17:47:59] *** Quits: john_johnk (~Thunderbi@232.58.140.77.rev.sfr.net) (Ping timeout: 256 seconds)
[17:47:59] <Berge> vbabiy: No, but you can create a constraint that does so
[17:48:21] <Berge> You have two timestamptz columns and want to make sure they're exactly 10 seconds apart?
[17:49:28] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[17:49:37] <Berge> hm, that's not a range, of course, sorry
[17:50:07] <vbabiy> Berge, I am using a DateTimeTZRange
[17:50:19] <Berge> vbabiy: That's not a postgres data type
[17:50:45] <vbabiy> Sorry I meant tstzrange
[17:51:13] <vbabiy> I was looking in to creating a constraint but I didn't understand how I would do that
[17:51:23] <Berge> vbabiy: I'll give an example, two secs
[17:51:23] <Myon> there's a function to compute the length of a tstzrange
[17:51:51] <Berge> Is there?
[17:51:52] <Myon> alter table foo add check (thatfunction(yourtstzcolumn) = '00:00:10')
[17:52:09] <Berge> I was thinking just doing upper(therange) - lower(therange)
[17:52:18] <Myon> or an operator
[17:53:54] *** Joins: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26)
[17:54:28] <Berge> vbabiy: Something like ALTER TABLE mytable ADD CONSTRAINT "range must be exactly 10 seconds" CHECK (UPPER(myrange) - LOWER(myrange) = '10 seconds'::interval)
[17:55:15] *** Quits: cloudbender (~cloudbend@user/cloudbender) (Quit: Connection closed)
[17:55:18] <Berge> I couldn't find Myon's function at https://www.postgresql.org/docs/current/functions-range.html#RANGE-FUNCTIONS-TABLE
[17:55:51] <Myon> yeah sorry I just remembered that the problem has a solution, and that's Berge's -
[17:56:02] <Berge> You can always create a function for it! (-:
[17:57:17] *** Quits: fordfrog (~fordfrog@gentoo/developer/fordfrog) (Quit: Leaving)
[18:01:22] *** Joins: tozhu (~tozhu@218.89.244.95)
[18:05:29] *** Quits: vbabiy (~vbabiy@24.88.40.231) (Remote host closed the connection)
[18:07:37] *** Joins: maroloccio (~marolocci@pousada3ja.mma.com.br)
[18:07:45] <rovanion> How do I increment the sequence created by a serial on a table? When doing some preseeding of my db I insert some fields with predefined id's, but when normal operation then takes over the sequence starts on 1 and inserts fails x times over until there is an empty number available.
[18:08:07] <rovanion> This preseed is only used during development.
[18:08:51] <Berge> rovanion: With setval()
[18:08:58] <rovanion> Thank you!
[18:09:10] <rovanion> So, setval(nextval()) or something like that.
[18:09:18] <Berge> SELECT setval('mysequence', 123)
[18:09:48] <Berge> Any particular reason you're using predefined IDs and not RETURNING?
[18:10:40] <rovanion> Barge: Using those hardcoded id's in other text files that are part of the preseed.
[18:11:56] <Berge> It's Berge. (-: (If you get it wrong, I won't be highlighted, and might miss your message)
[18:12:03] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[18:12:33] <rovanion> Sorry, Norwegian surename?
[18:12:58] *** Quits: jdavfsxd (~rvgzuuqp@gateway/tor-sasl/rvgzuuqp) (Quit: jdavfsxd)
[18:12:58] <Berge> It's indeed a very common Norwegian surname, but it happens to be my first name.
[18:13:05] <Berge> Which is still Norwegian, but very uncommon.
[18:14:20] <Berge> There's exactly 98 of us. (And 9445 people have it as a surname.)
[18:14:26] <rovanion> :D
[18:14:42] <ilmari> 98 with it as the _only_ first name,  135 with it as the first first name
[18:15:28] <ilmari> and 2595 with it as a middle name
[18:16:04] <rovanion> Bet you get a lot of confusion/mobbing around that growing up.
[18:16:48] <Berge> Loads
[18:16:59] <Berge> I still have people asking me what my actual first name is
[18:17:50] <kjetilho> ilmari: I wonder how many are called Berge Berge Berge
[18:18:31] <Berge> kjetilho: There's a place called Bergeberget! (It's right next to Styggberget.)
[18:18:51] <kjetilho> |-D
[18:19:08] *** Quits: psoo (~psoo@dslb-090-186-134-090.090.186.pools.vodafone-ip.de) (Ping timeout: 272 seconds)
[18:19:17] <Myon> you should relocate there!
[18:19:39] <Berge> I… really shouldn't!
[18:19:57] <Berge> It's absolutely middle of nowhere. (But I even had family right next to it for a while.)
[18:20:14] <Myon> yeah I guess it might be difficult to explain that address to people
[18:20:18] <Berge> Also, Styggberget literally means Ugly The Berge
[18:21:12] <rovanion> Or naughymountain?
[18:21:20] *** Joins: tozhu (~tozhu@218.89.244.95)
[18:21:28] <ilmari> this is getting well into #lounge territory …
[18:21:29] <Berge> Or that, I suppose. The name lacks context!
[18:21:42] <Berge> Especially since it's a completely flat place.
[18:22:29] <rovanion> Not too far from Trysil though if you want to hit the piste.
[18:22:31] *** Joins: Kohe (~Kohe@46.12.76.236.dsl.dyn.forthnet.gr)
[18:23:38] <Berge> yeah
[18:26:06] *** Quits: evdubs (~evdubs@user/evdubs) (Ping timeout: 272 seconds)
[18:26:56] *** Joins: tachoknight (~textual@2600:1700:3060:3ee0:f467:fda7:b67f:3b6)
[18:30:09] *** Joins: evdubs (~evdubs@user/evdubs)
[18:31:48] *** Quits: Vitto (~Vitto@se-14.nat.univ-paris4.fr) (Ping timeout: 272 seconds)
[18:34:05] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[18:34:45] *** Joins: tozhu (~tozhu@218.89.244.95)
[18:36:37] *** Quits: tozhu (~tozhu@218.89.244.95) (Client Quit)
[18:37:32] *** Joins: tozhu (~tozhu@218.89.244.95)
[18:39:22] *** Joins: gambl0re (~gambl0re@2607:fea8:a59f:c360::82dc)
[18:40:36] *** Quits: emhwfhrom^ (~emhwfhrom@50.235.176.163) (Ping timeout: 240 seconds)
[18:42:08] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[18:43:00] *** Joins: mordant (~mordant@69.174.145.100)
[18:43:53] <mordant> I have an old, old database that I'm migrating to v13.6 right now. I swear I had this working not too long ago, thinking maybe an update goofed it up. Have old encrypted data that I was able to decrypt using SET bytea_output='escape' previously, but that's no longer working
[18:44:41] <Myon> bytea_output='escape' is still a thing
[18:44:45] *** Joins: mattil (~mattil@87-92-28-123.bb.dnainternet.fi)
[18:44:53] *** Joins: sehrope (~sehrope@23-24-81-162-static.hfc.comcastbusiness.net)
[18:45:17] *** Joins: mattil_ (~mattil@helsinki.portalify.com)
[18:46:01] <mordant> why is it throwing this error still then? not returning the data
[18:46:27] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[18:46:36] <mordant> hang on i'll get the error back
[18:47:17] <mordant> ERROR:  decrypt error: Data not a multiple of block size
[18:48:30] <depesz> mordant: what's the exact query?
[18:48:37] <depesz> that caused this error.
[18:49:11] *** Quits: mattil (~mattil@87-92-28-123.bb.dnainternet.fi) (Ping timeout: 256 seconds)
[18:49:15] <mordant> SET bytea_output = 'escape'; select decrypt(taxid,'###key###','aes') as taxid from users where userid='username';
[18:49:34] <Myon> that's independent from bytea_output
[18:49:42] <Myon> the data has the wrong length
[18:50:06] <mordant> that's what exists on the old server, it imported as-is
[18:50:07] *** Joins: Klinda (~superleag@user/klinda)
[18:50:11] <mordant> I'm guessing old encryption
[18:50:23] <Myon> more likely data corruption
[18:50:29] <Myon> try the query on the old server
[18:50:29] <mordant> but how am I supposed to get that data out? I did have this working
[18:50:38] <mordant> let me verify, I did a new import recently, maybe it didn't carry over correctly
[18:50:42] <Myon> does it work for other rows?
[18:50:48] <mordant> no
[18:51:00] *** Joins: mattil (~mattil@87-92-28-123.bb.dnainternet.fi)
[18:51:03] <Myon> check the raw data on the old and new server
[18:51:07] <mordant> yea checking now
[18:51:16] <Myon> maybe one quoting layer got erroneously stripped or added
[18:51:31] <depesz> mordant: what is the datatype of users.taxid ?
[18:52:32] *** Joins: mrgz (~mrgz@201-42-0-191.dsl.telesp.net.br)
[18:52:36] <mordant> bytea
[18:53:15] <mordant> ok you're right, the data does not match
[18:53:19] <mordant> well, my bad
[18:53:41] <mordant> looks like all the '\' characters in the hash are doubled
[18:54:35] *** Quits: mattil_ (~mattil@helsinki.portalify.com) (Ping timeout: 256 seconds)
[18:55:05] *** Quits: randir (~randir@93.159.239.42) (Remote host closed the connection)
[18:55:24] *** Quits: mattil (~mattil@87-92-28-123.bb.dnainternet.fi) (Ping timeout: 240 seconds)
[18:55:27] <mordant> how fun
[18:55:38] *** Joins: randir (~randir@93.159.239.42)
[18:55:45] <mordant> if this previous dev was here I'd punch him right in the back of the head, he did so many things wrong
[18:56:56] *** Joins: john_johnk (~Thunderbi@232.58.140.77.rev.sfr.net)
[18:57:12] <mordant> any advice? I have to import it from a dump file, wondering if that's why. I remember trying to export a pgsql file and was not able to import it to the new one
[18:57:35] *** Joins: bmomjian (~bruce@momjian.us)
[18:57:50] <Myon> psql's \copy to/from would be best, that gets the quoting right
[18:58:24] *** Joins: Nekomander (~BadAdvice@user/badadvicecat)
[18:59:48] *** Quits: randir (~randir@93.159.239.42) (Ping timeout: 240 seconds)
[19:01:26] <mordant> it does look like it's just this 1 table, that makes it easier
[19:01:43] *** Joins: fef (~thedawn@user/thedawn)
[19:06:12] *** Quits: turlando (~turlando@user/turlando) (Ping timeout: 240 seconds)
[19:09:18] *** Quits: antlarr (~quassel@223.red-80-26-193.dynamicip.rima-tde.net) (Quit: bye!)
[19:10:41] *** Quits: mrhansen (~john@harry.mrhansen.id.au) (Read error: Connection reset by peer)
[19:11:16] *** Joins: mrhansen (~john@45.124.52.33)
[19:11:28] *** Joins: antlarr (~quassel@223.red-80-26-193.dynamicip.rima-tde.net)
[19:11:56] <mordant> did the import take them as a string and try to escape the slashes or something? curious how it might've happened
[19:12:05] *** Joins: fordfrog (~fordfrog@gentoo/developer/fordfrog)
[19:12:30] <Myon> possibly
[19:12:46] *** Joins: tradar (~tradar@user/tradar)
[19:12:48] <Myon> or COPY with/without a CSV option
[19:15:28] *** Quits: ben__ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Remote host closed the connection)
[19:18:35] *** Quits: tradar (~tradar@user/tradar) (Quit: tradar)
[19:19:40] *** Joins: emhwfhrom^ (~emhwfhrom@50.235.176.163)
[19:20:01] *** jkavalik_ is now known as jkavalik
[19:20:28] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[19:21:59] *** Joins: randir (~randir@2.92.196.208)
[19:22:33] *** Quits: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net) (Remote host closed the connection)
[19:22:56] *** Joins: ees-mobile (~ees-mobil@pool-108-18-30-46.washdc.fios.verizon.net)
[19:29:04] *** Joins: tozhu (~tozhu@218.89.244.95)
[19:29:30] *** Quits: krismatrix (~krismatri@205.178.37.15) (Quit: Leaving)
[19:30:41] *** Joins: shiranaihito_ (~textual@123-192-192-149.dynamic.kbronet.com.tw)
[19:33:07] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[19:33:14] *** Quits: shiranaihito (~textual@220-133-46-51.hinet-ip.hinet.net) (Ping timeout: 272 seconds)
[19:33:28] *** Joins: ne2k (~andy@212.250.187.98)
[19:38:36] *** Quits: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26) (Ping timeout: 240 seconds)
[19:42:27] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[19:42:52] *** Quits: emhwfhrom^ (~emhwfhrom@50.235.176.163) (Remote host closed the connection)
[19:43:24] *** Joins: tozhu (~tozhu@218.89.244.95)
[19:45:11] *** Joins: aremaref (~aremaref@160.72.16.74)
[19:45:12] *** Joins: aremaref_ (~aremaref@160.72.16.74)
[19:50:00] *** Joins: xocolatl (~xocolatl@193.32.126.231)
[19:53:25] *** Joins: Guest48 (~Guest48@a79-168-179-110.cpe.netcabo.pt)
[19:53:27] *** Quits: Guest48 (~Guest48@a79-168-179-110.cpe.netcabo.pt) (Client Quit)
[19:56:29] *** Joins: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26)
[19:57:15] *** Joins: Guest48 (~Guest48@a79-168-179-110.cpe.netcabo.pt)
[19:58:09] <Guest48> Hi there!
[19:58:10] <Guest48> Is there any simple way to update a foreign key column along with all the references in referenced tables ?
[19:58:43] <Kobaz> make sure your FK is on update cascadew
[19:58:46] <Kobaz> cascade
[19:58:48] <Guest48> I am doing it with python finding all the referenced tables and columns and executing update on each of them. Which is making to many update queries and its slow
[19:59:31] <Kobaz> ON UPDATE CASCADE
[19:59:33] <Guest48> Kobaz to make it update cascade, I must need to drop current FK constraint ?
[19:59:37] <Kobaz> yeah
[20:01:05] *** Joins: carragom (~textual@201.204.94.76)
[20:01:17] <Kobaz> also... avoid changing those if you can
[20:01:23] <Kobaz> use 'relational design'
[20:01:27] <Guest48> why isn't it by default? does it degrade performance?
[20:01:39] <Kobaz> relate things by ID... don't change the ID.. change the name of the thing in the base table
[20:02:10] <Guest48> Kobaz doing it because trying to migrate a database into another and both of them having some PK conflicts
[20:02:16] <Kobaz> ah
[20:02:29] <Kobaz> why isn't it the default? becuase the developers didn't set it up that way
[20:02:47] <Kobaz> postgres doesn't have that many defaults anyway
[20:02:54] <Guest48> what If I set and leave that way would database be slower?
[20:03:00] <Kobaz> the default is to have the user tell the db what it wants
[20:03:20] <Guest48> because I would need it only while migration not later
[20:03:22] <Kobaz> leaving the defintiion there doesn't affect speed
[20:03:31] <Guest48> okay thank you!
[20:03:37] <Kobaz> once you execute operations, then it would affect speed
[20:03:43] *** Quits: xinming (~xinming@115.221.11.48) (Ping timeout: 256 seconds)
[20:03:50] <Kobaz> if you're updating the id all the time, then it will cascade the update
[20:03:57] <Kobaz> if you're not updating the id, then it wont
[20:05:35] *** Joins: xinming (~xinming@115.221.11.48)
[20:06:19] *** Quits: rufito (~phil@186-79-29-179.baf.movistar.cl) (Quit: Leaving)
[20:07:44] *** Quits: tozhu (~tozhu@218.89.244.95) (Quit: tozhu)
[20:09:10] *** Joins: saml (~saml@cpe-74-73-80-162.nyc.res.rr.com)
[20:12:39] <saml> explain select * from tb where col = :someuuid;   shows Seq Scan even if there's btree index on col.  is it because table is too small (only a couple of rows)?
[20:13:38] <Myon> yes
[20:13:53] * saml adds more rows
[20:14:06] <Myon> indexes will only be used if the table is > 2 blocks
[20:17:17] *** Quits: Ergo^ (~ergo@91.238.59.144) (Remote host closed the connection)
[20:17:50] *** Joins: mizi (~mizi@user/mizi)
[20:17:51] *** Quits: Haudegen (~quassel@91.114.49.10) (Quit: Bin weg.)
[20:18:11] *** Joins: mowcat (~mowcat@host86-129-162-211.range86-129.btcentralplus.com)
[20:18:36] *** Quits: Nekomander (~BadAdvice@user/badadvicecat) (Ping timeout: 240 seconds)
[20:18:57] *** Joins: Nekomander (~BadAdvice@user/badadvicecat)
[20:24:36] *** Quits: jkavalik (~jkavalik@ip-78-102-141-139.net.upcbroadband.cz) (Ping timeout: 252 seconds)
[20:24:38] <mordant> well, using \copy still escapes the slashes in the encrypted strings. but WHY.
[20:25:04] <mordant> google is no help thus far
[20:28:03] <saml> "tb_pkey" PRIMARY KEY, btree ("userid", "usertype");  if I have this index,  select * from tb where userid = :x;  be using index when table grows?  adding a hundred row still does Seq Scan. I'm wondering if i should create index on userid  specifically
[20:30:07] <ilmari> saml: a hudred rows is still approximately zero
[20:30:43] <ilmari> add a few thousand and see. and do `vacuum analyze` on the table
[20:31:36] *** Joins: the_lanetly_052 (~the_lanet@194.135.154.213)
[20:32:51] *** Quits: the_lanetly_052_ (~the_lanet@194.135.169.19) (Ping timeout: 252 seconds)
[20:36:13] <Myon> mordant: but \copy also knows how to read that back corectly
[20:36:42] <saml> yeah adding thousands made query to use Bitmap Heap Scan
[20:37:33] <mordant> Myon, I see, I'll give it a shot then, thanks for the ongoing support
[20:39:23] *** Joins: tozhu (~tozhu@218.89.244.95)
[20:43:50] *** Quits: tozhu (~tozhu@218.89.244.95) (Ping timeout: 250 seconds)
[20:48:22] *** Quits: enoq (~enoq@2a05:1141:1f5:5600:b9c9:721a:599:bfe7) (Quit: enoq)
[20:50:53] <ne2k> mordant, I missed your original question, what were you trying to do with backslashes?
[20:51:54] <mordant> ne2k, I'm importing a database dump from an old postgres instance (7.4) to a new one, and i have one table with some encrypted bytea columns. When I import it, the slashes in the bytea columns are doubling up / being escaped
[20:52:00] <mordant> so im trying to work around that
[20:52:15] <Berge> Is the dump made with pg_dump from 7.4 too?
[20:52:27] <ne2k> "encrypted" in what way?
[20:52:36] <mordant> Berge, yes
[20:52:53] <mordant> ne2k with the built in encrypt(), aes
[20:53:24] *** Quits: aremaref_ (~aremaref@160.72.16.74) (Ping timeout: 240 seconds)
[20:53:24] *** Quits: aremaref (~aremaref@160.72.16.74) (Ping timeout: 240 seconds)
[20:53:27] <ne2k> mordant, if it's a bytea column, it surely doesn't matter what it contains? the encryption of not is irrelevant, surely? it's just... bytes? no?
[20:53:56] <ne2k> mordant, what is the format of the file you're trying to import? plain sql? what does an example row with one of these values look like?
[20:54:06] <depesz> mordant: afair, in 7.4 string 'a\nb' was "a", newline character, "b". now, it's literal "a\nb" - 4 charactres
[20:54:45] <Berge> mordant: Generally, the only supported way to upgrade via plain text dumps is to use pg_dump from the target version
[20:55:06] <Berge> Looks like you found one of the ways why
[20:55:50] <ne2k> Berge, can you run the new version of pg_dump on an old database, then?
[20:55:54] <mordant> ne2k, for example, \364z\350\337b\202*\246\214\250\205\344\306\265T\321 becomes \\364z\\350\\337b\\202*\\246\\214\\250\\205\\344\\306\\265T\\321 in the dump
[20:55:56] <Berge> ne2k: sure
[20:55:57] <koollman> ne2k: yes
[20:56:00] <Berge> ne2k: It's backwards compatible
[20:56:05] <Berge> But not forwards compatible
[20:56:11] <koollman> restore to 7.4, and use a modern pg_dump
[20:56:17] <ne2k> mordant, what is \364 supposed to represent?
[20:56:18] <mordant> access is the issue there, the previous dev left this thing to fester for 16 years
[20:56:28] <mordant> its on old hardware, on an abandoned OS
[20:56:33] <mordant> its a nightmare
[20:56:38] <Berge> mordant: Do you have 7.4 running?
[20:56:39] <mordant> ne2k, I have no idea
[20:56:40] <Berge> Or just the dump?
[20:57:04] <ne2k> surely it wouldn't be beyond the wit of man to spin up a 7.4 instance, restore the dump, install the new pg_dump and dump again in the new format
[20:57:06] <mordant> Berge I have it running, but it's on a machine running fedora core 3, locked behind a firewall not even the datacenter its in knows how to manage, old juniper box
[20:57:25] <mordant> it all sucks, getting the new pg_dump to connect to the old is going to be way more trouble than it's worth
[20:57:30] <koollman> I wouldn't touch the original system, or anything that still has 7.4 running :)
[20:57:46] <depesz> mordant: before loading the dump you can try to set "standard_conforming_strings = false"
[20:57:49] <mordant> koollman, yea no kidding
[20:57:52] <Berge> mordant: As people here are saying, your options are to connect a modern pg_dump to the running 7.4 instance, or install 7.4 somewhere and restore the dump to it
[20:57:53] *** Quits: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26) (Remote host closed the connection)
[20:57:58] <Berge> And then point a modern pg_dump to that new instance
[20:58:12] <mordant> can you even install 7.4 anymore?
[20:58:14] <Berge> koollman: Why wouldn't you?
[20:58:15] <mordant> its ancient
[20:58:16] <ne2k> Berge, or could you just upgrade the instance?
[20:58:24] <Berge> ne2k: How?
[20:58:25] <depesz> i think the dump is fixable. but proper way would be to use fresh pg_dump
[20:58:27] <mordant> no, it cannot be upgraded
[20:58:34] <mordant> not easily
[20:58:35] <koollman> Berge: because if it still exists, it is obviously not maintained. Could break for many reasons
[20:58:40] <ne2k> Berge, isn't there a thing to upgrade versions?
[20:58:43] <koollman> Berge: don't want that kind of responsibility
[20:58:58] *** Joins: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26)
[20:58:58] <Berge> koollman: Could it break it to point pg_dump to it?
[20:59:02] <mordant> depesz, I'll look into that, thank you
[20:59:04] <koollman> Berge: sure
[20:59:05] <Berge> ne2k: In 7.4? (-:
[20:59:13] <Berge> ne2k: Dump and restore, just like mordant is about to
[20:59:18] *** Quits: carragom (~textual@201.204.94.76) (Quit: Textual IRC Client: www.textualapp.com)
[20:59:18] <ne2k> ok
[20:59:27] <Berge> No pg_upgrade there
[20:59:31] <mordant> nope
[20:59:31] <ne2k> fair enough
[20:59:43] <Berge> koollman: er, ok, I wouldn't expect that to break
[21:00:17] <koollman> Berge: that's the tricky part. You don't expect it, which is why it hurts more ;)
[21:00:20] <mordant> if that was an option this whole thing could be avoided, I'm extracting this entire app setup onto modern hosting, and it's taken months. Guy didn't know what he was doing, built it a long time ago, then never updated anything, and I mean anything
[21:00:41] <Berge> koollman: You can always make a backup with pg_dump-from-7.4 to somewhere if you're worried, of course
[21:00:55] *** Joins: jkavalik (~jkavalik@ip-78-102-141-139.net.upcbroadband.cz)
[21:00:56] *** Joins: aremaref (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net)
[21:01:29] *** Joins: aremaref_ (~aremaref@107-198-1-161.lightspeed.irvnca.sbcglobal.net)
[21:01:38] <mordant> php4, OpenSSL 0.9.7a Feb 19 2003
[21:01:53] <koollman> mordant: brings back memories ... not always good one, though :)
[21:01:58] <mordant> yea, kill me
[21:02:39] *** Joins: enquora (~enquora@S0106f0f2498387f3.cg.shawcable.net)
[21:03:02] <koollman> mordant: get some vm or similar env running an OS from that time, use it to validate/transform the dump (and maybe some other things. php4 is not something you'll get easily in a modern env either :) )
[21:03:31] <mordant> depesz, I'm restoring the dump via psql connectionstring < file.dump, because other methods fail - can I still make use of the standard_conforming_strings ?
[21:03:47] <ne2k> mordant, https://github.com/postgres/postgres/tree/3f78df10b168b78232fdbefe1d9d84c37f5aaaab/src 7.4 source
[21:04:03] <mordant> koollman, I've updated the code to work on 5.3, baby steps
[21:04:10] <koollman> debian sarge might be a reasonable fit. (or do try to get fedora core 3, but I don't know if they have an archive system)
[21:04:30] *** Quits: enquora (~enquora@S0106f0f2498387f3.cg.shawcable.net) (Client Quit)
[21:04:31] <ne2k> presumably if we can find out how it handled outputting bytea with copy, it can be munged into a compatible format
[21:04:34] <koollman> (not as a final target, but as intermediary host that you can mess with and use such old software easily enough on it)
[21:04:42] <mordant> This is just 1 table of data, relatively small, and I had it working not 3 months ago. Do I really need to recreate the entire legacy environment just to import it again?
[21:04:54] <koollman> well, not for one table, no
[21:05:01] <mordant> honestly i'm tempted to just find and replace \\ with \ in a text editor
[21:05:05] <koollman> why was it working 3 month ago, then ? :)
[21:05:13] <mordant> if I knew that I wouldn't be in here asking for help, I don't know
[21:05:29] <mordant> wish I knew
[21:05:31] <koollman> the data format change isn't recent
[21:05:35] <Berge> mordant: What's wrong with the suggestions you've gotten?
[21:05:35] <mordant> nope
[21:05:39] *** Quits: jnnnnnnnnn (~jnnnnnnnn@157.97.164.5) (Remote host closed the connection)
[21:05:53] *** Joins: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com)
[21:05:59] <ne2k> mordant, what text file would you be wanting to replace it in? I thought the file had \xxx in it, which represents some byte, and when you import it, you're getting four bytes, the literal bytes for \, x and x and x
[21:06:02] <Berge> Did you try to import the dump you have with standard_conforming_strings=false?
[21:06:13] <mordant> Berge, I appreciate all the help, everyone is jumping in with support and I am grateful. Some of the solutions are way, way more work than I think is necessary
[21:06:16] <ne2k> Berge, he wasn't sure how to set that when doing psql <
[21:06:32] <mordant> yea
[21:06:33] <Berge> Set it in postgresql.conf
[21:06:38] <Berge> Or use ALTER SYSTEM
[21:06:48] <Berge> Or amend the dump file to start with SET standard_conforming_strings = 'false';
[21:06:58] <mordant> Berge I like that, let me give it a shot
[21:07:09] <Berge> I still fail to see how it's much work to use a modern pg_dump
[21:07:25] <Berge> mordant presumably has some sort of access to the old server
[21:07:28] *** Joins: funhouse (~funhouse@user/funhouse)
[21:08:19] *** Quits: xinming (~xinming@115.221.11.48) (Ping timeout: 256 seconds)
[21:08:45] <mordant> I do
[21:09:21] <mordant> Berge because I can't connect to the old server, I have about 20 reasons why that's a pain, and setting up a VM and rebuilding an old version of postgres to try and make it accessible to the new version seems like a stretch
[21:09:47] <ne2k> mordant, several people have said that it seems to be the most likely way to work
[21:09:56] *** Joins: xinming (~xinming@115.221.11.48)
[21:10:02] <mordant> and I'm no expert, theyre probably right
[21:10:11] <Berge> mordant: You have access to the old server, but can't connect?
[21:10:15] <mordant> but for one table, do I really need to create a virtual machine and rebuild 7.4 from source code?
[21:10:18] <mordant> Berge, correct
[21:10:23] <Berge> By access, I meant some way of opening a TCP connection (for instance through SSH tunneling)
[21:10:26] <Berge> What do you mean by access?
[21:10:35] <koollman> you can surely load data by transforming the file, but it is not easy to be sure you have the exact same data
[21:10:36] <ne2k> mordant, however, I still think that if we can get to the bottom of what the encoding in the files you have is supposed to represent, it can be munged into a format that a new version will be able to import
[21:10:37] <mordant> I can ssh, didn't know that was an option honestly
[21:10:38] <Berge> You don't have to build it from source.
[21:10:53] <Berge> mordant: So if you have SSH, then you can easily create a tunnel
[21:10:57] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[21:11:03] <Berge> ne2k: But it's still unspported
[21:11:38] <Berge> ssh -L1337:localhost:5432 old-server.example.com
[21:11:39] <ne2k> Berge, have you got domain sockets to work over ssh? I tried once and it was a pain in the butt. gave up and went to TCP. but he might not have the option if the machine doesn't have TCP enabled and he can't edit the config
[21:11:46] <Berge> And then pg_dump -h localhost -p 1337 ...
[21:11:55] <mordant> standard_conforming_strings = false did not work, changed how the string is stored to some other type of value
[21:11:56] <Berge> ne2k: yes, with a bit of socat-to-TCP
[21:11:59] <ne2k> ssh is supposed ot allow forwarding of domain sockets
[21:11:59] <mordant> \x695c3030303379772e5c333732555c3033315c3234325c303034215c3233377c5c3331375c323636
[21:12:16] <mordant> before I could just do SET bytea_output='escape' and i was good
[21:12:22] *** Joins: jnnnnnnnnn (~jnnnnnnnn@c-2172524e.016-77-73746f43.bbcust.telenor.se)
[21:12:22] <koollman> Berge: works in pure ssh too. Well, maybe not openssh from that era, but still, could work :)
[21:12:22] *** Joins: alioui (~medeva293@102.159.53.198)
[21:12:46] <ne2k> mordant, how are you ever going to know if it's right? what are you expecting the value to actually be? that looks like a perfectly reasonable bytea value to me
[21:12:49] <mordant> I'll look into a pg_dump over ssh then, because that might be doable, thanks for the suggestions
[21:12:55] <Berge> socat -d TCP6-LISTEN:5433,bind=::1,fork UNIX-CONNECT:/var/run/postgresql/.s.PGSQL.5432
[21:12:58] <Berge> Or something to that effect
[21:13:03] <mordant> ne2k, I expect it to look how it looks on the old version, because when it was working, they matched
[21:13:13] <Berge> koollman: What works in pure SSH? Tunneling unix sockets?
[21:13:24] <ne2k> mordant, "when it was working"? what are you talking about here, btw?
[21:13:31] <mordant> the encrypted strings
[21:13:33] <koollman> Berge: yes. openssh is pretty amazing, although postgresql makes it harder than it should be
[21:13:33] <ne2k> I thought you had a dump you needed to import and couldn't
[21:13:39] <mordant> ne2k, correct
[21:13:41] <Berge> ah, it does indeed, I'm just outdated
[21:13:44] <ne2k> so... wut
[21:13:55] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[21:13:57] <mordant> i updated my import, encrypted data carried over differently
[21:14:08] *** Joins: immibis_ (~hexchat@dynamic-089-204-139-245.89.204.139.pool.telefonica.de)
[21:14:08] <mordant> and I don't have a reason why, the process is the same I followed previously
[21:14:10] <ne2k> mordant, "updated my import"?
[21:14:15] <mordant> the data
[21:14:18] <mordant> I re-imported the data
[21:14:22] <mordant> to a newer version, for testing
[21:14:27] <ne2k> newer version of what?
[21:14:30] <mordant> data
[21:14:33] <ne2k> is it the same dump file in all cases?
[21:14:36] <koollman> Berge: something like: -L /tmp/.s.PGSQL.5432:/var/run/postgresql/.s.PGSQL.5432
[21:14:37] *** Joins: Guest79 (~Guest79@7.sub-174-197-195.myvzw.com)
[21:14:38] *** Quits: ben_ (~ben@host86-160-225-46.range86-160.btcentralplus.com) (Remote host closed the connection)
[21:14:41] <mordant> i have a nightly dump
[21:14:43] <Berge> koollman: yep, looks like it
[21:14:47] <mordant> i pulled the latetst one into new environment
[21:15:06] <Guest79> index btree (pay_to) includes (name);  select name from table where pay_to = 'x';
[21:15:07] <koollman> Berge: it would be slightly easier if psql could use a socket rather than a socket directory
[21:15:13] <Guest79>  why would this be bitmap heap scanning? shouldn't it be an index only since everything fetched/filtered on is in the index?
[21:16:45] <koollman> Berge: note that it can also show a nice property of abstraction layers ... -L 5432:/var/run/postgresql/.s.PGSQL.5432  (assuming nothing listen on tcp/5432 locally)
[21:16:52] <Berge> Guest79: If the visibility map is up to date
[21:17:06] *** Quits: immibis (~hexchat@dynamic-089-204-138-200.89.204.138.pool.telefonica.de) (Ping timeout: 272 seconds)
[21:17:28] <ne2k> mordant, can you show some rows from the dump from the version that worked, and the row that fails on the new dump?
[21:17:35] <Berge> Guest79: https://www.postgresql.org/docs/current/indexes-index-only-scans.html explains the method
[21:17:38] <mordant> ok, well, just for kicks, I did my text editor find and replace. Replaced all instances of \\ with \, and from what I can tell, it worked
[21:18:02] <Guest79> ty Berge
[21:18:07] <koollman> mordant: so, anyway, if you can ssh from something with a modern pg_dump, to the old postgresql server, you can generate a new dump easily enough
[21:18:20] <ne2k> mordant, I still have no idea what you're talking about. I thought the files had \235 in them, and that was getting import as four bytes, rather than one byte
[21:19:44] <mordant> ne2k, the byteas in the dump, dumped as strings with \235 etc, every backslash was doubled, because the dump was trying to escape them for whatever reason
[21:20:02] <mordant> i opened the dump as a text file, replaced all the escaped backslashes with just a single
[21:20:06] <Guest79> omg double thank you Berge, I've been at this for days
[21:20:11] <Guest79> have a great day sir
[21:20:12] <mordant> dumb
[21:20:15] <mordant> but it seems to be working
[21:20:50] <mordant> yep, works, wow. What a pain.
[21:21:11] <mordant> Ok, thank you everyone for all of your help, really. Situation is stupid and needlessly complicated, you all came through with a ton of good info and suggestions
[21:21:18] <ne2k> ok, that is not at all what I thought the problem was
[21:21:33] <mordant> ne2k, sorry, thought I explained it decently, but clearly not
[21:22:21] <ne2k> mordant, something must have changed at the end that generates the dumps
[21:23:03] <mordant> thats the one thing I can guarantee isn't changing. I must have fucked up the import script I have been working from somewhere, I'll check my revisions. Or maybe I did something that I didn't document trying to fix it last time? I honestly don't remember, it was months ago
[21:23:09] <mordant> stupid on my part either wya
[21:24:31] <ne2k> I'm still baffled as to how you think the thing that is doing the import could affect the content of the thing that is generated before you import it
[21:24:35] <ne2k> but hey
[21:25:00] *** Quits: xenoterracide (~xenoterra@2600:1700:71d4:81f:6e:2be2:8546:ba26) (Ping timeout: 240 seconds)
[21:25:00] *** Joins: Xof (~Xof@198-0-193-113-static.hfc.comcastbusiness.net)
[21:25:29] <mordant> I don't, im saying something changed is all
[21:25:37] <mordant> and i just said it couldve been something i changed that i didn't document
[21:25:55] *** Joins: econo (uid147250@user/econo)
[21:26:37] *** Quits: the_lanetly_052 (~the_lanet@194.135.154.213) (Ping timeout: 272 seconds)
[21:26:39] <ne2k> mordant, you said you guarantee that nothing has changed with the thing that generates the dumps. but the file you have, that is different, is generated by the thing that generates the dump
[21:27:01] <ne2k> so... logic brain mega super kachunk grind grind okay okay
[21:28:30] *** Quits: vladoski (~vladoski@2001:b07:add:d406:907e:4f:b57f:11c2) (Remote host closed the connection)
[21:29:37] *** Joins: Haudegen (~quassel@178.115.237.87.static.drei.at)
[21:30:38] <Berge> Guest79: haha, I'm glad to help
[21:35:01] *** Quits: rodo (~rodo@pop.92-184-105-149.mobile.abo.orange.fr) (Read error: No route to host)
[21:35:31] *** Joins: pvn (~Adium@85-195-230-32.fiber7.init7.net)
[21:36:09] *** Joins: ash_worksi (~ash_m@user/ash-m/x-3292451)
[21:36:12] *** Quits: Elodin (~elodin@user/elodin) (Ping timeout: 240 seconds)
[21:36:57] <ash_worksi> is a special variable populated I can raise a notice on to see what collision happened during a unique_violation ?
[21:37:39] <ash_worksi> (I do _not_ want to comment out the EXCEPTION)
[21:38:37] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[21:39:12] *** Joins: NCS_One (~NCS_One@bl11-90-133.dsl.telepac.pt)
[21:42:03] *** Joins: Rashad (~textual@2a01:9700:1a7c:8900:84a2:920e:e2a8:8ec)
[21:42:26] *** Quits: jkavalik (~jkavalik@ip-78-102-141-139.net.upcbroadband.cz) (Ping timeout: 272 seconds)
[21:48:34] *** Quits: mncheck (~mncheck@193.224.205.254) (Read error: Connection reset by peer)
[21:50:11] <StuckMojo> if you're using sync rep, does max_standby_streaming_delay still have any effect?
[21:51:18] *** Quits: john_johnk (~Thunderbi@232.58.140.77.rev.sfr.net) (Ping timeout: 272 seconds)
[21:51:33] *** Quits: maroloccio (~marolocci@pousada3ja.mma.com.br) (Quit: WeeChat 3.0)
[21:51:57] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[21:52:47] <StuckMojo> i.e. if it kicks in on a sync replica, that would cause a commit on the primary to block potentially
[21:53:35] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[21:54:48] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Ping timeout: 252 seconds)
[21:56:54] *** Joins: _mikey (~mikey@user/mikey/x-4335048)
[22:03:20] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[22:04:56] <ne2k> ash_worksi, CONSTRAINT_NAME? https://www.postgresql.org/docs/14/plpgsql-control-structures.html#PLPGSQL-ERROR-TRAPPING
[22:05:30] <ash_worksi> ne2k: I'll look at that, thanks
[22:05:50] <ne2k> 43.6.8.1. Obtaining Information About An Error
[22:05:52] *** Quits: ponsfrilus (~Thunderbi@vpn-253-161.epfl.ch) (Ping timeout: 272 seconds)
[22:06:17] <ne2k> Within an exception handler, one may also retrieve information about the current exception by using the GET STACKED DIAGNOSTICS command, which has the form:
[22:06:21] <ash_worksi> there is also a "DETAIL" option for RAISE, but I don't really understand how it's used
[22:07:16] <ne2k> ash_worksi, that's for you to supply detail on the error you are raising with raise
[22:07:32] *** Joins: ponsfrilus (~Thunderbi@adsl-178-39-226-100.adslplus.ch)
[22:07:50] <ash_worksi> oh?
[22:07:54] <ash_worksi> so do I need to use both?
[22:07:56] <ne2k> https://www.postgresql.org/docs/14/plpgsql-errors-and-messages.html#PLPGSQL-STATEMENTS-RAISE
[22:07:59] <ash_worksi> in order to raise a notice
[22:08:09] <ash_worksi> giving the constraint?
[22:09:15] <ne2k> ash_worksi, it would make sense to do raise notice using constraint = CONSTRAINT_NAME, surely?
[22:09:32] <ne2k> ash_worksi, it depends on what is consuming the notice and what it's going to do with it
[22:10:36] *** Quits: zemis (~zemis@ip-89-176-21-138.net.upcbroadband.cz) (Ping timeout: 240 seconds)
[22:10:38] <ash_worksi> oh
[22:10:56] <ash_worksi> thanks ne2k
[22:11:05] <ne2k> if all else fails, read the manual
[22:11:06] <ash_worksi> I'll read that section more closely
[22:11:18] <ne2k> the pg manuals really are very good indeed
[22:11:37] <ash_worksi> yes, but I didn't really know what I was looking for until now, I appreciate it :)
[22:17:16] *** Quits: ponsfrilus (~Thunderbi@adsl-178-39-226-100.adslplus.ch) (Ping timeout: 272 seconds)
[22:17:45] *** Joins: richard_h (~richard@2406:e001:8:a900:6e62:6dff:fe05:ae29)
[22:19:43] *** Quits: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[22:19:58] *** Quits: karabaja4 (~karabaja4@dh207-95-139.xnet.hr) (Remote host closed the connection)
[22:21:42] *** Quits: darutoko (~darutoko@37.21.204.207) (Quit: Leaving)
[22:22:58] *** Joins: jkavalik (~jkavalik@ip-78-102-141-139.net.upcbroadband.cz)
[22:23:48] *** Quits: otisolsen70 (~otisolsen@xd4ed80b5.cust.hiper.dk) (Ping timeout: 240 seconds)
[22:31:03] *** Quits: EvanCarroll (~ecarroll@68-78-105-35.lightspeed.hstntx.sbcglobal.net) (Ping timeout: 256 seconds)
[22:33:12] *** Quits: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net) (Read error: Connection reset by peer)
[22:34:38] *** Joins: rpkilby (~rpkilby@c-24-20-208-106.hsd1.or.comcast.net)
[22:38:29] *** Joins: mattt_ (~matt@2607:fea8:5740:aee0:75b3:a8f3:a252:b480)
[22:38:30] *** Quits: maret (~maret@nat-88-212-37-89.antik.sk) (Quit: maret)
[22:39:56] *** Parts: alioui (~medeva293@102.159.53.198) ()
[22:41:14] *** Joins: turlando (~turlando@93-42-250-112.ip89.fastwebnet.it)
[22:41:14] *** Quits: turlando (~turlando@93-42-250-112.ip89.fastwebnet.it) (Changing host)
[22:41:14] *** Joins: turlando (~turlando@user/turlando)
[22:43:04] *** Joins: r2ndur (~r2ndur@46-39-145-84.telset.ee)
[22:43:45] *** Quits: cliluw (~cliluw@47.147.77.43) (Remote host closed the connection)
[22:44:15] *** Joins: cliluw (~cliluw@47.147.77.43)
[22:45:21] *** Quits: sec^nd (~sec^nd@gateway/tor-sasl/secnd/x-45171752) (Remote host closed the connection)
[22:46:08] *** Joins: sec^nd (~sec^nd@gateway/tor-sasl/secnd/x-45171752)
[22:47:40] *** Quits: jkavalik (~jkavalik@ip-78-102-141-139.net.upcbroadband.cz) (Ping timeout: 272 seconds)
[22:50:10] *** Joins: jkavalik (~jkavalik@ip-78-102-141-139.net.upcbroadband.cz)
[22:50:39] *** Joins: CodeMouse92 (~CodeMouse@user/codemouse92)
[22:51:35] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[22:57:52] *** Joins: EvanCarroll (~ecarroll@198-0-91-126-static.hfc.comcastbusiness.net)
[22:57:56] *** Quits: mattt_ (~matt@2607:fea8:5740:aee0:75b3:a8f3:a252:b480) (Quit: My MacBook Air has gone to sleep. ZZZzzz…)
[22:59:14] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[23:02:21] *** Quits: Guest48 (~Guest48@a79-168-179-110.cpe.netcabo.pt) (Quit: Client closed)
[23:04:17] <ash_worksi> ne2k: so, I was not able to use `using constraint = CONSTRAINT_NAME` on pg12, I get an error unless I use `GET STACKED DIAGNOSTICS __constraint = CONSTRAINT_NAME; RAISE NOTICE USING constraint = __constraint_name;` is that what you meant or am I missing something?
[23:05:15] *** Joins: hoppity (~hoppity@66.222.130.38)
[23:05:15] *** Quits: hoppity (~hoppity@66.222.130.38) (Changing host)
[23:05:15] *** Joins: hoppity (~hoppity@user/hoppity)
[23:05:50] *** Joins: enoq (~enoq@2a05:1141:1f5:5600:b9c9:721a:599:bfe7)
[23:05:53] *** Joins: magla (~gelignite@55d4275f.access.ecotel.net)
[23:09:15] *** Quits: varioust (~varioust@gateway/tor-sasl/varioust) (Ping timeout: 240 seconds)
[23:10:05] *** Joins: mattt_ (~matt@2607:fea8:5740:aee0:75b3:a8f3:a252:b480)
[23:10:18] *** Quits: mattt_ (~matt@2607:fea8:5740:aee0:75b3:a8f3:a252:b480) (Client Quit)
[23:12:39] *** Joins: jdavfsxd (~rvgzuuqp@gateway/tor-sasl/rvgzuuqp)
[23:13:00] *** Joins: varioust (~varioust@gateway/tor-sasl/varioust)
[23:14:57] *** Quits: Junxter (~Junxter@222.95.164.193) (Read error: Connection reset by peer)
[23:16:12] *** Quits: veesh (~veesh@89.237.102.212) (Ping timeout: 240 seconds)
[23:20:55] *** Quits: bmomjian (~bruce@momjian.us) (Ping timeout: 256 seconds)
[23:22:16] *** Joins: bmomjian (~bruce@momjian.us)
[23:29:12] *** Joins: veesh (~veesh@89.237.103.109)
[23:29:41] *** Quits: furrymcgee (~devuan@cgn-89-1-211-93.nc.de) (Ping timeout: 256 seconds)
[23:31:11] *** Quits: Guest79 (~Guest79@7.sub-174-197-195.myvzw.com) (Quit: Client closed)
[23:38:51] *** Joins: kakashiAL (~kakashi@p5dd8ca04.dip0.t-ipconnect.de)
[23:39:49] *** Joins: john_johnk (~Thunderbi@232.58.140.77.rev.sfr.net)
[23:40:18] *** Quits: immibis_ (~hexchat@dynamic-089-204-139-245.89.204.139.pool.telefonica.de) (Read error: Connection reset by peer)
[23:40:19] *** Joins: Auron (Auron956@user/auron)
[23:40:23] *** Joins: immibis (~hexchat@dynamic-089-204-139-245.89.204.139.pool.telefonica.de)
[23:41:53] *** Quits: ba|ch (~user@p200300f3a700c4a06e44f7dadbf988c3.dip0.t-ipconnect.de) (Ping timeout: 250 seconds)
[23:42:15] *** Joins: jazzy (~jaziz@user/jaziz)
[23:45:42] *** Quits: fef (~thedawn@user/thedawn) (Quit: Leaving)
[23:48:20] *** Quits: uncleyear (~ian@pppoe.178-66-157-18.dynamic.avangarddsl.ru) (Remote host closed the connection)
[23:48:41] *** Quits: john_johnk (~Thunderbi@232.58.140.77.rev.sfr.net) (Ping timeout: 256 seconds)
[23:50:11] *** Quits: shiranaihito_ (~textual@123-192-192-149.dynamic.kbronet.com.tw) (Quit: My MacBook Air has gone to sleep. ZZZzzz…)
[23:51:15] *** Quits: acovrig (~acovrig@host-173-247-7-127.JENOLT2.epbfi.com) (Read error: Connection reset by peer)
[23:51:34] *** Joins: acovrig (~acovrig@host-173-247-7-127.JENOLT2.epbfi.com)
[23:51:58] *** Quits: immibis (~hexchat@dynamic-089-204-139-245.89.204.139.pool.telefonica.de) (Read error: Connection reset by peer)
[23:52:03] *** Joins: immibis_ (~hexchat@dynamic-089-204-139-245.89.204.139.pool.telefonica.de)
[23:52:35] *** Joins: uncleyear (~ian@pppoe.178-66-157-18.dynamic.avangarddsl.ru)
[23:53:37] <Net> Will GRANT ALL PRIVILEGES allow a user who's not a table's owner to alter the table structure (e.g. add unique constraints)?
[23:54:28] <ilmari> Net: what happened when you tried?
[23:56:28] <ilmari> «You must own the table to use ALTER TABLE.» - https://www.postgresql.org/docs/devel/sql-altertable.html
[23:56:29] <dostoyevsky2> Is there a way for psql to provide a password without using the commandline/environment?
[23:56:38] <ilmari> ??service
[23:56:39] <pg_docbot> https://www.postgresql.org/docs/current/static/libpq-pgservice.html :: https://www.postgresql.org/docs/current/static/libpq-envars.html
[23:56:44] *** Joins: ur5us (~ur5us@2406:e002:6804:8a01:fe29:d3cc:a0c3:c22)
[23:57:13] <ilmari> dostoyevsky2: you can use a pg_service file, see the first link there ^^
[23:58:07] <dostoyevsky2> ilmari: ah, great!
[23:58:09] *** Quits: gambl0re (~gambl0re@2607:fea8:a59f:c360::82dc) (Quit: WeeChat 3.3)
