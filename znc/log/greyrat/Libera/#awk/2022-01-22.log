[00:19:44] *** Joins: malina (~malina@user/malina)
[00:38:37] *** Quits: malina (~malina@user/malina) (Ping timeout: 240 seconds)
[00:47:20] *** Quits: otisolsen70 (~otisolsen@xd4ed80b5.cust.hiper.dk) (Quit: Leaving)
[00:51:23] *** Quits: user51 (~user51@176.228.148.215) (Remote host closed the connection)
[01:00:12] *** Parts: ham5 (iamfive@user/ham5) ()
[01:11:41] *** Joins: vlm (~vlm@user/vlm)
[01:12:31] *** Joins: seninha (~seninha@user/seninha)
[01:17:52] *** Joins: Torr (~Torr@user/torr)
[01:48:30] *** Joins: ifohancroft (~ifo@about/techtavern/ifohancroft)
[01:53:15] <ifohancroft> Hi! I have an unsorted file with about 118 million lines in the following format: integer,string Some lines have the same second column, but are not in consecutive order. Also, the number of lines with the same second column is unknown. I'm trying to do the following: 1. Find all lines with the same second column. 2. For all lines with the second column, I want the integers from the first column,
[01:53:17] <ifohancroft> except the smallest one for each group of duplicates.
[01:54:27] <ifohancroft> But honestly I have no idea how to do that. Even if I can get all duplicated lines with both first and second columns that is still enough as I can easily do the rest with a scripting language.
[01:55:47] <ifohancroft> Here is example content: https://gist.github.com/ifohancroft/77f2e989ea965d17caece2ba521f4ce9
[01:57:11] <ifohancroft> Here is an example of the expected result: https://gist.github.com/ifohancroft/a27a064a1b50ca9865f70de3b4274fc6
[01:58:52] <emanuele6> that doesn't make sense to me.
[01:58:59] <ifohancroft> Or this: https://gist.github.com/ifohancroft/eb05cf0927e676fc21d06efaa102c84e doesn't have to remove the second column and remove the lowest integer of each duplicate
[01:59:01] <emanuele6> 8 10 9??
[01:59:13] <ifohancroft> emanuele6: Sorry, let me try to explain the idea better
[01:59:48] <ifohancroft> you have asdf in the second column 3 times, once as 1,asdf then as 10,asdf and 9,asdf
[02:00:08] <ifohancroft> i want the first column of those lines, except for the smallest integer
[02:00:17] <ifohancroft> but for each duplicate group
[02:00:37] <ifohancroft> so you also have 8,bcd and 7,bcd in the example, in that case i need the '8'
[02:00:49] <ifohancroft> so you end up with 8, 10, 9 (i don't care about the sorting of the end result)
[02:01:33] <ifohancroft> basically i care for first column of each duplicated line (ideally without the smallest integer from each group of duplicates)
[02:02:03] <ifohancroft> or just getting all the full lines whose second columns are duplicated
[02:03:48] <emanuele6> you are not saying it, but you probably mean "or just getting all the full lines whose second columns are duplicated groupped by the second column in order of appearance" 
[02:04:17] <ifohancroft> yes, but i don't care about the grouping
[02:04:54] <emanuele6> then you only care about filtering out the non duplicate lines? and the order doesn't matter?
[02:05:01] <ifohancroft> yes
[02:05:14] <emanuele6> ok, that is fine.
[02:05:27] <emanuele6> otherwise it would have been insane
[02:05:57] <ifohancroft> yeah, i suck at explaining. I was trying to explaing that not all duplicate lines have the same second column and tried to use the word groups to explain that
[02:06:27] <ifohancroft> no idea why I didn't just say that i want to filter out the lines without duplicates in the second column
[02:08:09] <ifohancroft> I tried extracting the second column with awk and then grepping the multi column file but that didn't work because there are strings in the second column that are partial match inside of other strings in the second column so I was getting those as well
[02:10:35] <emanuele6> awk -F, 'f[$2]; { if (p[$2]) { p[$2] = 0; print f[$2] } else if (f[$2] == 0) { f[$2] = $1; p[$2] = 1 } }'
[02:10:55] <ifohancroft> Thank you! Let me try
[02:11:01] <ifohancroft> Would you mind explaining it
[02:11:12] <emanuele6> or even:   awk 'f[$2]; p[$2] { p[$2] = 0; print f[$2]; next } !f[$2] { f[$2] = $1; p[$2] = 1 }'
[02:11:43] <emanuele6> actually, hmm little inaccuracy
[02:11:50] <emanuele6> awk 'f[$2] != ""; p[$2] { p[$2] = 0; print f[$2]; next } !f[$2] { f[$2] = $1; p[$2] = 1 }'
[02:12:01] <emanuele6> otherwise if $2 is 0 it would not get printed
[02:12:32] <ifohancroft> that's fine, there is no case where $2 is just a 0. If it is, I have bigger problems than just the duplicates in the second column :D
[02:13:29] <emanuele6> ok, wait still wrong
[02:13:51] <emanuele6> awk -F, '$2 in f; p[$2] { p[$2] = 0; print f[$2]; next } !f[$2] { f[$2] = $0; p[$2] = 1 }'
[02:13:58] <emanuele6> now it should be good for real
[02:14:03] <emanuele6> basically
[02:14:42] <emanuele6> if   $2    is already a key to a value in the f associative array (has been encountered at least once), print the current line
[02:15:04] <ifohancroft> that sounds perfect! thank you!
[02:15:21] <emanuele6> that is only the    $2 in f;   part :)
[02:15:35] <ifohancroft> oh, sorry, go on :D
[02:15:56] <emanuele6> (i have noticed another inaccuracy let me fix it first (i only changed to $2 in f in one place)
[02:19:37] <emanuele6> ok, this should do
[02:19:39] <emanuele6> awk -F, '$2 in f { print; if (p[$2]) { p[$2] = 0; print f[$2] } next } { f[$2] = $0; p[$2] = 1 }'
[02:20:34] <emanuele6> if $2 is in f, do stuff (it won't be true the first time you encounter $2, so let's ignore it for now)
[02:21:16] <emanuele6> the second part sets is only executed when $2 is not in f because of the final next
[02:21:39] <emanuele6> if $2 is not in f already, set f[$2] to the current line and set p[$2] to 1
[02:22:39] <emanuele6> now, on a following record, if $2 has already been encountered (is in f), print the current line, and if p[$2] is set not 0, print f[$2] and p[$2] to 0 
[02:23:11] <emanuele6> the second time you encounter $2, p[$2] will be 1, so you will print f[$2] that is the first line that had $2
[02:23:53] <emanuele6> the third time and the following times you encounter $2, p[$2] will be 0 so you will only print the current line
[02:23:58] <emanuele6> that is it
[02:24:25] <ifohancroft> Thank you! I think I got it. I really need to sit on my ass and properly learn the full awk and the other cool stuff like grep, sed, etc
[02:24:39] <emanuele6> and   awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } { print } p[$2] { p[$2] = 0; print f[$2] }'
[02:24:43] <emanuele6> is probably more readable
[02:24:47] <emanuele6> just moved stuff around
[02:25:18] <emanuele6> could also be written as:  awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } 1; p[$2] { p[$2] = 0; print f[$2] }'
[02:25:50] <emanuele6> awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } p[$2] { p[$2] = 0; print f[$2] } 1'  that actually maybe is better since it keeps the original order
[02:26:19] <emanuele6> printf '%s\n' 1,2 3,4 1,5 6,2 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } p[$2] { p[$2] = 0; print f[$2] } 1'
[02:26:21] <emanuele6> oops
[02:26:24] <emanuele6> # printf '%s\n' 1,2 3,4 1,5 6,2 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } p[$2] { p[$2] = 0; print f[$2] } 1'
[02:26:25] <shbot> emanuele6: 1,2
[02:26:25] <shbot> emanuele6: 6,2
[02:26:30] <emanuele6> # printf '%s\n' 1,2 3,4 1,5 6,2 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } 1; p[$2] { p[$2] = 0; print f[$2] }'
[02:26:31] <shbot> emanuele6: 6,2
[02:26:31] <shbot> emanuele6: 1,2
[02:27:09] *** Quits: timm (~timm@user/timm) (Ping timeout: 268 seconds)
[02:27:14] <ifohancroft> # printf '%s\n' 1,2 3,4 1,5 6,2 7,2 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } 1; p[$2] { p[$2] = 0; print f[$2] }'
[02:27:15] <shbot> ifohancroft: 6,2
[02:27:15] <shbot> ifohancroft: 1,2
[02:27:15] <shbot> ifohancroft: 7,2
[02:27:18] <ifohancroft> awesome!
[02:27:41] <emanuele6> with 1 at the end it keeps the order in which they are encountered :)
[02:27:48] <emanuele6> # printf '%s\n' 1,2 3,4 1,5 6,2 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } p[$2] { p[$2] = 0; print f[$2] } 1'
[02:27:49] <shbot> emanuele6: 1,2
[02:27:50] <shbot> emanuele6: 6,2
[02:27:53] <emanuele6> oops
[02:27:59] <emanuele6> # printf '%s\n' 1,2 3,4 1,5 6,2 7,2 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } p[$2] { p[$2] = 0; print f[$2] } 1'
[02:28:00] <shbot> emanuele6: 1,2
[02:28:00] <shbot> emanuele6: 6,2
[02:28:00] <shbot> emanuele6: 7,2
[02:28:20] <emanuele6> but do note that it does not sort them and the first line is only printed when the second is encountered
[02:28:29] <ifohancroft> # printf '%s\n' 1,2 3,4 1,5 6,2 7,2 8,4 9,5 3,6 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } p[$2] { p[$2] = 0; print f[$2] } 1'
[02:28:35] <shbot> ifohancroft: 1,2
[02:28:35] <shbot> ifohancroft: 6,2
[02:28:35] <shbot> ifohancroft: etc... ( http://ix.io/3N4i )
[02:28:37] <emanuele6> # printf '%s\n' 1,2 3,4 1,5 3,5 6,2 7,2 | awk -F, '!($2 in f) { f[$2] = $0; p[$2] = 1; next } p[$2] { p[$2] = 0; print f[$2] } 1' | paste -s
[02:28:38] <shbot> emanuele6: 1,5     3,5     1,2     6,2     7,2
[02:28:51] <ifohancroft> yeah, I am fine that
[02:29:06] <emanuele6> yeah, obviously, you said order doesn't matter :)
[02:30:18] <ifohancroft> it finished. Seems good. Now I just need to go through them and remove the smallest int for each duplicate and remove the rest
[02:30:24] <ifohancroft> and print* the rest
[02:30:31] <emanuele6> ( you can use `|paste -s' at the end of the pipeline when the output will be more than 3 lines long, that prevents uploading the output to ix.io )
[02:30:48] *** Joins: BSaboia (~bsaboia@189.45.78.253)
[02:31:00] <ifohancroft> oh cool. thank you!
[02:31:48] *** Quits: BSaboia (~bsaboia@189.45.78.253) (Remote host closed the connection)
[02:33:04] *** Joins: smrtz (~smrtz@user/smrtz)
[02:34:16] <emanuele6> you could also reimplement `paste -s' in awk:  awk 'NR != 1 { printf "\t" } { printf "%s", $0 } END { print "" }'  :)
[02:42:41] *** Quits: mknod (~mknod@user/mknod) (Quit: mknod)
[02:44:17] *** Quits: smrtz (~smrtz@user/smrtz) (Quit: Client closed)
[02:50:27] <ifohancroft> that's fucking awesome! (sorry, not sure if cursing is allowed in here)
[02:53:21] *** Joins: sagax (~sagax_nb@user/sagax)
[02:56:35] *** Joins: kensanata (~user@user/kensanata)
[03:17:33] <e36freak> we give zero fucks
[03:19:40] <ifohancroft> awesome!
[03:20:03] <emanuele6> s/^/fucking /
[03:21:10] <e36freak> ifohancroft: how many unique strings are there, roughly? a shitload or is it mostly repeats?
[03:21:44] <e36freak> oh you want all *but* the smallest
[03:22:04] <e36freak> ideally you could solve the whole thing in one shot, but it would involve storing a lot in memory, or sorting the input
[03:22:19] <emanuele6> yeah, that is why i said it would be insane
[03:23:04] <e36freak> how big is your file? like, that's a lot of lines, but actual space-wise
[03:23:48] <e36freak> problem is you can't know which one's the smallest until you see all of them
[03:25:07] <e36freak> although you've already stored all the unique strings in memory with that solution, storing all of the numbers as well can't be that much more space
[03:25:34] <e36freak> ifohancroft: dos the output order have to be anything like the input order?
[03:27:36] <ifohancroft> e36freak: if I extract the second column and sort uniq it, i get about 50 million lines, so knowing probably the duplicated lines are probably only duplicated once (so two occurances) the unique lines (those with no duplicates) are probably about 18 million
[03:27:48] <nmz> does awk automatically convert a string to a number in memory? I imagine having "376248" takes more space than its integer equivalent
[03:28:00] <e36freak> nmz: look up "numeric string" in the man page
[03:28:09] <ifohancroft> the filesize is 3.2G
[03:28:23] <e36freak> but no, they're strings, sometimes they're just treated as numbers
[03:28:25] <ifohancroft> The order output order doesn't matter whatsoever
[03:28:43] <e36freak> 3.2G isn't crazy... you should have a good bit more ram than that
[03:29:15] <nmz> gnu parallel it
[03:29:20] <nmz> maybe
[03:29:41] <emanuele6> parallelisation doesn't reduce memory usage :)
[03:29:56] <ifohancroft> Basically that's an extraction of data from a database table and I need to find the duplicated entries (basically leaving the first occurance of each duplicate) and removivng them from multiple tables in the database, hence why needing the ids of the duplicates without the lowest int for each
[03:29:58] <e36freak> ifohancroft: and you only want duplicates in the output, and only all but the one with the smallest first field?
[03:29:59] <nmz> no but having a single thread on a single core for 3gb would take a bit of time
[03:30:12] <e36freak> ah, ok
[03:30:16] <e36freak> nmz: it's a storage issue
[03:30:18] <ifohancroft> e36freak: yeah, also there's 64G ram
[03:30:33] <e36freak> this is also a one time thing, i assume time isn't a major issue
[03:30:45] <e36freak> obviously you don't want it to take too long, but it doesn't need to be like <10ms
[03:30:47] *** Joins: BSaboia (~bsaboia@189.45.78.253)
[03:30:55] <e36freak> if it did, you'd be rolling a custom C program or something
[03:31:33] <ifohancroft> Nah, definitely doesn't have to be that fast and it's not a regular thing. Like if I'm unlucky, I may have to do that again like next year or in a couple of months (in case I find a bug in another script of mine again that allowed duplicate entries to creep in)
[03:32:06] <ifohancroft> I've had duplicate entries once or twice before but they were like 10 so I just removed them by hand
[03:33:17] <e36freak> ifohancroft: give me a sec, i'm gonna type it out in a file so it's easier to read
[03:33:51] <emanuele6> so you want to remove duplicates and keep the one with the lowest values, not the other way around?
[03:34:23] <emanuele6> (only keep duplicates, but remove the duplicate with the lowest value)
[03:34:35] <ifohancroft> e36freak: sure
[03:34:55] <ifohancroft> emanuele6: yeah, so I can basically make a comma separated string of all the IDs I need to remove from the database
[03:35:24] <emanuele6> wait so this is the output of a database?
[03:35:33] <emanuele6> like a SQL database?
[03:35:36] <ifohancroft> emanuele6: yeah
[03:35:49] <emanuele6> why don't you just use an SQL query to remove the duplicates?
[03:36:37] <ifohancroft> I don't think my SQL is good enough to come up with a solution with just plain SQL, also I have to remove entries from multiple tables using the duplicated IDs from the main one
[03:36:52] <ifohancroft> Also, I am not sure how fast it would be with a table with like 118 million records inside it
[03:37:28] <ifohancroft> But I am accepting suggestions on reading material so I can improve my SQL knowledge so I can do stuff like that in plain SQL
[03:37:31] *** Quits: BSaboia (~bsaboia@189.45.78.253) (Quit: This computer has gone to sleep)
[03:40:03] <nmz> #sql
[03:40:34] *** Joins: BSaboia (~bsaboia@189.45.78.253)
[03:40:43] <e36freak> http://ix.io/3N4B
[03:40:45] <nmz> should be faster too
[03:40:54] <emanuele6> it would be extremely fast in comparison to parsing a text file. that is why databases exist.
[03:40:55] <e36freak> but yeah, #sql would be a better option
[03:41:10] <e36freak> not that it wasn't an interesting problem
[03:41:40] <ifohancroft> Thank you! Yeah, I may have to start looking for plain SQL solutions first when I have similar problems
[03:42:04] <emanuele6> it depends on the sql dialect obviously, but it will be something as simple as something like this  https://0x0.st/oojn.txt
[03:42:25] <emanuele6> ( i copied it from here, the website has ads: https://karthiktechblog.com/sql/how-to-delete-all-duplicate-rows-but-keeping-one-in-sql )
[03:42:30] *** Quits: BSaboia (~bsaboia@189.45.78.253) (Client Quit)
[03:43:59] <ifohancroft> thank you! so to use those ids from that one table, but to delete the records from multiple tables where the id column has a different name in the different tables can I still use a single query or should I just delete from the other tables first then finally delete from the table that has the IDs?
[03:44:17] <ifohancroft> the DB is MariaDB btw
[03:45:01] <emanuele6> mariadb is mysql so probably compatible with that sql.
[03:45:20] <emanuele6> but try asking in #sql as nmz said, i don't know much sql since i don't use it much.
[03:47:06] <ifohancroft> thank you both so much!
[03:50:50] <emanuele6> e36freak: i was afraid of doing something like that since the file was that long, but i guess 118 million lines is not that close to INT_MAX afterall
[03:51:25] <e36freak> not sure how INT_MAX matters, though i'm also not sure how the hashmap is implemented
[03:51:50] <emanuele6> maybe DOUBLE_MAX, i don't really know
[03:52:18] <e36freak> that would make even less sense
[03:52:25] <emanuele6> anyway i was afraid of overflows for  ++strings[$2] since the file is huge
[03:53:06] <emanuele6> you think ++var performs math on the string representation of the number?
[03:53:35] <ifohancroft> oh yeah, speaking of overflows, i probably should have mentioned that the IDs start from 2 billion
[03:54:21] *** Joins: timm (~timm@user/timm)
[03:58:30] <emanuele6> e36freak: yeah, not DBL_MAX but the maximum integer that can be stored in a double without loss of information
[03:58:41] <emanuele6> # gawk 'BEGIN{ print 9007199254740992 + 1 }'
[03:58:42] <shbot> emanuele6: 9007199254740992
[03:58:49] <emanuele6> but that is a very huge number, so nevermind
[03:59:12] <emanuele6> and would not cause an underflow, it would just not increment the variable
[03:59:58] <emanuele6> s/under/over/
[04:01:23] <nmz> # gawk -M 'BEGIN{ print 9007199254740992 + 1 }'
[04:01:24] <shbot> nmz: 9007199254740993
[04:01:47] <emanuele6> oh, gawk can do bignum math, cool
[04:03:29] <e36freak> at the very least it's a 64 bit int
[04:04:31] <e36freak> for ++strings[$2] to overflow you'd need 2^63 duplicates of a single string
[04:04:53] <e36freak> shouldn't need bignum
[04:05:03] <e36freak> would just slow it way down
[04:06:08] <emanuele6> yeah, i didn't realise it was that far off from the max
[04:06:46] <emanuele6> <emanuele6 > i was afraid of doing something like that since the file was that long, but i guess 118 million lines is not that close to INT_MAX afterall
[04:07:43] <e36freak> it's really not that many
[04:08:23] <emanuele6> yeah
[04:08:35] <emanuele6> my bad
[04:08:54] *** Quits: kensanata (~user@user/kensanata) (Ping timeout: 268 seconds)
[04:14:32] *** Quits: timm (~timm@user/timm) (Ping timeout: 240 seconds)
[04:20:34] *** Quits: wurstwasser_2 (~frank.hol@aftr-62-216-207-210.dynamic.mnet-online.de) (Ping timeout: 250 seconds)
[04:52:07] *** Quits: seninha (~seninha@user/seninha) (Quit: Leaving)
[04:52:20] *** Quits: lavaball (felix@31.204.155.215) (Remote host closed the connection)
[05:30:18] *** Quits: vlm (~vlm@user/vlm) (Quit: "")
[06:48:39] *** Joins: Everything (~Everythin@37.115.210.35)
[07:33:51] *** Joins: malina (~malina@user/malina)
[07:42:57] *** Quits: Earnestly (~earnest@user/earnestly) (Ping timeout: 240 seconds)
[08:20:49] *** Quits: Thanatermesis (~Thanaterm@191.92.148.122) (Ping timeout: 256 seconds)
[08:44:57] *** Joins: bluedust (~bluedust@103.160.194.145)
[08:52:29] *** Joins: Earnestly (~earnest@user/earnestly)
[09:00:17] *** Joins: bluedust_ (~bluedust@81.92.205.22)
[09:02:57] *** Quits: bluedust (~bluedust@103.160.194.145) (Ping timeout: 240 seconds)
[09:23:21] *** Quits: larryv (~larryv@zsh/patchmanager/larryv) (Quit: larryv)
[09:33:26] *** Quits: joshbaptiste (~josh@2001:470:1f07:4d5:24::c2b5) (Ping timeout: 250 seconds)
[09:35:08] *** Quits: tirnanog (~tirnanog@user/tirnanog) (Quit: = "")
[10:16:05] *** Quits: Everything (~Everythin@37.115.210.35) (Quit: leaving)
[10:25:51] *** Joins: user51 (~user51@176.228.148.215)
[11:03:34] *** Quits: Earnestly (~earnest@user/earnestly) (Ping timeout: 250 seconds)
[11:13:19] *** Quits: roarde (~roarde@user/roarde) (Quit: Leaving)
[11:29:48] *** Joins: otisolsen70 (~otisolsen@xd4ed80b5.cust.hiper.dk)
[11:51:31] *** Quits: bluedust_ (~bluedust@81.92.205.22) (Remote host closed the connection)
[11:53:05] *** Joins: lavaball (felix@31.204.155.215)
[11:58:19] *** Joins: elastic_1 (~elastic_d@2a01:118f:822:9c00:f583:aa51:9ad4:d4fb)
[11:58:32] *** Quits: elastic_1 (~elastic_d@2a01:118f:822:9c00:f583:aa51:9ad4:d4fb) (Client Quit)
[11:58:54] *** Joins: elastic_1 (~elastic_d@2a01:118f:822:9c00:f583:aa51:9ad4:d4fb)
[12:00:02] *** Quits: elastic_dog (~elastic_d@2a01:118f:822:9c00:f583:aa51:9ad4:d4fb) (Ping timeout: 268 seconds)
[12:22:30] *** Joins: BSaboia (~bsaboia@189.45.78.253)
[12:25:08] *** Quits: BSaboia (~bsaboia@189.45.78.253) (Client Quit)
[12:36:26] *** Joins: bluedust (~bluedust@89.238.130.70)
[13:14:24] *** Joins: timm (~timm@user/timm)
[14:29:24] *** Quits: elastic_1 (~elastic_d@2a01:118f:822:9c00:f583:aa51:9ad4:d4fb) (Ping timeout: 250 seconds)
[14:33:01] *** Joins: Earnestly (~earnest@user/earnestly)
[14:33:31] *** Quits: bluedust (~bluedust@89.238.130.70) (Remote host closed the connection)
[14:34:35] *** Joins: elastic_1 (~elastic_d@2a01:118f:822:9c00:f583:aa51:9ad4:d4fb)
[15:59:15] *** Joins: seninha (~seninha@user/seninha)
[16:33:59] *** Quits: malina (~malina@user/malina) (Remote host closed the connection)
[17:32:45] *** Joins: bluedust (~bluedust@103.160.194.145)
[18:24:04] *** Quits: Torr (~Torr@user/torr) (Remote host closed the connection)
[18:32:25] *** Joins: Torr (~Torr@user/torr)
[19:06:36] *** Joins: Thanatermesis (~Thanaterm@191.92.148.122)
[19:20:37] *** Quits: timm (~timm@user/timm) (Ping timeout: 240 seconds)
[20:11:51] *** Quits: Earnestly (~earnest@user/earnestly) (Quit: WeeChat 3.4)
[20:13:43] *** Joins: Earnestly (~earnest@user/earnestly)
[20:24:30] *** Quits: lavaball (felix@31.204.155.215) (Remote host closed the connection)
[20:40:07] *** Quits: lopid (~lopid@user/lopid) (Quit: WeeChat 3.3)
[20:42:38] *** Joins: lopid (~lopid@user/lopid)
[20:55:59] *** Joins: lgc (~lgc@user/lgc)
[20:56:04] *** Quits: lgc (~lgc@user/lgc) (Client Quit)
[21:16:17] *** Quits: vinipsmaker (~vinipsmak@177.54.145.213) (Ping timeout: 240 seconds)
[21:34:41] *** Joins: lavaball (felix@31.204.155.215)
[21:45:20] *** Quits: ryu` (~ryusaku@user/ryu/x-3206151) (Ping timeout: 250 seconds)
[21:46:06] *** Joins: ryu` (~ryusaku@user/ryu/x-3206151)
[21:52:27] *** Joins: bluedust_ (~bluedust@185.248.85.22)
[21:55:57] *** Quits: bluedust (~bluedust@103.160.194.145) (Ping timeout: 240 seconds)
[21:56:32] *** Joins: larryv (~larryv@zsh/patchmanager/larryv)
[22:00:04] *** Joins: bluedust (~bluedust@103.160.194.145)
[22:03:15] *** Quits: ryu` (~ryusaku@user/ryu/x-3206151) (Ping timeout: 256 seconds)
[22:03:49] *** Quits: bluedust_ (~bluedust@185.248.85.22) (Ping timeout: 256 seconds)
[22:07:50] *** Joins: bluedust_ (~bluedust@103.160.194.145)
[22:08:02] *** Quits: bluedust (~bluedust@103.160.194.145) (Read error: Connection reset by peer)
[22:11:14] *** Quits: seninha (~seninha@user/seninha) (Quit: Leaving)
[22:11:51] *** Joins: seninha (~seninha@user/seninha)
[22:38:15] *** Joins: smrtz (~smrtz@user/smrtz)
[22:40:14] <smrtz> Hey everyone! I've got a text file where I'd like to replace every other line break with a comma. Here's an example of what I've got: https://bpa.st/Z7UA and what I'd like: https://bpa.st/VO4Q
[22:40:15] <smrtz> I think the best method is to set $0 to a variable, {next}, then print that variable "," $0, but I can't figure out setting the current line to a variable.  Any tips?
[22:44:43] *** Joins: ryu` (~ryusaku@user/ryu/x-3206151)
[22:45:28] <Patsie> { s = $0; if (getline) print s "," $0; else print s }
[22:46:55] <smrtz> Perfect, thanks!
[22:47:09] <emanuele6> in sed it would have been even simpler
[22:50:57] <smrtz> emanuele6 Oh?
[22:51:09] <emanuele6> N; y/\n/,/
[22:52:25] <smrtz> Oh sweet, thanks!
[22:56:26] <emanuele6> ( Patsie's code is a little inaccurate, it should be   getline>0   or  getline==1   getline returns -1 on error, 0 on EOF and 1 on success )
[22:57:07] *** Joins: roarde (~roarde@user/roarde)
[23:24:24] *** Joins: vinipsmaker (~vinipsmak@177.54.145.213)
