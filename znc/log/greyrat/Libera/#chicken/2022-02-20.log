[00:42:46] <curium> hello
[00:44:56] <curium> a question: given that I want to compute the sum of lengths of u8-vectors, is there any gain in writing a recursive function, instead of (apply + (map u8vector-length vectors)) ?
[00:45:27] <curium> (I saw the recursive version in srfi-160)
[01:13:12] *** Joins: sarna (~sarna@public-gprs390386.centertel.pl)
[01:13:50] *** Quits: sarna (~sarna@public-gprs390386.centertel.pl) (Remote host closed the connection)
[01:14:18] *** Joins: sarna (~sarna@public-gprs390386.centertel.pl)
[01:14:48] *** Quits: sarna (~sarna@public-gprs390386.centertel.pl) (Remote host closed the connection)
[01:16:21] *** Joins: sarna (~sarna@public-gprs390386.centertel.pl)
[01:16:48] *** Quits: sarna (~sarna@public-gprs390386.centertel.pl) (Remote host closed the connection)
[01:19:03] <jcroisant> curium: You might notice better performance with the recursive version, especially if you have a very large number of vectors, because it doesn't need to allocate a temporary list like map does. Also there is a limit to the number of arguments that can be passed to compiled procedures, so I think apply might have an error if there are more than 1000 vectors in the list.
[01:23:46] <curium> jcroisant: I see, thanks
[01:30:46] <jcroisant> That's all hypothetical though. The compiler might do some optimizations that change the balance, so if performance is important I'd recommend doing benchmarks to see which is really faster in practice
[01:36:48] <curium> In my situation performance is not so important, I just need to append 4 vectors once, and was copying code from srfi-160 to use with srfi-4 to reduce dependencies
[01:37:26] <curium> playing a bit trying to implement sha1 in chicken scheme :)
[01:37:43] *** Joins: ublx (~ublx@user/ublx)
[01:38:39] <jcroisant> Ah, cool :) In that case the apply/map version is totally fine, don't worry about it
[01:40:47] *** Joins: crumpo (~crumpo@user/crumpo)
[02:06:17] <johnjaye> i was just reading today about how people overoptimize at every level even when it's not logical to do so
[02:06:23] <johnjaye> i guess the optimization is real
[02:23:08] <curium> was there any particular example?
[02:28:29] *** Quits: rgherdt (~rgherdt@port-92-193-217-5.dynamic.as20676.net) (Remote host closed the connection)
[03:10:28] *** Quits: klovett (~klovett@107.127.17.177) (Quit: ZZZzzz…)
[03:37:07] *** Quits: ASau (~user@lmpc.drb.insel.de) (Ping timeout: 250 seconds)
[03:43:19] *** Quits: crumpo (~crumpo@user/crumpo) (Quit: Buh bye)
[03:50:37] *** Quits: skapata (~Skapata@user/skapata) (Ping timeout: 240 seconds)
[04:02:19] *** Joins: skapata (~Skapata@user/skapata)
[04:47:01] *** Joins: s-liao (~s-liao@101.86.96.21)
[05:00:09] *** Quits: johnjaye (~pi@173.209.64.74) (Ping timeout: 256 seconds)
[05:07:16] *** Joins: klovett (~klovett@107.127.17.177)
[05:16:35] *** Quits: s-liao (~s-liao@101.86.96.21) (Ping timeout: 256 seconds)
[05:25:11] *** Joins: s-liao (~s-liao@101.86.96.21)
[07:06:31] *** Quits: s-liao (~s-liao@101.86.96.21) (Ping timeout: 256 seconds)
[07:14:10] *** Quits: sts-q (~sts-q@212.53.219.164) (Ping timeout: 256 seconds)
[07:14:36] *** Joins: sts-q (~sts-q@212.53.219.169)
[07:18:24] *** Joins: s-liao (~s-liao@101.86.96.21)
[07:20:07] <jcowan> Programmers have been over-optimizing since before the dawn of time.
[07:20:25] <jcowan> The trick is to figure out when you can safely not optimize
[07:37:17] *** Quits: s-liao (~s-liao@101.86.96.21) (Quit: Client closed)
[07:58:46] *** Quits: skapata (~Skapata@user/skapata) (Remote host closed the connection)
[08:43:01] *** Quits: ublx (~ublx@user/ublx) (Quit: ublx)
[08:53:50] *** Quits: klovett (~klovett@107.127.17.177) (Quit: ZZZzzz…)
[09:43:28] *** Joins: s-liao (~s-liao@101.86.96.21)
[09:43:35] *** Quits: s-liao (~s-liao@101.86.96.21) (Client Quit)
[10:37:42] *** Quits: mdhughes (~mdhughes@user/mdhughes) (Remote host closed the connection)
[10:38:17] *** Joins: mdhughes (mdhughes@user/mdhughes)
[11:19:58] <Irvise> Adding to the optimisation comment. Compilers nowdays are also much smarter and cable that before. Some compilers can autovectorise and even multithread code automaticly...
[11:42:12] *** Joins: johnjaye (~pi@173.209.64.74)
[12:05:23] *** Joins: s-liao (~s-liao@101.86.96.21)
[12:11:47] *** Joins: rgherdt (~rgherdt@port-92-193-217-5.dynamic.as20676.net)
[13:13:16] *** Quits: s-liao (~s-liao@101.86.96.21) (Quit: Client closed)
[13:27:38] *** Joins: s-liao (~s-liao@101.86.96.21)
[13:28:00] <Bunny351> Irvise: I doubt they can do this for all but very specific or idiomatic cases.
[13:31:01] <Irvise> Bunny351: that is partially true. But in case of Fortran, gfortran and Intel's ICC and HPC Fortran compilers can already autoparallelise long loops. GCC enabled some basic SIMD optimisations @ -O2 (LLVM already did that for some time); Ada 2022 is adding the parallel keyword to tell the compiler to potentially parallelise the code...
[13:32:48] <Irvise> And the libraries (internal to the compiler or general use libraries) are already getting heavily optimised. Most programmers just don't have to screw their program design and use the correct algorithms given by those libraries. See Numpy in Python as an obvious example.
[13:34:37] <Bunny351> sure, that's all math-intensive, like sprfeading out calculations over sequences. that's mightily cool, yet the general case, non-trivial code, that's a different story
[13:48:43] *** Joins: crumpo (~crumpo@user/crumpo)
[13:55:08] <Irvise> I think why a lot of languages ecosystems are moving to a "let the libraries handle everything" programming style because of that. The C++ stl is being heavily increased in size and speed/efficiency. Same thing in Python, Rust, JS...
[13:55:34] <Irvise> Basically "I do not optimise because the libraries are already taking care of the expensive part"
[13:56:16] <Bunny351> I think that is in the end illusionary, to be honest...
[13:57:09] *** Joins: mdhughes_ (~mdhughes@user/mdhughes)
[14:01:20] *** Quits: mdhughes (mdhughes@user/mdhughes) (Ping timeout: 272 seconds)
[14:08:26] <Bunny351> this would be so cool: http://www.open-std.org/JTC1/SC22/WG14/www/docs/n2920.pdf
[14:08:47] <Bunny351> it even mentions CHICKEN
[14:23:06] <s-liao> C++ stl are more about the abstract. Some of them called it libs?
[14:25:32] *** Joins: s-liao92 (~s-liao@101.86.96.21)
[14:27:02] *** Quits: s-liao92 (~s-liao@101.86.96.21) (Client Quit)
[14:28:31] *** Quits: s-liao (~s-liao@101.86.96.21) (Ping timeout: 256 seconds)
[14:31:07] *** Joins: s-liao (~s-liao@101.86.96.21)
[14:31:58] <s-liao> I mean that from the abstract perspective to see the C++ STL
[14:49:24] *** Quits: s-liao (~s-liao@101.86.96.21) (Quit: Client closed)
[15:55:54] *** mdhughes_ is now known as mdhughes
[16:24:52] <vandusen> SPAM! SPAM! SPAM! by freeee
[16:26:01] *** Joins: ublx (~ublx@user/ublx)
[16:26:59] <vandusen> SPAM! SPAM! SPAM! by freeee
[16:44:14] *** Quits: fantazo (~fantazo@192-164-154-92.hdsl.highway.telekom.at) (Ping timeout: 256 seconds)
[16:46:29] <sjamaan> siiky: That file stuff was a quick hack I did for a customer project, and never used it again. I'm not surprised it doesn't work as advertised ;)
[17:00:40] *** Joins: s-liao (~s-liao@101.86.96.21)
[17:13:24] *** Joins: skapata (~Skapata@2804:14c:87b0:9d2e:409c:deea:cba7:6b6d)
[17:13:24] *** Quits: skapata (~Skapata@2804:14c:87b0:9d2e:409c:deea:cba7:6b6d) (Changing host)
[17:13:24] *** Joins: skapata (~Skapata@user/skapata)
[17:58:52] *** Quits: s-liao (~s-liao@101.86.96.21) (Quit: Client closed)
[18:25:54] *** Quits: skapata (~Skapata@user/skapata) (Ping timeout: 250 seconds)
[18:28:50] *** Joins: skapata (~Skapata@user/skapata)
[20:00:56] <vandusen> SPAM! SPAM! SPAM! by NGRE
[20:02:08] *** Joins: klovett (~klovett@107.127.17.177)
[21:28:59] *** Quits: paddymahoney (~paddymaho@cpe9050ca207f83-cm9050ca207f80.cpe.net.cable.rogers.com) (Ping timeout: 256 seconds)
[21:35:09] *** Joins: fantazo (~fantazo@192-164-154-92.hdsl.highway.telekom.at)
[22:29:44] *** Joins: paddymahoney (~paddymaho@cpe9050ca207f83-cm9050ca207f80.cpe.net.cable.rogers.com)
[23:50:31] *** Joins: cjb (~cjb@user/cjb)
