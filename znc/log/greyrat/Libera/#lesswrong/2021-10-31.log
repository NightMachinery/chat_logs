[00:01:04] <feepbot> <gwern> 'https://www.gwern.net/docs/ai/music/2019-12-04-gpt2-abc-alldata.tar.xz i only used 150K piano pieces because of google colab limitations actually this has been done by https://www.gwern.net/GPT-2-music i just try to train it myself to see if it work and yeah it worked' https://github.com/annasajkh/PianoGPT lol
[00:03:59] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[00:04:55] *** Quits: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de) (Ping timeout: 268 seconds)
[00:12:27] <feepbot> <gwern> https://twitter.com/jd_pressman/status/1454547292367720451/photo/1
[00:12:31] <|dbotdan> John David Pressman (@jd_pressman, 2021-10-30 20:34): ‚ÄòEpoch 24 of the danbooru 2020 500m diffusion training is learning to draw the arms.‚Äô Images: https://nitter.vxempire.xyz/pic/media%2FFC-W9R4VkAAZz7T.jpg%3Fname%3Dorig (description: a collage of pictures of a girl; confidence: 0.40)
[00:17:31] <feepbot> <gwern> 'not much around that's as satisfying sounding as starting up a big full-size ATX case crammed with raptors. starting up a big lathe or mill spindle, or maybe a large phase converter with a heavy lever switch come close, but they don't scratch that "i'm in a super computer lab" itch as much as the fantastic whir of a raptor RAID. utilitarian feature : you could hear a bad
[00:17:31] <feepbot> sector from anywhere in a large room.' https://news.ycombinator.com/item?id=29049423
[00:17:32] <feepbot> An ode to the 10,000 RPM Western Digital (Veloci)Raptor | Hacker News
[00:21:05] <ggreer> the motors in my car spin up to 18,000rpm. if you drop the rear seats they sound pretty fun
[00:23:29] <ggreer> https://www.youtube.com/watch?v=_Rg75JbVOpg
[00:23:30] <Robomot> Tesla Motor Test Bench - QA/Sale Prep (Custom control package) - YouTube (https://057tech.comMy new test bench for Tesla drive units.  Still needs some tweaking and cleanup, especially with the cooling side (just a temporary closed...)
[00:26:22] *** Joins: MerchantOfVenice (~patrick@user/merchantofvenice)
[00:28:31] <feepbot> <gwern> https://twitter.com/ATNPassion/status/1454281747097526273 RiversHaveWings: how - how does this work? it's somehow doing pseudo-3d with clip?
[00:28:32] <|dbotdan> All Technique No Passion (@ATNPassion, 2021-10-30 02:59): ‚ÄòFeline Tornado‚Äô Watch video: https://nitter.domain.glass/ATNPassion/status/1454281747097526273
[00:29:04] <ggreer> what the hell. that's really impressive
[00:29:05] <RiversHaveWings> gwern: i think it combines CLIP gradients with a separate thing that infers depth maps
[00:29:10] <RiversHaveWings> is it pytti
[00:29:47] *** Quits: Obormot\Gaia (~obormot@user/obormot) (Ping timeout: 264 seconds)
[00:29:49] <gwern> what does the depth map inferring do? do you penalize inconsistent depths?
[00:31:27] <RiversHaveWings> i don't actually know.
[00:31:30] <RiversHaveWings> I didn't write it
[00:32:05] <RiversHaveWings> Maybe they use it to 3D inpaint the image from a slightly different perspective and then CLIP optimize a bit more?
[00:43:36] *** Joins: Obormot\Gaia (~obormot@user/obormot)
[00:44:40] <shawwwn> a death map would be cool too
[00:44:58] <shawwwn> You could penalize based on danger
[00:45:43] <shawwwn> quanticle: https://twitter.com/atnpassion/status/1454281747097526273?s=21
[00:45:44] <|dbotdan> All Technique No Passion (@ATNPassion, 2021-10-30 02:59): ‚ÄòFeline Tornado‚Äô Watch video: https://birdsite.xanny.family/atnpassion/status/1454281747097526273
[00:47:48] <Obormot\Arcturus> gwern: Wow what the heck is this site
[00:50:00] <shawwwn> Antiwork has been my favorite sub for awhile https://www.reddit.com/r/antiwork/comments/qiijcc/posted_by_my_former_boss_who_wrote_this_on_her/hijya6u/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&context=3
[00:50:12] <feepbot> Posted by my former boss who wrote this on her Facebook page for her clients to see (the epitome of professionalism) : antiwork (34,025 votes and 4,991 comments so far on Reddit)
[00:50:41] <shawwwn> Someone watched one punch man and was like ‚Äúthis, but unironically‚Äù
[00:52:24] *** Quits: son0p (~ff@181.136.122.143) (Remote host closed the connection)
[00:57:51] <shawwwn> https://twitter.com/polyparadigm/status/1454296635383582724?s=21 the discovery of philosophy, 98 BC, colorized
[00:58:02] <|dbotdan> pumpkin-headed Joel üéÉ (@polyparadigm, 2021-10-30 03:58): ‚ÄòHmm‚Ä¶or maybe that, given a universe containing n things, the maxim number of bad things is n-2.‚Äô
[00:59:22] <shawwwn> Lol, Aristotle died in 322 BC. I‚Äôll never forget it again. https://www.google.com/amp/s/amp.knowyourmeme.com/memes/322
[00:59:23] <mjr> hmh, I'm not convinved
[00:59:35] <feepbot> 322 | Know Your Meme
[00:59:54] <[itchyjunk]> https://en.wikipedia.org/wiki/Prion
[00:59:55] <Robomot> Prion - Wikipedia (Prions are misfolded proteins with the ability to transmit their misfolded shape onto normal variants of the same protein. They characterize several fatal and transmissible neurodegenerative diseases in humans and many other animals.[ ‚Ä¶)
[01:00:04] <[itchyjunk]> How does one protein get another one to fold differently ? 
[01:00:11] <[itchyjunk]> i don't understand what it means
[01:00:22] <[itchyjunk]> Prions are misfolded proteins with the ability to transmit their misfolded shape onto normal variants of the same protein
[01:00:30] <[itchyjunk]> the "transmit their misfolded shape" part
[01:03:16] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[01:07:58] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 260 seconds)
[01:14:31] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[01:19:35] *** Quits: two2thehead (~user@124.195.209.131) (Ping timeout: 264 seconds)
[01:20:06] <pompolic> https://link.springer.com/chapter/10.1007%2F128_2011_165 that graphical abstract won me over
[01:20:07] <Robomot> Prion Protein and Its Conformational Conversion: A Structural Perspective | SpringerLink (Abstract The key molecular event in the pathogenesis of prion diseases is the conformational conversion of a cellular prion protein, PrPC, into a misfolded form, PrPSc. In contrast to PrPCthat is...)
[01:20:12] <pompolic> it seems it's not well known though
[01:24:16] *** Joins: two2thehead (~user@124.195.209.131)
[01:32:22] <[itchyjunk]> hmm
[01:34:17] <[itchyjunk]> heh that graphical abstract
[01:36:45] <two2thehead> "The biblical foundation for the best-selling Left Behind series‚Ä¶. In the twinkling of an eye, millions of people across the world vanish, resulting in highway catastrophes, plane crashes, utility breakdowns, and more. Chaos reigns. With the stage set, a dictator emerges who persecutes Christians horribly. But tribulation is about to give way to incredible joy - for the return of the King of Kings is at hand."
[01:37:06] <two2thehead> I'm always surprised to hear Jesus being referred to as 'King'
[01:37:22] * quanticle remembers reading a few of the Left Behind books, out of curiosity
[01:37:31] <two2thehead> Any good?
[01:37:49] <quanticle> The really funny thing is that, because the authors are hardcore Christians, they're constantly referring to bad things happening, but they can't actually bring themselves to show any of it
[01:38:01] <two2thehead> hm
[01:38:19] <quanticle> So it's like, the protagonists are always hearing about chaos and the forces of the antichrist rampaging in the distance, but their day-to-day lives are hardly changed
[01:38:34] <quanticle> Kind of hilarious in that sense
[01:43:24] <quanticle> Obormot\Arcturus: returning to this picture, my caption is, "BABA YAGA IS OPPOSED"
[01:43:27] <quanticle> https://i.imgur.com/lvgJ7qu.jpeg
[01:44:14] <gwern> https://www.reddit.com/r/marvelstudios/comments/77mooo/cate_blanchett_as_hela_with_director_taika/
[01:44:17] <Robomot> Cate Blanchett as Hela with director Taika Waititi on the set of Thor: Ragnarok. : marvelstudios (6.9k votes, 385 comments. 2.0m members in the marvelstudios community. A subreddit dedicated to Marvel Studios and the Marvel Cinematic Universe!)
[01:45:00] <gwern> https://knowyourmeme.com/memes/taika-waititi-and-cate-blanchett damn, that poster didn't even come up with the 'bread winning' caption
[01:45:01] <Robomot> Taika Waititi and Cate Blanchett | Know Your Meme (Taika Waititi and Cate Blanchett is an exploitable object labeling image macro series derived from a behind the scenes photo during the making of Thor: Ragnarok. It is used to the show when a less powerful individual is trying to impress or convince a more powerful individual, usually a dominant female or mother to comic effect.)
[01:45:15] <gwern> quanticle: the spice must flow
[01:45:38] <gwern> (incidentally, I found that by using google images with the query 'tall woman in green and black standing next to Taika Waititi' go go multimodal transformers)
[01:46:22] <quanticle> gwern: That would only make sense if Chonkers' eyes were blue
[01:46:32] <gwern> 'yellow' looks like spice, close enough
[01:47:15] *** Joins: adiabatty (~adiabatic@user/adiabatic)
[01:48:10] <quanticle> :|
[01:51:28] *** Quits: spxtr (~spxtr@user/spxtr) (Quit: leaving)
[01:51:59] *** Quits: adiabatty (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[02:13:30] *** Quits: MerchantOfVenice (~patrick@user/merchantofvenice) (Quit: Konversation terminated!)
[02:23:38] <feepbot> <gwern> https://www.lesswrong.com/posts/efc4JjrvZXYZbYwmQ/which-of-our-online-writings-was-used-to-train-gpt-3?commentId=2ZjFztH6tiLf7HbPB some thoughts on how to write for gpt-3
[02:23:38] <Robomot> GreaterWrong version of feepbot‚Äôs link: https://www.greaterwrong.com/posts/efc4JjrvZXYZbYwmQ/which-of-our-online-writings-was-used-to-train-gpt-3?commentId=2ZjFztH6tiLf7HbPB
[02:23:39] <feepbot> Which of our online writings was used to train GPT-3? - LessWrong (Comment by gwern - I can confirm that Twitter is generally not in Common Crawl, and that Gwern.net)
[02:24:36] *** Joins: son0p (~ff@181.136.122.143)
[02:29:25] <feepbot> <gwern> 'We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end
[02:29:25] <feepbot> training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.' https://arxiv.org/abs/2110.15343
[02:29:26] <feepbot> [2110.15343] Scatterbrain: Unifying Sparse and Low-rank Attention Approximation (Recent advances in efficient Transformers have exploited either the sparsity)
[02:33:50] <rmmh> apparently you can buy spicy bird seed. Capsaicin isn't spicy to birds, but it'll put off squirrels. 
[02:34:43] <gwern> makes sense. that's literally why capsaicin evolved in the first place
[02:36:17] *** Quits: src (~src@user/src) (Quit: Leaving)
[02:38:04] <Obormot\Arcturus> "Elon Musk should stick to what he knows." [in a discussion about autonomous vehicles and how supposedly Musk is wrong about some thing]
[02:38:08] <Obormot\Arcturus> Very lol
[02:45:52] <shawwwn> https://twitter.com/theshawwn/status/1454587662350880769?s=21
[02:45:53] <|dbotdan> Shawn Presser (@theshawwn, 2021-10-30 23:14): ‚ÄòI should‚Äôve dressed up as a Tensorflow error. I‚Äôd be terrifying.‚Äô
[02:46:47] <rmmh> "the name's Trace. Stack Trace."
[02:49:24] <gwern> rmmh: he'd need to rent a fat suit for that
[02:49:37] *** Quits: son0p (~ff@181.136.122.143) (Remote host closed the connection)
[02:54:24] <feepbot> <gwern> https://www.timesofisrael.com/camel-beauty-pageants-races-spur-high-demand-for-cloning/
[02:54:25] <feepbot> Camel beauty pageants, races spur high demand for cloning | The Times of Israel (Technology allows wealthy clients to replace their most beautiful or fast camel with one just like it)
[02:55:04] <shawwwn> https://twitter.com/stasbekman/status/1452797437890891779?s=21
[02:55:06] <|dbotdan> Stas Bekman (@StasBekman, 2021-10-26 00:41): ‚ÄòDid the backslash break camel's back? | At @BigscienceW 104B over the weekend we got a huge spike and looking at data we found OSCAR-en has ~4K records of 0.5M backslashes and nothing else. Which means 2M samples of just \\\\\\\\. | Stay tuned for updates. |
[02:55:06] <|dbotdan> https://github.com/bigscience-workshop/bigscience/blob/master/train/tr8-104B-wide/chronicles.md#2m-backslash-only-samples-in-our-dataset ‚Äô Images: https://nitter.1d4.us/pic/media%2FFClemNPVIAQLZHU.png%3Fname%3Dorig (description: chart, line chart; confidence:
[02:55:06] <|dbotdan> 1.00)
[02:59:38] <shawwwn> https://twitter.com/flockaroo/status/1454405159224754184?s=21
[02:59:38] <|dbotdan> Florian Berger (@flockaroo, 2021-10-30 11:09): ‚Äòfloat i,j,d,s,l,x=i=0.;vec3 p,V=vec3(-1,-1,1);for(;i<2e2;i++){p=(FC.xyz-r.xyx*.5)/r.x*x-t*.1;d=s=5.;p=fract(p*sign(mod(p,V+4.)-2.));for(j=4.;j>1.;j--){l=length(p.xy)*s;d=min(d,length(vec2(max(fract(l),++l-s),fract(p.z*s))-.5)/s);p.zxy=fract(p*V);}x+=d-.002;}o=-o+j/x;#„Å§„Å∂„ÇÑ„ÅçGLSL‚Äô Videos:
[02:59:38] <|dbotdan> https://twitr.gq/pic/video.twimg.com%2Ftweet_video%2FFC8VNBqXsAQvaCa.mp4
[03:04:39] <feepbot> <gwern> https://www.nytimes.com/2021/10/30/dining/louise-slade-dead.html
[03:04:39] <feepbot> Louise Slade, Scientist Who Studied the Molecules in Food, Dies at 74 - The New York Times (Her research focused on how to keep dough, bread, cookies and crackers tasting delicious even after weeks on a grocery store shelf.)
[03:09:39] <feepbot> <gwern> https://twitter.com/deliprao/status/1454301221905260546
[03:09:40] <|dbotdan> Delip Rao (@deliprao, 2021-10-30 04:16): ‚ÄòGithub Copilot is magic and addictive. I feel like I will forget how to code in a couple of years of using this, the way Google made me forget details of pretty much anything by being so good at recalling things for me.‚Äô
[03:10:11] <shawwwn> Really? This is the tweet that does well? https://twitter.com/theshawwn/status/1454589225551155200?s=21
[03:10:12] <|dbotdan> Shawn Presser (@theshawwn, 2021-10-30 23:21): ‚ÄòML engineers should get white lab coats. | I want one.‚Äô
[03:10:28] <shawwwn> Guess more people want white lab coats than I thought
[03:15:29] <feepbot> <gwern> https://threadreaderapp.com/thread/1394636356110979076.html wild
[03:15:29] <feepbot> Thread by @VincentGeloso: Econ Thread: My paper on the strange experiment of playing card money in 17th-18th centuries Canada (image below) with Bryan Cutsinger and Mathieu B√©dard was accepted in the European Revie...‚Ä¶
[03:16:11] <[itchyjunk]> Just finished the quantum thief. decent book :s
[03:19:10] <ggreer> https://i.imgur.com/63uwqFv.jpg went to the mall and they have a ‚Äúwifi oasis‚Äù
[03:24:24] <shawwwn> https://twitter.com/theshawwn/status/1454597334608797700?s=21
[03:24:35] <|dbotdan> Shawn Presser (@theshawwn, 2021-10-30 23:53): ‚ÄòTweeting for my notes: an eigenvector is a vector which, when transformed by a matrix, points the same way. | The eigenvalue is the factor by which that vector is scaled. | The rotation axis of a rotation matrix is one such eigenvector. | Not all matrices have eigenvectors.‚Äô
[03:29:36] <feepbot> <gwern> uploads https://www.gwern.net/docs/economics/2021-cutsinger.pdf
[03:32:58] *** Joins: son0p (~ff@181.136.122.143)
[03:34:37] <feepbot> <gwern> https://twitter.com/cavegremlin/status/1454493521054945287
[03:34:41] <|dbotdan> SHITTER (@cavegremlin, 2021-10-30 17:00): ‚Äòhappy Halloween  this year i decided to go as Sickos‚Äô Images: https://birdsite.xanny.family/pic/media%2FFC9mJJ5WEAMshLi.jpg%3Fname%3Dorig (description: a person holding up the hands; confidence: 0.44) | https://birdsite.xanny.family/pic/media%2FFC9mJJ3WYAAvQOB.jpg%3Fname%3Dorig
[03:35:09] * gwern puzzles over why his site greps for https://www.nature.com/articles/s41562-021-01117-5 aren't working and eventually pulls up the title in emacs: https://www.gwern.net/docs/www/misc/xwd-1635638689876211.png MOTHERFUCKERS nature put en-dashes in the title
[03:35:10] <Robomot> image/png (1694x87; 5 KB)
[03:35:11] <Robomot> A systematic review and meta-analysis of discrepancies between logged and self-reported digital media use | Nature Human Behaviour (There is widespread public and academic interest in understanding the uses and effects of digital media. ‚Ä¶)
[03:36:25] <gwern> (if you're wondering why the en-dashes look so weird, it's because my emacs replaces em-dash and en-dash with very obviously different dashes compared to hyphen)
[03:40:33] <Robomot> [Less Wrong [frontpage]] Why the Problem of the Criterion Matters by G Gordon Worley III - https://www.greaterwrong.com/posts/LkjpHGiELQzed8hdu/why-the-problem-of-the-criterion-matters
[03:42:23] *** Quits: Mateon1 (~Thunderbi@user/mateon1) (Ping timeout: 264 seconds)
[03:42:41] <ggreer> It would be nice if chunks of text had something around them to indicate whether they were ascii or unicode
[03:44:30] <gwern> like a highlight indicating "*something* in this line is non-ascii"?
[03:44:56] <adiabatic> notepad does that with its BOM and everyone hates it
[03:46:00] <gwern> wouldn't a BOM be filewide? that's not so useful. unicode usually just screws up the one line it's on
[03:48:14] *** Joins: adiabatty (~adiabatic@user/adiabatic)
[03:48:50] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (Ping timeout: 246 seconds)
[03:50:41] *** Joins: Lord_of_Life (~Lord@user/lord-of-life/x-2819915)
[03:51:02] <feepbot> <gwern> https://twitter.com/adamswaab/status/1454304337849585664 looks like it'd be degenerate cyclical without randomization or something
[03:51:02] <|dbotdan> Adam Swaab (@adamswaab, 2021-10-30 04:29): ‚ÄòTwo simple rules: | 1) Fill empty spaces | 2) Don't hit anything‚Äô Watch video: https://nitter.fdn.fr/adamswaab/status/1454304337849585664
[03:51:31] <adiabatic> just assume everything is unicode, then
[03:52:42] *** Quits: adiabatty (~adiabatic@user/adiabatic) (Ping timeout: 260 seconds)
[03:53:18] <ggreer> https://twitter.com/kittypurrzog/status/1454568694856630272
[03:53:19] <|dbotdan> Katie Herzog (@kittypurrzog, 2021-10-30 21:59): ‚ÄòSupply chain shortage is so bad I can‚Äôt even find razor blades to put in the mini Snickers this year‚Äô
[03:53:59] <adiabatic> hah
[03:56:19] <shawwwn> nshepperd: math senpai help interpret pls https://haoye.us/post/2019-12-05-interpreting-complex-eigenvalues/
[03:56:29] <feepbot> Interpreting Complex Eigenvalues of Real Matrices | Hao Ye (Introduction Setup The easy case (all eigenvalues are real) The hard case (complex eigenvalues) Demonstration Conclusions References   Introduction Lately, I‚Äôve been stuck in getting an intuition for exactly what is going on when a real  [snip])
[03:58:09] <gwern> (dashes were a mistake)
[03:58:31] <adiabatic> emacs was a mistake
[03:58:37] <shawwwn> lisp was a mistake
[03:59:03] <adiabatic> languages older than Unicode are a mistake. OSs (I'm including emacs) older than Unicode are a mistake.
[04:03:00] <shawwwn> First of all, which language is this blog post using?
[04:04:35] <Obormot\Arcturus> MacRoman is fine, l2p
[04:05:23] <shawwwn> Okay, so eigendecomposition sometimes results in complex eigenvectors
[04:05:28] <gwern> shawwwn: that's R
[04:05:46] <shawwwn> what‚Äôs solve(Q)
[04:06:03] <shawwwn> I thought it‚Äôs supposed to be inverse
[04:06:05] <gwern> shawwwn: note the 'R' tag at the top. also the source code blocks have the class 'r'
[04:06:18] <shawwwn> I‚Äôm on mobile
[04:06:28] <shawwwn> But yes I see the R tag now
[04:06:56] <gwern> as well, the '$'s and '<-' are pretty characteristic. you'll never see so many $s as when you're programming R, in both senses
[04:06:59] <shawwwn> So what‚Äôs Q_inv <- solve(Q) ?
[04:07:08] *** Quits: voltage_ (voltage@user/voltage) (Quit: Leaving)
[04:07:17] <gwern>  ?solve 'Solve a System of Equations Description: This generic function solves the equation ‚Äòa %*% x = b‚Äô for ‚Äòx‚Äô, where ‚Äòb‚Äô can be either a vector or a matrix.'
[04:07:52] <shawwwn> ‚ÄúOh‚Äù
[04:08:52] *** Joins: Mateon1 (~Thunderbi@user/mateon1)
[04:09:12] *** Joins: galambo_ (galambo@user/galambo)
[04:11:00] <saturn2> oh good, another problem of the criterion post
[04:12:31] *** Quits: galambo__ (galambo@user/galambo) (Ping timeout: 260 seconds)
[04:15:42] <Obormot\Arcturus> Oh no
[04:16:46] <kuudes> so many mistakes. I presume you guys are therefore mistake theorists? ;-)
[04:19:02] <Obormot\Arcturus> kuudes: We are engaged in a never-ending conflict with the mistake theorists of the world
[04:19:33] <gwern> I'm a mistake theorist because I think humans are too ignorant, lazy, and stupid to have meaningful conflicts
[04:20:08] <gwern> lots of humans can't even fill out a form about political views the same way twice, how can they be considered agents in a conflict with sides
[04:20:21] <Obormot\Arcturus> No, only most humans are too ignorant, lazy, and stupid to have conflicts; but some are also evil, and they exploit the laziness, stupidity, and ignorance of the masses; thence, conflicts
[04:20:34] <kuudes> are you guys now having a conflict?
[04:20:45] <Obormot\Arcturus> kuudes: It would be a mistake to conclude that
[04:22:42] <Obormot\Arcturus> gwern: Also, people can easily be considered agents in a conflict if their actions consistently have the consequence of causing them to be on one side of a conflict, despite their apparent failure to take apparently purposive actions like "filling out a form about political views", which is irrelevant bullshit that nobody actually cares about and thus nobody has any incentive to do "correctly", unlike real things that matter
[04:24:43] <Obormot\Arcturus> "But because the problem seems removed from everyday reality when described in the abstract, I think a lot of people fail to notice when they‚Äôre grappling with an instance of it." ... after reading a bunch of SCPs, I can no longer read sentences like this without thinking "an instance? oh no, is this one of the sort that spawns copies or something"
[04:25:06] <kuudes> I feel conflicted
[04:26:11] <mst> Obormot\Arcturus: in another universe, you'd probably make an excellent hire for the antimemtics division :D
[04:26:22] <Obormot\Arcturus> Haha
[04:26:43] <saturn2> people make mistakes while filling out surveys because they're too busy fighting their conflicts
[04:31:44] <feepbot> <gwern> https://twitter.com/TaliaRinger/status/1454537651495784459 tfw your twitter stalker shows up at the part with a caption "wait, this is just a pretty girl with the word 'ipod' written on her!"
[04:31:47] <|dbotdan> Talia Ringer (@TaliaRinger, 2021-10-30 19:56): ‚ÄòI heard Halloween costumes are supposed to be scary, so I went with something outright adversarial‚Äô Images: https://bird.trom.tf/pic/media%2FFC-OR46WEAIDMli.jpg%3Fname%3Dorig (description: a person wearing a white shirt; confidence: 0.40) | https://bird.trom.tf/pic/media%2FFC-OSOYXIAYnw98.jpg%3Fname%3Dorig
[04:32:00] <kuudes> meh. we still have to move the clocks this night
[04:32:06] <gwern> inside you there are _n_ conflict/mistake theorists. political physicists are still working out the details of the constituent 'quant' particles, but have given them names like 'strange' 'up' down' 'left' 'right' 'tankie' etc
[04:35:28] <saturn2> well, microsoft didn't fall for it
[04:40:28] <feepbot> <gwern> 'This video dates back to about 2010, and is the origin of the phrase "I've forgotten how to count that low" that was recently referenced in a blog post and on hacker news' https://www.youtube.com/watch?v=3t6L-FlfeaI
[04:40:29] <feepbot> I just want to serve 5 terabytes. - YouTube
[04:42:17] *** Joins: GvP (~GvP@ip70-162-85-176.ph.ph.cox.net)
[05:31:45] *** Quits: srhm (~srhm@user/srhm) (Ping timeout: 268 seconds)
[05:34:15] <ggreer> https://twitter.com/plu_ought/status/1454618468662218753 we‚Äôre so domesticated that we killed off every other species that walked upright ü§î
[05:34:19] <|dbotdan> Alex F. ü™ß (@plu_ought, 2021-10-31 01:17): ‚ÄòI‚Äôve often thought about this as an eternal ‚Äúlarval state,‚Äù adding our barely recognizable sexual dimorphism as one of its signature qualities, but this @SamoBurja passage more accurately describing it as a domestic state has altered my thoughts!‚Äô Images: https://nitter.skrep.in/pic/media%2FFC_XyORVgAQjgJy.jpg%3Fname%3Dorig (description: table;
[05:34:19] <|dbotdan> confidence: 0.74)
[05:44:52] *** Quits: GvP (~GvP@ip70-162-85-176.ph.ph.cox.net) (Quit: Going offline, see ya!)
[05:46:53] *** Joins: GvP (~GvP@ip70-162-85-176.ph.ph.cox.net)
[05:48:57] *** Joins: adiabatty (~adiabatic@user/adiabatic)
[05:52:18] <feepbot> <gwern> https://twitter.com/ThanksThoth/status/1454625434637721601
[05:52:18] <|dbotdan> Rod (üôÇüëç) (@ThanksThoth, 2021-10-31 01:45): ‚ÄòMaaaan fuck this Halloween party I wish I was home playing Factorio‚Äô
[05:53:38] *** Quits: adiabatty (~adiabatic@user/adiabatic) (Ping timeout: 260 seconds)
[05:57:19] <feepbot> <gwern> https://www.reddit.com/r/wikipedia/comments/qj9uav/one_critic_wrote_of_pierre_brassaus_works_brassau/
[05:57:20] <feepbot> One critic wrote of Pierre Brassau's works: "Brassau paints with powerful strokes, but also with clear determination. He is an artist who performs with the delicacy of a ballet dancer." After Pierre was revealed to be a chimpanzee, the critic maintained that his work was the best in the exhibition.[snip] (203 votes and 6 comments so far on Reddit)
[06:07:38] <quanticle> ggreer: A "wifi oasis" implies that the rest of the mall *does not* have wifi, which is... horrifying to consider
[06:09:34] <gwern> a wifi oasis implies the existence of nomadic bandwidth raiders whose ferocity is as legendary as their hospitality
[06:09:48] <quanticle> gwern: Yes, you just described ggreer
[06:14:48] <feepbot> <gwern> https://www.chocosphere.com/default/category/bargains/clbt-5kg-11lb-gianduja-bloc-cs.html mmmm.... 5kg of hazelnut milk chocolate... only $90...
[06:14:50] <feepbot> Callebaut GIA-145 Gianduja Block - 5Kg ‚Äì Chocosphere (This Belgian Gianduja is made in the Italian tradition! Callebaut GIA starts with their signature mild Belgian milk chocolate, then blends it with freshly roasted Hazelnuts ground to a flour-like consistency. The result is a wonderful silky te [snip])
[06:15:18] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[06:18:05] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 264 seconds)
[06:23:58] <quanticle> I'm reading Neville Maxwell's _India's China War_ (a history of the 1962 India-China conflict), and I saw this footnote, which I think might be useful:
[06:24:00] <quanticle> "Although even recent dictionaries give the words 'demarcate' and 'delimit' as synonyms, they have taken on distinct meanings; the distinction, crucial to any discussion of boundaries, was drawn by Henry McMahon in 1897. As he spelled it out later: '"Delimitation" is have taken to comprise the determination of a boundary line by treaty or otherwise and its definition in written, verbal terms;
[06:24:02] <quanticle> "Demarcation", to comprise the actual laying down of a boundary line on the ground, and its definition by boundary pillars or other physical means.'"
[06:24:27] <quanticle> Delimit: "draw a boundary on the map"
[06:24:34] <quanticle> Demarcate: "make a boundary on the territory"
[06:25:10] <quanticle> Now, interestingly enough, Maxwell states, "It is impossible to demarcate a boundary without delimiting it first", meaning that the boundary on the map has to exist prior to the boundary in the territory
[06:25:11] <gwern> de-mark-it /squint
[06:31:58] <quanticle> Also the entire episode with Gulab Singh is a hilarious bit of principal-agent shenannigans. Gulab Singh is a Sikh ruler, originally from Jammu. Unlike the other Sikhs, he throws in with the British in the Anglo-Sikh war, and with British support, conquers Kashmir. Then, not satisfied, he turns eastward, and sets his sights on Tibet. His armies conquer Ladakh (an independent kingdom, allied with
[06:32:00] <quanticle> Tibet), but while they're overwintering there in the mountains, the Tibetans surround them, beseige them, and destroy the army. But then Tibetan counterattack is met by Singh's strategic reserve at Leh, and is defeated. This leads to an armistice between Singh and the Tibetans, but the British aren't satisfied. The British come to a deal with Singh where they'll recognize his claim over both
[06:32:02] <quanticle> Jammu and Kashmir, keep him separate from the British Raj, on the condition that he not invade Tibet again.
[06:32:46] <quanticle> And thus the British fuck over Jammu and Kashmir by putting a Hindu ruler over a Muslim population *and* also set the seeds for the India-China conflict by leaving the border between Tibet and Ladakh undemarcated.
[06:33:11] <quanticle> Absolutely impressive, a masterclass in fucking over the future. Only the British could pull off something like this
[06:33:57] <superz_> https://old.reddit.com/r/ControlProblem/comments/qj6j67/china_has_already_reached_exascale_on_two/hiozes2/ ok guys, anyone disagree on this user's comment? My impression is that the US is at the lead in developing AGI first...
[06:33:58] <Robomot> Buck-Nasty comments on "China Has Already Reached Exascale ‚Äì On Two Separate Systems" (FP16 4.4 exaflops; but kept secret?) (An arms race that the US is unlikely to be able to win. China is no Soviet Union, it's PPP GDP (which is what matters for Chinese R&D) is already...)
[06:34:03] <quanticle> The British were worried that Singh would drag them into a war with Imperial China.
[07:07:50] *** Quits: namespace (~op@user/namespace) (Remote host closed the connection)
[07:15:46] *** [_] is now known as [itchyjunk]
[07:46:46] *** Quits: Urchin[emacs] (~user@user/urchin) (Ping timeout: 245 seconds)
[07:49:47] *** Joins: adiabatty (~adiabatic@user/adiabatic)
[07:54:26] *** Quits: adiabatty (~adiabatic@user/adiabatic) (Ping timeout: 260 seconds)
[08:19:55] *** Joins: adiabatty (~adiabatic@user/adiabatic)
[08:20:30] <PapuaHardyNet> oh yeah, nanowrimo has begun
[08:24:42] <superz_> cool
[08:33:22] <Obormot\Arcturus> Man what the hell is this gworley post
[08:33:51] <Obormot\Arcturus> How did he even manage to get obsessed with this bizarre "problem"
[08:39:44] <saturn2> meditation?
[08:48:00] <rmmh> did anyone else play Galidor Quest? It was a weird Lego thing where you went around fighting people, and when you won you could "Glinch" and swap body parts with them
[08:48:01] <superz_> https://www.greaterwrong.com/posts/Xs7ag4gsiA6zspmsD/the-problem-of-the-criterion I think Obormot means this
[08:48:02] <Robomot> The Problem of the Criterion - LessWrong 2.0 viewer (I keep finding cause to discuss the problem of the criterion, so I figured I'd try my hand at writing up a post explaining it. I don't have a great track record on writing clear explanations, but I'll do my best and include lots of links you can follow to make up for any inadequacy on my part. ‚Ä¶)
[09:20:46] <Obormot\Arcturus> saturn2: Yeah maybe
[09:21:47] <CoJaBo> Cool, so one of our sites has had an option for years where users can receive their payments mothly.
[09:22:52] <rmmh> hmmmm https://m.media-amazon.com/images/I/71X6i2Y-BsL._AC_SL1500_.jpg
[09:22:53] <Robomot> image/jpeg (1500x1500; 169 KB)
[09:39:15] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[09:53:11] *** Quits: two2thehead (~user@124.195.209.131) (Ping timeout: 264 seconds)
[09:55:29] *** Joins: two2thehead (~user@124.195.209.131)
[10:01:56] *** Quits: Fusxfaranto (~Fusxfaran@cpe-75-85-179-208.san.res.rr.com) (Ping timeout: 246 seconds)
[10:04:55] <rsaarelm> http://aleph.se/andart2/fiction/bright-hunger/
[10:04:56] <Robomot> Bright hunger ‚Äì Andart II
[10:06:47] <portmanteau> what's bezos'?
[10:22:56] <PapuaHardyNet> meditation: the one reliable way for a rat to go insane
[10:23:34] *** Quits: phil_ (uid429774@id-429774.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[10:26:40] <Obormot\Arcturus> quanticle: https://imgur.com/gallery/tSPJfhe
[10:26:40] <Robomot> Couldn't believe my eyes - Imgur
[10:26:56] <PapuaHardyNet> imagine a game of chicken, where you and your friends do a marathon meditation session and the one who continues to meditate the longest wins
[10:38:19] <rsaarelm> https://fakenous.net/?p=2632
[10:38:21] <Robomot> The Fallacy of Rationalism ‚Äì Fake Nous (I‚Äôve spent a fair amount of time thinking about how human thinking goes wrong. You could call this ‚Äúapplied epistemology‚Äù. It‚Äôs really striking how often human thinking goes wrong, and in how many ways. Anyway, it seems to me that there is a cluster of misguided thoughts and ways of reasoning that are generally associated‚Ä¶)
[10:39:48] <Obormot\Arcturus> oh no
[10:40:29] <Obormot\Arcturus> Why does Michael Huemer insist on cranking out this sort of tedious bullshit
[10:45:52] <adiabatty> he didn't get invited to any parties this weekend
[10:46:34] <adiabatty> me, I'm rearranging my expenses categories
[10:51:06] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Remote host closed the connection)
[10:51:26] *** Quits: adiabatty (~adiabatic@user/adiabatic) (Remote host closed the connection)
[10:51:47] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[10:52:03] <PapuaHardyNet> adiabatty: what software do you use to track your expenses
[10:52:32] <rsaarelm> The Huemer thing sounds broadly right to me? Is it tedious because it's misguided somehow or beating on trivialities?
[10:56:26] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 260 seconds)
[10:59:09] *** Joins: Urchin[emacs] (~user@user/urchin)
[11:00:15] *** Joins: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de)
[11:02:39] <synthmeat> yt literally recommends crypto scams to me
[11:03:13] <synthmeat> like, look at this, 15k people are watching this now and it's going on for 6 hours. no comments, no live chat, not even reasonable amount of dislikes https://www.youtube.com/watch?v=9D7zQP0XPZA
[11:03:14] <Robomot> Vitalik Buterin Explains How 1 Ethereum Could Reach OVER $15,000 PER COIN! +Bitcoin 100k Prediction - YouTube (Vitalik Buterin Explains How 1 Ethereum Could Reach OVER $15,000 PER COIN! +Bitcoin 100k Prediction üöÄüöÄüöÄHit Like, Share, and Subscribe for more daily crypt...)
[11:03:51] <linear> just saw that linked on HN lol
[11:03:57] <linear> definitely not helping the viewercount
[11:03:58] <PapuaHardyNet> synthmeat: how do you feel about yt thinking you are an absolute fool
[11:04:32] <superz_> man... just use invidious to avoid the youtube recomendation algorithms.
[11:04:39] <PapuaHardyNet> yep! invidious is great
[11:04:50] <PapuaHardyNet> I recommend invidious.sethforprivacy.com and iteroni.com
[11:04:51] <Robomot> Invidious (An alternative front-end to YouTube)
[11:05:02] <PapuaHardyNet> if they don't work then try invidious.namazso.eu
[11:05:03] <Robomot> Invidious (An alternative front-end to YouTube)
[11:05:19] <PapuaHardyNet> if that doesn't work then try piped.kavin.rocks
[11:05:58] <superz_> to add on to PapuaHardyNet's list: https://redirect.invidious.io/
[11:05:59] <Robomot> Select instance - Invidious
[11:06:04] <PapuaHardyNet> linear: I assume it is time to sell, based on the proliferation of scams
[11:07:05] <linear> it's always like this tbh
[11:07:18] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[11:11:00] <PapuaHardyNet> https://invidious.namazso.eu/channel/UChPsCwzLIghlUKodG5zijfA
[11:11:01] <Robomot> Elegant Sister (ES) - Invidious
[11:11:09] <PapuaHardyNet> this is my favorite channel for discovering doujin music
[11:11:47] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[11:13:03] <ggreer> https://i.imgur.com/atzgJjs.jpg finished moving everything from my old place to my new place. Both places have 3 floors
[11:14:20] <CoJaBo> Logged into a webapp after moving it to new server. Main portal screen says:    Today is day
[11:14:39] <CoJaBo> So I guess. Today is day.
[11:15:34] <superz_> ggreer now has giga chad legs.
[11:16:04] <CoJaBo> Lol, try working in a building with no elevator
[11:16:17] <CoJaBo> Like 11th floor or something
[11:16:27] <CoJaBo> All stairs
[11:16:47] <CoJaBo> Entrence isn't even at ground level either. You go up, down, across, then up. Madness.
[11:18:23] <PapuaHardyNet> up, down, across, then up? are you sure this is an office building
[11:18:29] <CoJaBo> Yep
[11:18:50] <CoJaBo> I don't think it was built that way, but it is now
[11:19:05] <PapuaHardyNet> I see, incremental construction led to this
[11:19:35] <CoJaBo> Yeh, it's built as kind of a, growth out the side of a parking garage
[11:20:39] <CoJaBo> The actual entrance to the main part of the structure is on the roof of the parking garage, which you access via stairs bolted onto the side facing a courtyard
[11:21:10] <CoJaBo> Weirdest building I've ever seen, and one of our other clients was hosted out of what apparently used to be a brothel
[11:22:47] <PapuaHardyNet> btw, I made that list of youtube alternatives into a txt file in case you want to refer to it later: https://apotheosis.tilde.institute/youtube_alternatives.txt
[11:22:48] <Robomot> text/plain; charset=utf-8 (143 bytes)
[11:24:05] <CoJaBo> I wonder how many code violations these places have; they gotta be going for a WR
[11:25:29] <CoJaBo> The brothel one (I dunno if it really was, but that was the explanation I was given; riverfront property with NO windows) was interesting. All interior fixtures removed, and CAT5e just duct-taped to the walls
[11:25:32] <ggreer> superz_: my gf must have over 100lbs of clothes‚Ä¶ and she‚Äôs constantly complains about how she has nothing to wear
[11:32:41] <superz_> ggreer: haha, which reminds of this meme: https://nitter.42l.fr/pic/media%2FFC-W74_WYAUTplx.jpg%3Fname%3Dorig
[11:32:42] <Robomot> image/jpeg (1100x1414; 161 KB)
[11:34:56] <Obormot\Arcturus> https://www.datasecretslox.com/index.php/topic,4874.msg313/topicseen.html#new
[11:34:57] <Robomot> Powerology so far (Powerology so far)
[11:35:11] <Obormot\Arcturus> "I'll grovel and write a disclaimer here because I have empathy with the fascism anxiety impulse and I know a lot of you guys are Jewish: I am not an anti-semite. I love many Jews, such as Jesus Christ, St. Paul, the apostles, etc, Ron Unz, Paul Gottfried, and Author Jensen."
[11:35:20] <Obormot\Arcturus> What a sentence
[11:35:52] <ggreer> superz_: accurate
[11:35:57] <superz_> indeed
[11:37:49] <rsaarelm> JuliusBranson is a very special person.
[11:47:08] <PapuaHardyNet> how do I ensure that my files don't bitrot? is there a filetype agnostic way of checksumming them? perhaps a file system?
[11:47:23] <PapuaHardyNet> what I currently have in mind is using IPFS and its automatic hashing ability to "checksum" the data
[11:47:25] <Obormot\Arcturus> zfs
[11:47:41] <PapuaHardyNet> right, I'll read up on zfs
[11:48:03] <ggreer> Btrfs and zfs both have checksums
[11:48:13] <Obormot\Arcturus> btrfs is terrible though
[11:48:17] <ggreer> I use btrfs
[11:48:23] <Obormot\Arcturus> I'm sorry to hear it
[11:48:52] <PapuaHardyNet> from experience I know that btrfs and systemd do not go well together
[11:48:54] <ggreer> I haven‚Äôt noticed anything besides the lack of fscking
[11:48:58] <Obormot\Arcturus> (Also APFS has checksums but I didn't mention that because "switch to macOS" isn't really viable advice for most people)
[11:49:00] <PapuaHardyNet> I recall dbohdan experiencing something similar
[11:49:35] <Obormot\Arcturus> (Oh actually APFS only checksums metadata, so never mind anyhow)
[12:39:31] <PapuaHardyNet> man, zfs is so complicated to get started with on arch linux
[12:39:43] <PapuaHardyNet> it is ridiculous, I thought all I would need to do is do a mkfs
[12:55:55] <ivan> I use btrfs and it's not terrible
[13:02:13] <PapuaHardyNet> btrfs has given me difficulties, but I'll look into it again
[13:04:35] <mjr> zfs is a bit of a foreigner on linux, though apparently distros have done some work to integrate it. Also it _is_ just a more complex filesystem than the usual ones to start with.
[13:08:13] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[13:08:58] *** Joins: galambo (galambo@user/galambo)
[13:11:11] *** Quits: galambo_ (galambo@user/galambo) (Ping timeout: 264 seconds)
[13:12:59] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[13:44:29] *** Quits: bildramer (~bildramer@2a02:587:6207:7b00:dd6a:d998:40b5:bae2) (Quit: alway rember happy day)
[13:44:55] *** Joins: bildramer (~bildramer@2a02:587:6207:7b00:7578:683e:43a2:2892)
[14:08:08] *** Joins: src (~src@user/src)
[14:09:34] *** Joins: galambo_ (galambo@user/galambo)
[14:09:57] <PapuaHardyNet> the product is called Google Slides. The URL is https://docs.google.com/presentation/u/0/
[14:09:58] <Robomot> Google Slides: Sign-in (Access Google Slides with a free Google account (for personal use) or Google Workspace account (for business use).)
[14:11:30] *** Quits: galambo (galambo@user/galambo) (Ping timeout: 260 seconds)
[14:12:47] <nshepperd> i went on a 18km hike today and now my body hurts
[14:12:56] <nshepperd> i skipped leg day
[14:18:52] <PapuaHardyNet> wow nshepperd, you are TOUGH
[14:19:15] <nshepperd> heh really
[14:22:13] <PapuaHardyNet> well, for me at least! A 10km walk would surely take me out of commission for days, 18km seems impossible for me
[14:23:26] <pompolic> "https://fakenous.net/?p=2632
[14:23:28] <Robomot> The Fallacy of Rationalism ‚Äì Fake Nous (I‚Äôve spent a fair amount of time thinking about how human thinking goes wrong. You could call this ‚Äúapplied epistemology‚Äù. It‚Äôs really striking how often human thinking goes wrong, and in how many ways. Anyway, it seems to me that there is a cluster of misguided thoughts and ways of reasoning that are generally associated‚Ä¶)
[14:23:29] <pompolic> dammit
[14:23:41] <pompolic> well, that's the context anyway
[14:23:48] <pompolic> "Rationalists are tempted to identify concepts with definitions. " excuse me wat
[14:24:14] <pompolic> does "Words are Hidden Inferences" and related concepts not exist
[14:25:58] <RiversHaveWings> pompolic: i don't think that's about us
[14:26:59] <pompolic> that would explain it at least
[14:28:37] <pompolic> so it's the Descartes-Spinoza-Leibniz thing?
[14:28:43] <RiversHaveWings> Yeah
[14:28:53] *** Joins: voltage_ (voltage@user/voltage)
[14:29:10] * pompolic shakes fist at Yudkowsky for domainsquatting
[14:29:26] <pompolic> domainsquatting in thingspace
[14:33:33] <nshepperd> only real exercise I've done for a long time too
[14:34:29] <nshepperd> pompolic: yeahhh that's why he tries to get people to say 'rationality' for lw stuff
[14:38:08] *** Quits: src (~src@user/src) (Quit: Leaving)
[14:38:17] *** Joins: src (~src@user/src)
[14:38:23] <pompolic> ah, so it was externally applied?
[14:54:00] <feep> yep
[14:54:09] <feep> people went "wait, wasn't there a thing called rationalism"
[14:54:13] <feep> "oh this must be that thing
[14:54:15] <feep> "
[14:54:16] <feep> thingspace aliasing
[14:56:51] <pompolic> protip: name yourself after a long-discredited group and nobody will be able to criticize you
[14:57:11] <PapuaHardyNet> is there any novel or writing like Grendel (https://www.goodreads.com/book/show/676737.Grendel) where the protagonist is considered a monster by other people and the protagonist must come to grips with it?
[14:57:12] <Robomot> Grendel by John Gardner (Grendel book. Read 2,171 reviews from the world's largest community for readers. The first and most terrifying monster in English literature, from the gr...)
[14:57:12] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[14:57:19] <pompolic> (not to imply this is what happened, but i did extrapolate from LW/"rationalism")
[14:57:33] <feep> yeah nobody will *have* to criticize you, because you're already discredited
[14:57:43] <feep> that's like "ninjas can't get you if you're on fire" tier
[14:58:33] <pompolic> shoot the cyberdemon until it dies
[15:00:37] <pompolic> now, i wasn't being serious, but I wonder if, if you don't care about credibility, and/or if you want to filter against highly reputation-conscious people, it would actually be useful to do this
[15:00:53] <pompolic> then again, if you don't care about credibility, you also don't care about being criticized
[15:01:54] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 260 seconds)
[15:08:30] *** Quits: galambo_ (galambo@user/galambo) (Ping timeout: 260 seconds)
[15:16:46] *** Joins: galambo (galambo@user/galambo)
[16:02:05] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[16:02:06] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[16:05:32] <bildramer> pretty sure if a group starts calling itself "posadists" it will very soon come to refer to that group and not the old posadists
[16:06:07] *** Quits: [_] (~itchyjunk@user/itchyjunk/x-7353470) (Client Quit)
[16:06:09] <bildramer> except, of course, everyone will also say "posadists have wacky UFO beliefs" no matter if they actually do
[16:06:28] <bildramer> it won't last very long
[16:07:21] <bildramer> maybe you'll get better results by picking multiple names, all of them also referring to other extant groups
[16:07:42] <bildramer> "we're the liberals, also known as libertarians"
[16:07:55] <pompolic> lol
[16:08:33] <pompolic> i wonder if it generally holds that the connotations are stickier than the denotation
[16:08:41] <pompolic> e.g. "posadists have wacky UFO beliefs"
[16:27:11] <[itchyjunk]> I'm still fiddling with my broken code for some reason. i change my error to a squared error and it magically converges for when 1 is expected but not for 0
[16:27:24] <[itchyjunk]> Isn't every UFO belief "wacky" in some sense?
[16:27:29] <RiversHaveWings> [itchyjunk]: your neural net code?
[16:27:44] <[itchyjunk]> yes my "neural net" code
[16:27:59] <[itchyjunk]> "neural net suffering from MS or something" code
[16:28:08] <RiversHaveWings> ah
[16:28:12] <[itchyjunk]> Roughly following someone else :
[16:28:13] <[itchyjunk]> https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d?gi=82b465a60240
[16:28:16] <Robomot> Implementing the XOR Gate using Backpropagation in Neural Networks | by Siddhartha Dutta | Towards Data Science (Implementing logic gates using neural networks help understand the mathematical computation by which a neural network processes its inputs to arrive at a certain output. This neural network will deal‚Ä¶)
[16:28:41] <[itchyjunk]> Expected:0 Actual:0.9996922485832813 Error:-0.9996922485832813
[16:28:41] <[itchyjunk]> Expected:1 Actual:0.9995527873435514 Error:4.4721265644864783E-4
[16:28:53] <[itchyjunk]> so when 1 is expected, it's convergin towards 1.
[16:29:03] <[itchyjunk]> when 0 is expected, it is still converging towards 1 it seems
[16:29:15] <RiversHaveWings> oh no
[16:29:42] <RiversHaveWings> Did you write the gradient wrong
[16:30:25] <[itchyjunk]> has to be
[16:30:51] <[itchyjunk]> will try to check if mine and this guy's is different in a min
[16:32:58] <pompolic> is "linearly separable" something like "you can draw a line on the plot that cleanly splits the two groups"
[16:33:28] <[itchyjunk]> yes
[16:33:56] <pompolic> \o/
[16:35:06] <RiversHaveWings> pompolic: yes but in arbitrary numbers of dimensions, and the thing that splits the groups is a hyperplane
[16:35:16] <[itchyjunk]> y_0 is the predicted output by that neuron?
[16:37:20] <[itchyjunk]> wait, he doesn't even use the "Error" he computed by mean square to compute the backprop thing?
[16:37:45] <pompolic> RiversHaveWings: oh yeah, that stands to reason. was focused on the actual 2d plot in the image
[16:37:54] <pompolic> in the article*
[16:38:12] <pompolic> concept: generalized
[16:41:30] <RiversHaveWings> [itchyjunk]: "error = expected_output - predicted_output" is the thing though?
[16:41:50] <RiversHaveWings> That is the adjoint (backward pass) of 1/2 squared error
[16:43:13] <RiversHaveWings> the derivative of  x^2 / 2 is x
[16:43:51] <[itchyjunk]> i thought error was 1/2(exp_output - predicted)^2
[16:43:58] <[itchyjunk]> i used to have it expected - predicted
[16:44:05] <kuudes> "the derivative of  x^2 / 2 is x" wait
[16:44:09] <kuudes> isn't that x/4?
[16:44:17] <RiversHaveWings> no?
[16:44:22] <kuudes> eh, no you, you are correct
[16:44:22] <feep> nah, cause 2x/2
[16:45:49] <[itchyjunk]> I also don't see a learning rate in his computation :<
[16:46:47] <RiversHaveWings> [itchyjunk]: they manually multiply by lr?
[16:47:00] <RiversHaveWings> "hidden_weights += inputs.T.dot(d_hidden_layer) * lr"
[16:47:58] <feep> alright! what are we doing today
[16:48:01] * feep checks his kanboard
[16:48:20] <[itchyjunk]> oh
[16:48:34] <feep> public import? public import.
[16:48:39] <[itchyjunk]> read the stuff he was saying but didn't glance at the code.. hm
[16:49:27] <[itchyjunk]> guess i am a little lost in the py code since i decided to do this in java ;_;
[16:50:13] <feep> why would yo do that tho
[16:50:23] <feep> at least use a cool language like haskell, or rust, or D, or neat~~~
[16:50:27] <[itchyjunk]> I am having to learn java for a class
[16:50:31] <feep> oh no :(
[16:50:45] <[itchyjunk]> "Accelerated object oriented programming" class
[16:50:50] <feep> pffh
[16:50:54] <feep> ain't nothing accelerated about java
[16:50:56] <[itchyjunk]> so i figured i'd do this in java for now
[16:51:12] <feep> also java-style/C++-style oop is shit
[16:51:23] <feep> it's just a bad way to program
[16:51:46] <feep> I mean, objectively a bad way to program, like, I can prove this
[16:52:42] <feep> tl;dr if you have an array of vectors, every object in that array will carry around its vtable, this violates the ... um ... what's actually the name for that principle
[16:52:44] <feep> normalization?
[16:52:55] <feep> normalization is like the root key to good software design
[16:53:18] <feep> if you truly understand sql normalization, you will also understand good software design
[16:54:07] <feep> and working around this in java leads to abominations like what minecraft had to do to get fast terrain gen, ie. flat ubyte[
[16:54:09] <feep> []*
[16:54:46] <feep> only java doesn't even have courage of its convictions on that, like, because it does leave escape hatches like byte, int, float, etc. on some level it *knows* it's doing it wrong!
[16:55:11] <feep> grah. sorry. hate java.
[16:55:23] <[itchyjunk]> it was a must take class
[16:55:29] <feep> right, I'm not angry at you
[16:55:30] <[itchyjunk]> for my major, they forced one down my throat
[16:55:36] <feep> I'm angry at james gosling :p
[16:55:42] <[itchyjunk]> i wanted to take a py class but that didn't "count"
[16:55:46] <[itchyjunk]> i am pissed too, tbh
[16:56:19] <[itchyjunk]> for some reason, a py class that dealt with numpy didn't count as a programming class requirement for my major
[16:56:26] <feep> fucking shit.
[16:56:29] <feep> fuck.
[16:56:42] <feep> it's not like python is literally the biggest language on the planet or something
[16:56:43] <feep> smh
[16:56:47] <[itchyjunk]> choice was to take this class or not graduate this semester though
[16:56:55] <feep> right, I mean, sure, you do what you gotta
[16:57:03] <feep> at least java REALLY isn't hard to learn.
[16:57:20] <[itchyjunk]> final project is writing a tower defense game but can't really use random java libraries
[16:57:30] <[itchyjunk]> have to using Swing which hasn't been updated in 15 years
[16:57:33] <feep> fffk
[16:58:01] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[16:58:10] <feep> srsly, everything about good software design was laid down in the 1970s. learn database normalization.
[16:58:28] <feep> <george lucas>normal forms are the key to EVERYTHING.</george>
[16:58:58] <feep> okay. what was I doing.
[16:59:00] <feep> public import!
[16:59:26] <[itchyjunk]> what were you publicly importing?
[16:59:37] <feep> nothing because I don't have that feature yet XD
[17:02:47] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[17:02:47] <[itchyjunk]> what does "outer layer" mean? output layer?
[17:02:52] <feep> where?
[17:03:11] <[itchyjunk]> https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d?gi=82b465a60240
[17:03:13] <Robomot> Implementing the XOR Gate using Backpropagation in Neural Networks | by Siddhartha Dutta | Towards Data Science (Implementing logic gates using neural networks help understand the mathematical computation by which a neural network processes its inputs to arrive at a certain output. This neural network will deal‚Ä¶)
[17:03:18] <[itchyjunk]> says deltaW is outer layer weight
[17:03:29] <[itchyjunk]> that's the hidden to output layer weight?
[17:03:31] <feep> oh, that's input or output layer I think
[17:03:43] <feep> I *think* they mean output here
[17:03:55] <feep> cause .. input doesn't have weights
[17:04:34] <[itchyjunk]> so the weights between my hidden to output, ty
[17:05:55] <feep> I think they do those separately cause the outputs usually have a different transfer function than the hidden layers?
[17:07:07] <[itchyjunk]> in my case, they are all same sigmoid
[17:14:20] <feepbot> <gwern> syncs https://www.gwern.net/docs/genetics/cloning/index
[17:14:20] <feepbot> /docs/genetics/cloning/ Directory Listing ¬∑ Gwern.net (Annotated bibliography of files in the directory /docs/genetics/cloning/, most recent first.)
[17:15:31] <[itchyjunk]> oh ohh. the output weight computation given there and what i am doing actually matches
[17:18:18] <[itchyjunk]> ah looks like my hidden layers don't match
[17:18:56] <feep> // TODO yet another import class? struct? we've written this line three times now...
[17:19:00] <feep> bleh, I've done this code wrong
[17:19:08] <feep> I have like ... *three* import classes if I take that line
[17:20:44] <[itchyjunk]> More the import, better the economy
[17:20:47] <[itchyjunk]> or was that export?
[17:25:21] <feep> unclear.
[17:30:22] <feepbot> <gwern> https://twitter.com/SilverVVulpes/status/1454722292425048065
[17:30:22] <|dbotdan> loaded dice genetic gambler (@SilverVVulpes, 2021-10-31 08:09): ‚ÄòThis is why David Reich discovering what's essentially the indo-aryan invasion didn't bother me one bit. Sure it has cooties from bad people but everyone involved is long dead with impact in those living today limited. It's called decoupling. Try it sometime‚Äô
[17:32:03] <two2thehead> kuudes, s0ph1a feep : I'm trying to improve my look. How should I dress from now on? https://i.redd.it/0vnngccxtzq61.jpg
[17:32:03] <Robomot> image/jpeg (1920x1280; 412 KB)
[17:32:33] <feep> hahaha
[17:33:01] <two2thehead> I'm leaning towards the Akatsuki jacket and katana combo
[17:33:15] <[itchyjunk]> is that x_0 in "outer weight" and x_0 in hidden weight the same?
[17:33:22] <[itchyjunk]> it's the result of different partials :s
[17:33:31] <gwern> two2thehead: the nazi black sun t-shirt is lit tho
[17:33:35] <feep> two2thehead: guy in the middle *would* be countersignalling
[17:33:42] <feep> If he could just raise his head!
[17:33:43] <two2thehead> gwern, that's a bit too much imo
[17:33:52] <two2thehead> feep, yeah, that was what I was thinking
[17:33:56] <gwern> two2thehead: you could just do the hitler 'stache then
[17:34:18] <feep> two2thehead: that said, yellow shirt dude cracking up is my spirit animal
[17:34:20] <gwern> meanwhile, I'm trying to figure out what's wrong with the black dude or earpod guy
[17:34:28] <[itchyjunk]> if it is, d/dw_h (error) = d/dw_0(error) * some stuff
[17:35:01] <two2thehead> feep, that or the pussy ni**a shirt
[17:35:25] <feep> I keep laughing at this picture
[17:35:27] <feep> this is amazing
[17:35:47] <RiversHaveWings> [itchyjunk]: what are you looking at? is it the python implementation, which isn't loading for me?
[17:35:50] <two2thehead> A brave fashion inspiration to us all :V
[17:36:20] <[itchyjunk]> no it's the text in his blog, the image
[17:36:38] <[itchyjunk]> partial of E wrt w_0 for him and me is identical
[17:36:47] <[itchyjunk]> i am looking at partial of error wrt w_h now
[17:37:20] <[itchyjunk]> (t-y_0)*y_0*(1-y_0)*x_0 in that, the x_0's are same?
[17:38:30] <RiversHaveWings> [itchyjunk]: i think they are the same?
[17:38:58] <[itchyjunk]> so i can temporary store my previous compute and use it.. i vaguely remember trying to do something like that from last tutorial too.. makes more sense
[17:39:00] <RiversHaveWings> That's how backprop works, you reuse the partials you computed for later layers in the computations for earlier layers?
[17:39:01] <[itchyjunk]> thanks!
[17:39:32] <RiversHaveWings> it would be really slow if you couldn't do that
[17:39:45] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[17:39:52] <RiversHaveWings> but since you can then computing a backward pass for an arbitrarily deep network is around the same computing cose as computing the forward pass
[17:41:01] <RiversHaveWings> *cost
[17:41:24] <[itchyjunk]> just need to figure out the rest of the terms there
[17:41:46] <RiversHaveWings> in fact if you know you are going to compute a backward pass you can pre-allocate a buffer the same shape as each weight and the same shape as each activation array, I think.
[17:42:14] <RiversHaveWings> to store partials to reuse
[17:43:04] <[itchyjunk]> ah. i haven't actually considered larger NN's yet i just realized haha. so focused on getting this hello world going ..
[17:44:31] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 260 seconds)
[17:44:31] <[itchyjunk]> the color he uses for w_0 is reaaallly hard to see for me.
[17:44:38] <[itchyjunk]> is it true for anyone else?
[17:44:57] <feep> lemme check
[17:45:30] <gwern> I am going to suggest again, as I did like a week ago, that instead of doing this in java using what is apparentl the worst NN tutorial ever written, that you take an actual working tutorial like karpathy's pure python implementation, and, like, read it
[17:45:31] <feep> [itchyjunk]: green?
[17:45:53] <feep> yeah that's a known thing, green is the "brightest" color
[17:46:08] <feep> ie. full saturation green is the closest to white, the background
[17:46:35] <feep> https://en.wikipedia.org/wiki/Relative_luminance
[17:46:36] <Robomot> Relative luminance - Wikipedia (Relative luminance Y follows the photometric definition of luminance L including spectral weighting for human vision, but while luminance L is a measure of light in units such as c d / m 2} , Relative luminance Y values are normalized as 0.0 to 1.0 (or 1 to 100), with 1.0 (or 100) being a theoretical perfect reflector of 100% reference white.[ ‚Ä¶)
[17:46:54] <[itchyjunk]> oh, i didn't know green was the brightest color.. interesting
[17:47:17] <[itchyjunk]> gwern, a different tutorial is in use now :s
[17:47:56] <[itchyjunk]> i was comparing and contrasting my back prop with this one and i dunno what 2 terms he's using is, put once i figurre that out, i think i have it!
[17:48:10] <[itchyjunk]> w_0 and x_h that is
[17:49:13] <feep> [itchyjunk]: well, it's not inherently about green, our green receptors are just more sensitive
[17:49:46] <[itchyjunk]> Is it because our sun's output band maximizes around the green as well?
[17:49:57] <feep> maybe, but like
[17:50:02] <[itchyjunk]> or something like that, i vaguely know there is some correlation between green and our sun
[17:50:02] <feep> you'd expect it to be the weakest from that.
[17:50:04] <feep> I don't know.
[17:51:11] <feep> some people say it's cause green is in the middle so you get response from red and blue cells too
[17:51:44] <ggreer> This is also why it‚Äôs so damn hard to make good looking green color schemes
[17:51:56] <ggreer> Blue is easy. Our eyes suck at seeing blue
[17:52:15] <PapuaHardyNet> everything is blue, his pills, his hands, his jeans
[17:52:53] <[itchyjunk]> his balls
[17:53:37] <feep> da ba dee da ba dai
[17:57:22] *** Quits: two2thehead (~user@124.195.209.131) (Ping timeout: 260 seconds)
[17:58:38] <feepbot> <gwern> https://twitter.com/ATNPassion/status/1454270664177688578
[17:58:38] <|dbotdan> All Technique No Passion (@ATNPassion, 2021-10-30 02:15): ‚ÄòTentacle textures on this one‚Äô Watch video: https://nitter.net/ATNPassion/status/1454270664177688578
[17:59:11] *** Joins: two2thehead (~user@124.195.209.131)
[18:00:59] <RiversHaveWings> [itchyjunk]: i pasted the python code into a jupyter notebook and it seemed to work
[18:01:09] <feep> ah, machine learning <3
[18:01:54] <[itchyjunk]> great! i think if i get this working, i'll repent for my sins of using java by installing py in my free oracle vps and doing this all in py from now on
[18:02:20] <feep> not like any of this like ... relevantly uses java
[18:02:30] <feep> new BackpropagationPassFactory
[18:02:38] <[itchyjunk]> but i still can't tell what w_0 and x_h means
[18:02:47] <feep> x_h as opposed to x_l
[18:03:05] <[itchyjunk]> hmm, don't understand
[18:03:08] <feep> is joke
[18:03:20] <feep> on old x86 processors, registers used to be split into high and low bits
[18:03:26] <[itchyjunk]> ah
[18:03:45] <feep> so x_h would be the upper 8 bits of x, and x_l the lower 8 bits
[18:04:40] <feep> this is cause x86 started out as a 16-bit arch, where it'd occasionally be useful to have two 8-bit regs instead of one 16-bit reg
[18:05:35] <feep> and because ~compatibility~, all that code still works~
[18:06:15] <[itchyjunk]> i did a "hello world" for riscv a month or so bad and it was an effort. i manage to store 2 numbers, compare them and return the smallest. thats all i know about assembly
[18:06:32] <feep> oh huh, even cooler. there's an even *older* 8-bit processor, that the 8086 was trying to be compatible with, hence al/ah
[18:07:43] <[itchyjunk]> sounds like a lot of baggage from being backward compatible
[18:07:45] <feep> https://en.wikipedia.org/wiki/Intel_8080#/media/File:KL_Intel_i8080_Black_Background.jpg look at this cutie
[18:07:46] <Robomot> Intel 8080 - Wikipedia (The Intel 8080 ("eighty-eighty") is the second 8-bit microprocessor designed and manufactured by Intel. It first appeared in April 1974 and is an extended and enhanced variant of the earlier 8008 design, although without binary compatibility.[ ‚Ä¶)
[18:08:00] <feep> that's basically what x86 is lol. "a lot of baggage from being backward compatible."
[18:08:38] <feep> amusingly, the 8080 is pretty much *exactly* 1000 behind current processors.
[18:09:11] <[itchyjunk]> so my input is either w_0 or x_h
[18:09:40] <RiversHaveWings> [itchyjunk]: ported that network to pytorch p quickly and it works there with autograd ofc
[18:09:53] <feep> 1000 lower frequency, 1000 lower resolution, 1000*1000 less transistors
[18:10:24] <[itchyjunk]> hmm so there is numpy, on top of it is scikit, on top of it is tensorflow and on top of it is pytorch?
[18:10:27] <feep> moore's law is one hell of a drug
[18:10:30] *** Joins: _inky (~inky_@46.36.117.129)
[18:10:36] <feep> tensorflow is not necessarily on top of numpy
[18:10:46] <[itchyjunk]> RiversHaveWings, if you're trying to make me jealous, it's working
[18:10:50] <feep> cause gpu support
[18:10:53] <RiversHaveWings> [itchyjunk]: "X_h is the input to the hidden layer, which are the actual input patterns from the truth table."
[18:11:06] <feep> also I don't think pytorch is on top of tensorflow at all, though could be wrong
[18:11:07] <[itchyjunk]> ahh, i thought a tf used numpy in it's implementation or somesuch
[18:11:15] <feep> maybe it does for the cpu backend
[18:11:22] <[itchyjunk]> RiversHaveWings, it says that? .. i just reread and missed it
[18:11:23] <RiversHaveWings> [itchyjunk]: tf is sort of numpy-compatible but does not actually use it
[18:11:26] <feep> but like, people who tf on cpu have lost control of their life etc
[18:11:32] <feep> ah ic
[18:11:39] <RiversHaveWings> pytorch is a separate tensor library from numpy altogether
[18:11:40] <[itchyjunk]> okay what's w_o ?weight from input to hidden?
[18:11:49] <[itchyjunk]> no that can't be
[18:11:59] <feep> <[itchyjunk]> that's impossible!
[18:13:05] <RiversHaveWings> [itchyjunk]: i think w_o is the weights of the output layer?
[18:14:15] <RiversHaveWings> like their actual values
[18:14:29] <[itchyjunk]> well, i don't use weights in the other calculation
[18:14:52] <[itchyjunk]> partial of error wrt w_0 doesn't any weights
[18:15:06] <[itchyjunk]> why would there be weight when computing this partial?
[18:15:08] <RiversHaveWings> i think that is the letter o for "output" and not a zero, also
[18:15:20] <[itchyjunk]> oh
[18:16:05] <ggreer> https://twitter.com/elonmusk/status/1454808104256737289
[18:16:06] <|dbotdan> Elon Musk (@elonmusk, 2021-10-31 13:50): ‚ÄòIf WFP can describe on this Twitter thread exactly how $6B will solve world hunger, I will sell Tesla stock right now and do it.‚Äô
[18:16:26] <[itchyjunk]> hmm still dunno what w_o is
[18:16:31] <RiversHaveWings> [itchyjunk]: because you have to backpropagate "through" the weights of layers that come after to get values for the layers before
[18:16:53] <RiversHaveWings> hm
[18:16:53] <RiversHaveWings> so
[18:17:09] <RiversHaveWings> w_o is the "output_weights" matrix in the python code
[18:17:38] <[itchyjunk]> weight between hidden layer and output layer?
[18:17:46] <kuudes> ggreer, wow, nice!
[18:17:57] <RiversHaveWings> yes
[18:20:04] <RiversHaveWings> [itchyjunk]: i found a subtle bug in their python code btw, rather two bugs that cancel out
[18:21:01] <[itchyjunk]> lmao
[18:21:06] <RiversHaveWings> "output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr" should be -=
[18:21:11] <[itchyjunk]> how long does your thing take to converge?
[18:21:12] <RiversHaveWings> It's gradient *descent*, not ascent.
[18:21:20] <[itchyjunk]> oh
[18:21:37] <RiversHaveWings> But they also wrote "error = expected_output - predicted_output" which flips the sign there too
[18:21:43] <RiversHaveWings> So it works as written
[18:22:49] <RiversHaveWings> [itchyjunk]: if you use lr 1 and run for 2000 steps or so the loss goes low enough to confirm it works
[18:23:05] <[itchyjunk]> https://bpa.st/5JRA code back to not doing anything useful
[18:23:06] <Robomot> View paste 5JRA
[18:23:09] <two2thehead> kuudes, s0ph1a feep : hmm https://www.youtube.com/watch?v=M2QJ9iyGQ48
[18:23:11] <Robomot> Watch This Dinosaur Fall Into‚Ä¶A Cactus! ü¶ñüåµ - YouTube (‚ù§Ô∏è Check out the Gradient Dissent podcast by Weights & Biases: http://wandb.me/gd¬†üìù The paper "Medial IPC: accelerated incremental potential contact with me...)
[18:23:14] <RiversHaveWings> [itchyjunk]: lr 10 seems to work too
[18:23:23] <kuudes> good. I hope WFP takes its best minds and creates a good response and we will then get hunger solved. sincerely. this seems like a high stakes situation
[18:23:31] <[itchyjunk]> hmm my lr is 0.0025
[18:24:23] <two2thehead> kuudes, wat?
[18:24:36] <RiversHaveWings> [itchyjunk]: yeah you can use higher lr on smaller networks usually
[18:25:00] <[itchyjunk]> this is what i see with lr 1
[18:25:01] <[itchyjunk]> https://bpa.st/6MDQ
[18:25:01] <Robomot> View paste 6MDQ
[18:25:18] <ggreer> Spoiler alert: wfp is totally incompetent. They just wanted to dunk on Elon. Also world hunger has been solved. More people die of obesity each year than of starvation/malnutrition.
[18:25:18] <RiversHaveWings> How is your error ever negative?
[18:25:25] <kuudes> https://twitter.com/elonmusk/status/1454808104256737289 <- two2thehead, ggreer linkes that
[18:25:25] <RiversHaveWings> It's a sum of squared errors
[18:25:26] <|dbotdan> Elon Musk (@elonmusk, 2021-10-31 13:50): ‚ÄòIf WFP can describe on this Twitter thread exactly how $6B will solve world hunger, I will sell Tesla stock right now and do it.‚Äô
[18:25:29] <RiversHaveWings> Squares cannot be negative
[18:25:37] <two2thehead> hm
[18:26:00] <[itchyjunk]> System.out.println("Expected:" + expectedValue[pos] + " Actual:" + output + " Error:" + (expectedValue[pos]-output));
[18:26:06] <RiversHaveWings> oh
[18:26:10] <two2thehead> I know for a fact that one non-trivial part of world hunger is corruption derived problems
[18:26:17] <RiversHaveWings> oh you are printing the difference, not the loss
[18:26:26] <two2thehead> so that's one major problem money by itself cannot fix
[18:26:39] <[itchyjunk]> right, thought that was more useful for debugging, should i be printing the loss?
[18:27:04] * feep grep improtes
[18:27:06] * feep ...
[18:27:10] * feep ^Wimports
[18:27:36] <kuudes> well, north korea is a major part of world hunger, so...
[18:28:16] <two2thehead> yeah, but do you really believe that money is the main problem Best Korea has and not, say, the Kim Dynasty?
[18:28:48] <RiversHaveWings> [itchyjunk]: huh, i'm also running all four inputs through the net at once
[18:28:55] <RiversHaveWings> like in a batch of four
[18:29:04] <RiversHaveWings> it should work one at a time too i think
[18:30:39] <RiversHaveWings> [itchyjunk]: the loss is usually what is printed, if only bc it's a single number for a whole batch and it's easier to verify it's going down by inspection/plotting
[18:31:41] <RiversHaveWings> Especially for losses that are not sums of squared errors
[18:34:52] <nshepperd2> kuudes: he'd have to raise an army and conquer north korea. would probably cost more than $6B alone
[18:35:54] <RiversHaveWings> [itchyjunk]: pytorch port https://gist.github.com/crowsonkb/e5c1b239d357508eea8342ecdeed0228
[18:35:55] <Robomot> manual_network_pytorch_port.py ¬∑ GitHub (GitHub Gist: instantly share code, notes, and snippets.)
[18:36:13] <[itchyjunk]> hmm
[18:37:07] <[itchyjunk]> can't tell what my loss it
[18:37:16] <RiversHaveWings> can't tell?
[18:37:24] <[itchyjunk]> that pytorch code is pretty small
[18:37:50] <RiversHaveWings> you can compute it?
[18:38:12] <RiversHaveWings> for a single example it is 1/2 the square of your error
[18:38:39] <RiversHaveWings> for multiple examples you have to decide whether to sum or mean, my pytorch code takes the mean
[18:38:44] <RiversHaveWings> (also it doesn't divide by 2)
[18:38:50] <RiversHaveWings> (it's just mean squared error)
[18:39:29] <[itchyjunk]> ah
[18:39:42] <RiversHaveWings> [itchyjunk]: actually i updated it to divide by 2
[18:39:52] <RiversHaveWings> so the gradient should be equivalent for batch size 1
[18:43:24] <[itchyjunk]> loss is all over the place, something must be really wrong
[18:44:14] <mst> [itchyjunk]: maybe you configured it to output low res .jpg ?
[18:44:36] <mst> (this is a loss.jpg pun, do not accidentally think I'm trying to be remotely helpful here, sorry not sorry)
[18:45:04] <feep> is this gradient
[18:45:59] <RiversHaveWings> ...someone should take the spatial gradient of loss.jpg lol
[18:46:20] <[itchyjunk]> like wtf
[18:46:21] <[itchyjunk]> https://bpa.st/5LOQ
[18:46:21] <Robomot> View paste 5LOQ
[18:47:46] <RiversHaveWings> [itchyjunk]: it looks like it is going down *really slowly*
[18:48:21] <[itchyjunk]> it's with lr 1 :s 
[18:48:40] <feep> [itchyjunk]: you probably want to compute loss over all four learning steps or something
[18:48:57] <feep> rn I think you're mostly seeing the divergence from the input pattern
[18:49:01] <feep> also lr 1 is insane, don't lr 1
[18:49:13] <[itchyjunk]> back to 0.0025?
[18:49:16] <feep> definitely
[18:50:17] <feep> Result:
[18:50:19] <feep>   69/69 successful
[18:50:21] <feep> woo! \o/
[18:50:22] <feep> also haha funny number
[18:51:36] <PapuaHardyNet> learning rate = 1? what
[18:51:49] <RiversHaveWings> [itchyjunk]: it goes *really slowly* at first when i try it
[18:51:49] <feep> network: "THIS IS THE MOST IMPORTANT SAMPLE EVER!!!!!!"
[18:51:53] <RiversHaveWings> i need to do a bunch of steps
[18:52:02] <feep> network: "HOLY FUCKING SHIT---"
[18:52:07] <PapuaHardyNet> hahahah
[18:52:07] <RiversHaveWings> This is like the smallest network that can possibly learn xor
[18:52:13] <two2thehead> kuudes, s0ph1a feep : So apparently apple found a good balance between slim and light laptop design vs battery life https://www.youtube.com/watch?v=BwRwSwrHLEI
[18:52:14] <pompolic> high school be like
[18:52:15] <Robomot> Apple MacBook Pro 2021 Review: The 21-Hour Laptop? | WSJ - YouTube (Apple‚Äôs switch from Intel to its own M1 Pro and M1 Max chips pays off big time in battery life. But how much extra juice do you really get with the new MacBo...)
[18:52:15] <RiversHaveWings> It is not *good* at learning it.
[18:52:19] <two2thehead> More competition is good
[18:52:23] <feep> two2thehead: nice. I want one ;.;
[18:52:26] <feep> but like
[18:52:27] <feep> with linux
[18:52:28] <[itchyjunk]> https://bpa.st/LOPA
[18:52:29] <Robomot> View paste LOPA
[18:52:37] <[itchyjunk]> yeah it seems to be going down reaaaaalllyl slowly
[18:52:38] <RiversHaveWings> [itchyjunk]: if you increase the hidden layer size it learns considerably faster
[18:52:50] <feep> golden xor ticket hypothesis?
[18:52:50] <RiversHaveWings> but do like 40k steps or something, idk
[18:53:02] <two2thehead> well if you want to play the odds, apple hardware probably has the best driver support vs any random laptop
[18:53:04] <[itchyjunk]> currently it's infinite loop
[18:53:07] <RiversHaveWings> oh
[18:53:17] <feep> two2thehead: I mean uh. thinkpads are pretty good.
[18:53:22] <two2thehead> also rip touchbar
[18:53:26] <two2thehead> feep, that too :V
[18:53:35] <feep> and apple are ... not? like, the M1 especially
[18:53:41] <feep> there's an ongoing project for linux support
[18:53:42] <two2thehead> but I'm talking randomly selected
[18:53:45] <feep> but it's afaik not production ready
[18:53:59] <feep> https://asahilinux.org/about/
[18:54:00] <Robomot> About - Asahi Linux
[18:54:09] <RiversHaveWings> [itchyjunk]: how many steps are you actually doing
[18:54:13] <[itchyjunk]> since error is going down, does that imply it's working? or it still might just be some error
[18:54:29] <RiversHaveWings> idk, there's a saying "neural networks want to work"
[18:54:30] <[itchyjunk]> let me throw some print steps in there to see
[18:54:37] <[itchyjunk]> lol
[18:54:44] <mst> feep: I do have a bit of envy for the M1 processor but I'm too attached to trackpoints ;)
[18:54:44] <RiversHaveWings> Which means you can have huge bugs in your code and your loss still goes down and it appears to be "fine"
[18:55:17] <feep> mst: wait wh, no watchpoint support?
[18:55:18] <RiversHaveWings> And sometimes it actually is fine still and sometimes you are just one or more orders of magnitude away from the goodness of the results you *would* get if your code was correct
[18:55:42] <mst> feep: eh?
[18:55:59] <feep> mst: OH, the trackpoint mouse
[18:56:04] <feep> I thought that was a weird name for debug watchpoints
[18:56:14] <mst> heh
[18:56:18] <[itchyjunk]> 600k steps ish
[18:56:21] <RiversHaveWings> oh
[18:56:27] <PapuaHardyNet> the trackpoint is great!
[18:56:28] <RiversHaveWings> yeah you should get results by then i think
[18:56:41] <mst> apple's touchpads are *amazingly* good in my experience, like noticeably better than anybody else's I've ever used
[18:56:51] <mst> but still still doesn't stop me disliking the entire approach
[18:57:10] <feep> idk, I'm a touchpad person
[18:57:15] <feep> never got used to the wigglemouse
[18:57:27] <mst> touchpads don't work for me at all UX-wise
[18:57:40] <mst> mice do, but unfortunately don't work for me at all RSI-wise
[18:57:45] <mst> so trackpoint addiction it is
[18:59:27] <feep> fair fair
[18:59:35] <feep> mst: did you consider getting one of the weird gripmice?
[18:59:44] <RiversHaveWings> lr 1 worked for me on this problem *shrug*
[19:00:13] <[itchyjunk]> i must still have the hidden layer computation wrong
[19:00:16] <feep> (product search for rsi mouse)
[19:00:25] <[itchyjunk]> because the other one matches both tutorials
[19:00:31] <feep> "vertical mouse", apparently
[19:01:28] <PapuaHardyNet> mst: kinesis vertical mouse?
[19:01:42] <[itchyjunk]> https://bpa.st/Q7KQ
[19:01:42] <Robomot> View paste Q7KQ
[19:01:54] <nshepperd2> the best lr with SGD can be basically anything theoretically, bc its not invariant to reparameterization
[19:01:55] <feep> logitech have one too
[19:02:33] <nshepperd2> like the units of lr are params^2/loss
[19:02:34] <feep> [itchyjunk]: really taking advantage of that oop
[19:02:36] <feep> lol
[19:03:25] <pompolic> trackball gang
[19:03:30] <RiversHaveWings> [itchyjunk]: the net you wrote has no biases, did you mean to do that
[19:03:44] <feep> // so that the room will be empty
[19:03:50] <pompolic> i'm on my second or third logitech m570 because they are pieces of shit
[19:04:00] <mst> feep: the problem seemed to be as much as anything else the having to move my hand off the keyboard and back ... but realistically, the fact that I have an approach that's entirely comfortable for me leaves me with little incentive to run the experiment
[19:04:07] <pompolic> also because i play shooters with them
[19:04:11] <mst> PapuaHardyNet: ^^ same answer really
[19:04:12] <feep> mst: fair fair~
[19:04:31] <[itchyjunk]> RiversHaveWings, yes i thought it would work without biases as well
[19:04:56] <RiversHaveWings> [itchyjunk]: oh, i just turned them off on my pytorch impl and loss still went down but it was not able to get near 0
[19:05:13] <RiversHaveWings> like with them in it gets to like 5.109970152261667e-05 in 10k steps
[19:05:56] <RiversHaveWings> without them it gets to 0.04030681774020195 in 10k steps
[19:06:15] <feep> if public imports work now, I'm grabbing my scooter and going for a ride~
[19:06:22] <[itchyjunk]> 0.1 ~ 700k steps here
[19:06:29] <feep> that sounds broken
[19:06:40] <RiversHaveWings> also mine with with batch size 4 and lr 10
[19:06:40] <[itchyjunk]> :(
[19:06:43] <RiversHaveWings> was with
[19:06:55] <[itchyjunk]> my lr is 0.0025
[19:06:58] <[itchyjunk]> i'll try 10
[19:07:29] <RiversHaveWings> idk it may be too high if you are not taking the mean over the whole batch
[19:09:20] <[itchyjunk]> million steps in 0.025 lr
[19:09:21] <[itchyjunk]> https://bpa.st/C25A
[19:09:22] <Robomot> View paste C25A
[19:10:09] <RiversHaveWings> [itchyjunk]: ok so your loss is 0.0038380180779594582
[19:10:28] <RiversHaveWings> Like if you take the mean of four adjacent steps.
[19:11:32] <feep> Subscribe if you print the mean to stdout.
[19:12:29] <RiversHaveWings> which, uh, is better than i expected given that you aren't using biases and you really need them for this
[19:13:16] <[itchyjunk]> ah, so it's basically a success?!
[19:14:41] <nshepperd2> xor with hidden=2 and no biases is hard mode lol
[19:15:05] <[itchyjunk]> I asked for "hello world of neural networks" and this was recommended :(
[19:15:15] <RiversHaveWings> well the tutorial had biases
[19:15:21] <[itchyjunk]> 2 input neuron, 1 hidden layer of 2 neuron and one output neuron
[19:15:21] <RiversHaveWings> the *python code* for it did anyway
[19:15:36] <[itchyjunk]> oh this tutorial used bias, yes
[19:15:38] <RiversHaveWings> It kind of glossed over them in the text
[19:15:48] <[itchyjunk]> right, so did the previous tutorial
[19:15:54] <[itchyjunk]> i seems to be good at finding them
[19:16:03] <RiversHaveWings> anyway if you use like 5 or 10 neurons in the hidden layer it tends to go better also
[19:16:10] <[itchyjunk]> i looked at karpathy's tutorial and it will take a lot of work for me to get through all that
[19:16:27] <RiversHaveWings> like just more than two.
[19:17:17] * feep drags "public import" to the done column <3
[19:18:21] <RiversHaveWings> feep: ooh :)
[19:20:08] * feep ports [itchyjunk]'s code to neat, whee
[19:20:24] <[itchyjunk]> so i'd not bother trying to learn numpy and directly start with pytorch?
[19:20:31] <RiversHaveWings> [itchyjunk]: yes
[19:20:31] <[itchyjunk]> i know some basic py and that's about it
[19:20:39] <RiversHaveWings> numpy doesn't have autograd
[19:21:00] <[itchyjunk]> great, i'll try to get into pytorch! 
[19:21:04] <RiversHaveWings> So you will have to write gradient computation yourself and it *will* break (I have done this so many times lol)
[19:21:33] <RiversHaveWings> there is also JAX which has a mostly drop-in replacement for numpy and has autograd
[19:21:40] <RiversHaveWings> that is a Google thing
[19:22:37] <RiversHaveWings> i don't like it as much, i feel it's less well-documented/has much worse examples and tutorials than pytorch
[19:22:51] <RiversHaveWings> i only used it because i wanted to run on free TPUs
[19:22:58] <RiversHaveWings> pytorch is bad on tpus, don't bother with it
[19:23:37] <gwern> I thought it was fine for a few TPUs, it was just driving pods that's bad
[19:24:03] <RiversHaveWings> gwern: there are random footguns that if you run into them will trigger transfers from the tpu to the host and back
[19:24:12] <RiversHaveWings> If you cannot work around one you are dead in the water
[19:24:22] <[itchyjunk]> heh heh, i doubt i'll get to tpus
[19:24:34] <RiversHaveWings> pytorch was really designed with gpus in mind
[19:24:57] <RiversHaveWings> JAX on CPU is also supposed to be kinda bad, pytorch on CPU is good
[19:25:01] <[itchyjunk]> well my free vps has no gpu and my 15 year old laptop probably doesn't have any supported library
[19:25:08] <[itchyjunk]> so i'll just play with cpu's for now probably
[19:25:18] <[itchyjunk]> ah, pytorch sounds great
[19:25:25] <feep> okay, something is wrong with my code XD I get stuck at loss 0.125
[19:26:07] <feep> oyeah I was initializing everything to 0
[19:26:08] <RiversHaveWings> [itchyjunk]: if you want free GPUs and TPUs you can use google colab
[19:26:32] <feep> hm, not sure if it's that I switched to float, but loss does not seem to want to go below 0.001
[19:26:45] <PapuaHardyNet> itchy, you are a math undergrad, right? or maybe I'm confusing you with someone else
[19:26:50] <RiversHaveWings> feep: it's gonna be bad if you didn't add biases
[19:26:58] <RiversHaveWings> also float32 is fine for neural nets
[19:27:02] <feep> hm idk then.
[19:27:34] <RiversHaveWings> try a different random init idk
[19:27:46] <feep> okay, I forgot how fast C-likes are
[19:27:50] <feep> Loss at step 29280000: 0.000795
[19:27:56] <RiversHaveWings> Ahah
[19:28:04] <feep> (that's after a few seconds)
[19:28:10] <feep> tbf it's an extremely trivial network
[19:28:15] <nshepperd2> hehe
[19:28:16] <Urchin[emacs]> synthmeat: crypto scams happen, just report them when you see them
[19:28:26] <RiversHaveWings> I feel like the smallest net that can possibly learn xor is maybe going to be a bit dependent on the init
[19:28:34] <feep> anyway seems to be working https://gist.github.com/78c4112d2d3c238aac7ebb901f6b8127
[19:28:35] <Robomot> nntest.nt ¬∑ GitHub (GitHub Gist: instantly share code, notes, and snippets.)
[19:29:21] <RiversHaveWings> ahh this is still using the weird code that has double sign flips and does gradient ascent on the negated loss
[19:29:37] <feep> "gradient ascent on the negated loss".. band name?
[19:29:42] <RiversHaveWings> ehehe~
[19:30:06] <feep> and yeah this is just copypaste from [itchyjunk]'s paste
[19:30:57] * feep puts "double math" as the next TODO item
[19:31:03] <Obormot\Arcturus> "> Communism is a hedonistic doctrine
[19:31:03] <Obormot\Arcturus> I suspect this notion stems from a fundamental misunderstanding of the meaning of the word 'party' in both political and recreational contexts due to lack of first-person experience with either."
[19:31:14] <feep> haha
[19:32:05] <RiversHaveWings> Eheh. You don't actually need double for NNs
[19:32:14] <feep> right, I want it anyway tho
[19:32:16] <RiversHaveWings> :)
[19:32:39] <feep> compiler is sitting at 21kloc
[19:32:43] <feep> pretty okay for a C-like, I think
[19:33:13] <RiversHaveWings> feep: does your language have like, stuff that would let you implement generic NNs easily. like operator overloading
[19:33:25] <feep> yeah
[19:33:28] <RiversHaveWings> ah
[19:33:28] <feep> well hm, hang on
[19:33:38] <feep> I want it to have operator overloading, but I can't remember if I added it yet XD
[19:33:40] <RiversHaveWings> Ahh
[19:33:43] <PapuaHardyNet> oh hey, that would be a sweet idea
[19:33:49] <PapuaHardyNet> writing a neural network in C99
[19:33:52] * feep looks up ASTBinaryOp
[19:33:54] <PapuaHardyNet> suckless neural networks
[19:34:19] <RiversHaveWings> PapuaHardyNet: https://bellard.org/libnc/
[19:34:20] <Robomot> LibNC: C Library for Tensor Manipulation
[19:34:22] <feep> ah, I don't *yet*
[19:34:27] <feep> but I could easily add it when the need arises
[19:34:32] <RiversHaveWings> Bellard has a GPT-J-6B implementation using libnc
[19:34:34] <PapuaHardyNet> Rivers: https://github.com/100/Cranium
[19:34:34] <Robomot> GitHub - 100/Cranium: ü§ñ   A portable, header-only, artificial neural network library written in C99 (ü§ñ   A portable, header-only, artificial neural network library written in C99 - GitHub - 100/Cranium: ü§ñ   A portable, header-only, artificial neural network library written in C99)
[19:34:37] <RiversHaveWings> And an inference demo for it
[19:34:41] <RiversHaveWings> On the web
[19:34:45] <nshepperd2> https://media.discordapp.net/attachments/730484623028519072/904400195230515241/hm.png "beautiful dramatic fantasy painting of üß¨/üíò/üïè/üîä/üõ∞/üö´/ü¶ë/ü´Ä by greg rutkowski" hehe
[19:34:45] <Robomot> image/png (2058x1030; 3.2 MB)
[19:34:51] <RiversHaveWings> https://bellard.org/textsynth/
[19:34:51] <Robomot> Text Synth
[19:34:54] <PapuaHardyNet> bellard sounds sweet!
[19:34:59] <feep> nshepperd2: damn that is beautiful
[19:35:09] <RiversHaveWings> nshepperd2: wow
[19:35:31] <RiversHaveWings> I figure if libnc works for something as big as GPT-J-6B it must be ok
[19:35:49] <feep> tbf bellard is like some kind of ancient wizard
[19:35:50] <nshepperd2> eheh :)
[19:36:33] <feep> anyway, I promised myself a scooter roundtrip so afk~
[19:36:43] <nshepperd2> ooh i should do these emoji prompts with InfoLOOB
[19:38:33] <feep> !
[19:38:45] <feep> if you ask gpt-2 "Is X true? [yes|no]"
[19:38:49] <feep> you get its "first guess"
[19:39:16] <pompolic> thank you for this wizardry
[19:39:18] <feep> I wonder if you would get an improvement by making it complete "Is X true? yes, because" and then selecting whichever is less maximally likely
[19:39:30] <nshepperd2> gotta figure out all_gather
[19:39:34] <feep> ie. actually prompt it to think about a reasonable explanation
[19:39:58] <gwern> gpt-2? probably not. too stupid. asking for explanations seems to help a bit on gpt-3
[19:40:33] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[19:40:45] <RiversHaveWings> plain C doesn't have operator overloading though so stuff can be a pain
[19:41:19] <PapuaHardyNet> Rivers, could you explain why operator overloading is so important?
[19:41:35] <PapuaHardyNet> why not simply use functions everywhere instead of operators?
[19:44:29] <RiversHaveWings> PapuaHardyNet: it just makes your code look messy
[19:45:16] <PapuaHardyNet> aesthetics aren't an imperative
[19:45:23] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[19:45:37] <PapuaHardyNet> however, if you said that verbosity decreases comprehensibility, I would agree with you
[19:45:52] <PapuaHardyNet> I recall there's some ideal length of a function - something like 50-100 loc
[19:45:53] <mst> if you're trying to express mathematics in code, being able to actually write out the mathematics so it bears some actual resemblance to the original equation can be extremely helpful in terms of readability
[19:46:09] <PapuaHardyNet> mst: well, that makes sense
[19:46:13] <mst> matrix_multiply(m1, m2) versus m1 * m2
[19:46:52] <mst> all other cases of operator overloading are much more debetable, although I do think there are plenty of cases where the reduced verbosity is worth the trade-offs
[19:48:04] <PapuaHardyNet> W*x + b becomes mat_add(mat_mul(W, x), b)
[19:48:10] <PapuaHardyNet> doesn't seem all that painful to me tbh
[19:48:30] <PapuaHardyNet> perhaps I haven't dealt with a big enough scale of a codebase
[19:48:33] <mst> as the equation becomes more complicated, the pain becomes much more salient
[19:48:46] <PapuaHardyNet> right, I see
[19:49:41] <mst> one of the annoying things about programming language ergonomics is that examples simple enough that the example doesn't get in the way of understanding the trade-offs ... are also often too simple an example to properly *see* how the trade-offs would play out in practice
[19:49:56] <RiversHaveWings>             ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \
[19:49:56] <RiversHaveWings>                 (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()
[19:49:56] <RiversHaveWings>             adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()
[19:50:12] <RiversHaveWings> That is one of the worst things in my current project
[19:51:08] <median> what is the project?
[19:51:32] <RiversHaveWings> it's for sampling from diffusion models
[19:51:58] <PapuaHardyNet> mst: I love the phrase "programming language ergonomics"
[19:52:38] <nshepperd2> ...what is the gradient of an all_gather
[19:52:43] <mst> PapuaHardyNet: I think it better captures the domain we're trying to navigate trade-offs within than anything else
[19:52:52] <nshepperd2> am i going to get a multiplier of 8 or something
[19:55:18] <RiversHaveWings> nshepperd2: with pytorch?
[19:56:28] *** Quits: two2thehead (~user@124.195.209.131) (Quit: Leaving)
[19:56:54] <nshepperd2> with jax
[19:57:08] <RiversHaveWings> oh
[19:57:26] <nshepperd2> like if i pmap something which takes the gradient of something that does all_gather... i'm not even sure what that means
[19:58:51] <RiversHaveWings> nshepperd2: you can all_gather inside a pmap?
[19:58:58] <RiversHaveWings> like, at all?
[19:59:09] <RiversHaveWings> idk very much about jax tbh
[19:59:34] <nshepperd2> yeah
[20:00:14] <nshepperd2> it returns a tensor that has an 8 as the first axis. like containing the values from each core
[20:00:41] <RiversHaveWings> oh
[20:01:22] <nshepperd2> the gradient rule for it is lax_numpy.sum(all_to_all(...)) which kind of looks like it replicates the gradient to each core
[20:01:27] <RiversHaveWings> probably yeah
[20:01:38] <RiversHaveWings> i wouldn't think you'd get a multiplier unless you did a psum
[20:02:31] <nshepperd2> so i guess yeah my infoloob gradient will be multiplied by 8? bc it will add together 8 instances of the same gradient computed on each core?
[20:03:04] <RiversHaveWings> oh
[20:03:26] <RiversHaveWings> wouldn't you get the same gradient on each core
[20:03:28] <RiversHaveWings> i guess
[20:03:32] <RiversHaveWings> maybe
[20:03:37] <RiversHaveWings> ^^;;
[20:04:32] <nshepperd2> yeah it's confusing ^_^
[20:05:43] <nshepperd2> it is running now and hasn't NaNed so i guess the gradient isn't too big at least
[20:05:49] <RiversHaveWings> ^_^
[20:10:50] <feepbot> <gwern> uploads https://www.gwern.net/docs/economics/2017-rubenstein.pdf
[20:11:04] <rmmh> http://randomfootage.homestead.com/pumpkinctscan.html
[20:11:04] <Robomot> PumpkinCTScan
[20:15:42] <PapuaHardyNet> if transformers are language models that predict the next wordvector in a sequence, how do you use it to get a probability of a sequence being part of the corpus the model is trained or finetuned on?
[20:16:35] <PapuaHardyNet> s/trans/ some trans/
[20:17:54] <RiversHaveWings> PapuaHardyNet: they give you, for each subsequence, the conditional probabilities of the next token in the sequence
[20:17:58] <RiversHaveWings> over all possible tokens
[20:18:06] <RiversHaveWings> um, for each prefix, not each subsequence
[20:18:39] <RiversHaveWings> oh, hm, wait
[20:18:50] <RiversHaveWings> "how do you use it to get a probability of a sequence being part of the corpus the model is trained or finetuned on" You can't actually do this.
[20:19:28] <gwern> you can sorta in practice. people have demonstrated extracting specific information from large models, because they do so much memorization/interpolation
[20:19:35] <RiversHaveWings> You can use them to get the probability of a sequence *in the distribution defined by the model*
[20:19:47] <RiversHaveWings> By multiplying together the conditional probabilities for each token.
[20:19:53] <RiversHaveWings> (You do this in the log domain)
[20:20:16] <gwern> (this is one reason gmail's autocomplete is so constrained. worried about it leaking private info)
[20:20:19] <RiversHaveWings> aahh
[20:20:24] <RiversHaveWings> ahh
[20:21:31] <PapuaHardyNet> so for each prefix subsequence, find the probability of the next token, then multiply them all?
[20:21:36] <RiversHaveWings> Yes
[20:21:58] <RiversHaveWings> When I have done this I have started the sequence with an "end of text" token
[20:22:12] <RiversHaveWings> So the first token of the input sequence had a probability for it.
[20:22:22] *** Joins: Fusxfaranto (~Fusxfaran@cpe-75-85-179-208.san.res.rr.com)
[20:23:01] <RiversHaveWings> The way transformers work, you can get the conditional probabilities for the entire sequence, in the log domain, in one evaluation
[20:23:10] <RiversHaveWings> Then you add them all
[20:23:17] <RiversHaveWings> And you get a log probability.
[20:23:30] <nshepperd2> that gives you the probability that a random sequence of that length drawn from the model's distribution is equal to your sequence
[20:23:53] <RiversHaveWings> it's related to the number of bits needed to compress the sequence
[20:24:04] <RiversHaveWings> using the frequency tables output by the model and an optimal code
[20:24:07] <PapuaHardyNet> I see! and you get the probability for a specifix token using what exactly? 
[20:24:11] <nshepperd2> i guess you can estimate "the probability that your sequence is anywhere in the dataset" by take 1 - (1 - that)^(dataset size in tokens)
[20:24:49] <RiversHaveWings> PapuaHardyNet: it will output, for every token, a vector of log probabilities for the next token.
[20:25:03] <nshepperd2> probably in log domain
[20:25:06] <PapuaHardyNet> oh yeah, my bad
[20:25:08] <RiversHaveWings> You index into it using the index of the actual next token
[20:26:46] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[20:30:08] <feepbot> <gwern> uploads https://www.gwern.net/docs/economics/2000-griffeth.pdf
[20:31:03] <nshepperd2> RiversHaveWings: https://irc.zlkj.in/uploads/15d2532536d9926e/infoloob.png InfoLOOB
[20:31:04] <Robomot> image/png (2058x1030; 5.7 MB)
[20:31:13] <RiversHaveWings> oh wow
[20:31:27] <RiversHaveWings> it enhanced the differences
[20:32:17] <nshepperd2> yep!
[20:32:38] <nshepperd2> the witch on a broomstick for the Bowl of Hygieia is kind of interesting
[20:33:22] <gwern> bowl of hygiene?
[20:33:48] <nshepperd2> this thing: üïè
[20:34:19] <nshepperd2> trying again divided by 8~
[20:34:57] <nshepperd2> bc it does look a bit artifacty in a way that suggests too high cgs
[20:39:01] <nshepperd2> uh wait
[20:39:05] <nshepperd2> i did this wrong
[20:39:10] <RiversHaveWings> oh?
[20:39:21] <nshepperd2> i was computing infoloob *across the cutouts*
[20:39:30] <RiversHaveWings> ohhh
[20:39:43] <RiversHaveWings> Yeah I took the mean of the cutout embeds when I did it
[20:39:48] <nshepperd2> i'm not even sure what that would mean ^^;;
[20:40:04] <nshepperd2> yeah i'm gonna do that now
[20:40:13] <RiversHaveWings> It is going to try to make the individual cutout embeds repel
[20:40:26] <RiversHaveWings> It cannot really do this that well but it will try
[20:40:34] <RiversHaveWings> And the result will be bad
[20:42:39] <nshepperd2> ah yeah
[20:46:40] <Robomot> [Astral Codex Ten] Open Thread 196 - https://astralcodexten.substack.com/p/open-thread-196
[20:51:41] <feepbot> <gwern> 'I can see the horde of Tesla fanboys approaching over the horizon with their pitchforks. Only a full moon can stop them!' https://www.reddit.com/r/SelfDrivingCars/comments/qj6m6j/waymo_pokes_tesla_on_twitter_makes_fun_of_common/
[20:51:44] <feepbot> Waymo Pokes Tesla on Twitter, Makes Fun of Common Autopilot Mistake : SelfDrivingCars (46 votes and 93 comments so far on Reddit)
[20:52:59] <pompolic> reddit links to an article that includes the tweets
[20:53:12] <pompolic> this fills me with inchoate anger for some reason
[20:54:32] <gwern> as long as it's not embeds that'll break in a few months
[20:57:16] <pompolic> to explain a bit further: i have enough browser tabs open to ~completely exhaust memory on this computer. in practice this just means every page has a chance of loading extremely slowly
[20:57:48] <pompolic> so the impact of indirection is magnified
[20:57:52] <feep> home~
[20:58:02] <pompolic> compared to, say, including a screenshot of the tweet
[20:58:26] <feep> [17:16] <PapuaHardyNet> aesthetics aren't an imperative [17:16] <PapuaHardyNet> however, if you said that verbosity decreases comprehensibility, I would agree with you
[20:58:30] <feep> this is *why* aesthetics matter :P
[20:59:26] <mst> feep: that's why I used 'ergonomics' instead
[21:00:00] <feep> right but aesthetics have the same effect
[21:00:12] <feep> or rather, I'm not convinced there is actually a difference there.
[21:00:23] <pompolic> *puts on vr headset* there's no difference
[21:01:40] <Robomot> [Less Wrong [frontpage]] Both or Nothing by Chris_Leong - https://www.greaterwrong.com/posts/hdW2QHYgbSt5bbTQc/both-or-nothing
[21:02:46] <feep> also I can report that my scooter range is down from 19 to 17.3km
[21:02:48] <mst> feep: I'm convinced ergonomics is a more constructive framing since it's about usability, not prettiness
[21:02:52] <feep> probably cause it's getting cooler outside
[21:03:07] <feep> mst: it just feels like ... professionalismwashing the terminology.
[21:03:40] <feep> anyway I can give this range number with confidence cause I arrived home after a 17.3km round trip with like 5km/h speed, rolling to a gentle stop at my house door~
[21:03:48] <feep> er, with like 5km/h speed remaining*
[21:04:47] <mst> feep: I think that's a mistake - for example I'd describe golang as having pretty reasonable ergonomics within the context of an aesthetic that doesn't work for me at all
[21:06:43] <feep> I mean, ergonomics are also subjective
[21:06:49] <feep> would you say golang is ergonomic for you?
[21:07:32] <feep> unrelatedly would you say rice with onion and cheese is delicious :ponders dinner:
[21:08:48] <PapuaHardyNet> playing doom (the original one) made me nauseous. smh
[21:11:43] <mst> now this is the cyberpunk future I actually want: https://twitter.com/GammaCounter/status/1454174999711993856
[21:11:46] <|dbotdan> Alan (@GammaCounter, 2021-10-29 19:55): ‚ÄòThat neolithic amber bear everyone has seen? Well, @boursier made a 3D model *from the photo* , printed the bears, used those to create silicone molds, and then created edible, cola-flavored gummi bears. HE MADE GUMMI BEARS BASED ON A 3500 YEAR-OLD BEAR. https://www.instagram.com/p/CVnsXKav2uzIAKWZwPzueMuD1IIWHCmV_-Kp7Y0/?utm_medium=copy_link ‚Äô Images:
[21:11:46] <|dbotdan> https://nitter.vxempire.xyz/pic/media%2FFC5EcX4XsAM9yOd.jpg%3Fname%3Dorig (description: a person holding a small brown object; confidence: 0.39) | https://nitter.vxempire.xyz/pic/media%2FFC5EclwXEAI2KUJ.jpg%3Fname%3Dorig | https://nitter.vxempire.xyz/pic/media%2FFC5Ec1ZWUAE7yGu.jpg%3Fname%3Dorig |
[21:11:46] <|dbotdan> https://nitter.vxempire.xyz/pic/media%2FFC5EdD9WQAE6qiQ.jpg%3Fname%3Dorig
[21:12:50] <PapuaHardyNet> that's not cyberPUNK at all tho
[21:13:09] <feep> it's not cyber either..
[21:13:17] <pompolic> nor is it the future
[21:13:53] * feep is still in love with his definition as "punks being in the cyber"
[21:13:56] <feep> you know, matrix
[21:14:07] <feep> tron
[21:14:39] <pompolic> the cyber
[21:17:02] <PapuaHardyNet> super hilarious: I try to play othercide or prototype and if I have a browser open too, my x230 literally dies
[21:17:08] <PapuaHardyNet> within a minute of the game running
[21:17:10] <feep> haha
[21:17:13] <pompolic> ignoring that the projection operation is not a bijection
[21:17:15] <feep> manual fan control to 100%?
[21:17:27] <pompolic> that is to say, can't be unambiguously inverted
[21:17:34] <pompolic> (i think)
[21:17:54] <pompolic> what would the inverse operation be called? unprojection? deprojection? 
[21:18:06] <feep> rejection
[21:18:25] * pompolic ponders
[21:18:27] <PapuaHardyNet> oh no, the fan is always screaming as I run the game. I don't get why if the browser isn't open the game doesn't crash the system. it obviously isn't a RAM thing - has to be that the CPU overheats and commits seppuku
[21:18:41] <pompolic> does the browser use the GPU?
[21:18:48] <feep> log a cpu/gpu temperature chart as it goes
[21:19:21] <PapuaHardyNet> good idea, what linux software can I use to log it? also I have no discrete gpu, this is an x230 with an i3 5th gen
[21:19:41] <PapuaHardyNet> oh wait, I could simply use sensors piped into a file
[21:19:50] <feep> yeah exactly
[21:20:01] <feep> watch 'sensors |tee -a temperaturelog.txt'
[21:25:02] <feepbot> <gwern> uploads https://www.gwern.net/docs/statistics/bias/2015-simonsohn.pdf
[21:30:02] <feepbot> <gwern> 'Women on Tinder are more selective than men on Tinder.' https://www.sciencedirect.com/science/article/pii/S0272775719301104 shocku
[21:30:03] <feepbot> Are men intimidated by highly educated women? Undercover on Tinder - ScienceDirect
[21:30:45] <PapuaHardyNet> "Women on tinder are more selective than men on tinder" = water is wet
[21:31:11] <saturn2> PapuaHardyNet: i'd suspect a bug related to gpu memory allocation
[21:36:12] <feepbot> <gwern> 'When considering all subjects, about one-third (33.2%) of our profiles (hereafter: ‚Äòthe evaluated profiles‚Äô) received a (super)like. However, this conceals remarkable differences between the male subjects and female subjects. Indeed, male subjects (super)liked 61.9% of the female evaluated profiles, while female subjects (super)liked only 4.5% of the male evaluated
[21:36:12] <feepbot> profiles. These findings are in line with previous research on online dating in general (Fiore et al., 2010, January, Todd et al., 2007) and on Tinder in particular (Tyson, Perta, Haddadi & Seto, 2016).'
[21:36:35] <ggreer> damnit. my old landlord is trying to charge me for professional carpet cleaning because she's paranoid about covid
[21:36:57] <ggreer> she's also not vaccinated but... she's never been particularly consistent
[21:37:09] <gwern> (ah yes, because people get covid by huffing carpets all the time)
[21:37:16] <saturn2> lmao
[21:37:52] <ggreer> instead of being nice and spending days cleaning everything, I should have trashed the place and forfeited my security deposit
[21:38:19] <ggreer> I'll probably get some of it back, but if I'm going to be chastised for shit like this... eh it's more fun to burn the money and make her upset
[21:40:08] <ggreer> the worst part of it is that the place was dirtier when I moved in. I had to clean kitty litter out of the air conditioner
[21:41:28] <nshepperd2> RiversHaveWings:  https://irc.zlkj.in/uploads/0ed38ecebd22e3ea/infoloob2.png after a few false starts ^_^
[21:41:28] <Robomot> image/png (2058x1030; 5.9 MB)
[21:42:01] <ggreer> on the bright side I can warn the new tenant about the landlord's behavior. we'll be neighbors
[21:42:08] <RiversHaveWings> nshepperd2: oh wow those differences between the prompts are enhanced
[21:43:17] <saturn2> nshepperd2: are these supposed to represent emojis?
[21:43:44] <nshepperd2> RiversHaveWings: yeah! it matches the spherical_dist_loss one now but enhanced
[21:43:48] <nshepperd2> saturn2: yep
[21:44:08] <nshepperd2> "beautiful dramatic fantasy painting of üß¨/üíò/üïè/üîä/üõ∞/üö´/ü¶ë/ü´Ä by greg rutkowski"
[21:45:15] <nshepperd2> gonna generate more overnight and go to bed now~
[21:45:41] <saturn2> hm, some of them make sense, others i don't understand
[21:47:09] <PapuaHardyNet> saturn2: seems like it - while the temp logs show the CPU at ~95 Centigrade, that shouldn't make the CPU kill itself
[21:47:21] <PapuaHardyNet> I'll try running the game again, but this time without any open browsers
[21:47:26] <superz_> https://www.gnu.org/education/how-i-fought-to-graduate-without-using-non-free-software.html
[21:47:27] <Robomot> How I Fought To Graduate Without Using Nonfree Software  - GNU Project - Free Software Foundation
[21:48:09] <saturn2> PapuaHardyNet: i'd say that's probably in the emergency shutdown range
[21:50:11] *** Joins: spxtr (~spxtr@user/spxtr)
[21:50:46] <saturn2> PapuaHardyNet: what do you get from cat /sys/class/thermal/thermal_zone0/trip_point_*
[21:55:47] <feepbot> <gwern> https://arxiv.org/abs/1607.01952 https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.319.9429&rep=rep1&type=pdf https://www.pnas.org/content/104/38/15011.long
[21:55:48] <feepbot> [1607.01952] A First Look at User Activity on Tinder (Mobile dating apps have become a popular means to meet potential partners.)
[21:57:53] <PapuaHardyNet> saturn2: 100000 critical, 90000 passive, 80000 active
[21:59:04] <PapuaHardyNet> right, looks like I'm hitting the limits of this machine. need to buy a proper workstation to play a video game :/
[21:59:15] <gwern> 'On another occasion, I objected to using a Windows VM for a penetration testing exercise. The professor remarked that one would not be a good penetration tester if restricted to testing only libre servers. I gave up on responding to him, but I think proprietary platforms should be considered insecure by default due to, for instance, possible backdoors they may have.'
[21:59:42] <PapuaHardyNet> he isn't wrong
[21:59:49] <Obormot\Arcturus> A terrible day. XO Creperie, one of my favorite restaurants in the city (in fact I was just mentioning it the other day), has folded.
[22:01:11] <gwern> it was cursed once you praised it. murphy's law
[22:01:22] <Obormot\Arcturus> Hard to believe that they're finally gone. The place stood around closed and empty for *literal years* - I don't know what the heck kind of arrangement they'd come to with their landlord - finally reopening shortly before the pandemic started; they lasted this long, through all the restrictions, closures, everything, business seemed to be picking back up... and now they're gone
[22:01:25] <pompolic> pressing F
[22:01:35] <Obormot\Arcturus> Place is cleared out, real estate dealer "for rent" sign, etc.
[22:02:48] <Obormot\Arcturus> (The head waiter found a new job in a place half a block away - good for him. He was exceptionally good at his job)
[22:03:04] <mst> yay for him, boo for everything else
[22:06:00] <Dyo> Obormot\Arcturus: considering you live in new york, how many favorite restaurants do you have?
[22:06:40] <Obormot\Gaia> So my mother and I went to a sushi place down the block for lunch instead. This fucking place, lol. Standard-issue mediocre-to-good sushi joint, the gimmick being "the hostesses are cute young Russian girls in skintight leather outfits; also, the special rolls have Russian-culture-based joke names"
[22:06:42] <kiboneu> the liquor store right next to my apartment building entrance just folded
[22:06:42] <saturn2> PapuaHardyNet: maybe you need new thermal paste. or just to prop up the computer so there's airflow around the bottom
[22:07:06] <rmmh> Obormot\Gaia: who picked the place? :thinking:
[22:07:16] <Obormot\Gaia> (I got the Putin roll, which was *much* better than the Medvedev roll)
[22:07:37] <kiboneu> rusko japanese fusion
[22:07:39] <Obormot\Gaia> Dyo: Not that many. There's so much mediocre shit. (Or, if you like, I have refined tastes, as befitting a New Yorker)
[22:07:42] <gwern> Obormot\Gaia: you'd think they'd be the same thing
[22:07:56] <rmmh> (it's funnier if your mom is taking you to Russian Hooters)
[22:08:12] <Obormot\Gaia> It wasn't Hooters! They were very tasteful skintight leather outfits
[22:08:33] <kiboneu> star trek had skintight outfits
[22:08:37] <Obormot\Gaia> Exactly
[22:08:46] <kiboneu> its the future
[22:08:47] <rmmh> Xootepc
[22:08:52] <Obormot\Gaia> lols
[22:09:15] <saturn2> ah, sushi, the ancient russian delicacy
[22:09:42] <rmmh> saturn2: have you read the stories about russians smuggling caviar out of the US?
[22:09:58] <saturn2> no
[22:10:44] <kiboneu> i remember walking through the russian flea markets with my mom and trying caviar for the first time there
[22:10:50] <Obormot\Gaia> Meanwhile, for the first time in years, I found a bakery in Bensonhurst that had black & white cookies
[22:10:53] * gwern is disappointed that trying out various permutations of 'brooklyn sushi restaurant with waitresses in leather outfits' doesn't work in google images. ah well. it can't win them all.
[22:10:56] <Obormot\Gaia> Review is forthcoming.
[22:11:06] <PapuaHardyNet> saturn2: yeah, I probably will try adding new thermal paste
[22:11:19] <[itchyjunk]> well, installed pytorch and ran RiversHaveWings code. guess i am a bit of a AI person as well now i suppose
[22:11:24] <RiversHaveWings> :)
[22:11:30] <rmmh> gwern: it's Obormot\Gaia's modus operandi, bragging about how NYC has "cheap food" and "good bagels" and "sexy sushi waitresses" to tease us
[22:11:32] <gwern> (google images did think https://www.opentable.com/jado-sushi was relevant, but I don't see anything about the waitresses being russian or in skintight leather)
[22:11:34] <Robomot> Jado Sushi Restaurant - New York, NY | OpenTable (Jado Sushi, Casual Dining Sushi cuisine. Read reviews and book now.)
[22:11:58] <gwern> rmmh: after all, he lives in what is objectively the best city in the world. you forgot the water too
[22:12:09] <kiboneu> i want a cyber bar / cafe. the world hasn't delivered yet
[22:12:31] <rmmh> what is a cyber bar
[22:12:38] <rmmh> you go there, order a drink, and then sext with an AI?
[22:12:45] <kiboneu> like cyberia from neuromancer or sel
[22:13:13] <gwern> or _hackers_. get some pizza while poring over old dumpsterdived AT&T manuals
[22:13:21] <kiboneu> oh yeah exactly
[22:13:26] <mst> gwern: isn't that just a hackspace?
[22:13:28] <kiboneu> roller blade trax
[22:13:36] <gwern> mst: no. it has much hotter people in it
[22:14:00] <kiboneu> angelina jolie hacker waifu
[22:14:04] <Obormot\Arcturus> gwern: https://www.yelp.com/biz/masuta-japanese-fusion-mitoushi-brooklyn
[22:14:05] <Robomot> MASUTA JAPANESE FUSION - MITOUSHI - 433 Photos & 292 Reviews - Japanese - 1714 Sheepshead Bay Rd, Brooklyn, NY - Restaurant Reviews - Phone Number - Menu (Delivery & Pickup Options - 292 reviews of Masuta Japanese Fusion - Mitoushi "Cosy Japanese Restaurant in Brooklyn near Sheepshead Bay with great tasting & fresh sushi and sashimi, do try the Oyster Shots, they are super yummy.")
[22:14:20] <Obormot\Arcturus> "So my husband and I ordered 3 rolls And couple appetizers . Only got one soy sauce to dip. When I called and asked they said one soy sauce per 3 rolls. Haha
[22:14:20] <Obormot\Arcturus> Is this because of new ownership?  I said so why did u give me two chip sticks then . So instead of apology , Russian girl on the phone was just being Russian."
[22:14:43] <kiboneu> i hope one day i can run a bar like that
[22:14:50] <kiboneu> without caring that its losing money
[22:15:26] <mst> Obormot\Arcturus: the important thing is getting sufficient wasabi though
[22:15:45] <kiboneu> gwern: wouldn't it be badass to also have a sushi bar in the cyber bar?
[22:16:01] <kiboneu> like usually the bars are /in/ the sushi place, but not the other way around
[22:16:13] <kiboneu> like a show of dominance
[22:16:19] <gwern> Obormot\Arcturus: pfft, I don't see *any* leather in these photos. and all of the waiters seem to be male, and dressed in boring black cottons or something. there's one woman in a blue shirt. no wonder google image fails
[22:16:48] <kiboneu> Obormot has selective attention / memory
[22:16:50] <gwern> kiboneu: the problem is, like dna lounge, you ultimately do run low on money even if you're fine with losing money
[22:17:10] <kiboneu> yeah rip
[22:17:21] <kiboneu> but also... sf
[22:17:47] *** Quits: _inky (~inky_@46.36.117.129) (Ping timeout: 264 seconds)
[22:18:06] <Obormot\Arcturus> gwern: I am very confident in the accuracy of my description
[22:18:38] <kiboneu> one time a german coworker noted that *all* of my shirts were german, even though only 2 of my 10 shirts were german
[22:18:47] <gwern> (where do people get all these russian girls anyway)
[22:19:09] <Obormot\Arcturus> Um, this is Brooklyn. They're not hard to find
[22:19:10] <gwern> (do you pick them up with the candy and coffee at the russian grocery store in brooklyn)
[22:20:23] <spxtr> I saw dune in imax last night and my hearing is still a little off. Having not seen an action movie in theaters for years, is this volume level the new normal?
[22:20:42] <gwern> how often do you see imax movies?
[22:20:52] <spxtr> I think the last one I saw was in like 2005 or something
[22:20:57] <gwern> every imax I've been to has been extremely proud of its loud
[22:21:01] <spxtr> ah
[22:21:11] <gwern> also, dune was pretty loud too imo
[22:21:33] <kiboneu> is it feel-in-your-body loud?
[22:21:44] <Urchin[emacs]> I guess not going to the theater to see it was a good decision
[22:22:47] <spxtr> it was a little strange. They cranked up the music very loud but kept the dialogue low. I could barely hear that "fear is the mind killer" bit, which was sad
[22:25:22] <gwern> Urchin[emacs]: not the imax version, anyway
[22:25:28] <gwern> I think it was worth big-screening
[22:25:46] <spxtr> yeah def worth seeing
[22:25:56] <rmmh> yeah the audio mix was weird in a few places
[22:26:12] <rmmh> that night scene in the desert canyon was weirdly quiet too
[22:26:24] <rmmh> spxtr: what row were you in?
[22:27:00] *** Joins: _inky (~inky_@46.36.117.129)
[22:28:53] <spxtr> like 2/3 of the way up the theater
[22:28:59] <spxtr> row K
[22:29:51] <ggreer> I watched at home with subtitles. totally saved the movie
[22:30:05] *** Quits: nanotube (~nanotube@user/nanotube) (Ping timeout: 264 seconds)
[22:30:17] <rmmh> "mumble movies works for Christopher Nolan,"
[22:30:41] <kiboneu> wait they release the dvds so soon after theater releases?
[22:30:48] <ggreer> wtf is a dvd?
[22:30:54] <rmmh> it had a simultaneous theatrical and streaming release
[22:30:57] <kiboneu> oh, stream?
[22:31:01] <kiboneu> ah
[22:31:09] <rmmh> and streaming release means you have high quality streamrips immediately :)
[22:31:18] <kiboneu> nice, yeah i was just thinking that
[22:32:05] <kiboneu> i don't have any streaming subscriptions and i hate smelling human farts
[22:36:28] <adiabatic> you could get a streaming subscription for a single month
[22:36:39] <gwern> 'hbo max'
[22:36:45] <adiabatic> ^
[22:36:57] <gwern> adiabatic: assuming they don't make you sit around an hour on phone support trying to argue you out of it
[22:37:22] <adiabatic> gwern: do it on iOS and you're just dealing with Apple. ezpz
[22:37:51] <adiabatic> besides, if they dick you around and don't "let" you, can't you put a stop order on your credit card?
[22:38:32] <gwern> also not exactly the click of a button
[22:40:13] <kiboneu> yeah add more uncertainty of time and money in my life, that sure sounds like a solution
[22:41:07] <adiabatic> I mean, I'm probably never gonna bother to see it
[22:41:27] <adiabatic> but there _are_ options that don't involve smelling others' farts if your mask is leaky
[22:43:39] <kiboneu> right
[22:44:11] <kiboneu> buy an iphone for the ephemeral promise of 2-click customer satisfaction that has no guarantee of staying that way 5 years from now
[22:44:20] <kiboneu> i wouldn't rank that option very highly
[22:45:04] <PapuaHardyNet> you could pirate it
[22:45:20] <kiboneu> i'm jesting around mostly
[22:46:54] <kiboneu> PapuaHardyNet: yeah i think it's the only option
[22:47:07] <kiboneu> the most rational one
[22:52:08] <feepbot> <gwern> uploads https://www.gwern.net/docs/sociology/2016-ong.pdf
[22:57:08] <feepbot> <gwern> 'The book reports chilling research about predatory males. Researchers videotaped individuals walking down the same block in New York City. The tapes were shown to 53 prison inmates convicted of violent crimes. Inmates showed strong consensus on who they would victimize. They chose individuals who moved in an uncoordinated manner, with a stride that was too short or too
[22:57:08] <feepbot> long for their height. In another set of studies, researchers found an association between men‚Äôs chosen targets and women‚Äôs self-reported frequency of having been sexually victimized in the past. This suggests that women suffering from unwanted sexual encounters inadvertently emit cues that predatory males can detect.'
[22:57:08] <feepbot> https://quillette.com/2021/04/30/when-men-behave-badly-a-review/
[22:57:09] <feepbot> When Men Behave Badly‚ÄîA Review (A review of When Men Behave Badly: The Hidden Roots of Sexual Deception, Harassment, and Assault by David M. Buss, Little, Brown Spark, 336 pages (April 2021)  David Buss   Professor David M. Buss, a leading evolutionary psychologist, states in the introduction of  [snip])
[23:01:40] <Obormot\Arcturus> Well... this isn't literally the worst black and white cookie I've ever had... but it's not great
[23:03:11] <Obormot\Arcturus> Thick, dense, and fairly dry cookie, overpowering the frostings; I'll be very generous and assume that if there's any subtle flavors in the cookie base, I just can't taste them, but I rather doubt it. Frostings are underwhelming too. The only good thing I have to say is that they're not too sweet. The white frosting has too much salt, though (there's a flaw you don't encounter every day!)
[23:03:57] <Obormot\Arcturus> White frosting honestly looks and tastes like cornstarch paste.
[23:03:59] <kiboneu> perhaps the baker's taste buds are affected by a recent bout of covid, leading to oversalting
[23:04:06] <Obormot\Arcturus> (With salt added.)
[23:04:39] <Obormot\Arcturus> Black frosting is not bad. Best part of the cookie (though that's not saying much).
[23:04:51] <Obormot\Arcturus> Overall, not bothering to even finish it, having eaten less than half.
[23:05:07] <Obormot\Arcturus> Rating: two out of five cookies.
[23:05:19] <kiboneu> 1/3rd of a cookie on a scale from 0 to 1 cookie
[23:05:23] <kiboneu> ah
[23:05:32] <kiboneu> 2/5th of a cookie
[23:10:33] <feepbot> <gwern> https://www.oglaf.com/crossroad/ obormot and the nyc lw meetups, apparently
[23:10:34] <feepbot> Midnight at the Crossroads
[23:12:36] <Obormot\Arcturus> Hardly a realistic depiction of the attractiveness of your average LW meetup-goer
[23:12:46] <gwern> let me dream
[23:17:47] <feepbot> <gwern> https://twitter.com/sdellavi/status/1408440286297030664
[23:17:47] <|dbotdan> Stefano DellaVigna (@sdellavi, 2021-06-25 15:01): ‚Äò4/N Things change in the 1980s-90s, with election rate of female about tracking the share females among econs w/ pubs. Then in last 15 yr share female among new Fellows rises sharply. Last 10 years: females more likely to be elected cond on pubs and cites. Things did get better!‚Äô
[23:18:19] <Obormot\Arcturus> https://www.dropbox.com/sh/n5y6x3isa7r0vya/AAAmvI-_m0OAULIGAgbEbFF7a?dl=0
[23:18:19] <Robomot> Dropbox - the_people_s_black_and_white_cookie - Simplify your life (Shared with Dropbox)
[23:18:23] <Obormot\Arcturus> The cookie in question
[23:18:36] <Obormot\Arcturus> (Why the folder name? Because the baker is called "The People's Bakery")
[23:18:40] <Obormot\Arcturus> *bakery
[23:19:37] <gwern> the people's leavening front, based on what you said
[23:19:44] <Obormot\Arcturus> Indeed
[23:19:56] <Obormot\Arcturus> They're going to be receiving any Orders of Lenin anytime soon, that's for sure
[23:20:00] <Obormot\Arcturus> *not going
[23:22:28] <Obormot\Arcturus> https://www.greaterwrong.com/posts/39Ae9JEoGCEkfiegr/recommending-understand-a-game-about-discerning-the-rules#comments ... people downvoting my comments here, amazing
[23:22:30] <Robomot> Recommending Understand, a Game about Discerning the Rules - LessWrong 2.0 viewer [initializeCommentControls()MondSemmel 28 Oct 2021 14:56 UTC 10 pointsIf you have 100+ karma and would like me to sponsor you a copy of the game, just reply to this comment or send me a private message, and include a link to your Steam profile ‚Ä¶] (I play a lot of video games. ‚Ä¶)
[23:24:51] * feep upvoted them
[23:29:52] <feepbot> <gwern> https://twitter.com/paulnovosad/status/1407317137883111424  https://www.nber.org/papers/w28937 so much valuable human capital formation
[23:29:54] <|dbotdan> Paul Novosad (@paulnovosad, 2021-06-22 12:38): ‚ÄòScholarship recipients (ie. high school grads) report *less* satisfaction with their jobs than the control group, though still holding out hope that things will get better.‚Äô Images: https://birdsite.xanny.family/pic/media%2FE4fLM-pXMAINMq-.png%3Fname%3Dorig (description: text; confidence: 0.61)
[23:32:08] <ivan> Obormot\Arcturus: if you run greaterwrong, surely you can fix the score on your comments there to be correct
[23:32:23] <PapuaHardyNet> greaterwrong is a frontend, not a backend
[23:32:35] <ivan> that's the joke
[23:32:49] <PapuaHardyNet> hahaha.
[23:35:15] <saturn2> (i run it, obormot did most of the ui design)
[23:35:33] <ivan> ah
[23:38:52] *** Joins: CryptoDavid (uid14990@id-14990.uxbridge.irccloud.com)
[23:40:34] <feepbot> <gwern> http://eprints.lse.ac.uk/110873/1/Working_Paper_67.pdf#page=2 https://twitter.com/devarbol/status/1407469599122198534
[23:40:37] <|dbotdan> v üçÇ (@devarbol, 2021-06-22 22:44): ‚ÄòClark finally published a normal working paper about Hungary, the same mood, usual persistence coefficients of 0.6-0.8, aristocrats are overrepresented, communism didn't do a lot to mobility: | http://eprints.lse.ac.uk/110873/1/Working_Paper_67.pdf ‚Äô Images: https://nitter.fdn.fr/pic/media%2FE4hWHaDXEAAWFnp.jpg%3Fname%3Dorig (description: a screenshot of a
[23:40:37] <|dbotdan> computer; confidence: 0.47)
[23:43:30] <Robomot> [Less Wrong [frontpage]] Resurrecting all humans ever lived as a technical problem by RomanS - https://www.greaterwrong.com/posts/CKWhnNty3Hax4B7rR/resurrecting-all-humans-ever-lived-as-a-technical-problem
[23:47:53] *** Joins: adiabatty (~adiabatic@user/adiabatic)
[23:48:31] <feepbot> <gwern> 'When we were kids, my brother studied a railroad atlas any time he wasn't in school or church. This thing was a monster, large book with hundreds of pages of maps. Literally county by county across the Unites States, all railroad lines and yards. I'd go to sleep Friday night while he sat in an overstuffed chair, studying. I'd wake up at 7:30 AM Saturday... he hadn't moved.
[23:48:31] <feepbot> Still there, a few pages further into the atlas. As an adult, he worked for the Illinois Central as a hump yard operator. Later the Iowa Pacific, and ultimately found himself doing customer service for a small line in Texas, mainly shipping sand. His job frequently included finding 'lost' cars somewhere in the US, on any line that happened to be in that car's route. He was really
[23:48:31] <feepbot> good at it. One notable call had him searching railroad yards in Missouri. He had a strong hunch that the misplaced car was on a spur where that yard stored cars that had lost their way. But he needed to confirm. He could have called that yard, but getting the right person to check logs or go hunt was always a hassle. So... He Googled up the yard, zoomed in on the spur and noted
[23:48:36] <feepbot> that Google helpfully displayed several businesses in a strip mall backed up to the spur. Mike dialed up 'Patty's Nail Salon' and the call went something like: "Hi Patty, this is Craig with the Iowa Pacific. I'm tracking down a misplaced car and I have reason to believe it is sitting on the spur outside your back window. Would you mind checking for a car with ID 123876?" Extended
[23:48:36] <feepbot> pause.... "I'll be right back!" Sure enough, there is was.' https://news.ycombinator.com/item?id=29041729 autism powers
[23:48:37] <feepbot> Unusual Locomotives | Hacker News
[23:53:31] <feepbot> <gwern> uploads https://www.gwern.net/docs/technology/carbon-capture/2020-longman.pdf
[23:58:32] <feepbot> <gwern> 'As of 1980 the population of Alamagan was one family. The people resided on Anatahan when school was not in session' https://en.wikipedia.org/wiki/Anatahan wouldn't that average out to less than 1 family then
[23:58:33] <feepbot> Anatahan is an island in the Northern Mariana Islands in the Pacific Ocean, and has one of the most active volcanoes of the archipelago. Formerly inhabited, the island currently does not have any population due to the always-present danger of volcanic eruptions.
[23:59:20] <catern> lol https://ctan.org/
[23:59:21] <Robomot> CTAN: Comprehensive TeX Archive Network
