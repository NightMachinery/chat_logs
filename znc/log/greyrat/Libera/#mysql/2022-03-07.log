[00:31:37] *** Quits: Tabmow (~tabmow@user/tabmow) (Ping timeout: 240 seconds)
[00:33:53] *** Joins: Tabmow (~tabmow@user/tabmow)
[01:03:16] *** Joins: [sr] (~kvirc@pal-213-228-163-73.netvisao.pt)
[01:04:09] <thumbs> [sr]: you made it
[01:09:19] <[sr]> hi thumbs, well just checking the "normality", i got surprised on what happened to freenode, havent used in a year or so
[01:09:23] <[sr]> sad about it
[01:09:27] *** Quits: masber (~masber@213.55.226.25) (Quit: Client closed)
[01:10:54] <[sr]> so libera now, it's ok, updated!!
[01:11:49] <thumbs> yes, remove the freenode connections forever, it's dead
[01:15:37] *** Quits: [sr] (~kvirc@pal-213-228-163-73.netvisao.pt) (Quit: KVIrc 5.0.0 Aria http://www.kvirc.net/)
[01:16:56] *** Joins: [sr] (~kvirc@pal-213-228-163-73.netvisao.pt)
[01:17:02] <[sr]> ops, wrong botton!!
[01:17:08] <[sr]> thumbs:  yes, done it!
[01:17:13] <thumbs> don't touch that dial
[01:18:01] <[sr]> at least we'll all toghther, which is great, different place, same people, like going on vacation with the friends to a new place ;)
[01:19:07] <thumbs> hence why I redirected you here, yes.
[01:19:14] <thumbs> we want to get our old community back
[01:19:44] <thumbs> you will also see some new faces since the libera move
[01:20:19] *** Quits: [sr] (~kvirc@pal-213-228-163-73.netvisao.pt) (Read error: Connection reset by peer)
[01:20:39] *** Joins: [sr] (~kvirc@pal-213-228-163-73.netvisao.pt)
[01:20:48] <[sr]> damn i have to fix this kvirc version
[01:21:19] <[sr]> i was saying, i like that people are all here, first of all in my opinion there's the human relations
[01:21:29] <[sr]> we should keep that (ok virtually)
[01:21:49] <thumbs> well, I recall you being part of the group. This is what matters.
[01:22:20] <[sr]> yap, i felt like that, and thanks for recognizing that!!
[01:23:20] <[sr]> my last 12 month's or so were to crazy that didn't had time for nothing, hope to start getting more free time for myself
[01:28:57] *** Quits: podeni (~podeni@188.27.129.34) (Ping timeout: 240 seconds)
[01:30:47] <[sr]> well, going to get stuff ready for tomorow, 10pm here, be well, chat tomorow or so
[01:31:28] <thumbs> all right
[02:53:44] *** Quits: Tabmow (~tabmow@user/tabmow) (Ping timeout: 256 seconds)
[02:54:58] *** Joins: Tabmow (~tabmow@user/tabmow)
[02:58:49] *** Joins: lamneth (~IceChat9@modemcable213.189-81-70.mc.videotron.ca)
[04:11:00] *** Quits: Guest3952 (~vit@chello085216193138.chello.sk) (Ping timeout: 240 seconds)
[05:39:57] *** littlepap_ is now known as littlepap
[05:44:32] *** Quits: ChmEarl (~chmearl@user/chmearl) (Quit: Leaving)
[07:08:33] *** Joins: zumba_addict (~zumba_add@c-71-194-58-34.hsd1.il.comcast.net)
[07:11:36] <zumba_addict> Hey folks, someone here 3 weeks ago helped me fixed a login issue. I remember the created user looked fine in the tables. I couldn't remember what the solution was. I'm on Debian
[07:12:27] <zumba_addict> I should have documented it because it looks like the fix had didn't have to do something with the table
[07:14:10] <thumbs> zumba_addict: what login issue?
[07:14:33] <zumba_addict> i created a user using create user and also used grant command
[07:14:41] <zumba_addict> login from command line using mysql
[07:15:12] <zumba_addict> maybe we can find a log since that was my last question in this channel
[07:15:30] <thumbs> zumba_addict: you can find the logs, yes
[07:15:48] <zumba_addict> maybe thumbs it was you who helped last time
[07:15:54] <thumbs> zumba_addict: or you could re-state the actual problem, or error(s)
[07:16:45] <zumba_addict> I'm on debian. I created a user using mysql. Also executed this `GRANT ALL PRIVILEGES ON wpdb.* TO 'wpuser'@'localhost';`
[07:17:08] <thumbs> zumba_addict: so what's the problem?
[07:17:09] <zumba_addict> went back to prompt and ran mysql -u wpuser -p
[07:17:39] <zumba_addict> I am getting this error ` Access denied for user 'wpuser'@'localhost' (using password: YES)`
[07:18:05] <thumbs> how did you create the user?
[07:18:21] <zumba_addict> CREATE USER 'wpuser'@'localhost' identified by 'dbpassword';
[07:18:51] <thumbs> ok, what version of MySQL?
[07:19:17] <zumba_addict> it is mariadb-server-10.5
[07:19:32] <thumbs> okay
[07:20:46] <thumbs> zumba_addict: select user, host, password, plugin, authentication_string from mysql.user;
[07:21:29] <zumba_addict> sure, one sec
[07:21:54] <zumba_addict> i looked at my mysql from 3 weeks ago, looks like you asked me to run `SHOW VARIABLES LIKE 'secure_file_priv';`
[07:21:58] <zumba_addict> running it now, one sec
[07:22:18] <thumbs> zumba_addict: that variable is for something else
[07:24:50] <zumba_addict> https://bpa.st/MAKA
[07:25:04] <zumba_addict> ah got it, it's for the load infile you helped me :)
[07:25:19] <thumbs> that looks like a sane mysql table
[07:26:02] <zumba_addict> sane is synonym to ok or good right?
[07:26:16] <zumba_addict> i'll try it again
[07:26:22] <zumba_addict> maybe i mistyped the password
[07:26:47] <thumbs> yes, it looks like a simple case of typing the wrong password
[07:27:01] <zumba_addict> it worked
[07:27:10] <zumba_addict> let me check spelling in wordpress now
[07:27:18] <thumbs> or even try mysql -uwpuser -ppasswordherealbeititsinsecure
[07:28:16] <zumba_addict> not sure why wordpress is failing to connect
[07:28:21] <zumba_addict> mysql works
[07:28:29] <thumbs> zumba_addict: then #wordpress can help further
[07:28:44] <zumba_addict> yep
[07:28:47] <thumbs> zumba_addict: note that user@localhost means that wordpress needs to run on the same host,
[07:29:03] <zumba_addict> yep, it's on the same host
[07:29:36] <zumba_addict> oh my bad
[07:30:04] <zumba_addict> wrong db
[07:30:05] <zumba_addict> lol
[07:30:15] <thumbs> great move.
[07:31:15] <zumba_addict> yay, I see the wp installer webpage now
[07:31:22] <thumbs> good job.
[07:31:28] <zumba_addict> thank you again
[07:31:43] <thumbs> assume you did something simple and stupid, it works every time.
[07:31:51] <zumba_addict> yep :D
[07:31:58] *** Quits: Vacuity (~Vacuity@user/vovo) (Ping timeout: 256 seconds)
[07:33:44] *** Joins: Vacuity (~Vacuity@user/vovo)
[08:19:42] *** Quits: rvalue (~rvalue@user/rvalue) (Read error: Connection reset by peer)
[08:19:57] *** Joins: rvalue (~rvalue@user/rvalue)
[08:34:16] *** Joins: ChmEarl (~chmearl@user/chmearl)
[08:50:28] <KnowledgeShark>  If anyone could take a peek at my json 2 mariadb python script I am working on from scratch. Still very very alpha. I can insert 5 rows from the json file; and then it chokes on an empty array on row 6 (which I have no idea where to begin to "if empty array -> skip json key" (Logic) ) My paste is here: https://dpaste.com/CX5BGP6Y8 (My program ends at "input" when I go into the 11 Column Version; once I figure out how to do my 2 Column Version).
[08:50:33] <KnowledgeShark> o_O ... Any direction for remedy would be helpful! Thank you in advance!
[09:02:50] *** Quits: ChmEarl (~chmearl@user/chmearl) (Quit: Leaving)
[09:17:46] <skyfall> Is there a `print` like feature in mysql shell? Like, I'd like to view the result of `ENCRYPT()` function without having to first insert it into some dummy table and SELECTing it.
[09:23:45] *** Joins: funnybunny (~funnybunn@user/funnybunny)
[09:29:10] <funnybunny> I have a search page on a website and then a dropdown that orders records by some predefined orders. I want to know how many concurrent users the DB can handle searching and ordering by various orders. I searched and mysqlslap came up often. Is it a good tool for that or is there something else?
[09:35:18] <funnybunny> I should add that I tried mysqlslap a bit and had a problem where it used up all the connections to my test DB at once, which I don't think is realsitic for the scenario I was trying to simulate, which is X users *roughly* searching at the same time, not *exactly*
[09:43:53] *** Joins: podeni (~podeni@188.27.129.34)
[10:02:31] *** Quits: ferdna (~ferdna@user/ferdna) (Quit: Leaving)
[10:34:03] *** Joins: Naktibalda (~Naktibald@88.135.22.17)
[10:54:46] *** Quits: tosaraja_ (~quassel@192.89.120.58) (Quit: http://quassel-irc.org - Chat comfortably. Anywhere.)
[10:55:38] *** Joins: tosaraja (~quassel@192.89.120.58)
[11:17:31] *** Quits: Naktibalda (~Naktibald@88.135.22.17) (Read error: Connection reset by peer)
[11:20:13] *** Joins: Naktibalda (~Naktibald@88.135.22.17)
[11:46:32] <Xgc> funnybunny: Exactly how did you run mysqlslap?  With what parameters/options?
[11:47:43] <Xgc> funnybunny: It should use only the number of sessions you specify.
[12:01:59] <Isotopp> skyfall: select "hello, world!";
[12:03:43] <funnybunny> Xgc: Thanks for helping, but I decided to test the DB load via the JSON API instead of the server directly so that the webserver can be taken into account
[12:03:50] <Isotopp> skyfall: ie select aes_encrypt("plaintext", "secret_password") as result;
[12:04:14] <Isotopp> skyfall: the function encrypt() no longer exists. it was part of mysql 5.6, a version of mysql that is no longer supported and should not be used.
[12:04:46] <funnybunny> Xgc: I think I told it like 3000 concurrent connections or something to simulate 3000 users, but all it did was eat up the available 1000 connections or so and sent the DB CPU up to 100%
[12:05:24] <Xgc> funnybunny: Ok. So that's what you expected.  You told it to use all connections, essentially.
[12:05:35] <funnybunny> The webserver would probably keep a few connections open only and manage hundreds of requests, so in order to simulate that I will simulate JSON API calls instead
[12:05:37] <Isotopp> funnybunny: "3000 users" can mean any number of connection, and any number of *simultaneous* queries
[12:05:48] <Isotopp> what is the host language you are using?
[12:05:57] <funnybunny> What ado you mean "host language"
[12:06:00] <Isotopp> that is, what is the application in?
[12:06:05] <funnybunny> Kotlin
[12:06:08] <funnybunny> With Spring Boot
[12:06:13] <Isotopp> Ok, I read that as a kind of java.
[12:06:21] <funnybunny> Yes, it's like Java++
[12:06:25] <Isotopp> how many client machines will connect to the database host?
[12:06:47] <funnybunny> Machines... well it's on AWS with probably like 5 or 6 servers
[12:06:52] <Isotopp> I am assuming kotlin like most java will have a single server process using threads, and utilizes a thread pool?
[12:06:55] <funnybunny> No idea how many connections per server
[12:07:00] <funnybunny> That would be up to Spring Boot
[12:07:09] <Isotopp> That can be configured, i presume
[12:07:12] <funnybunny> Yeah
[12:07:22] <Isotopp> say 5 machines and 50 connections per pool (per machine, that is)
[12:07:23] <funnybunny> I haven't done it personally
[12:07:31] <Isotopp> that ends up having 250 connections coming in to mysql
[12:07:53] <Isotopp> it is unlikely that all 250 connections will be running queries back-to-back without pause
[12:07:58] <funnybunny> Right
[12:08:06] <Isotopp> how many vCPU on the client kotline instances?
[12:08:15] <Isotopp> estimate is fine
[12:08:19] <funnybunny> Uh, maybe 8 I don't remember
[12:08:47] <Isotopp> ok, 5 machines, 8 cores per, -> 40 vCPU in total, unlikely to use more than 50 concurrent queries, unless async bullshit happening
[12:09:07] <Isotopp> async stuff will also increase number of connections, as the protcol needs one connection per open query
[12:09:16] <Isotopp> you would know if you are using async anything
[12:09:24] <funnybunny> Yeah, so I could use mysqlslap and a shell script to simulate delay, but I think to just be sure I will simulate JSON calls just like the real web application would do
[12:09:34] <Isotopp> 50 concurrent queries, actually running, would create a load of 50 on the target machine
[12:09:41] <Isotopp> the target machine would be unhappy
[12:10:25] <funnybunny> Yeah, mysqlslap really is a slap huh
[12:10:30] <funnybunny> Not like a peppering of bullets lol
[12:10:38] <Isotopp> that is assuming a memory saturated database ("the working set of the database resides in ram", "the number of disk reads requested increases with load, but the number of disk reads does not")
[12:10:58] <Isotopp> such databases can be size on cpu saturation alone, which is easy
[12:11:16] <funnybunny> Uh
[12:11:21] <funnybunny> The DB is like 30 GB I think
[12:11:29] <Isotopp> we use 32 vcpu bare metal blades, 128 gb of memory, 2 TB of database size. they are stable up to a load of 24, but that is a 2 tier model with no connection pool
[12:11:30] <funnybunny> Let me see how much RAM the DB has
[12:11:39] <Isotopp> a 30 gb database is likely to be memory saturated
[12:12:34] <Isotopp> "stable to a load of 24", but because of the deployment model of the database, the load is actually spiky and a load of 24 in the 1min average is anything between 50 and 8 concurrent queries when viewed in "show processlist" running in 1/10s intervals
[12:13:08] <funnybunny> We have about 3000 users at one time
[12:13:32] <funnybunny> I don't really know how many concurrent queries that amounts to
[12:13:57] <funnybunny> I just assume they are say all pressing the search button every 30 seconds or something
[12:15:16] <funnybunny> The DB server has 32 GB of memory and 4 vCPUs
[12:15:24] <funnybunny> Also there are two read replicas
[12:16:04] <Isotopp> are you doing connection splitting?
[12:16:09] <funnybunny> What is that?
[12:16:16] <Isotopp> reads to the replicas, writes to the primary
[12:16:41] <funnybunny> Oh, I'm not sure. I think maybe AWS handles that automatically
[12:16:43] <Isotopp> it can be done by having two database handles in the code, or by using a proxy that listens in to the statements and sends them to different targets.
[12:16:45] <funnybunny> We are using Aurora
[12:16:46] <Isotopp> no
[12:16:48] <Isotopp> it cannot
[12:17:04] <funnybunny> Oh, let me check the app then
[12:17:11] <Isotopp> if you are using two different database handles in the code, the code needs to be adjusted
[12:17:35] <funnybunny> I think Aurora automatically load balances the databases with one DNS though
[12:17:49] <Isotopp> if you are using a proxy, you would know from the endpoint you connect to. also, using java means you are likely using transactions. there are three verbs for beginning a transaction, and two make connection splitting impossible
[12:18:00] <Isotopp> aurora offers differend endpoints.
[12:18:07] <funnybunny> Yes
[12:18:27] <Isotopp> if you start a transaction with BEGIN or BEGIN WORK, the system must assume a r/w txn
[12:18:48] <Isotopp> the verb START TRANSACTION requires a statement of intent, READ WRITE or READ ONLY and can be split be a proxy automatically
[12:19:23] <Isotopp> that is, using a proxy for connection splitting, all explicit transactions always end up at the primary UNLESS the code is written so that it uses START TRANSACTION <intent>
[12:19:59] <Isotopp> https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Overview.Endpoints.Types
[12:20:17] <Isotopp> you'd need to use cluster or reader endpoints, that is, have 2 handles in the app for aurura to split connections
[12:20:54] <Isotopp> you could use RDS proxy, but that is subject to the proxy limitations I just explained wrt to txns
[12:22:05] <funnybunny> Hm, it looks like we may only be using the replicas for our batch server...
[12:22:18] <Isotopp> That is, aurora and rds proxy provide you with the usual means to split connections but you still have to make your application aware of these for them to actually do a thing for you
[12:22:28] <funnybunny> OK
[12:23:36] <funnybunny> OK yeah, we are not using the replicas for search
[12:23:49] <funnybunny> Just for the batch server
[12:24:16] <funnybunny> So we have a 30 GB database and a single server with 8 vCPUs and 32 GB of RAM
[12:26:41] <Isotopp> yes. likely memory saturated, good for a target load of 5-6 or so, so around 5-6 concurrent queries
[12:26:58] <funnybunny> What does memory saturated mean? That the DB fits in RAM or doesn't?
[12:28:10] <funnybunny> I guess you just mean all the memory is being used? Not sure how that factors in
[12:28:54] <Isotopp> memory saturated means that the database working set is kept in memory
[12:29:16] <Isotopp> in metrics, it means that increasing load will (may) increase disk reads requested, but the disk reads to not actually go up
[12:29:29] <funnybunny> OK
[12:29:40] <Isotopp> the disk read load of the database is independent of the actual query load.
[12:29:46] <funnybunny> I see
[12:29:52] <funnybunny> Makes sense
[12:29:55] <Isotopp> "kb/s does not go up, as cpu and qps go up"
[12:29:56] *** Joins: masber (~masber@213.55.226.100)
[12:30:13] *** Quits: Mids_IRC (~midfairyd@2001:470:69fc:105::3f22) (Quit: You have been kicked for being idle)
[12:30:14] <Isotopp> scaling an io bound database is MUCH harder
[12:30:26] *** Joins: Cale38 (~Cale@cable-tku-50dd48-166.dhcp.inet.fi)
[12:30:29] <Isotopp> having an IO bound aurora will not last long, as you pay disk reads.
[12:30:29] *** Joins: Mids_IRC (~midfairyd@2001:470:69fc:105::3f22)
[12:30:32] <Isotopp> you bleed money
[12:30:53] <funnybunny> I would think even if the DB size exceeded the RAM we would still be memory saturated since the data being searched is only a fraction of that 32
[12:31:08] <funnybunny> We have other big tables
[12:31:16] <Isotopp> likely, yes, but using aurora means you MUST monitor and alert ond disk reads.
[12:31:19] <Isotopp> or io in general
[12:31:26] <Isotopp> or the metrics that link directly to aurora cost
[12:31:28] <funnybunny> OK good to know
[12:31:50] *** Parts: Mids_IRC (~midfairyd@2001:470:69fc:105::3f22) ()
[12:31:56] <funnybunny> So I have another question about query optimization
[12:32:03] <funnybunny> Well first
[12:32:19] <funnybunny> I don't understand MySQL composite indexes
[12:32:41] <Isotopp> having multiple types of query load ('search', 'reporting' and so on) it may be useful to have replicas (or endpoints) pointing to different boxen to segregate workloads, by the way.
[12:32:43] <funnybunny> I don't understand how they are implemented under the hood or what they are actually for
[12:32:58] <Isotopp> under the hood, a composite index does not exist
[12:33:12] <funnybunny> OK, so I was trying to optimize a query from first principles...
[12:33:14] <funnybunny> I can explain that
[12:33:19] <Isotopp> the database makes an image (a bit string) from the parts of a table that make up an index
[12:33:36] <Isotopp> it then uses the bitstring as the key-part in a b-tree (or, b+ tree for the pk)
[12:33:49] <funnybunny> BTW, is a B-tree a binary tree?
[12:33:55] <Isotopp> yes, a balanced tree
[12:34:09] <funnybunny> Balanced binary tree?
[12:34:15] <Isotopp> a tree where the longest path from the root to any leaf is at most one step longer than the shortest path
[12:34:22] <Isotopp> it is not binary
[12:34:26] <funnybunny> OK
[12:34:36] <Isotopp> it is n-ary, where n is 16 kb block size divided by index entry size
[12:34:50] <funnybunny> Can it find an index in log n time?
[12:35:00] <funnybunny> Like for example
[12:35:04] <Isotopp> assuming 32 byte index entries, 16384/32 is 2^14/2^5 = 14-5 = 2^9 = 512-ary
[12:35:09] <funnybunny> Say I have an index on a datetime
[12:35:26] <funnybunny> Can it find the first index where the datetime >= now() in log n time?
[12:35:27] <Isotopp> to for a table with n records, it is log512(n) index lookups
[12:36:04] *** Quits: Cale38 (~Cale@cable-tku-50dd48-166.dhcp.inet.fi) (Quit: Client closed)
[12:36:16] <Isotopp> since this is a log with a large base, the number of lookups is constant and the content is approximately 4 for all interesting cases.
[12:36:23] <funnybunny> Oh, so it is even better than log2(n)
[12:36:23] <Isotopp> constant is 4 for ...
[12:36:37] <funnybunny> I don't know how to make such a thing
[12:36:46] <Isotopp> we have books about this
[12:37:07] <funnybunny> If I read about Btrees will I understand?
[12:37:33] <Isotopp> i do not know
[12:37:45] <Isotopp> InnoDB is literally an implementation of a book,
[12:37:45] <Isotopp> https://www.amazon.de/Transactional-Information-Systems-Algorithms-Concurrency-ebook/dp/B01253U1BC
[12:37:49] <funnybunny> OK
[12:37:50] <funnybunny> So
[12:38:02] <Isotopp> This is an advanced university text book, one does not read these, you work through them in 1-4 semesters
[12:38:12] <funnybunny> I assuming finding the splitting index is roughly constant...
[12:38:17] <funnybunny> The next thing I want to ask it
[12:38:18] <funnybunny> *is
[12:38:48] <funnybunny> Say I want to find records where col1 >= now() and then order by the insertion time.
[12:38:49] <Isotopp> also, https://github.com/jeremycole/innodb_ruby
[12:39:10] <funnybunny> If I have two indices stored together, col1_index and ins_index
[12:39:14] <Isotopp> here is an implementation of the internals of innodb in ruby, as done by @jcole as a hobby project
[12:39:22] <Isotopp> there is a set of blog articles that accompanies this
[12:39:39] <Isotopp> it is outdated wrt to the current server, but still immmensely useful for study
[12:40:06] <funnybunny> I find the col1_index, then I *have* to do a O(n) scan through the cached records sorted by ins_index and purge ones that are less than the col1 splitting index right?
[12:40:18] <funnybunny> Does that make sense?
[12:40:21] <Isotopp> I do not understand your question
[12:40:24] <funnybunny> OK
[12:40:39] <funnybunny> So I have records with a departure time and an insertion time
[12:40:46] <Isotopp> do you have one single compount index index or do you have a table with two indexes?
[12:41:09] <funnybunny> I assume I can cached the records sorted by both departure time and insertion time
[12:41:30] <funnybunny> So I want all the records where departure time >= now() sorted by insertion time
[12:41:56] <Isotopp> innodb will always select an index (from multiple that can be used) to speed up the selection (the where clause)
[12:42:01] <funnybunny> I assume I can the index of the first record where departure time >= now() in constant time
[12:42:04] <funnybunny> *find the
[12:42:09] <Isotopp> the selected index then MAY also be used to speed up an ORDER BY clause
[12:42:15] <Isotopp> but if not, manual sorting has to be done
[12:42:41] <Isotopp> an index on a b-tree can be used to speed up any number of equalities and one range
[12:42:59] <Isotopp> for example, WHERE a = 10 AND b = 20 AND c = 10 is a point lookup on the index(a,b,c)
[12:43:00] <funnybunny> Then once I have that index I would have to go through all of the records sorted by insertion time and remove any that have a departure time index that is < than the first index where departure time >= now()
[12:43:19] <Isotopp> but also WHERE a = 10 AND b = 20 AND c>10 can profit from INDEX(a,b,c)
[12:43:31] <funnybunny> No I am not doing any =
[12:43:45] <Isotopp> on the other hand WHERE a = 10 AND b>10 AND c = 10 can use only INDEX(a,b).
[12:43:49] <funnybunny> I don't think it's possible to do anything faster than O(n) but this is what I want to check
[12:44:05] <Isotopp> If you have INDEX(a,b,c) it can be used, but only the INDEX(a,b) prefix of it will actually be contributing
[12:44:26] <funnybunny> I don't understand INDEX(a,b) in the first place. I don't understand a composite index
[12:45:01] <Isotopp> an index reads through a table, extracts the column or columns named, pastes them together forming a bit string, and then sorts by the bit string
[12:45:09] <Isotopp> (collations make things more complicated)
[12:45:23] <Isotopp> the sorted list of bit strings will contain pointers to all records of origin
[12:45:23] <funnybunny> OK
[12:45:58] <Isotopp> so you go through the index, find the bit string that matches, and from there follow the pointers to the records of origin (or not, if the index is covering)
[12:46:03] <funnybunny> So if I have a range check on one column and then a sort on another it seems pretty useless
[12:46:20] <Isotopp> with a b-tree that is not solvable in one lookup
[12:46:26] <Isotopp> many equalities, one range
[12:46:50] <Isotopp> a range is any inequality, a<10, b like "prefix%", BETWEEN a AND b and also ORDER BY
[12:47:05] <funnybunny> Right
[12:47:07] <Isotopp> also, WHERE x IN (const, const, const)
[12:47:08] <funnybunny> So I have 2 ranges
[12:47:11] <Isotopp> yyes
[12:47:20] <funnybunny> Thus I will not benefit from a composite index
[12:47:23] <Isotopp> your order by will not profit from the index.
[12:47:43] <funnybunny> Well, that matches up with my assumption from first principles
[12:47:44] <Isotopp> that means, the query will be listed in EXPLAIN as 'using temporary'
[12:47:51] <Isotopp> it will not be resolved streaming
[12:48:04] <Isotopp> it will materialized the result set in the server in an implicit temporary table
[12:48:07] <Isotopp> that table will be sorted
[12:48:15] <Isotopp> the sorted result table will be emitted
[12:48:33] <funnybunny> But am I right that if I want to cache the records in multiple sorted orders, I need an index for each order I want?
[12:48:51] <Isotopp> define 'cache' ?
[12:49:07] <Isotopp> if you have a query WHERE a= 10 ORDER BY b, this will profit from INDEX(a,b)
[12:49:13] <funnybunny> Literally use some of the DB memory or disk to hold all the records in a particular sorted order
[12:49:25] <Isotopp> if you have a query WHERE a= 10 ORDER BY c, you would also need INDEX(a,c) to avoid temporary files
[12:50:00] <Isotopp> please check otu https://www.slideshare.net/isotopp/explain-explain-no-presenter-notes
[12:50:11] <Isotopp> or https://www.slideshare.net/isotopp/explaining-explain with presenter notes
[12:50:32] <funnybunny> Should I really care if it has to look up the original record. I mainly care about it not having to sort things again
[12:51:06] <Isotopp> SELECT a,c FROM t WHERE a=10 ORDER BY c. Assume INDEX(a,c) exists.
[12:51:06] <funnybunny> So if I can cache ORDER BY c, but I need to look up the record to get 'a' that seems OK
[12:51:21] <Isotopp> The query can be resolved by going to the index alone, this is called a covering index.
[12:51:38] <funnybunny> But I generally want to pull almost every column from the record
[12:51:38] <Isotopp> it is also sorted in the index in the order demanded by the query, so no sorting
[12:52:00] <Isotopp> SELECT a,b,c FROM t WHERE a = 10 ORDER BY c
[12:52:10] <Isotopp> this is resolvable without sorting
[12:52:15] <Isotopp> but the column b is not present in the index
[12:52:18] <Isotopp> the index is not covering
[12:52:32] <Isotopp> it needs to take the row pointer and visit the row of origin, extract b to resolve
[12:52:41] <funnybunny> Yeah, but still no sorting takes place right
[12:52:50] <funnybunny> Just a lookup
[12:52:56] <Isotopp> precisely
[12:53:07] <Isotopp> it wil still be a lot of disk seeks
[12:53:11] <funnybunny> I don't think that's a big deal, especially if my DB is in RAM
[12:53:26] <Isotopp> because the data in innodb is physically stored in primary key order, which is not c order
[12:53:41] <Isotopp> when you are using NVME storage OR the data is in memory, it matters not
[12:53:53] <Isotopp> well, it does, but that is complaining at a very high level
[12:53:54] <funnybunny> What I actually have is like SELECT a,b,c,d,e,f,g,h,i,j,k,l ORDER by c
[12:54:11] <funnybunny> It seems a little silly to make a composite index of all those columns
[12:54:22] <Isotopp> SELECT data FROM t WHERE id = const is resolved by our databases in 0.15ms (150µs) or less
[12:54:31] <Isotopp> funnybunny: yes
[12:54:45] <funnybunny> OK
[12:55:01] <Isotopp> SELECT * FROM t ORDER BY c will be reading the entire table in c order
[12:55:10] <Isotopp> so either giant sort (no index exists at all)
[12:55:41] <Isotopp> or index(c), so the database generates pairs of (c, row pointer) in the index, then read all of c and using the row pointer each time
[12:55:50] <Isotopp> now
[12:55:54] <Isotopp> 2 interesting facts:
[12:56:08] <Isotopp> 1. the primary key is special. it defines the physical order or rows in the table
[12:56:30] <Isotopp> the primary key in innodb is a B+ tree, that is, the data pages are part of the b-tree that is defined by the primary key
[12:56:41] <Isotopp> a b-tree where the leaf nodes are data pages
[12:56:55] <Isotopp> (a b* tree also has data in the tree, innodb does not use b*)
[12:57:26] <Isotopp> 2. all non-primary indexes are called secondary. the row pointer innodb uses in secondary keys is the primary key, the id
[12:57:34] <Isotopp> so index(c) is actually pairs (c, id)
[12:57:47] <Isotopp> the database knows that. select id from t where c= 10 will be covering
[12:58:22] <funnybunny> That makes sense
[12:58:37] <Isotopp> it makes changes of any values that are part of the pk very extremely expensive
[12:58:47] <Isotopp> update t set id = 10 where id =20
[12:58:55] <Isotopp> physically moves the row in the tree
[12:59:06] <Isotopp> NEVER change a value that is part of a PK
[12:59:15] <funnybunny> Yeah, I didn't design much of the DB, but we don't use the PK id for anything in our application logic I think
[12:59:21] <Isotopp> this is instadeath, especially in Aurora.
[12:59:38] <funnybunny> I assume the engineer who designed it knew that
[12:59:45] <Isotopp> in aurora, you pay for IO. updating a pk is 2 IOs minimum
[12:59:53] <Isotopp> bwhahaha.
[13:00:02] <funnybunny> lol
[13:00:09] <Isotopp> I made a very well paid living finding this kind of assumptions not being honored for many years.
[13:00:35] <funnybunny> Wow, I would be lucky to get passed on any savings at my job for stuff like that
[13:01:45] <Isotopp> so you said SELECT a, b, c, ... ORDER BY c
[13:01:48] <Isotopp> no where clause.
[13:02:03] <funnybunny> Yeah, so there might be a few where clauses sometimes
[13:02:09] <Isotopp> queries that have 0 or more joins and scan one table completely are called reporting queries or dumps
[13:02:21] <Isotopp> there is little that can be done to optimize these.
[13:02:33] <funnybunny> No, it's not really that
[13:02:36] <Isotopp> SELECT a, b, c, ... FROM t ORDER BY c
[13:03:02] <Isotopp> SELECT a,b,c, ... FROM t LEFT JOIN expansion1 ON t.code = expansion1.code_lookup ORDER BY c
[13:03:05] <Isotopp> and so on
[13:03:06] <funnybunny> So like the page is a search, and if you don't put any advanced search params in there will be no where clauses,  but if there are there would be
[13:03:35] <Isotopp> this assumes t.code is an integer like 1, 2, 3 and expansion.code_lookup turns 1 onto red, 2 into yellow, 3 into green or similar
[13:03:41] <funnybunny> But I assume that a record lookup has to take place anyway since we are still getting other columns unless every single column were in the index
[13:04:14] <Isotopp> this scans t in full and looks up a number of code expansions to turn encoded values in t into their string values using th expansion lookup tables
[13:05:05] <Isotopp> yes, you will have lookups, even if an index is used.
[13:05:20] <Isotopp> since the index will only be on the columns you index and the primary key as the tail part
[13:05:24] <Isotopp> (the row pointer)
[13:05:30] <funnybunny> I'm pretty sure one engineer just added like a bunch of columns to a composite index for no reason hoping it would go faster
[13:06:24] <Isotopp> probably
[13:06:39] <funnybunny> Like he saw it made explain say "using index" so he added it? I'm not sure
[13:06:43] <Isotopp> this kind of search job is not actually a primary showcase application for a relational database, and especially not mysql
[13:07:07] <Isotopp> a lot of people take the data from the table as a k/v pair, dump it into elastic and use this to search
[13:07:15] <Isotopp> get back id value, use this to go into the mysql
[13:07:19] <funnybunny> Yeah
[13:07:26] <Isotopp> this assumes the search finds few rows, not dumping all of the mysql
[13:07:35] <funnybunny> We do use Elastic for other searches
[13:07:43] <funnybunny> I think the reason we don't for this one is
[13:07:49] <funnybunny> We want the results to be realtime
[13:07:50] <Isotopp> but this search "use a value in all columns" is not a good mysql search
[13:08:10] <funnybunny> And with Elastic you would have to constantly rebuild indices
[13:08:14] <Isotopp> "realtime", only 30 GB of data. Is the data changing a lot?
[13:08:19] <funnybunny> Yeah
[13:08:21] <Isotopp> I see
[13:08:30] <funnybunny> People edit stuff a lot
[13:08:40] <Isotopp> Where I work we have this kind of data, very volatile, >100 changes per second
[13:08:57] <Isotopp> we actually wrote a java in memory database with a lot of business logic in it to make this fast
[13:09:00] <funnybunny> People are also deleting stuff and adding stuff a lot
[13:09:03] <Isotopp> but this is the core of the biz
[13:09:25] <Isotopp> domas dealt with this with mysql, when he was admin at wikipedia.
[13:10:06] <Isotopp> https://dom.as/2007/01/26/mysql-covering-index-performance/ (article from 2007)
[13:11:10] <funnybunny> How is a java in memory database better than an SQL database where everything is in memory?
[13:12:10] <funnybunny> I guess there is just overhead from SQL being transactional
[13:12:36] <funnybunny> BTW
[13:13:00] <funnybunny> Our DB is 30 GB but the number of searchable records on any given day is only like 10000
[13:13:10] <Isotopp> it is more complicated, we store prices of hotel rooms in a database, optiimized for writing
[13:13:12] <funnybunny> Because the rest are outdated
[13:13:26] <funnybunny> So we could probably put them in a non-transactional database for searching
[13:13:28] <Isotopp> we see many hundred changes per second, whenever a room is listed for a specific date, or when it is sold.
[13:14:00] <Isotopp> we have an engine that reads the change stream for the database, and turns it into a second systems input, optimized for reading. it cannot always keep up, but it tries
[13:14:11] <Isotopp> the second system, optimized for reading, uses a LOT more space,
[13:14:17] <Isotopp> and is the in-memory java database.
[13:14:23] <funnybunny> Oh, OK
[13:14:33] *** Quits: PeGaSuS (WeeChat@user/pegasus) (Remote host closed the connection)
[13:14:44] <Isotopp> it also knows a lot about the queries, and about business logic for hotel prices and room types, and conditions (conditional pricing)
[13:14:48] <funnybunny> What is the lag?
[13:15:00] <funnybunny> Between the read and write databases
[13:15:08] <Isotopp> the lag is variabel, during peaks several seconds, during off-peak it keeps up
[13:15:12] *** Joins: PeGaSuS (WeeChat@user/pegasus)
[13:15:16] <funnybunny> Hm
[13:15:24] <Isotopp> the read-optimized database is using for looking (searching)
[13:15:32] <Isotopp> the write-optimized database is also read, but onl
[13:15:37] <Isotopp> y during bookign
[13:15:51] <funnybunny> I feel like for our site we have so few records, like we're not that big, it matters fuck all what we do
[13:15:52] <Isotopp> at some point in the funnel a customer is switched from look to book
[13:16:27] <funnybunny> But I have to make sure anyway
[13:16:31] <funnybunny> So
[13:16:31] <Isotopp> otoh, in our system the stuff is running on bare metal blades, so the price of a system is constant, no matter if we install memcached, elastic or mysql
[13:17:05] <funnybunny> We have like 10000 records live at anytime and the rest go out of date never to be seen again right
[13:17:08] <Isotopp> if this were running in AWS (at one point it will) it would have to be architected very differently because the price for 1 vcpu and 4 GB of memory depends very much on the service you are running
[13:17:25] <funnybunny> These 10000 records are getting edited, deleted, inserted actively
[13:17:30] <Isotopp> funnybunny: in that case, why not a file in a ramdisk on a server with 64 GB memory?
[13:17:41] <Isotopp> funnybunny: that sounds a lot like wikipedia?
[13:18:24] <funnybunny> Well, I said never to be seen again but
[13:18:45] <funnybunny> Actually they are just never to be seen again from search, but users can still view past records they uploaded
[13:18:53] <funnybunny> So the first thing I want to do is
[13:19:05] <funnybunny> Every day move all the old records to another table
[13:19:16] <funnybunny> And keep only the 10000 new records in one table
[13:19:43] <Isotopp> makes a lot of sense.
[13:19:48] <funnybunny> Then I'm pretty sure I can do the dumbest SQL imaginable and it will still run fine for 3000 users using the site at the same time
[13:20:01] <funnybunny> What we want to do is
[13:20:04] <Isotopp> mysql is a transactional database, and it makes sense to keep only active records that are being transacted on in a table
[13:20:05] <funnybunny> We already have the search
[13:20:29] <funnybunny> We have a dropdown that has predefined sorts, sort by X, sort by Y
[13:20:45] <funnybunny> But the data is presented tabularly and ideally would like to be able to click any column and sort by that column
[13:20:55] <funnybunny> Maybe even 2 or n columns
[13:21:21] <funnybunny> I think with 10000 records it should be no prob, but I have to simulate this before we ship such a feature to make sure our DBs can handle it
[13:21:59] <funnybunny> So first I'm just going to do the table moving thing and then simulate with like no indexes or like very simple indexes and see how that goes
[13:22:10] <funnybunny> Then I will see if I can optimize the query further
[13:22:25] <Isotopp> if you show table status like "the table"
[13:22:29] <Isotopp> what is your average row length
[13:22:38] <Isotopp> (the question only makes sense on tables with many rows)
[13:22:42] <funnybunny> What do you mean?
[13:22:55] <funnybunny> Is that a command?
[13:23:02] <funnybunny> Do you mean like how many bytes in a row?
[13:23:03] *** Joins: ejjfunky (~ejjfunky@125.164.16.115)
[13:23:11] <Isotopp> SHOW TABLE STATUS LIKE '...'\G
[13:23:14] <Isotopp> yes
[13:23:30] <Isotopp> or SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '...';
[13:23:38] <Isotopp> there is a column that says average row len
[13:23:39] <funnybunny> OK let me try
[13:24:03] <Isotopp> AVG_ROW_LENGTH in I_S.TABLES, if you want that
[13:25:35] <Isotopp> AVG_ROW_LENGTH is calculated by data_length/table_rows, so it will be way too large for small tables.
[13:25:46] <funnybunny> 312
[13:25:57] <Isotopp> around 30ish rows per pagge
[13:25:59] <Isotopp> page
[13:26:04] <funnybunny> This is for all the records
[13:26:10] <funnybunny> For 10000 active records...
[13:26:22] <Isotopp> arund 300 ish pages for 10k rows
[13:26:30] <funnybunny> 688
[13:26:48] <funnybunny> I moved the active records to another table to test the performance of a small tablr
[13:26:51] <funnybunny> *table
[13:26:54] <funnybunny> Like in a test DB
[13:26:55] <Isotopp> 8000 kb (around 8 MB) for 10.000 rows assuming 300 byte/record
[13:29:38] <funnybunny> That's really small
[13:29:47] *** Quits: PeGaSuS (WeeChat@user/pegasus) (Remote host closed the connection)
[13:30:24] *** Quits: masber (~masber@213.55.226.100) (Quit: Client closed)
[13:30:59] <Isotopp> nothing you do with indexes matters much on a thing this tiny in RAM
[13:31:55] *** Joins: PeGaSuS (WeeChat@user/pegasus)
[13:32:26] <funnybunny> Yeah
[13:32:51] <funnybunny> I still haven't thought about how to move stuff out of this table...
[13:33:11] <funnybunny> Like when a user edits a record, whether it's in the active table or not it should just work
[13:34:57] <funnybunny> I guess the edit could just send update statements to both tables. I haven't really thought about the application logic yet
[13:35:08] <funnybunny> Anyway, I'm gonna go. Enough work for today
[13:35:18] <funnybunny> Ty for talking, I might be around later
[13:36:58] *** Quits: tosaraja (~quassel@192.89.120.58) (Read error: Connection reset by peer)
[13:37:19] *** Joins: tosaraja (~quassel@192.89.120.58)
[13:58:40] *** Joins: Guest3952 (~vit@chello085216193138.chello.sk)
[13:59:00] *** Quits: funnybunny (~funnybunn@user/funnybunny) (Ping timeout: 256 seconds)
[14:05:01] *** Quits: zumba_addict (~zumba_add@c-71-194-58-34.hsd1.il.comcast.net) (Quit: Client closed)
[14:24:08] *** Joins: MrBech (~MrBech@2a09:bac0:65::82b:e04d)
[14:24:44] *** Quits: MrBech (~MrBech@2a09:bac0:65::82b:e04d) (Client Quit)
[15:11:48] *** Quits: Sleepy63 (~Sleepy63@user/sleepy63) (Ping timeout: 240 seconds)
[15:13:07] *** Joins: Sleepy63 (~Sleepy63@user/sleepy63)
[15:19:04] *** Quits: podeni (~podeni@188.27.129.34) (Ping timeout: 272 seconds)
[15:52:01] *** Quits: rvalue (~rvalue@user/rvalue) (Read error: Connection reset by peer)
[15:52:14] *** Joins: rvalue (~rvalue@user/rvalue)
[16:00:18] *** Joins: vit (~vit@chello085216193138.chello.sk)
[16:00:42] *** vit is now known as Guest5584
[16:00:48] *** Joins: podeni (~podeni@188.27.129.34)
[16:03:55] *** Quits: Guest3952 (~vit@chello085216193138.chello.sk) (Ping timeout: 256 seconds)
[16:05:48] *** Joins: sultand (~sultand@office.rackhosting.com)
[16:34:07] <thumbs> KnowledgeShark: try asking the python folks instead
[16:42:02] <Isotopp> KnowledgeShark: the term array has no meaning in python. it is likely a thing with [], that is the class list in python.
[16:42:22] <Isotopp> KnowledgeShark: an empty list, [] or list(), compares to False in Python
[16:42:25] <thumbs> oh, Isotopp is a python buff
[16:42:38] <Isotopp> KnowledgeShark:
[16:42:40] <Isotopp> if mylist:
[16:42:50] <Isotopp>   the_list_isnt_empty()
[16:42:56] <Isotopp> else:
[16:43:00] <Isotopp>   list_is_empty()
[16:43:13] <Naktibalda> can python do a oneliner here?
[16:43:18] <Isotopp> No
[16:43:41] <Isotopp> Python has no semicolons, and whitespace/indentation has the semantic of {} in c-like languages
[16:45:23] <Isotopp> KnowledgeShark: Something with a comma is a tuple, which is immutable. Often () are required to make the parser recoginize things: mytuple = ( 1, 2 )
[16:45:44] <Isotopp> Something with [] is a list: mylist = [ 1, 2]. It it mutable, you can append, delete, pop and so on
[16:46:02] <Isotopp> Something with {} is a dict, mydict = { "a": "b", "c": "d" }
[16:46:16] <Isotopp> Python allows trailing commas is lists and dicts, so [ 1, 2, ]
[16:46:26] <Isotopp> this makes such enumerations easy to extend.
[16:46:33] <thumbs> my OCD would hate that
[16:46:54] <Isotopp> Imagine the [ and ] on separate lines, and the items all have the form "....",
[16:47:03] <Isotopp> you ocd will like that
[16:47:31] <Isotopp> In the end, nobody formats python by hand. black (the python formatter without options) or the jetbrains IDE do that
[16:48:04] <thumbs> I suppose most languages are getting like this, yes. Hand-writing code is a thing of the past
[16:49:23] <Isotopp> Tuples also appear on the LHS of an assignment: a, b = somefunction()
[16:49:32] <Isotopp> somefunction() will somewhere return 1,2
[16:49:49] <Isotopp> this also works with lists, and using * with nonmatching item counts:
[16:49:55] <Isotopp> a, *b = [ 1, 2, 3 ]
[16:49:59] <Isotopp> a is 1, b is 2,3
[16:50:15] <thumbs> I'll take my perl5 code over that any day
[16:50:38] <Isotopp> perl5 has about zero integration and tooling support (I work at Booking, we use a lot of Perl)
[16:50:58] <Isotopp> X-Ray, Sonarcube, initially no Prodfiler support, no Jetbrains
[16:51:14] <Isotopp> the language is not easily parseable, and ambigous.
[16:51:29] <Isotopp> You need Python to parse it properly (The Lark parser can parse Perl)
[17:00:15] <steve> Does the command line tool myisamchk benefit more from optimized CPU or optimized memory?
[17:01:04] <steve> ie: if I was going to use an amazon ec2 instance to run the command and rebuild an index, should I go for a C (compute) type instance or an R (memory) type instance
[17:01:16] <steve> Or maybe even disk-optimized?
[17:01:47] <steve> I'm sure the difference will be small, but if I can knock 2-3 hours off an 18 hour process, I'm all for it
[17:02:53] <thumbs> steve: myisamchk is irrelevant for any modern deployment
[17:03:27] <steve> I can appreciate that, but I'm kinda stuck with this specific task
[17:03:41] <thumbs> so you have legacy MyISAM tables?
[17:03:46] <steve> :)
[17:04:08] <steve> The story of my life
[17:04:36] <steve> If I was doing this from the ground up, I'd do something innodb based
[17:07:23] <steve> My boss (not an IT guy) decided that it needed to be myisam because, "if we ever delete it, we want to reclaim the space. you can't do that with innodb"
[17:07:37] <steve> I know that's not necessarily true
[17:07:57] <steve> But now I'm stuck with a 1tb MyISAM table
[17:08:07] <steve> And I can't even blame my stupid boss because he's dead
[17:08:28] <steve> Well, I could blame him, but it would fall on deaf ears
[17:10:08] *** Quits: rvalue (~rvalue@user/rvalue) (Remote host closed the connection)
[17:10:21] *** Joins: rvalue (~rvalue@user/rvalue)
[17:18:04] <Isotopp> steve: if you need to run myisamchk, you are a decade behind. why are you using myisam tables?
[17:18:38] <Isotopp> steve: in general, myisamchk and other utilities tend to be io bound when used at scale. you will need memory to build or rebuild indexes, and iops
[17:18:57] <Isotopp> steve: you will also need to provide a config to actually use these resources
[17:20:03] <Isotopp> in general, if you meet a cpu bound database, there is something seriously wrong
[17:21:46] <Isotopp> one instance of myisamchk is single threaded and will use at most a single core
[17:22:27] <thumbs> steve: also, 18 hours is optimistic
[17:22:46] <Isotopp> the mysql server can also run REPAIR TABLE, and also do so automatically on server start, doing the equivalent work. it supposedly is in theory capable of doing that multithreaded and in parallel, but last time I checked that code was broken and you had to config that to single thread
[17:23:09] <Isotopp> so if you have multiple tables in mysam that need repair, you will neeed to orchestrate that somehow manually, eg using gnu parallel or something
[17:23:14] <Isotopp> none of that is safe for production
[17:23:42] <thumbs> his setup is not safe for production, period
[17:25:19] <thumbs> so the decision to go with MyISAM to reclaim space (which is the cheapest thing to scale) was ludicrous
[17:25:35] <Isotopp> the myisamchk will be rebuilding the index, if the index is damaged. the MYI file contains ALL the indexes, how large is your MYI file?
[17:25:58] <Isotopp> thumbs: yes. also, innodb reclaims free pages
[17:26:15] <steve> ~200gb MYI file
[17:26:22] <thumbs> hah
[17:26:35] <steve> I've repaired larger before
[17:26:54] <Isotopp> steve: myisamchk will read the myd file front to back, and rebuild the indexes, to recreate the myi file. a lot of memory, and a myisamchk config that allows to use that memory
[17:27:20] <Isotopp> https://dev.mysql.com/doc/refman/8.0/en/myisamchk-memory.html
[17:27:33] <steve> Thanks, Isotopp
[17:27:48] <steve> I assumed it was mostly a memory thing
[17:28:02] <Isotopp> note how that page still deals in megabytes and that the default are single digit megabytes
[17:28:11] <Isotopp> that is how old this is
[17:28:36] <steve> Oh trust me, I know...
[17:29:05] <steve> Now that the boss is gone, I'm starting a campaign to convert everything to something more modern
[17:29:27] <Isotopp> where i work, we did change to innodbb more than a decade ago by runnign ALTER TABLE ENGINE=InnoDB, and then where required batch insert/update/delete by creating explicit transactions in around 1000 row batches
[17:29:51] <Isotopp> that is, find those loops, add a counter and if i%1000 == 0, commit, start transaction read write.
[17:30:02] <Isotopp> put start transaction read write before the loop, and commit behind
[17:30:10] <steve> That's almost exactly what I was planning on doing, hah
[17:30:17] <Isotopp> that way, you have wrapped a bulk update into a set of nice 1000 rows txns
[17:30:36] <Isotopp> in our case that was fit for a 15bn USD turnover company, so you should be fine
[17:31:13] <steve> How'd you deal with indexes? I know that inserting a lot into an indexed table is a slow operation
[17:31:25] <steve> Did you just add them after the fact?
[17:32:31] <thumbs> that is faster for large number of rows, yes
[17:32:35] <Isotopp> for bulk loads, we load into a table with only a primary key defined, then do a single alter table that creates all SK
[17:33:05] <Isotopp> in innodb, having a table with no defined primary key is a big problem, make sure all tables have a PK
[17:33:33] <Isotopp> innodb profits from the loaded data to be sorted in pk order
[17:33:35] <Isotopp> a lot
[17:34:05] <steve> Good to know!
[17:35:19] <Isotopp> innodb data is physically stored in primary key order on disk. if the loaded data is in pk order, the rightmost page is always filled, split and then the btree would have to be rebalanced. this is the worst case scenario for a btree. innodb knows that and has extra code (the "change buffer") to detect and handle this, turning it into a bulk load best case.
[17:36:15] <Isotopp> that also makes auto_increment as a pk a very good choice. a_i sorts data in insert order (temporal order), and usually new data is also hot data. that creates a tiny working set and makes mysql use a lot less memory.
[17:36:53] <Isotopp> and finally optimize table, alter table t engine =innodb or bulk loads and insert... select ... operate on existing data in pk order, so the change buffer stuff pays off all of the time, big time.
[17:38:49] <steve> Any considerations for a database that's entirely read oriented? The only time these tables would ever be written to would be during the initial load
[17:40:35] <Isotopp> you can investigate and experiment with innodb compression
[17:40:42] <Isotopp> ON A SECOND INSTANCE
[17:40:54] <Isotopp> and if that suits you, go ahead
[17:42:00] <Isotopp> https://blog.koehntopp.info/2021/09/09/mysql-two-kinds-of-compression.html and https://blog.koehntopp.info/2021/09/14/mysql-page-compression-revisited.html. not having writes will help
[17:58:02] *** Quits: yano (~yano@ircpuzzles/staff/yano) (Quit: WeeChat, the better IRC client, https://weechat.org/)
[18:00:22] *** Joins: yano (~yano@ircpuzzles/staff/yano)
[18:00:53] *** Quits: yano (~yano@ircpuzzles/staff/yano) (Client Quit)
[18:01:20] *** Joins: yano (~yano@ircpuzzles/staff/yano)
[18:01:39] *** Quits: yano (~yano@ircpuzzles/staff/yano) (Client Quit)
[18:04:13] *** Joins: yano (~yano@ircpuzzles/staff/yano)
[18:05:43] <steve> Isotopp: this is so helpful - thank you!
[18:20:42] *** Joins: mooz (~none@173.208.98.70)
[18:43:31] *** Joins: bytestream (~bytestrea@user/bytestream)
[18:55:50] *** Joins: passage (~passage@user/passage)
[19:00:28] *** Quits: foobaz212 (~foobaz212@152.37.103.80) (Quit: Client closed)
[19:00:44] *** Joins: foobaz212 (~foobaz212@152.37.103.80)
[19:57:12] *** Quits: foobaz212 (~foobaz212@152.37.103.80) (Quit: Client closed)
[19:57:28] *** Joins: foobaz212 (~foobaz212@152.37.103.80)
[19:59:04] *** Quits: sultand (~sultand@office.rackhosting.com) (Quit: Client closed)
[20:33:26] *** Joins: yuesbeez (uid458354@id-458354.tinside.irccloud.com)
[20:42:34] *** Quits: foobaz212 (~foobaz212@152.37.103.80) (Quit: Client closed)
[20:42:52] *** Joins: foobaz212 (~foobaz212@152.37.103.80)
[21:19:23] *** Joins: DrowningElysium (uid190788@user/drowningelysium)
[21:47:55] *** Quits: ejjfunky (~ejjfunky@125.164.16.115) (Read error: Connection reset by peer)
[22:01:15] *** Quits: wolfshappen (~waff@irc.furworks.de) (Ping timeout: 256 seconds)
[22:01:53] *** Joins: wolfshappen (~waff@irc.furworks.de)
[22:04:57] *** Quits: jrenken (~jrenken@user/jrenken) (Read error: Connection reset by peer)
[22:05:15] *** Joins: jrenken (~jrenken@user/jrenken)
[22:10:06] *** Quits: podeni (~podeni@188.27.129.34) (Ping timeout: 272 seconds)
[22:25:25] *** Quits: foobaz212 (~foobaz212@152.37.103.80) (Quit: Client closed)
[22:25:43] *** Joins: foobaz212 (~foobaz212@152.37.103.80)
[22:36:34] *** Joins: Rockwood (~Rocky@user/rocky)
[22:36:36] <Rockwood> hi
[22:37:34] <Rockwood> i ve question about DB structure for roles and permissions
[22:47:38] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[23:01:34] <[sr]> Rockwood: say it, and if anyone know how to help they'll
[23:01:43] <[sr]> *knows
[23:03:10] *** Quits: Rockwood (~Rocky@user/rocky) (Quit: The Time is Over)
[23:12:06] *** Joins: Hokedli (~lasliedv@gateway/tor-sasl/hokedli)
[23:22:36] *** Quits: Hokedli (~lasliedv@gateway/tor-sasl/hokedli) (Remote host closed the connection)
[23:25:47] <mooz> hi, is there a way to run a count on how many binary logs there are on the system from the cli?
[23:32:27] <[sr]> ls -l bin.log*|wc -l
[23:32:52] *** Quits: shibboleth (~shibbolet@user/shibboleth) (Remote host closed the connection)
[23:33:17] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[23:34:15] <mooz> [sr]: yeah, was talking specifically from the mysql cli, since I don't have access to the linux part
[23:34:29] *** Joins: ghostlines (~ghostline@ip-131-161-085-155.v4.isp.telem.sx)
[23:40:08] *** Quits: shibboleth (~shibbolet@user/shibboleth) (Remote host closed the connection)
[23:42:35] <ghostlines> howdy, after importing a database from MySQL version 5.6 to 8.0 I noticed a varbinary column has leading 0x3 notation is this normal?
[23:43:10] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[23:43:46] *** Quits: shibboleth (~shibbolet@user/shibboleth) (Remote host closed the connection)
[23:45:11] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[23:46:13] <thumbs> mooz: check the expire_log_days variable
[23:46:27] <thumbs> ghostlines: that isn't a supported upgrade path
[23:49:19] <ghostlines> @thumbs thanks - I'll downgrade
[23:49:56] <mooz> thumbs: thanks.. I'm a idiot ... what I wanted was mysql --login-path=host -Bs -h ... -e "show binary logs;" | wc -l
[23:58:59] <ghostlines> the db dump was done on an instance running v5.6. Should first upgrade to v5.7 > v8.0 then create a new dump before I can load it on v8.0?
