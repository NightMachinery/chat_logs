[00:00:32] *** Quits: epony (epony@user/epony) (Ping timeout: 240 seconds)
[00:02:00] *** Quits: OlCe (~user@amontsouris-156-1-23-81.w92-151.abo.wanadoo.fr) (Ping timeout: 256 seconds)
[00:10:45] *** Joins: edgar-rft (~edgar-rft@ip-109-193-249-223.um39.pools.vodafone-ip.de)
[00:12:31] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[00:14:45] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[00:24:03] <moon-child> https://lwn.net/Articles/869140/
[00:24:03] -ixelp- x86 User Interrupts support [LWN.net]
[00:29:01] <sm2n> wonder if amd will ever get it or if it's going to be like the intel specific profiling stuff
[00:34:01] *** Quits: rogersm (~rogersm@90.166.180.250) (Quit: Leaving...)
[00:39:17] *** Quits: jeosol (~jeosol@user/jeosol) (Quit: Client closed)
[00:42:36] <Aurora_v_kosmose> Do non-local exits negatively affect performance? Or is the case bringing that question to mind simply complaining about another language's design?
[00:43:06] <White_Flame> I presume both taking the exit and setting up the scope has costs
[00:43:31] <shka> Aurora_v_kosmose: not really, but setting up the scope is a small overhead
[00:43:40] <White_Flame> I'm not aware of any lisp implementation that does the "zero overhead exception" model of resolving everything at throw time via the PC
[00:43:58] <Aurora_v_kosmose> I see.
[00:44:03] <shka> thing is, without the setup overhead you are in the C++ situation
[00:44:10] <shka> where exceptions are free
[00:44:17] <shka> UNLESS YOU ARE ACTUALLY USING IT
[00:44:18] * Aurora_v_kosmose refrains from posting Lisp superiority in the related thread
[00:44:38] <shka> and when you are using exceptions, they are very expensive
[00:44:48] <shka> and because C++, useless in practice
[00:46:03] <shka> C++ is a crazy outlier here
[00:49:19] <moon-child> I would expect exceptions to be appreciably cheaper in practice in lisp than c++
[00:49:47] <moon-child> because lisp does not use scoped destruction for memory management
[00:51:01] *** Quits: cosimone (~user@93-47-230-95.ip115.fastwebnet.it) (Quit: ERC (IRC client for Emacs 27.1))
[00:52:06] <White_Flame> well, every with-* scope, unwind-protect, catch, etc has its own cleanup management
[00:52:32] <White_Flame> unwinding the dynamic scope in any form needs to resolve all of that
[00:52:49] <neominimum> re: user interrupts. does that mean we will get hardware timer based interrupts delivered to userspace programs?
[00:53:04] <White_Flame> but yeah, C++ destrucutors are probably way more often called in such circumstances than the lisp unwinding
[00:54:26] <neominimum> so we could achieve pre-emptive multitasking without the rigmarole that Erlang went through to fake it.
[00:54:40] *** Joins: kevingal (~quassel@2a02:8084:4140:f300:b4c5:b7b4:fceb:af93)
[00:55:14] <White_Flame> uh, what's wrong with threads?
[00:55:45] <White_Flame> and if you want to timeslice things, you can pause/resume threads from the outside anyway
[00:55:55] <phoe> holy shit
[00:55:58] <neominimum> they give no control over how to schedule execution
[00:56:01] <phoe> look at this madlad
[00:56:03] <phoe> https://github.com/quicklisp/quicklisp-projects/issues/2135
[00:56:04] -ixelp- Please add MK:DEFSYSTEM to quicklisp. · Issue #2135 · quicklisp/quicklisp-projects · GitHub
[00:56:11] <moon-child> White_Flame: yes, just saying I expect there to be way fewer of those in lisp than in c++
[00:56:27] <moon-child> phoe: haha
[00:57:01] <White_Flame> neominimum: what control do you seek?
[00:57:07] <phoe> aaahahahaha https://gitlab.common-lisp.net/mantoniotti/mk-defsystem/-/blob/master/defsystem.lisp#L2567-2572
[00:57:08] -ixelp- defsystem.lisp · master · Marco Antoniotti / MK-DEFSYSTEM · GitLab
[00:58:04] <White_Flame> he's not technically wrong
[00:58:09] <White_Flame> load ordering can be a pain in the butt sometimes
[00:58:10] <phoe> yes
[00:58:15] <phoe> the wording though
[00:59:05] <hayley> "they give no control over how to schedule execution" Good.
[01:03:38] <sm2n> lol
[01:03:38] <hayley> (Alternately, there are things called "mutexes" and "semaphores"...)
[01:03:46] <sm2n> I heard you liked build systems...
[01:03:58] <White_Flame> as well as affinity & priority
[01:04:29] <sm2n> neominimum: AIUI not in the current state
[01:05:09] <neominimum> It would be possible to have very lightweight proceses that need to operate concurrently numbering in the millions without having to pay the associated memory costs or processing overheads of spinning up huge amounts of threads or managing thread pools. You can pin a scheduler thread on each core of the processor and have them manage the execution in what ever way suits whether that is round robin style or first come first served etc.
[01:05:10] <sm2n> the interrupts are sent from one process to another, and if you wanted to write your own scheduler, it'd have to have to pin a core, I think
[01:05:48] <White_Flame> well, what's "possible" is infinite, and you can't tune to that
[01:06:05] <White_Flame> if there's some specific problem at hand, then it can have specific controls suited to its nuances
[01:07:03] <White_Flame> and if you've got millions of tasks in flight, managing and orchestrating them is going to be heavyweight, and require a lot of code
[01:07:10] <White_Flame> if it's to be performant
[01:08:07] <White_Flame> it's usually not a great idea in a situation like that for all of them to be long-running processes that can have unpredictably blocking I/O or whatever
[01:08:24] <White_Flame> but rather have some sort of data checkpoints that they fill in on each run
[01:08:34] <sm2n> White_Flame: Joe says hi
[01:08:43] <White_Flame> hello, mike
[01:08:56] <White_Flame> and yeah, BEAM is a heavyweight thing wiht a lot of code
[01:09:45] <White_Flame> I'm still a fan of the separated heaps, and wish that that were an option in many languages, but it's something that's hard to make it not all-or-none
[01:10:21] <White_Flame> I presume HiPE has a lot of code injection for checkpoints and whatnot
[01:10:53] *** Quits: shka (~herr@109.231.0.226) (Ping timeout: 256 seconds)
[01:13:58] *** Quits: iamFIREcracker (~iamFIREcr@user/iamfirecracker) (Ping timeout: 256 seconds)
[01:14:26] *** Joins: iamFIREcracker (~iamFIREcr@user/iamfirecracker)
[01:17:28] <hayley> MIT announces APL 2 https://news.mit.edu/2022/new-programming-language-high-performance-computers-0207
[01:17:29] -ixelp- A new programming language for high-performance computers | MIT News | Massachusetts Institute of Technology
[01:17:45] <Aurora_v_kosmose> A second one?
[01:18:58] <neominimum> White_Flame: all good points. Typically the only schedulers available to implement in userspace are cooperative in nature so the potential to have preemptive scheduling like the kernel has with less work than went into the development of BEAM would be magnificent. 
[01:19:08] *** moon-child is now known as bowl-of-petunias
[01:19:14] <bowl-of-petunias> Oh, no, not again.
[01:19:18] *** bowl-of-petunias is now known as moon-child
[01:19:25] <Aurora_v_kosmose> moon-child: ?
[01:19:26] <sm2n> lol
[01:22:04] <Alfr> moon-child, are you crashing again?
[01:23:24] <neominimum> White_Flame: I would settle quite happily for a mostly cooperative scheduler with a single limit on how long a process can monopolise the thread of execution. Granted, even that implies a bit of work to achieve.
[01:24:09] <moon-child> neominimum: you can do it anyway with signals
[01:24:19] <moon-child> watchdog thread
[01:24:58] <sm2n> aka a low resolution timer
[01:30:21] <sm2n> hmm
[01:30:31] <sm2n> I wonder how gambit's runtime works
[01:30:47] <neominimum> moon-child: Oh, I was not aware. My only concern is that due to the kernel maintaining the schedule one could not reliably enforce strict timing constraints, as the watchdog thread is liable to be paused/resumed at the kernel's discretion.
[01:31:15] <moon-child> sure, yes.  You wouldn't use this for hard realtime
[01:31:32] <moon-child> but then, you wouldn't use _linux_ for hard realtime anyway, so
[01:32:37] <neominimum> true
[01:33:37] <sm2n> in practice, you can adjust the nice value/tune the scheduler and even pin a core if you were running such a setup in production
[01:33:44] <sm2n> you only really want soft realtime
[01:39:04] <neominimum> sm2n: Okay cool. I was looking into this stuff about a year ago and I don't know how I missed this way of achieving soft-realtime preemptive scheduling.
[01:39:31] <sm2n> you may also find https://gitlab.haskell.org/ghc/ghc/-/wikis/commentary/rts/scheduler interesting
[01:39:32] -ixelp- scheduler · Wiki · Glasgow Haskell Compiler / GHC · GitLab
[01:40:08] <sm2n> All of these break on ffi of course, but BEAM does too
[01:40:35] <sm2n> I've been meaning to look into gambit's threading, but haven't gotten around to it
[01:41:57] <moon-child> I thought haskell solves the ffi problem by using a dedicated ffi thread?
[01:42:26] <sm2n> that's what I would do, but if it does, it's not written up in the documentation
[01:42:34] <sm2n> and I have not looked at the code
[01:43:06] <sm2n> "A list of worker OS threads."
[01:43:16] <sm2n> That does seem to indicate that you are correct
[01:45:11] <sm2n> Wow, haskell RTS is NUMA-aware
[01:45:13] <sm2n> that is pretty cool
[01:45:47] <sm2n> s/haskell/ghc/ of course
[01:46:58] <sm2n> moon-child: I don't think it does
[01:47:21] <sm2n> It seems to indicate that the number of os threads (which they call capabilities) are fixed
[01:48:04] <neominimum> I honestly thought that preemptive scheduling was not possible in userspace, haha. I considered Erlang to be pseudo-preemptive in that the standard library was rewritten to (cooperatively) constantly transfer control back to a scheduler.
[01:48:24] <sm2n> it isn't, really
[01:48:35] <sm2n> pseudo- is the best you are going to get
[01:49:02] <sm2n> Also, consider that exposing high resolution timers to userspace is a really bad idea from a security perspective
[01:49:29] <moon-child> why?
[01:49:31] <neominimum> oh I see, I guess the watchdog thread method is also pseudo-preemptive
[01:49:40] <sm2n> So I doubt scheduling will ever be an unprivileged operation (SASOS when?)
[01:49:44] <sm2n> moon-child: side-channels
[01:49:49] <moon-child> I mean, you have rdtsc.  Doesn't get much higher precision than that
[01:50:31] <sm2n> hmm, I wasn't aware of that
[01:50:42] <moon-child> you mean like spectre stuff?  People have used a thread incrementing a shared counter in a loop to pull that off
[01:50:51] <sm2n> I know that that's the official rationale for perf et al being locked up
[01:50:56] <moon-child> doesn't need to be an actual timer
[01:50:57] <sm2n> yeah, I know
[01:51:06] *** Quits: clothespin (~awolven@c-73-209-95-92.hsd1.il.comcast.net) (Remote host closed the connection)
[01:51:31] <sm2n> pinning a cpu is a lot more unusual compared to if everyone was using counters all the time though
[01:51:55] <moon-child> some relevant discussion regarding webgpu, starting from this https://lists.w3.org/Archives/Public/public-gpu/2019Aug/0006.html
[01:51:56] -ixelp- Re: Some Feature requests. from Doug Moen on 2019-08-06 (public-gpu@w3.org from August 2019)
[01:53:20] *** Joins: epony (epony@user/epony)
[01:53:20] <sm2n> Perl has taint checking???
[01:53:39] <moon-child> well, it was designed by aliens
[01:54:35] <sm2n> larry wall was/is a lisper‽
[01:55:30] <moon-child> I think he said of lisp's syntax that it 'has all the visual appeal of fingernail clippings mixed with oatmeal
[01:55:32] <moon-child> '
[01:57:59] <sm2n> some alien he is
[02:01:51] <sm2n> moon-child: interesting thread, thanks
[02:02:19] <sm2n> So having locked up performance counters is security theatre?
[02:02:54] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 260 seconds)
[02:04:40] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[02:05:53] <sm2n> <https://www.kernel.org/doc/html/latest/admin-guide/perf-security.html>
[02:05:53] -ixelp- Perf events and tool security — The Linux Kernel documentation
[02:06:08] <gilberth> Hmm, when I map sRGB to linear RGB, I would need 3.7 additional bits per component.
[02:06:12] <sm2n> "Content of architectural execution context registers (e.g., RIP, RSP, RBP on x86_64), process user and kernel space memory addresses and data, content of various architectural MSRs that capture data from this category." yeesh
[02:10:04] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[02:12:02] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[02:15:31] <gilberth> Ok. More homework. Is what RENDER does gamma-corrupted? Is what FreeType does gamma-corrupted? How to tell?
[02:17:08] <gilberth> Is the underlying sampling even sound? *sigh*
[02:31:47] <hayley> https://social.xenofem.me/notice/AFqlG6e8uLpyzm3s9o
[02:33:03] <hayley> "I considered Erlang to be pseudo-preemptive in that the standard library was rewritten to (cooperatively) constantly transfer control back to a scheduler." C code? The VM handles preemption, sure, it'll switch processes every N "reductions" on BEAM instructions. But there are no instructions to count in C.
[02:33:45] <sm2n> iirc NIFs are annotated with reduction counts now
[02:33:56] <sm2n> but that doesn't help you if the C code blocks
[02:34:02] <hayley> But such a scheduler is still preemptive to me; you can't convince yourself that using scheduling as a synchronisation primitive might be a good idea.
[02:34:27] <sm2n> that's a nice definition
[02:36:41] <hayley> Though it's never a good idea IMO. A preemptive scheduler can avoid lossage w.r.t latency that can happen with cooperative scheduling, and you only win if you use one core, which will become a worse idea over time. But I've said this too many times in #lispcafe already.
[02:38:43] <hayley> But, if you limit yourself to a fraction of the performance of your machine, then sure, you can pretend yield points are a good synchronisation primitive.
[02:40:02] *** Joins: dre (~dre@2001:8003:c932:c301:880:1cdd:3f72:271d)
[02:48:26] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Remote host closed the connection)
[02:48:49] *** Joins: Aurora_v_kosmose (~LispyLigh@user/lispylights)
[03:02:54] *** Joins: OlCe (~user@amontsouris-156-1-23-81.w92-151.abo.wanadoo.fr)
[03:09:30] *** Quits: v3ga (~v3ga@2603-6080-5204-3b35-0000-0000-0000-18ad.res6.spectrum.com) (Quit: WeeChat 3.4)
[03:09:54] *** Joins: v3ga (~v3ga@2603-6080-5204-3b35-0000-0000-0000-18ad.res6.spectrum.com)
[03:10:11] <hayley> https://www.youtube.com/watch?v=CiRBQ_hSNt0
[03:10:12] -ixelp- Emerson, Lake & Palmer - Nut Rocker - YouTube
[03:22:44] <neominimum> I have the barest of knowledge in these things, but I used pseudo-preemptive in the sense that the process isn't technically 'interrupted' based on a hardware timer interrupt, but itself decides to return the execution context to some scheduler of it's own volition. That's the whole need for tracking reductions because information is required for the process to decide when to do this. To me it achieves a similar result and maybe that is all 
[03:22:44] <neominimum> that matters but I thought the distinction was important.
[03:29:50] *** Quits: kevingal (~quassel@2a02:8084:4140:f300:b4c5:b7b4:fceb:af93) (Remote host closed the connection)
[03:31:04] <hayley> http://timonoko.github.io/jemma/smuisti.html One way to solder memory chips.
[03:31:04] -ixelp- S-memory
[03:32:34] <hayley> Well, from the user's perspective (assuming they don't write their own FFI code, which is hopefully uncommon) it just looks like preemption. Many GC systems (Java, ML, SBCL on Windows even) also use similar checks, rather than preempting wherever, but that is usually so that the GC knows how to parse the stack when the application is preempted.
[03:38:05] <hayley> Wikipedia claims "In computing, preemption is the act of temporarily interrupting an executing task, with the intention of resuming it at a later time. This interrupt is done by an external scheduler with no assistance or cooperation from the task" But, sure "cooperation" is hard to define. On such a GC system the compiler inserts safepoints (which Doligez-Leroy bluntly calls "cooperate") which is still transparent to the programmer.
[03:40:07] <neominimum> or rather I think from the users perspective it looks concurrent, if they even have the vaguest notion of what that means. to me preemptive or cooperative is an implementation detail.
[03:41:24] <neominimum> i see what you mean though
[03:42:45] <neominimum> the programmer may not know that there is yield points inserted into their code hence it may seem preemptive to them but cooperative from an implementation standpoint
[03:45:04] <neominimum> I would define assistance or cooperation as not just what the programmer explicitly declares, but also what the compiler/interpreter does before runtime.
[03:51:16] <gilberth> After I managed to get xtrace behave, I noticed that Cairo uses the depreciated "Trapezoids" call. So much for that.
[03:52:20] <gilberth> PPRINT with xtrace would have been nice.
[03:53:23] <hayley> White_Flame: Doligez-Leroy goes brrr
[03:56:11] <hayley> Hm, as well as compressing slots, we can reorder array indices too, to try to make iterations use the smallest stride. Is there much work on that?
[04:02:39] <White_Flame> neominimum: no, preemptive vs cooperative is absolutely a user-exposed detail
[04:02:56] <White_Flame> it changes how you have to code to the APIs
[04:03:33] <White_Flame> for instance, javascript chains of closures are all cooperatively distinguished
[04:04:28] <White_Flame> and that's up to the user to chop up their code into what can run full bore or not
[04:08:30] *** Joins: clothespin (~awolven@c-73-209-95-92.hsd1.il.comcast.net)
[04:13:07] *** Quits: Oddity (~Oddity@user/oddity) (Ping timeout: 256 seconds)
[04:16:48] *** Joins: aeth_ (~aeth@user/aeth)
[04:16:54] *** Quits: aeth (~aeth@user/aeth) (Killed (NickServ (GHOST command used by aeth_)))
[04:16:59] *** aeth_ is now known as aeth
[04:19:50] *** Quits: OlCe (~user@amontsouris-156-1-23-81.w92-151.abo.wanadoo.fr) (Ping timeout: 256 seconds)
[04:23:42] <neominimum> White_Flame: Sure, I don't doubt that, but that doesn't mean it *has* to be exposed to the user. I get the impression that those compromises are only relevant when operating in a synchronous runtime environment as the user has to manage what an asynchronous runtime would hopefully do on it's own. Anyway what I said was in reference to the notion that the definition of preemptive and pseudo-preemptive was a false distinction. I think the 
[04:23:42] <neominimum> distinction matters because of the associated consequences that each approach entails, namely in runtime cost.
[04:30:30] *** Joins: Oddity (~Oddity@user/oddity)
[04:37:09] <White_Flame> yes it has to be.  It's a difference in programming style
[04:37:22] <White_Flame> if it doesn't affect your programmign style, then it's not cooperative vs preemptive
[04:37:49] <White_Flame> eg, the user either has to or does not have to cooperate in order for multitasking to occur
[04:38:00] <White_Flame> if it happens without the user, then the code does not have to bother being cooperative
[04:39:08] <White_Flame> and can just assume it "owns" the whole CPU
[04:39:37] <White_Flame> now, there's probably other non-preemptive, non-cooperative systems that can be identified, but preemptive is the primary non-cooperative _programming model_
[04:39:59] <White_Flame> how the system is preempted is an implementation detail
[04:40:32] <White_Flame> for instance, if some system only task swaps on I/O blockages, then there's still no cooperative programming model, unless the user makes dummy reads to simulate a "yield"
[04:40:40] <White_Flame> and that immediately makes it cooperative
[04:41:12] <White_Flame> the distinction also doesn't have to do wiht cost
[04:41:30] <White_Flame> although cooperative will likely be lower overhead
[04:42:44] <White_Flame> anything that is literally exposed can be optimized for more, at the cost of more programmer complexity and loss of abstraction
[04:43:16] <White_Flame> the fastest code would be constructed with mind to assembly level control, eg the ultimate lack of abstraction
[04:43:27] <White_Flame> and exposure of all available CPU processes
[04:43:36] <White_Flame> (process as in thing it can do, not OS process)
[04:45:25] <White_Flame> regarding Erlang, the Erlang code is preemptive, while the code generated by HiPE is cooperative, as is the BEAM bytecode interpreter
[04:52:55] <neominimum> Okay hmm, I didn't realise that they represented explicit programming models. I figured what was left after compilation/interpretation was what defined the particular concurrent nature of the system. I see what you mean,  but I'm going to have to bow out of this discussion now as I'm at the limit of my knowledge. You've given me a lot to consider.
[04:54:40] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Quit: WeeChat 3.4)
[05:07:12] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[05:08:22] <gilberth> Ah, that's interesting the Trapezoid requests get a top and bottom horizontal y-coordinate and then unclipped left an right edges coming from the original polygon. As I said one should consider this the private interface between Cairo and the X server.
[05:22:18] <gilberth> Aha! And those trapezoids are ordered from left to right. And thus are the direct output of a sweep line alogrithm.
[05:29:03] <White_Flame> neominimum: I am also very voluminous in exploring exactly how to convey it :)
[05:31:44] *** Joins: Jacobis9000 (~jonaholuf@147.148.185.32)
[05:31:46] <Jacobis9000> hi
[05:32:51] <Jacobis9000> so I was just thinking, interesting that in computer science there can be multiple paradigms (and all the consequences thereof) within the same program, you can even write object-oriented LISP these days, it literally blows my mind
[05:33:11] <White_Flame> "computer science" is about the science of computation, not about programming
[05:33:30] <White_Flame> (and even then it's not technically a science as it's a constructed model)
[05:33:36] <Jacobis9000> well quite, but programming is part of computer science
[05:33:50] <White_Flame> yes, when it's incorrectly classified ;)
[05:34:09] <White_Flame> programming is the engineering/development part of things, not the "science" part
[05:34:17] <White_Flame> */craft
[05:34:38] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[05:34:40] <Jacobis9000> That depends on what you're doing with your programming
[05:34:48] <Jacobis9000> Or what programs you're writing
[05:35:06] <contrapunctus> Jacobis9000: hey ^^ «you can even write object-oriented LISP these days» hasn't that been the case since the 90s? 😀️
[05:35:32] <Jacobis9000> hehe
[05:35:39] <Jacobis9000> It's totally mind-blowing
[05:36:11] <gilberth> contrapunctus: 70s.
[05:36:30] <contrapunctus> ^ ah, yes
[05:36:32] <Jacobis9000> Listening to an interesting radio programme about the internet called Tubes: Behind the Scenes of the Internet, it's really nice
[05:37:28] <neominimum> some abstract models of computation lend themselves well to an explicit programming paradigm but language design itself isn't really computer science.
[05:38:13] <White_Flame> there is only turing completeness, everything else is an abstraction
[05:38:33] <White_Flame> or a pragmatism
[05:38:35] <Jacobis9000> Why I was interested in multiple-simultaneous-paradigms was in the notion of a paradigm as a restriction, and the understanding of paradigms as essentially rules, then it is comparable to poetry in that the rules in place allow for the freedom of expression
[05:38:55] *** Quits: random-nick (~random-ni@87.116.167.125) (Ping timeout: 256 seconds)
[05:39:16] <Jacobis9000> but it has got me interested in learning Assembly
[05:39:26] <White_Flame> but note that every expression in every code block in most programming languages isn't all that far removed from BASIC.  It's just the trappings around the expressions, variable assignments, etc and how they're organized
[05:39:31] <White_Flame> but that core still remains
[05:39:37] <gilberth> Not all programming languages are ideological.
[05:40:29] <gilberth> In sense that they believe that a certain paradigm is the holy grail and forbid everything else. Or at least make it hard to not obey that religion.
[05:41:31] <Jacobis9000> ah yes, there is an ideological dimension to programming paradigms, but this is precisely because they are essentially arbitrary restrictions on the manner of human-machine interface
[05:41:39] <Jacobis9000> invented in the mind
[05:41:53] <Jacobis9000> and the mind is frequently ideological, but can be occasionally philosophic
[05:42:25] *** Joins: iamFIREc1 (~iamFIREcr@user/iamfirecracker)
[05:42:34] <Jacobis9000> I see what you mean yes
[05:43:33] <gilberth> One prime example is the recent trend to not have a GOTO statement. Is there any reason to cripple a language by not having a GOTO than to force the idea GOTO => Bad code [in all cases] onto the programmer?
[05:44:13] <Jacobis9000> I think the idea was that the freedom of the GOTO statement is simply too much
[05:44:21] <neominimum> White_Flame: not a problem, voluminous isn't always bad :)
[05:44:54] <gilberth> The consequence is that you wind up writing while (state != FINAL_STATE) switch (state) { case ..; state = ...; break; ... }.
[05:45:09] *** Quits: iamFIREcracker (~iamFIREcr@user/iamfirecracker) (Ping timeout: 256 seconds)
[05:45:11] <Jacobis9000> heh yeah
[05:46:10] <Jacobis9000> ooh wow the YT album switched me from contemporary chamber music to lo-fi dance just at the point I wanted
[05:46:14] <Jacobis9000> it's getting better
[05:46:32] <gilberth> Jacobis9000: Exactly. The mindset is: "I, the magnificent language, have found the root of all evil, and I forbid it to make the world a better place."
[05:46:38] <Jacobis9000> album\algorithm 
[05:46:59] <gilberth> * language designer
[05:47:36] <Jacobis9000> I agree, let there be the apple of desire, let there be GOTO and see the manifestations!
[05:48:38] <hayley> I just cannot be bothered to work out GOTO from just message passing.
[05:48:45] <Jacobis9000> do you write poetry, gilberth?
[05:48:55] <neominimum> If you want goto, program in assembly... for everything else there's Mastercard
[05:48:56] <hayley> BLOCK/RETURN is my only "primitive" here.
[05:49:09] <Jacobis9000> lol neo
[05:49:13] <gilberth> Jacobis9000: Me? No way, I am a bad writer.
[05:49:25] <gilberth> hayley: No tail calls?
[05:49:33] <hayley> https://plover.com/%7Emjd/misc/hbaker-archive/MetaCircular.html "TAGBODY/GO" EMULATED BY "CATCH/THROW"
[05:49:34] -ixelp- ACM Lisp Pointers V, 4 (Oct/Dec 1992), 11-20.
[05:49:43] <hayley> gilberth: Tail sends, sure. But still no GOTO.
[05:49:58] <gilberth> hayley: Good enough for me.
[05:50:13] <hayley> All methods are straight-line code, and you make a loop by `self`-reference.
[05:50:31] <gilberth> Bad are languages which don't have either [specified] tail calls nor a GOTO.
[05:50:53] <hayley> i.e. (define (bla x) (something) (bla x))
[05:51:13] <Jacobis9000> I wonder if the absence of GOTO makes everything more readable, it's like the left to right, top to bottom rule in English.
[05:51:25] <gilberth> That's fine. I would call that a GOTO.
[05:51:56] <hayley> Dijkstra complained about a not-lexically-scoped GOTO IIRC. But when possible, using structured control flow does look nicer.
[05:52:11] <hayley> However, good luck if you want to compile a DFA or something.
[05:52:12] <gilberth> Jacobis9000: Well, GOTO has its uses.
[05:53:36] <gilberth> hayley: As I said: while (state) switch (state) ... the only way. It's silly without a GOTO or tail calls your language becomes an awkward target for any compiler. I repeat myself.
[05:53:39] * hayley continues ruining RISC memes.
[05:54:00] <Jacobis9000> There's a simple structure to essays, it goes like this: introduction: list arguments, body: make arguments, conclusion: list conclusions of arguments. You could imagine it like a program where you have to name your functions, define your functions, and set the return values throughout and particularly at the end. Follow that structure and you always get top marks.
[05:54:01] <White_Flame> yeah, GOTO is especially useful if you're going to compile higher level forms down in the same language
[05:55:06] * hayley uploaded an image: (165KiB) < https://libera.ems.host/_matrix/media/r0/download/matrix.org/TlIGsJaPXfdzUKcqMQTcuuNU/risc.jpg >
[06:08:11] *** Quits: dre (~dre@2001:8003:c932:c301:880:1cdd:3f72:271d) (Ping timeout: 250 seconds)
[06:13:22] *** Joins: iamFIREcracker (~iamFIREcr@user/iamfirecracker)
[06:14:52] *** Quits: iamFIREc1 (~iamFIREcr@user/iamfirecracker) (Ping timeout: 256 seconds)
[06:31:01] <gilberth> Ok. Fat lines are also rendered with trapezoids. That is the polyline is converted to a polygon first with Cairo.
[06:32:59] <gilberth> It's nice. Only 390 coordinates 32-bit each send for a single line.
[06:34:30] <gilberth> That's only 9750% of what is needed. Very efficient.
[06:35:54] <clothespin> my program keeps bugging out and i'm doing a demo next saturday
[06:36:33] <gilberth> Didn't you say your version would be fine?
[06:37:10] <clothespin> it's gonna hafta be fine
[06:37:33] <gilberth> Have fun, then! :-)
[06:56:38] <gilberth> Here is what I get of Cairo for a polyline: <http://clim.rocks/gilbert/cairo-polyline.svg>
[07:10:43] *** Quits: waleee (~waleee@h-98-128-229-110.NA.cust.bahnhof.se) (Ping timeout: 256 seconds)
[07:37:57] *** Quits: Jacobis9000 (~jonaholuf@147.148.185.32) (Ping timeout: 240 seconds)
[07:38:14] <White_Flame> that's some decent trapezoiding
[07:39:48] <gilberth> Yep. I wonder if it is directly generated or by first generating a bunch of polygons for the "stems" and the joints and caps and finally subject that to a polygon clipper using Bentley-Ottmann.
[07:40:33] <White_Flame> it looks like the rounded parts are divided evenly across the arc
[07:41:01] <White_Flame> and then on any horizontal span that has a point, it generates a trapezoid
[07:41:07] <White_Flame> s/point/vertex/
[07:42:21] <White_Flame> I doubt it bothers generating polygons, but rather just tesselates the outline first
[07:43:45] <gilberth> Well, at the round caps it's not evenly in y. It looks more like being so in phi.
[07:44:55] <White_Flame> right, "across the arc", angularly
[07:45:35] <White_Flame> I don't know if it tries to line up points across the arc at the same y, though
[07:45:46] <White_Flame> the left one has some doubled-up lines where they don't line up
[07:46:14] <gilberth> Well, the task is far from trivial. I could imagine going the polygon way, because it would catch all corner cases nicely.
[07:48:18] <gilberth> White_Flame: The double line you see at the left-top comes from the point where the right contour of that very cap beings. That is, where you go from the straight stem into the arc for that cap.
[07:48:43] <White_Flame> ah, so it is
[07:49:10] <White_Flame> so I bet it does take into account pairing up the y-coord on both sides of the arc, and not just an even division
[07:50:00] <White_Flame> probably starts from the left end of hte arc, as the right side is the one that has an uneven spacing right at the end on both arcs
[07:50:29] <White_Flame> all 3 arcs, including the bottom one
[07:50:48] <gilberth> You would still get the same, if you generate a rectangle for the stem and then a half circle with a constant delta phi for the cap and then subject both to Bentley-Ottmann aka a sweep line.
[07:50:52] <White_Flame> the middle arc is interestingly even
[07:51:27] <gilberth> Yep.
[07:52:57] <White_Flame> I don't think so, if it was just blind arc division starting from one side, then there'd be many more horizontal lines that that algo would find
[07:53:00] <gilberth> It's late. Perhaps I do some homework tomorrow and read source code. Perhaps also see what Ghostscript does.
[07:53:28] <White_Flame> but with circles, it would be easier to do even division on an absolute x/y axis instead of either from the left/right point of the arc
[07:53:34] <White_Flame> yet it still lines up at the left side
[07:53:35] <gilberth> White_Flame: Why would there be more horizontal lines?
[07:53:43] <White_Flame> so I'm not sure what sort of hinting/balancing it's doing
[07:53:54] <White_Flame> because there's no reason for the sample points to line up on both sides of the arc in Y
[07:55:12] <gilberth> When you define your zero angle as pointing up and go from there, lhs and rhs y-coordinates would match.
[07:55:27] <White_Flame> right, but then you wouldn't get the left/right ends of the arc lining up with the line
[07:55:32] <White_Flame> at least not as evenly as this does
[07:56:00] <gilberth> Which line do you refer to?
[07:56:02] <White_Flame> you'd get some little segment making up the difference on both sides
[07:56:07] <White_Flame> where the arc meets the line
[07:56:30] <White_Flame> if it started at the "top" or whichever cardinal direction, what are the odds that it would evenly hit the straight line it butts with?
[07:56:36] <White_Flame> without a tiny little make-up segment
[07:57:10] <gilberth> You mean the straight part of the contour. It happens. Watch like the last and next to last horizontal lines of both top caps are nearer to each other then the other lines.
[07:57:11] <White_Flame> looking at the top 2 arcs, it lines up with the arc->line transition very evenly on its left side
[07:57:34] <gilberth> Look closer, White_Flame. :)
[07:57:52] <White_Flame> on the right sides of each ard, there is a shorter make-up line
[07:57:55] <White_Flame> *arc
[07:58:07] <White_Flame> same thing with the bottm arc
[07:58:33] <White_Flame> for the bottom arc, its left side has a seemingly unusually long beginning segment, too
[07:59:13] <White_Flame> so I wouldn't be surprised if there's some level of hinting, cleanup, etc going on
[07:59:14] <gilberth> Yes, but that would be consistent with going from a zero angle pointing up with a constant delta phi and finally close the missing gap, which might be smaller.
[07:59:55] <White_Flame> I don't see that being a natural consequence
[08:00:13] <White_Flame> but if course we only have a sample size of 4 arcs here, to try to extrapolate what it's doing
[08:00:15] <gilberth> Perhaps just a tolerance to allow for a longer final gap closed to avoid too tiny closing gaps.
[08:00:16] <White_Flame> *of
[08:00:42] <White_Flame> yet the top arcs do have tiny closing gaps
[08:00:50] <gilberth> Sure, as I said, I ought to do my homework and read the source.
[08:01:04] <White_Flame> anyway, an interesting thign
[08:01:35] <White_Flame> you should sell an NFT of the image ;-D
[08:01:50] <gilberth> To get rich quick(tm)?
[08:03:59] <gilberth> Anyhow, I am dragged to a domain, I didn't intend to learn too much about.
[08:04:21] <White_Flame> I've done vector rendering of this sort back in the Palm 68k days
[08:04:43] <White_Flame> I'm somewhat of a pixelophile when it comes to things like this :)
[08:05:50] <gilberth> A pixelophile? Nice word. Yeah, it has its merits and could certainly drag you in.
[08:06:26] <White_Flame> however, I didn't tesselate it into trapezoids, but rather directly rendered each row from the arc's intersection with the raster line
[08:06:50] <White_Flame> (non-antialiased, AA was kind of unfeasable at the time)
[08:07:27] <gilberth> Well, that would an approach I'd use for rasterization. Just intersect each scanline with whatever you want to draw.
[08:08:41] <White_Flame> right, and then between each pair of rasterlines you can turn those points into trapezoids, too, for AA
[08:08:47] <gilberth> Pixels can be attractive. I once was into 3d rendering. Triggered by the advent of Quake and wanting to figure out how to do that that fast on those slow machines we had.
[08:09:12] <White_Flame> (though that rasterline-sampled form of AA wouldn't be quite as accurate)
[08:09:52] <gilberth> I still would need to educate myself on how sampling for AA is supposed to work.
[08:10:20] <White_Flame> at the level of trapezoids, you can just do an area calculation and skip sampling
[08:10:53] <gilberth> area as in area of the pixel at hand covered?
[08:10:58] <White_Flame> however, if you have butted-together individually rendered vectors, then true superspampling would have them butt properly
[08:11:00] <White_Flame> without seams
[08:11:02] <White_Flame> yes
[08:11:41] <White_Flame> eg, if you have 1 polyline covering 50% of a line, and do 50% alpha, then another polyline covers the other half of the pixel and also does a 50% alpha with what's now there, you end up with a 25% transparent seam leakage
[08:12:13] <White_Flame> so you have to track that between vectors, and the easiest way to do that is a supersampled grid
[08:12:20] <gilberth> Yep, I am aware. You somehow destroyed information.
[08:12:43] <White_Flame> the hardest way to do that is to combine all vectors in a single pass
[08:13:28] <White_Flame> (which means adding another vector to the canvas requires recomputing everything again)
[08:13:51] <White_Flame> ((though you could clip to the dirty region of the newly-added one's bounding box)
[08:13:53] <White_Flame> )
[08:14:02] <gilberth> Can't you use a two state approach? First raster the shape to just covering values, which should add up and then "press" the ink though that mask?
[08:14:24] *** Joins: semz_ (~semz@user/semz)
[08:14:26] <White_Flame> you can't just "add up" numerically
[08:14:52] <White_Flame> if 2 coverages overlap exactly, and that overlap is 33%, then the total pixel coverage is still 33%, not 66%
[08:15:16] <White_Flame> and of course if they have zero overlap between each other, then the total pixel coverage then gets up to 66%
[08:15:18] <gilberth> Oh dear, indeed.
[08:15:41] <White_Flame> so you need to combine them geometrically first, though you can do that even after trapeziodicalization
[08:15:55] <White_Flame> *trapezoidification
[08:16:23] <gilberth> Is that a word? I was wondering the other night.
[08:16:25] <White_Flame> *supertrapecalifragisticexpializoidious
[08:16:41] <White_Flame> it's a sensible usage of suffixes, if not a canonical word
[08:16:54] *** Quits: clothespin (~awolven@c-73-209-95-92.hsd1.il.comcast.net) (Ping timeout: 250 seconds)
[08:16:54] *** Quits: semz (~semz@user/semz) (Ping timeout: 250 seconds)
[08:17:14] <gilberth> See. Too difficult words and too much math in general. I'll switch careers.
[08:17:36] <gilberth> Not that I would have any to begin with. :)
[08:20:43] <gilberth> I wonder what other libraries I could have a look at.
[08:23:59] <White_Flame> which one is this?
[08:24:12] *** semz_ is now known as semz
[08:24:55] <gilberth> Which? The sample SVG I posted? That is what Cairo sends via the RENDER extension.
[08:25:04] <White_Flame> oh, which library it was.  Cairo then
[08:25:44] <gilberth> I captured the X11 requests with xtrace and then displayed the trapezoids with CLIM.
[08:53:27] *** Joins: dave0 (~davezero@069.d.003.ncl.iprimus.net.au)
[09:03:59] *** Joins: ln43 (~ln43@user/ln43)
[09:04:26] <ln43> g mornin'
[09:04:36] <dave0> maw ln43 
[09:08:16] *** Joins: dre (~dre@2001:8003:c932:c301:1c5e:3f1c:f7a4:be5b)
[09:11:10] <moon-child> gilberth: 'how sampling for AA is supposed to work'  I ran into this problem recently (and see earlier discussion with 3b).  It seems there is no canonical answer, because the problem is not really well defined; you just try to do something that looks good, whatever that means
[09:16:09] *** Quits: ln43 (~ln43@user/ln43) (Quit: Connection closed)
[09:20:36] *** Quits: dre (~dre@2001:8003:c932:c301:1c5e:3f1c:f7a4:be5b) (Ping timeout: 250 seconds)
[09:25:10] *** Joins: jeosol (~jeosol@user/jeosol)
[10:19:48] <hayley> https://consequence.net/2022/02/elon-musk-neuralink-brain-chips-monkeys-died/
[10:19:48] -ixelp- 15 of 23 Monkeys with Elon Musk's Neuralink Brain Chips Reportedly Died
[10:20:09] <ck_> not what I expected the real life 12 Monkeys to turn out like
[10:26:18] <dave0> that's a bit rough
[10:27:57] <dave0> Neuralink chips were implanted by drilling holes into the monkeys’ skulls
[10:28:22] <dave0> musk seems to be turning into a supervillian
[10:29:36] <moon-child> wasn't he always?
[10:31:38] <dave0> yeah but i didn't know he was torturing animals
[10:34:45] <moon-child> fair enough
[10:34:52] *** Joins: shka (~herr@109.231.0.226)
[10:35:43] <dave0> that's how serial killers start :-p
[10:51:54] <White_Flame> I'm sure they were sufficiently sedated
[10:53:28] <edgar-rft> the serial killers?
[11:06:58] <White_Flame> them, too
[11:11:41] <dave0> i'm weird on putting computer chips under the skin in my hand.. no swearing way someone's gonna put a chip in my brain
[11:12:12] <White_Flame> I wouldn't wholly be against output-only inserts
[11:12:24] <ck_> sent from my iPlant
[11:12:27] <White_Flame> get be some dr octopus arms
[11:12:31] <White_Flame> *me
[11:13:14] <moon-child> do you trust the manufacturers of those sensors?
[11:13:16] <moon-child> see e.g. oculus
[11:13:41] <White_Flame> trust is a singular thing, not a group thing
[11:15:59] <moon-child> sure
[11:16:09] <moon-child> put it another way: is there anyone you would trust to produce such sensors?
[11:19:51] *** Joins: ogamita (~pjb@2a01:cb11:8036:90c2:828:5888:3c6b:3bf8)
[11:26:57] *** Quits: ogamita (~pjb@2a01:cb11:8036:90c2:828:5888:3c6b:3bf8) (Ping timeout: 240 seconds)
[11:30:28] *** Joins: treflip (~user@user/treflip)
[12:02:47] *** Joins: ln43 (~ln43@user/ln43)
[12:07:34] *** Quits: treflip (~user@user/treflip) (Remote host closed the connection)
[12:09:47] *** Joins: treflip (~user@user/treflip)
[12:28:08] *** Quits: ln43 (~ln43@user/ln43) (Quit: Connection closed)
[12:30:18] *** Joins: cosimone (~user@93-44-184-23.ip98.fastwebnet.it)
[13:00:44] <hayley> https://www.intel.com/content/www/us/en/newsroom/opinion/thoughts-blockchain-custom-compute-group.html
[13:00:57] <hayley> "We expect that our circuit innovations will deliver a blockchain accelerator that has over 1000x better performance per watt than mainstream GPUs for SHA-256 based mining" Not really a high standard. ASICs exist??
[13:05:42] <hayley> In fact, 1000x the energy efficiency of a RX 580 is approximately 1/10 of what you get from ASICs these days. So it damned well be over 1000x better.
[13:09:13] *** Joins: OlCe (~user@amontsouris-156-1-23-81.w92-151.abo.wanadoo.fr)
[13:09:46] <hayley> (Closer to 1/6 but I'd only expect things to be in the ball park w.r.t energy efficiency. Of course, the best way to achieve efficiency is to avoid useless work, but...)
[13:53:38] *** Quits: notzmv (~zmv@user/notzmv) (Ping timeout: 260 seconds)
[13:58:45] <neominimum> Apress has sent me an email urging me to take advantage of their employee discount, 40% off! Only thing is I have not been employed by them, and I feel like I would remember if I had worked for them in the past. It make no sense
[14:03:40] <dave0> what do they sell? 40% sounds like a good deal
[14:03:57] <ck_> can't trust feelings tbh
[14:08:14] *** v3ga is now known as v3gajerusalem
[14:13:54] <hayley> "But then, I remembered the greatest legacy of the Self Project:  Javascript!   (argh!!)"
[14:15:16] <neominimum> dave0: Books! :)
[14:15:43] <dave0> go for it!
[14:19:03] <hayley> "I followed the link http://objective.st/ and read: 'Objective-S is the first general purpose programming language.' Were the others chopped liver?" -- David Ungar
[14:19:03] -ixelp- Objective-S: Home
[14:20:28] *** Quits: aeth (~aeth@user/aeth) (Ping timeout: 250 seconds)
[14:22:32] *** Joins: aeth (~aeth@user/aeth)
[14:39:38] *** Quits: cosimone (~user@93-44-184-23.ip98.fastwebnet.it) (Remote host closed the connection)
[14:40:05] *** Joins: cosimone (~user@2001:b07:ae5:db26:c24a:d20:4d91:1e20)
[14:44:13] *** Joins: iamFIREc1 (~iamFIREcr@user/iamfirecracker)
[14:46:53] *** Quits: iamFIREcracker (~iamFIREcr@user/iamfirecracker) (Ping timeout: 256 seconds)
[14:53:22] *** Quits: OlCe (~user@amontsouris-156-1-23-81.w92-151.abo.wanadoo.fr) (Ping timeout: 256 seconds)
[15:15:27] *** Joins: iamFIREcracker (~iamFIREcr@user/iamfirecracker)
[15:16:55] *** Quits: iamFIREc1 (~iamFIREcr@user/iamfirecracker) (Ping timeout: 256 seconds)
[15:31:05] *** Quits: shka (~herr@109.231.0.226) (Ping timeout: 256 seconds)
[15:47:47] *** Joins: iamFIREc1 (~iamFIREcr@user/iamfirecracker)
[15:49:43] *** Quits: iamFIREcracker (~iamFIREcr@user/iamfirecracker) (Ping timeout: 250 seconds)
[15:55:02] <hayley> https://twitter.com/geofflangdale/status/1492272669454049282 wtf based Geoff Langdale
[15:55:20] <hayley> "If [those results are] from the "languages shootout", it's more of a measure of how much effort language enthusiasts have put in to micro-optimize a bunch of tiny benchmarks"
[16:10:28] *** Joins: notzmv (~zmv@user/notzmv)
[16:15:52] *** Joins: random-nick (~random-ni@87.116.167.125)
[16:56:39] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[17:07:39] *** Quits: cosimone (~user@2001:b07:ae5:db26:c24a:d20:4d91:1e20) (Read error: Connection reset by peer)
[17:07:52] *** Joins: cosimone (~user@2001:b07:ae5:db26:c24a:d20:4d91:1e20)
[17:17:57] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 240 seconds)
[17:20:05] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[17:23:41] *** Quits: dave0 (~davezero@069.d.003.ncl.iprimus.net.au) (Quit: dave's not here)
[17:32:36] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[17:34:34] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[17:36:25] *** Quits: Oddity (~Oddity@user/oddity) (Remote host closed the connection)
[17:49:11] *** Joins: christophergray (~christoph@186.151.40.100)
[17:52:45] *** Joins: waleee (~waleee@h-98-128-229-110.NA.cust.bahnhof.se)
[18:15:41] *** Quits: christophergray (~christoph@186.151.40.100) (Quit: WeeChat 3.4)
[18:34:14] *** Quits: treflip (~user@user/treflip) (Quit: rebooting)
[18:49:06] *** Quits: cosimone (~user@2001:b07:ae5:db26:c24a:d20:4d91:1e20) (Remote host closed the connection)
[19:12:13] *** Joins: cosimone (~user@93-44-184-23.ip98.fastwebnet.it)
[19:26:30] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[19:28:33] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[19:32:45] <gilberth> Good morning #lispcafe!
[19:35:24] <Alfr> Hello, gilberth.
[19:36:13] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[19:37:57] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[19:39:40] <gilberth> I'd really like to adopt the PS graphics model for CLIM, but somehow I don't like its state-full API having a current path. Hmm.
[19:40:32] <White_Flame> that doesn't seem like that big of a blocker.  just take a constructed path instead of exposing a bunch of stuff to construct one "live"
[19:40:57] <White_Flame> that current path model is usually just a syntactic helper for making code shorter
[19:41:32] <gilberth> I'm wondering if that really is so bad. A "medium" with CLIM already has state like the CTM, the ink to use, the text style, etc.
[19:42:02] <White_Flame> (with-path-construction ....) can contain that without being part of permanent implicit graphics context state
[19:42:52] <gilberth> White_Flame: The way I see that is that with PS you have procedural path construction in contrast to functional path construction where a path would be an object you compose from primitive path elements.
[19:44:40] <gilberth> White_Flame: Indeed. Perhaps sth like (STROKE-PATH medium (CONSTRUCTING-PATH ... (MOVE-TO ...) (LINE-TO ...)) :INK +RED+)? That would fit well with CLIM, give me procedural path construction while also giving me the option to make paths objects proper.
[19:45:52] <gilberth> Oh well, API design is not easy.
[19:46:59] <gilberth> Anyhow, everybody adopted the PS model and hence it kind of is the lingua franca. I want it for CLIM, too --- No need to be an alien here.
[19:50:06] <White_Flame> did any of the graphical lisp machines do general vector graphics of this style well?
[19:50:18] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[19:50:23] <White_Flame> or was it more just direct drawing stuff?
[19:51:44] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[19:52:57] <gilberth> White_Flame: All I have seen so far was not path based, but draw-line here, draw-rectangle there, etc. X11 is the same way.
[19:53:08] <White_Flame> ok, thought so
[19:55:32] <gilberth> Actually I prefer that kind of API though it has its flaws when drawing a pixel is not idempotent anymore as with AA. Some DRAW-PATH primitive would help. But: Somehow I also want to transport the procedural API and if it is only for (1) not feeling alien and (2) easy porting of existing (non-Lisp) code.
[19:58:36] <gilberth> I need more coffee and pondering. CLIM mediums (media?) already have state. In my CLIM that state is dynamically bound anyway, so having a current path wouldn't be too bad. Hmm.
[20:00:06] <gilberth> What could be puzzling though is that output recording won't work on the path construction primitives, but a stroke or fill operation would be recording with path and all drawing options like colour to use etc.
[20:00:15] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[20:01:55] <gilberth> In so far "recording" and "replay" is a misnommer. Perhaps it was intended intially as just replaying all graphics requests, while we actually record the resulting graphical output, not the sequence of API calls.
[20:02:25] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[20:05:45] <gilberth> BTW Who came up with the PS model? Is that really an invention by Adobe, or was there some prior system based on path construction and stroking and filling with an ink?
[20:09:53] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[20:11:48] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[20:28:37] <White_Flame> wiki says "The concepts of the PostScript language were seeded in 1976 by John Gaffney at Evans & Sutherland,[2][3][4] a computer graphics company. At that time Gaffney and John Warnock were developing an interpreter for a large three-dimensional graphics database of New York Harbor."
[20:28:57] <White_Flame> and then warnock went and founded adobe in 1982
[20:31:19] <White_Flame> and xerox made the laser printer in the meantime, and made the lagnauges Press and Interpress
[20:31:30] <White_Flame> which also influenced PS
[20:33:17] <White_Flame> it doesn't specifically say where the notion of path construction & filling came from, but those would be languages to peek into if you're so inclined
[21:03:24] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[21:05:29] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[21:19:34] <gilberth> White_Flame: <http://clim.rocks/gilbert/cairo-polyline-subject-to-bentley-ottmann.svg> That's the polygon Cairo computes for my polyline. It then is processed by Bentley-Ottmann to yield the trapezoids. The red dots are the vertexes of the polygon.
[21:20:01] <gilberth> So it does indeed first come with with a polygon for the polyline rendering.
[21:21:12] <White_Flame> interesting on the bottom
[21:21:52] <White_Flame> and the upper right is a bit asymmetric
[21:22:50] <gilberth> Yep. For the caps and joints Cairo keeps a prototype "pen" which is the vertexes of a full circle for the given line thickness. It then copies vertexes from that prototype in the appropriate range.
[21:23:46] <gilberth> So the gaps we see come from rounding the real arc phase to the discrete phases in the prototype pen.
[21:23:55] <White_Flame> I guess all the AA vector stuff in mcclim & mezzano is rooted in cl-vecto
[21:24:39] <gilberth> mcclim has a Cairo backend.
[21:25:15] <gilberth> And isn't cl-vecto a rasterizer?
[21:30:37] <White_Flame> yes
[21:31:23] <White_Flame> and of course that includes paths and all that
[21:31:35] <White_Flame> polylines, joints, butts, etc
[21:31:57] <gilberth> Anyhow, all the corner case are handled by the sweep line algorithm, which I have. The rest seems easy. I may attempt to write a polyline triangulation baase on just that.
[21:32:35] <gilberth> The sweep line part is the hard part, but we have rationals.
[21:33:04] <White_Flame> I don't understand why that would be hard, it's basically just rooted in a sort & linear intersections
[21:33:53] <gilberth> White_Flame: You need to find all intersections between all the edges. And be numerically stable, which is the part where rationals help.
[21:34:12] <White_Flame> ah, I guess so
[21:35:40] *** Joins: clothespin (~awolven@c-73-209-95-92.hsd1.il.comcast.net)
[21:36:14] <gilberth> And that's the Bentley-Ottmann algorithm. It's clever and finds all intersections in O(n log n). If you don't use rationals, you could wind up with paradox situations which would really break the algorithm.
[21:37:48] <White_Flame> so your input lines coordinates are integers instead of floats?
[21:39:16] <gilberth> You sweep the figure from left to right halting at "event points", which are your intersections. You keep a current list of interesting segments, which is all the segments below your current y-coordinate. You keep those segments sorted from left to right. You only look at neighboring segments for intersections. However when you round an intersection you get, the sub-segments you get may flip their order nullifying assumptions made.
[21:40:05] <gilberth> White_Flame: Well, fixed point. Rounding the input coordinates is fine. Rounding the intersections is not.
[21:40:33] <gilberth> I scale the input coordinates to some range, which seems reasonable.
[21:43:20] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 250 seconds)
[21:45:11] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[21:49:52] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[21:51:33] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[21:55:09] *** Joins: OlCe (~user@amontsouris-156-1-23-81.w92-151.abo.wanadoo.fr)
[23:13:10] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[23:15:19] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[23:23:56] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[23:25:44] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[23:30:05] *** Joins: Nselm (~Nselm@p200300d56f4b3f232fdd2f2b08ee3842.dip0.t-ipconnect.de)
[23:45:13] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 256 seconds)
[23:47:06] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[23:59:58] <epony> White_Flame, there is no point to doubt lisp machines performance is it?
