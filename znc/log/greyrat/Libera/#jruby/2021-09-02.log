[00:01:14] <lopex[m]> https://www.wykop.pl/cdn/c3201142/comment_1625778375j0CLfbtEBJVRVoLTzhWH2c.jpg
[00:07:09] <lopex[m]> numbers?
[00:15:01] *** Joins: Freeky (~freaky@user/freaky)
[00:15:51] *** Quits: Freaky (~freaky@user/freaky) (Ping timeout: 276 seconds)
[00:16:44] *** Freeky is now known as Freaky
[01:21:14] *** Joins: lopex (uid4272@id-4272.tooting.irccloud.com)
[05:04:03] <byteit101[m]> headius: if you are referring the the ruby < ruby < java super loop issue, I am available to help debug that
[05:05:06] <byteit101[m]> (regarding the "JI one")
[05:09:05] <headius> byteit101 yeah that is the last blocker for 9.3 now
[05:13:44] <byteit101[m]> I know when I last tried to debug it, I got stuck trying to figure out where the super calls had been registered
[07:43:10] *** Quits: lopex (uid4272@id-4272.tooting.irccloud.com) (Quit: Connection closed for inactivity)
[09:13:04] <headius> I'll be available to work on it tomorrow but whatever you can figure out will help
[22:44:49] <headius> enebo: did you say something about a metaspace error recently?
[23:12:19] <enebo[m]> headius: I said it in regards to a cI run at some point
[23:12:40] <headius> I am seeing a job fail with metaspace errors: https://github.com/jruby/jruby/issues/6800
[23:13:05] <headius> neither of the dependencies I updated seem to have any changes related to classloading so I'm not sure what changed
[23:14:37] <enebo[m]> perhaps VM resources changed
[23:15:07] <headius> it's possible but I have not seen it on other jobs... but you have?
[23:16:39] <enebo[m]> no I am not sure.  I saw it on one and I think it was travis
[23:16:54] <enebo[m]> perhaps meatspace on that job is due to something specific to it?
[23:18:30] <headius> this was for rubyspec fast with jit, which does create a lot of classes, but it only failed on 8
[23:18:42] <headius> maybe they updated 8 recently and it has different meatspace heuristics now
[23:22:44] <enebo[m]> meatspace
[23:23:16] <enebo[m]> honestly it is the only thing I can call it now and forever more
[23:24:30] <ahorek[m]> I saw the error before dependecy changes, it's related to jit_threshold: 0 / that fast+jit job
[23:25:47] <ahorek[m]> maybe it's just OOM, travis jobs have some memory restrictions
[23:28:51] <headius> ahorek: but you have not seen something we changed to cause this?
[23:29:32] <ahorek[m]> no, I don't think any recent change could cause this
[23:30:15] <headius> as far as I know Travis does not publish their changes very well so this could be any number of causes on their end
[23:30:39] <headius> new JDK8 using more memory, reduced memory in the VM, new or additional services taking more memory
[23:30:40] <ahorek[m]> and it doesn't always fail
[23:31:02] <headius> I could bump it up but we are naturally reluctant to do that without an explanation
[23:31:56] <headius> we do -XX:MaxMetaspaceSize=512M for mspec runs currently
[23:33:14] <ahorek[m]> yeah, that should be high enough, a memory profile could help perhaps?
[23:34:11] <headius> hmmm perhaps
[23:34:34] <headius> I'll try to run locally with 8 this afternoon and see if I can profile (or if I get the errors myself)
[23:38:02] <headius> I'm going to merge those two small dep updates and if we see it on master we'll know something's up
