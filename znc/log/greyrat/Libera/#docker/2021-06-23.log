[05:25:44] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[05:30:24] *** Quits: vlm (~vlm@user/vlm) (Quit: "")
[05:34:40] *** Joins: vlm (~vlm@user/vlm)
[05:48:37] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 268 seconds)
[05:49:49] *** Joins: m1n10n (georgecloo@cpe-76-184-204-251.tx.res.rr.com)
[06:01:28] *** Quits: goldfish (~goldfish@user/goldfish) (Ping timeout: 252 seconds)
[06:04:20] *** Joins: pete443 (~pete@user/pete443)
[06:04:24] *** Quits: pete443_ (~pete@user/pete443) (Ping timeout: 252 seconds)
[06:08:07] *** Joins: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net)
[06:12:44] *** Quits: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net) (Client Quit)
[06:28:26] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Remote host closed the connection)
[06:29:46] *** Quits: Brainium (~brainium@user/brainium) (Remote host closed the connection)
[06:29:59] *** Joins: Brainium (~brainium@user/brainium)
[06:32:18] *** Quits: Brainium (~brainium@user/brainium) (Client Quit)
[06:33:20] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Remote host closed the connection)
[06:34:10] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[07:08:31] *** Quits: tyson2 (~user@bras-base-toroon0624w-grc-17-50-101-91-221.dsl.bell.ca) (Remote host closed the connection)
[07:11:12] *** Joins: night_wulfe_ (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[07:12:31] *** Quits: sesquisentient (uid38151@id-38151.stonehaven.irccloud.com) (Quit: Connection closed for inactivity)
[07:14:25] *** Quits: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 265 seconds)
[07:15:15] <Lutin> artok trying to avoid IPv6 is a very good idea ;)
[07:26:44] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[07:34:05] *** Joins: Brainium (~brainium@user/brainium)
[07:34:35] *** Quits: dalan (~dalan@110-175-186-185.tpgi.com.au) (Quit: dalan)
[07:35:58] *** Joins: dalan (~dalan@110-175-186-185.tpgi.com.au)
[07:41:07] *** Joins: versageek (~versageek@wikimedia/Versageek)
[07:45:10] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Ping timeout: 268 seconds)
[07:46:56] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[08:00:34] *** justBull is now known as justK
[08:05:18] *** Quits: Brainium (~brainium@user/brainium) (Quit: Konversation terminated!)
[08:05:27] *** Quits: dalan (~dalan@110-175-186-185.tpgi.com.au) (Quit: dalan)
[08:06:00] *** Joins: dalan (~dalan@110-175-186-185.tpgi.com.au)
[08:07:18] <cryocaustik> hey all - any idea why python would not be finding the "TEST_URL" environment url when passing it in through the compose file? https://bpa.st/Z6HQ
[08:07:56] <cryocaustik> if I run the same image using `docker run -e TEST_URL=XXX app:latest` then it works fine
[08:10:30] *** Joins: endigma61 (~endigma@134.41.133.214)
[08:11:05] *** Joins: tang^ (~doofus@2604:3d09:47c:f970:e46f:ebf5:a9:e73e)
[08:12:06] <doc> cryocaustik: considered using env_file instead?
[08:12:34] <doc> seems like that should be working 
[08:12:47] <cryocaustik> it is a single variable I am trying to throw inside, so I don't think it really warrants a whole file
[08:12:48] *** Quits: endigma61 (~endigma@134.41.133.214) (Read error: Connection reset by peer)
[08:12:54] *** Quits: endigma6 (~endigma@134.41.133.214) (Ping timeout: 265 seconds)
[08:13:08] * doc checks to see if his have environment anywhere
[08:15:50] <doc> if you exec into it and do printenv | grep TEST_URL does it show up?
[08:15:56] <doc> compose looks fine
[08:16:59] *** Joins: endigma6 (~endigma@134.41.133.214)
[08:17:56] <doc> cryocaustik: ^
[08:17:56] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Quit: WeeChat 3.2)
[08:19:10] <cryocaustik> let me try that
[08:21:32] *** Quits: endigma6 (~endigma@134.41.133.214) (Ping timeout: 252 seconds)
[08:21:43] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[08:23:25] <cryocaustik> https://bpa.st/2XDA nothing for that variable
[08:23:48] <doc> ok, narrows it down to just a docker thing at least. Weird, seems like it should work ok
[08:23:55] <doc> I'm out of ideas
[08:25:14] <doc> cryocaustik: try the value in quotes
[08:26:55] *** Quits: zakame (~zakame@user/zakame) (Ping timeout: 265 seconds)
[08:28:36] <cryocaustik> I tried single quote, double quote, and no quote 
[08:28:36] <cryocaustik> https://bpa.st/FEZA
[08:28:42] *** Joins: zakame (~zakame@user/zakame)
[08:30:48] <doc> sorry then, no idea :/
[08:31:13] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Write error: Connection reset by peer)
[08:37:37] *** night_wulfe_ is now known as night_wulfe
[08:39:01] <cryocaustik> I appreciate the help!
[08:44:08] <cryocaustik> just tried the docker run -e method and printenv shows the variable =/ 
[08:49:41] *** Quits: tkazi (~tkazi@user/tkazi) (Quit: Disconnected)
[08:51:01] *** Joins: tkazi (~tkazi@user/tkazi)
[08:51:01] *** Quits: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br) (Read error: Connection reset by peer)
[08:51:15] *** Joins: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br)
[09:01:52] *** Quits: keypusher (keypusher@user/keypusher) (Ping timeout: 252 seconds)
[09:16:49] *** Joins: onizu (uid373383@id-373383.highgate.irccloud.com)
[09:18:15] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[09:22:35] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[09:27:52] *** Joins: richard_h (~richard@2406:e001:8:a900:6e62:6dff:fe05:ae29)
[09:29:28] *** Quits: tang^ (~doofus@2604:3d09:47c:f970:e46f:ebf5:a9:e73e) (Quit: So as I was sayinSQUIRREL)
[09:38:57] *** Joins: tang^ (~doofus@2604:3d09:47c:f970:e46f:ebf5:a9:e73e)
[09:39:06] *** Quits: tang^ (~doofus@2604:3d09:47c:f970:e46f:ebf5:a9:e73e) (Client Quit)
[09:39:59] *** Joins: HaMsTeRs (~mx@14-0-228-101.static.pccw-hkt.com)
[10:17:42] *** Joins: gearnode (~gearnode@2a01cb000ce2c1008ca2093f6cef466a.ipv6.abo.wanadoo.fr)
[10:19:27] *** Joins: TheCoffeMaker (~TheCoffeM@user/thecoffemaker)
[10:21:05] *** Quits: HaMsTeRs (~mx@14-0-228-101.static.pccw-hkt.com) (Quit: Konversation terminated!)
[10:26:18] *** Quits: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963) (Ping timeout: 265 seconds)
[10:36:12] *** Quits: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br) (Read error: Connection reset by peer)
[10:36:41] *** Joins: arinov_ (~arinov@213.194.126.155)
[10:36:52] *** Joins: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br)
[10:54:55] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Remote host closed the connection)
[10:55:25] *** Joins: thc202 (~thc202@user/thc202)
[10:56:16] <geirha> cryocaustik: Are you sure the container got restarted after you modified docker-compose.yml?
[10:57:40] <geirha> docker-compose up -d && docker-compose exec app printenv
[11:00:24] *** Joins: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963)
[11:02:35] *** Quits: Furai (~Furai@cookiehoster.furai.pl) (Quit: WeeChat 3.2)
[11:03:24] *** Joins: rsx (~dummy@ppp-188-174-156-207.dynamic.mnet-online.de)
[11:05:47] *** Joins: Furai (~Furai@cookiehoster.furai.pl)
[11:12:41] *** Quits: ueberall (ueberall_l@user/ueberall) (Remote host closed the connection)
[11:13:55] *** Quits: Furai (~Furai@cookiehoster.furai.pl) (Quit: WeeChat 3.2)
[11:16:31] *** Joins: ueberall (ueberall_l@user/ueberall)
[11:22:39] *** Joins: pvdp (~Pieter@static-n58-105-183-94.rdl4.qld.optusnet.com.au)
[11:24:36] *** Quits: arinov_ (~arinov@213.194.126.155) (Ping timeout: 258 seconds)
[11:26:55] *** Quits: jamiejackson (~jamiejack@207.172.87.34) (Ping timeout: 258 seconds)
[11:27:53] *** Joins: Furai (~Furai@cookiehoster.furai.pl)
[11:31:32] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Remote host closed the connection)
[11:31:56] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[11:35:56] *** Joins: kikijiki (~Thunderbi@user/kikijiki)
[11:37:15] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Ping timeout: 258 seconds)
[11:38:05] *** Quits: sleepingice (~khaalidsu@206.189.80.9) (Remote host closed the connection)
[11:39:16] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[11:45:26] *** Joins: vidbina (~vid@x4dbf57a5.dyn.telefonica.de)
[11:47:30] *** Quits: jjakob (~quassel@2a01:260:8028:10f0::62) (Ping timeout: 240 seconds)
[11:49:03] *** Joins: jjakob (~quassel@2a01:260:8028:10f0[11:50:49] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[11:52:14] *** Church is now known as aleph
[11:56:14] *** Quits: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br) (Read error: Connection reset by peer)
[11:56:30] *** Joins: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br)
[12:02:40] *** Quits: cmc (~methos@gateway/tor-sasl/cmc) (Remote host closed the connection)
[12:03:41] *** Joins: TomyWork (~TomyLobo@p200300e80f133c00dc0e5b5162ee8ae8.dip0.t-ipconnect.de)
[12:03:43] *** Joins: cmc (~methos@gateway/tor-sasl/cmc)
[12:05:28] *** Joins: arinov_ (~arinov@212.156.215.30)
[12:12:56] *** Quits: rsx (~dummy@ppp-188-174-156-207.dynamic.mnet-online.de) (Quit: rsx)
[12:14:44] *** Joins: goldfish (~goldfish@user/goldfish)
[12:23:11] *** Joins: tex (~super@user/dix)
[12:44:10] *** Joins: night_wulfe_ (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[12:45:03] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Remote host closed the connection)
[12:45:48] *** Joins: arcarius (~ilius@75-169-68-172.slkc.qwest.net)
[12:46:42] *** Joins: varaindemian (~varaindem@86.124.78.162)
[12:47:20] *** Quits: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 268 seconds)
[12:48:34] *** Quits: ilius (~ilius@c-76-27-78-11.hsd1.ut.comcast.net) (Ping timeout: 268 seconds)
[12:51:03] *** Joins: wattux (~wattux@212.51.23.106)
[12:52:17] *** Joins: SparkleKraken (~SparkleKr@97e407ec.skybroadband.com)
[13:16:58] *** Quits: MC_Raw (uid504644@id-504644.charlton.irccloud.com) (Quit: Connection closed for inactivity)
[13:30:12] *** Quits: kikijiki (~Thunderbi@user/kikijiki) (Quit: kikijiki)
[13:43:03] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[13:46:44] *** Joins: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net)
[13:55:10] *** Quits: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br) (Read error: Connection reset by peer)
[13:55:36] *** Joins: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br)
[13:56:27] *** Joins: jonifen (~jonifen@user/jonifen)
[14:03:18] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Remote host closed the connection)
[14:04:10] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[14:05:28] *** Quits: Ohlov3 (~Ohlov3@dyndsl-091-248-233-151.ewe-ip-backbone.de) (Ping timeout: 252 seconds)
[14:10:20] *** Joins: Lutin (~Lutin@user/lutin)
[14:11:34] <Lutin> morning all!
[14:14:17] *** Joins: jkwnki (~jkwnki@p4fedbaf0.dip0.t-ipconnect.de)
[14:15:20] <artok> afternoon
[14:17:36] *** Joins: bancroft (~bancroft@bras-base-mtrlpq02hsy-grc-09-76-68-189-147.dsl.bell.ca)
[14:18:09] <bancroft> is there a limit to how many layers my docker image can have? 
[14:20:04] <Lutin> artok yo! you are early or late :D
[14:21:54] *** Quits: rowbots (~dirgeable@h69-129-115-197.arvdco.broadband.dynamic.tds.net) (Ping timeout: 240 seconds)
[14:31:48] *** Quits: wattux (~wattux@212.51.23.106) (Quit: Client closed)
[14:37:54] <artok> 1pm here
[14:40:32] *** Quits: Lutin (~Lutin@user/lutin) (Ping timeout: 265 seconds)
[14:44:51] *** Joins: Lutin (~Lutin@user/lutin)
[14:45:27] <Lutin> artok so you didn't ran out of beer yet ?
[14:48:02] <artok> ran out of beer long time ago
[14:49:46] *** Joins: keypusher (keypusher@user/keypusher)
[14:50:02] <Lutin> and why didn't you say so we can beer you ?
[14:54:27] *** Joins: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9)
[14:54:34] <mihael> anyone used vagrant with docker? 
[14:56:47] <Lutin> nope, I doubt if I would
[14:56:52] *** Joins: Crassus (~Crassus@user/crassus)
[14:57:30] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Read error: Connection reset by peer)
[14:57:41] *** Joins: thiras (~thiras@user/thiras)
[14:59:42] <mihael> Yeah, I have an existing proiject with vagrant/virtualbox. But since virtualbox is not supported on the new Apple M1 chip, I'm currently looking for an alternative
[15:03:53] *** Joins: SomeWeirdAnon (~shwn@2a02:8109:abf:ffb4:6906:756f:b2f4:2084)
[15:07:25] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-17-50-101-91-221.dsl.bell.ca)
[15:07:28] <a[15:54:24] <tabakhase> (with that "based on centos" the note that the hub is your yum. - if you build an image for something that has an official image you better have a very good reason todo so... shouldnt have to touch those base-os-images until youre quite a bit into the docker-journey ;-) )
[15:54:36] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[15:55:12] <mihael> In vagrant, I can build the vm, configure some network stuff, then have vm.provision "ansible" to provision with via ansible. I was wondering how to do that in Docker Compose
[15:56:25] <tabakhase> "docker is not a vm" - containers, and those are ontop very "opinionated" so yea, forget all that :D docker does different....
[15:56:31] *** SomeWeirdAnon is now known as SomeAwayAnon
[15:58:24] <tabakhase> so ya not gonna build a docker with all your stuff, but rather you make a compose that pulls down a mysql, memcache and tomcat - and you "build" your app out of tomcat with a small Dockerfile and raises that all up
[15:59:12] <mihael> Sorry for the confusion, I'm a bit confused myself, hehe. What's I'm trying to do is translate this vagrantfile: https://pastebin.com/NihL4NFJ into docker-compose
[15:59:28] *** Quits: keypusher (keypusher@user/keypusher) (Remote host closed the connection)
[16:00:18] *** Joins: keypusher (keypusher@user/keypusher)
[16:01:36] *** Quits: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9) (Quit: Client closed)
[16:12:13] *** Joins: lithium (~lithium@user/lithium)
[16:12:42] *** Quits: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963) (Ping timeout: 252 seconds)
[16:13:01] *** Joins: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963)
[16:15:02] *** Quits: richard_h (~richard@2406:e001:8:a900:6e62:6dff:fe05:ae29) (Quit: Leaving.)
[16:25:38] *** Quits: vidbina (~vid@x4dbf57a5.dyn.telefonica.de) (Ping timeout: 268 seconds)
[16:31:14] *** Joins: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9)
[16:31:41] <mihael> got disconnected, oh, I guess the web client can't retrieve history
[16:33:17] *** Joins: rewrit3 (~rewrit3@user/rewrit3)
[16:34:21] *** Quits: pvdp (~Pieter@static-n58-105-183-94.rdl4.qld.optusnet.com.au) (Remote host closed the connection)
[16:37:22] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 268 seconds)
[16:41:01] *** Quits: lucerne (~lucerne@ip202.ip-51-178-215.eu) (Read error: Connection reset by peer)
[16:41:32] *** Quits: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9) (Quit: Client closed)
[16:42:38] *** Joins: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9)
[16:42:53] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[16:56:24] *** Quits: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br) (Quit: The Lounge - https://thelounge.chat)
[16:57:22] *** Joins: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br)
[17:15:57] *** Joins: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net)
[17:18:41] *** Quits: SirScott (~SirScott@c-67-176-100-163.hsd1.co.comcast.net) (Quit: Ping timeout (120 seconds))
[17:19:05] *** Joins: SirScott (~SirScott@c-67-176-100-163.hsd1.co.comcast.net)
[17:20:31] *** Quits: jkwnki (~jkwnki@p4fedbaf0.dip0.t-ipconnect.de) (Ping timeout: 268 seconds)
[17:22:30] *** Joins: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de)
[17:31:06] *** Quits: gschanuel (~gschanuel@200-181-252-244.user3p.brasiltelecom.net.br) (Changing host)
[17:31:06] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[17:31:20] *** Quits: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net) (Quit: andydude)
[17:33:18] *** Quits: realies (~realies@user/realies) (Ping timeout: 264 seconds)
[17:34:05] *** Quits: zakame (~zakame@user/zakame) (Ping timeout: 268 seconds)
[17:34:46] *** Joins: realies (~realies@user/realies)
[17:34:58] *** Quits: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9) (Ping timeout: 246 seconds)
[17:37:04] *** Joins: ariedro (~ariedro@user/ariedro)
[17:37:40] *** Quits: realies (~realies@user/realies) (Client Quit)
[17:38:05] *** Joins: realies (~realies@user/realies)
[17:39:13] *** Joins: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9)
[17:43:44] <tabakhase> ye as said that isnt a 1:1 thing -- docker can have "fat containers" that you kinda threat like a vm - and youd run your ansible-build inside the docker-build using a Dockerfile - but you still need mods, as there is no actual OS running, you need a init-system (s6, runit, dumb-init...) where at that point it been simpler to just go the dockerish way from the start kinda ;-)   <- only thing 
[17:43:45] <tabakhase> you missed mihael
[17:44:44] *** Quits: jaskal (jaskal@user/jaskal) (Ping timeout: 252 seconds)
[17:45:15] <tabakhase> and ignore compose for the "first few minutes" - its just a fancy way to store multiple docker-commands -- well worth using, but can hinder a little on the first understanding on whats going on i guess...
[17:45:16] *** Quits: cmc (~methos@gateway/tor-sasl/cmc) (Ping timeout: 244 seconds)
[17:45:16] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Ping timeout: 244 seconds)
[17:45:28] *** Quits: varaindemian (~varaindem@86.124.78.162) (Ping timeout: 246 seconds)
[17:46:30] *** Joins: jaskal (jaskal@user/jaskal)
[17:47:54] <mihael> tabakhase thanks, can I trigger an ansible provision within Dockerfile? 
[17:47:55] *** Joins: cmc (~methos@gateway/tor-sasl/cmc)
[17:50:00] <tabakhase> mihael see above, technically yes, do you actually want this? (as in "beeing able to reuse your vagrant installers") thats more a no...
[17:50:31] <mihael> ah, so what's the best way to approach this?
[17:50:44] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Ping timeout: 268 seconds)
[17:50:57] <mihael> Manually run a container then run ansible-playbook against it?
[17:53:18] *** derikei is now known as klanh
[17:53:21] <tabakhase> dockerize it and rip out ansible ;-) - or at least modify your ansible roles to be dockerable and build corresponding images
[17:53:26] *** klanh is now known as klmr
[17:53:55] <tabakhase> for more details wed kinda need to know what your playbook entails
[17:54:20] *** Joins: rodybig (~rodybig@103.134.135.42)
[17:55:01] *** rodybig is now known as qasidy
[17:56:26] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Quit: WeeChat 3.2)
[17:56:39] <qasidy> Hello I am running a kubernetes cluster locally on my machine. I'm trying to see how much CPU the container is using but docker stats and kubectl top nodes do not agree with each other. 
[17:57:32] <qasidy> For example, docker stats shows that the container's cpu usage hits > 100% when I add load to the server but kubectl top nodes only goes to 3-6%
[17:57:33] <qasidy> Why 
[17:57:35] <qasidy> Why is that?
[17:57:37] <mihael> tabakhase Okay, thanks, I guess I'll just need to try it out
[17:57:54] *** Quits: jazzy (~jaziz@2600:380:c06b:9daa:64dc:a2b0:2f22:7e0f) (Ping timeout: 264 seconds)
[17:57:57] <qasidy> I should clarify the setup: I am running KIND to create the kubernetes cluster locally. 
[17:58:33] <qasidy> KIND runs a single "control plane" container in my host machine which houses all the subsequent application containers
[17:58:43] <qasidy> Two layers of containers
[17:58:48] <qasidy> essentially
[17:58:52] *** Joins: zakame (~zakame@user/zakame)
[17:59:40] <tabakhase> mihael ye... its a bit of headbanging, docker doesnt rly have a learnign "curve" - its more a siries of steps ;D - but in the end its not really thaaat much extra if you got some linux basics and then things will click together pretty quickly once you got over the first hill ;-)
[17:59:53] <qasidy> Therefore it does make sense to compare docker stats (on the control plane container on the host) and kubectl top nodes (on the kubernetes node -- which is basically the control plane container as it is the "node" housing the entire cluster)
[18:00:09] <programmerq> qasidy▸ I haven't used kind much, but I have seen that the kind components themselves can get a little cpu hungry. in fact, my laptop fan is spinning likely because I restarted docker desktop which likely started a kind cluster I created a while back.
[18:00:18] <mihael> tabakhase Thank. you for helping me out. :)
[18:00:21] <qasidy> So then why don't the two dashboards agree with each other? :(
[18:01:28] <qasidy> programmerq Sure, that is understandable. Couple more details -- the server I'm using to run this is beefy - 32 cores. And regardless, under low load, there's no problem. Docker stats is under 50% and kubectl top is at 0%
[18:01:56] <qasidy> It's when I increase the load on my server (which is basically the app housed in the container), that things get hairy. 
[18:02:20] <programmerq> speculating-- it's possible that the kubectl top is calculating against the entire server's resources, but docker stats may be calculating against any set limits.
[18:02:34] <tabakhase> mihael and its def. worth it :P -- if you want some "easy to start" - i usually recommend building CI pipeline or such first (there one can do dirty thigns and ignore a few rules while messing around ;D) while having a neat mix of having to build own things/mod tooling to fit your needs and using existing tooling (so linting, testing, coverage and such) --- and you see pretty quick its like 
[18:02:35] <tabakhase> 5 different commands and 10 different arguments that do 99% of the work
[18:02:48] <qasidy> programmerq I also checked the cpu limits for docker -- sudo docker container inspect control-plane-container | grep -i cpu
[18:03:18] <qasidy> I get 0 for everything. CPUShares, NanoCPUs, CPUPeriod, CPUQuota -- you get the picture. 
[18:03:59] <programmerq> not sure then-- it'll boil down to both tools using a different calculation of some sort.
[18:04:07] <programmerq> but I don't know the details. someone else might.
[18:04:09] <qasidy> Does 0 imply limit as 0? Or just that it's the lowest priority if eviction is required? A github issue suggests the latter: https://github.com/kubernetes/kubernetes/issues/86244
[18:04:36] <qasidy> programmerq Yes, you're probably right. Kubectl might be calculating against entire server's max. 
[18:05:28] <programmerq> some tools show % by core capacity. so a 32 core system with all 32 cores would show 3200% being used maximum.
[18:05:43] <qasidy> Oh man. 
[18:06:35] <tabakhase> if you think thats funny, go look how the load number is calculated :D :D
[18:07:03] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Quit: The Lounge - https://thelounge.chat)
[18:07:22] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[18:08:21] *** Quits: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9) (Quit: Client closed)
[18:08:55] <programmerq> yeah, linux load average calculation is pretty nutso. I'd argue that it hasn't been really useful since the days where single core servers were de facto
[18:08:56] <qasidy> tabakhase you mean the issue with instantaneous load number going above 100%?
[18:09:11] *** Joins: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9)
[18:10:32] <qasidy> programmerq So I think I agree that they're just measuring against different things. That makes sense. 
[18:10:44] <tabakhase> https://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html
[18:10:51] <tabakhase> that breaks it down pretty good
[18:11:03] <qasidy> What is the right way to check the actual limits placed on a container? This cpu: 0 thing is just confusing. 
[18:11:18] <qasidy> tabakhase will check that out. That url seems super familiar. 
[18:11:22] <programmerq> that just sounds like "no limit"
[18:11:51] <qasidy> Then shouldn't my docker stat and kubectl top agree with each other?
[18:12:01] <qasidy> Because even if they are measuring against different things
[18:12:16] <programmerq> if they're measuring different things, then they could show different numbers.
[18:12:33] <programmerq> regardless of limit or not, they can still be measuring different things.
[18:12:40] <qasidy> docker container now has the entire host's cpu at its disposal so that's what it is comparing its usage to
[18:12:47] <qasidy> hmm
[18:12:53] <qasidy> I don't quite follow
[18:13:03] <qasidy> Okay, so basically why is the docker container's usage hitting 100?
[18:13:05] <programmerq> whether there is a limit or not, a docker container can "see" all the cpus on the host.
[18:13:19] <programmerq> what does top show on the docker host?
[18:13:37] <qasidy> Sorry by host do you mean the actual host or the control-plane container?
[18:14:06] *** Quits: fey- (~fey@user/fey) (Read error: Connection reset by peer)
[18:14:06] <programmerq> the actual host
[18:14:32] *** Joins: fey- (~fey@user/fey)
[18:14:42] <qasidy> Yes, so when I put the server under high load, all the cores react and go to 10% - 15%
[18:14:52] <qasidy> But non of them are even close to being full
[18:15:09] <qasidy> So presumably the container can run on all the cores but just have some limitation on how much it can use?
[18:16:56] <programmerq> any given process can only use up to one core-- whether it's dockerized or not.
[18:17:19] <qasidy> Oh. Then I don't really understand what's going on. 
[18:17:27] <programmerq> unless it has multiple threads
[18:18:33] *** Quits: realies (~realies@user/realies) (Remote host closed the connection)
[18:18:55] *** Joins: realies (~realies@user/realies)
[18:19:28] <qasidy> programmerq I think I know why. The load generator itself resides on the same host. So when it starts up to create the requests and generate high load, it runs multiple threads. 
[18:19:52] <qasidy> Of course, this is not a great way of load testing. I'm not bothered about that right now. Just trying to figure out the cpu limit for the container atm. 
[18:20:07] <qasidy> So I guess, the multiple threads just cause the cores to go to 10%?
[18:20:40] <qasidy> I just ran the load generator to hit example.com instead of the service sitting inside the host. 
[18:20:46] <qasidy> All cores light up again. 
[18:20:58] <qasidy> But ~not as much~
[18:22:10] <qasidy> And the load generator is supposed to use only 8 threads. So it can't possibly account for the simultaneous jump in all cores. 
[18:23:20] <qasidy> Ran the load generator to hit the internal service again. The cpu graphs are certainly different from when it was hitting the external resource. All cores react immediately and go to 15-25% as opposed to some cores spiking to 10%
[18:23:55] <qasidy> programmerq What about containers within containers like in KIND? Are they all constrained to use the core the parent container is using?
[18:24:02] *** Joins: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net)
[18:31:14] <qasidy> Also if someone can clarify, as long as KIND's application code or my service's application code is multi-threaded, all CPU cores would be simultaneously used right? If not, I understand that a single process's single thread can only run on 1 core at a time. 
[18:33:50] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Quit: The Lounge - https://thelounge.chat)
[18:34:46] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[18:41:21] *** Joins: geirha1 (~geirha@user/geirha)
[18:41:50] *** Quits: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com) (Quit: WeeChat 3.0.1)
[18:43:51] *** Quits: geirha (~geirha@user/geirha) (Ping timeout: 268 seconds)
[18:44:08] *** geirha1 is now known as geirha
[18:44:42] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[18:46:22] <s17> htop could tell you
[18:46:28] *** Joins: clf59 (~clf59@user/clf59)
[18:51:24] *** Quits: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de) (Ping timeout: 250 seconds)
[18:51:27] *** Quits: clf59 (~clf59@user/clf59) (Quit: Textual IRC Client: www.textualapp.com)
[18:51:51] *** Joins: clf59 (~clf59@user/clf59)
[18:52:01] *** Quits: clf59 (~clf59@user/clf59) (Client Quit)
[18:52:18] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[18:53:29] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[18:57:58] <programmerq> qasidy▸ a container is just some linux kernel namespaces and stuff to lie to a group of processes. if you don't have any limits or cpu affinity set for a container, any process in that container (even if it's in a child set of namespaces since it's in a containerized container), it can still run any number of processes on any core.
[18:58:43] <programmerq> so if I have a container with no limits, and two single threaded processes are running in it, then they run like any non-containerized process on the host as far as cpu scheduling goes.
[18:59:12] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Ping timeout: 250 seconds)
[19:12:54] *** Quits: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9) (Quit: Client closed)
[19:16:58] *** Joins: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de)
[19:19:58] *** Joins: Smeagol (~Bugz000@telf-15-b2-v4wan-161011-cust323.vm31.cable.virginm.net)
[19:20:56] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:21:13] <Smeagol> Hey i'm trying to unpack a docker image to run on the host OS as i'm sick of having to dip into an "internal" shell to get things done... https://nginxproxymanager.com/advanced-config/ this is the package i'm trying to unpack, i have a python script  here which says it can do it, i can't seem to get the docker container to export tar to STDout though, the name appears to evade me... even the examples seem to just 
[19:21:13] <Smeagol> *magically* pull these unrelated names from nowhere, could anyone shed light on how best to go about this?
[19:23:19] *** Quits: SparkleKraken (~SparkleKr@97e407ec.skybroadband.com) (Quit: Leaving)
[19:28:10] *** Joins: ausserz (~ausserz@user/ausserz)
[19:28:38] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[19:29:49] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:30:38] <ada_> Smeagol: not sure what you're trying to accomplish here
[19:30:56] <ada_> Smeagol: what is your end goal?  why are you "unpack[ing] a docker image to run on the host OS"
[19:31:45] *** Joins: austin_ (~ausserz@user/ausserz)
[19:32:14] <Smeagol> for easier access to the various config files
[19:32:18] *** Joins: manuel1985 (~manuel198@62.99.131.178)
[19:32:34] <Smeagol> https://bugz000.co.uk/i/LpEqs0.png
[19:33:02] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[19:33:25] <Smeagol> unless you know where these are, i can't find them from the host OS, only once on an "internal shell" 
[19:33:48] <Smeagol> WAIT
[19:33:54] <ada_> im confused
[19:33:58] <Smeagol> rofl... i'm an idiot dont worry
[19:33:58] *** Joins: vidbina (~vid@x4dbf57a5.dyn.telefonica.de)
[19:33:59] <Smeagol> found them
[19:34:03] <Smeagol> -facedesk- ...
[19:34:13] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:34:18] <Smeagol> it was really very obvious 
[19:34:30] <Smeagol> well that was embarassing, thankyou for the help regardless rofl
[19:35:16] *** Quits: ausserz (~ausserz@user/ausserz) (Ping timeout: 258 seconds)
[19:36:25] *** Quits: austin_ (~ausserz@user/ausserz) (Ping timeout: 258 seconds)
[19:36:33] *** Joins: strudl (~strudl@user/strudl)
[19:36:36] <manuel1985> On my machine, docker seems to have prevented ip forwarding between two unrelated network interfaces. (https://unix.stackexchange.com/questions/655332/whats-preventing-my-ping-packets-from-getting-routed?noredirect=1#comment1232595_655332) Does anyone have any background info on that? Is this a known bug, or is it due to how docker works, or is there a configuration setting I overlooked?
[19:36:39] <Smeagol> i was doing the equivalent of "oh look a pot-hole in the road.... let's dig the whole road lower so the pot-hole goes away" 
[19:37:21] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[19:37:55] <manuel1985> Oh someone already provided the answer in the question I linked: https://docs.docker.com/network/iptables/#docker-on-a-router
[19:38:31] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:41:44] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[19:42:55] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:44:51] *** Quits: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net) (Ping timeout: 258 seconds)
[19:46:07] *** Parts: manuel1985 (~manuel198@62.99.131.178) (Leaving)
[19:46:08] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[19:46:18] *** Joins: oxum (~oxum@106.203.210.162)
[19:47:19] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:49:25] *** Quits: TomyWork (~TomyLobo@p200300e80f133c00dc0e5b5162ee8ae8.dip0.t-ipconnect.de) (Quit: Leaving)
[19:49:34] *** Joins: sesquisentient (uid38151@id-38151.stonehaven.irccloud.com)
[19:50:33] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[19:51:14] *** Quits: oxum (~oxum@106.203.210.162) (Ping timeout: 252 seconds)
[19:51:42] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:54:07] *** Joins: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net)
[19:54:51] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[19:56:02] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[19:56:21] *** Quits: tex (~super@user/dix) (Ping timeout: 258 seconds)
[19:58:35] <qasidy> Is it possible to increase the CPU limit for a running docker container?
[19:59:12] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[20:00:10] *** Joins: palindrome (~vargasj@109.77.70.228)
[20:00:23] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[20:00:30] *** Joins: endigma6 (~endigma@134.41.133.214)
[20:00:56] *** endigma6 is now known as endigma
[20:03:33] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[20:04:12] <ada_> qasidy: docker update --help
[20:04:45] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[20:04:56] *** Joins: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9)
[20:05:44] *** Joins: levifig (~textual@2001:8a0:e800:2300:3c96:8614:5b4d:8e0b)
[20:06:40] <statusbot> Status update: Docker Hub is not displaying the repositories. This incident has been identified and resolved. -- https://status.docker.com/pages/incident/533c6539221ae15e3f000031/60d35507310b19053a7d7804
[20:07:08] <ada_> qasidy: you can update the cpu limit on a container while it's running
[20:07:31] *** Quits: thanas (~thanas@user/thanas) (Quit: ZNC - https://znc.in)
[20:08:00] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[20:09:10] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[20:09:30] *** Joins: thanas (~thanas@user/thanas)
[20:11:30] <palindrome> considering the docker-compose.yaml explained here https://github.com/aws-observability/aws-otel-collector/blob/main/docs/developers/docker-demo.md, where it points config to /etc/otel-agent-config.yaml. How can I write that file before doing that?
[20:12:25] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[20:12:29] *** Joins: mikeputnam (~mikeputna@wilug/mikeputnam)
[20:13:38] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[20:15:04] *** Quits: doc (~doc@user/doc) (Quit: Things to do)
[20:15:16] *** Joins: Brainium (~brainium@user/brainium)
[20:15:38] <qasidy> @ada hey thanks. Let me look at docker update. 
[20:16:22] <qasidy> ada_ I ran docker container inspect and it shows that the container uses 0 CPU. Why is that?
[20:16:31] <qasidy>             "CpuShares": 0,
[20:16:32] <qasidy>             "NanoCpus": 0,
[20:16:32] <qasidy>             "CpuPeriod": 0,
[20:16:33] <qasidy>             "CpuQuota": 0,
[20:16:33] <qasidy>             "CpuRealtimePeriod": 0,
[20:16:34] <qasidy>             "CpuRealtimeRuntime": 0,
[20:16:34] <qasidy>             "CpusetCpus": "",
[20:16:35] <qasidy>             "CpusetMems": "",
[20:16:35] <qasidy>             "CpuCount": 0,
[20:16:36] <qasidy>             "CpuPercent": 0,
[20:16:51] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[20:18:02] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[20:19:50] <GyrosGeier> I believe these are just the limits, not the actual usage
[20:20:56] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[20:21:12] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[20:21:57] <programmerq> please don't multiline paste in the channel
[20:22:22] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[20:22:30] <qasidy> Sorry my bad. 
[20:22:42] <qasidy> GyrosGeier Yes but what does it mean to have 0 limit?
[20:23:08] <GyrosGeier> no limit
[20:23:19] *** Quits: levifig (~textual@2001:8a0:e800:2300:3c96:8614:5b4d:8e0b) (Quit: Textual IRC Client: www.textualapp.com)
[20:23:26] *** Joins: levifig (~levi@mt.levifig.com)
[20:25:32] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Read error: Connection reset by peer)
[20:26:44] *** Joins: betelgeuse (~john2gb@94-225-47-8.access.telenet.be)
[20:26:51] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[20:28:30] *** Quits: betelgeuse (~john2gb@94-225-47-8.access.telenet.be) (Client Quit)
[20:33:47] <ada_> palindrome: the whole line says that it's using the file from 1 folder up "../config.yaml"  to mount into the container at /etc/otel-agent-config.yaml
[20:33:57] <ada_> palindrome: I assume the original "config.yaml" is contained in that repo
[20:37:48] *** Quits: arinov_ (~arinov@212.156.215.30) (Ping timeout: 252 seconds)
[20:38:12] *** Quits: cliluw (~cliluw@47.147.80.149) (Ping timeout: 265 seconds)
[20:38:54] *** Joins: cliluw (~cliluw@47.147.80.149)
[20:39:46] *** Smeagol is now known as Bugz000
[20:40:39] *** Quits: mihael (~mihael@2001:4454:2c7:6800:fdc0:acab:5394:aab9) (Quit: Client closed)
[20:43:46] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[20:44:07] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[20:45:47] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[20:46:08] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[20:46:14] *** Quits: treethought (treethou@138.68.49.251) (Ping timeout: 252 seconds)
[20:46:39] *** Joins: treethought (treethou@138.68.49.251)
[20:46:45] *** Joins: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com)
[20:49:50] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[20:50:11] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[20:54:38] *** Quits: treethought (treethou@138.68.49.251) (Remote host closed the connection)
[20:55:00] *** Joins: tang^ (~doofus@2604:3d09:47c:f970:8571:cbb4:5901:49bf)
[20:57:06] *** Quits: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de) (Quit: WeeChat 2.8)
[20:57:26] *** Joins: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de)
[20:57:30] *** Joins: treethought (treethou@138.68.49.251)
[20:58:26] *** Quits: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de) (Client Quit)
[20:58:43] *** Joins: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de)
[20:58:56] *** Quits: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de) (Client Quit)
[20:59:13] *** Joins: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de)
[20:59:17] *** Quits: jkwnki (~jkwnki@p2e579aae.dip0.t-ipconnect.de) (Client Quit)
[21:02:26] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[21:02:47] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[21:04:27] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[21:04:48] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[21:04:49] *** Joins: fabienwang (~fabienwan@user/fabienwang)
[21:10:29] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[21:10:45] *** Quits: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net) (Quit: andydude)
[21:10:50] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[21:12:24] <qasidy> GyrosGeier I understand that 0 means no limit but what happens when you have CPU contention? Does this container get low priority?
[21:13:43] <programmerq> same thing that happens to any other process on the host when there is contention-- the kernel scheduler kicks in.
[21:16:51] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[21:17:12] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[21:17:56] <qasidy> programmerq Is there any way to prioritize this container over others?
[21:18:50] <qasidy> Meaning can I use docker update to say "whenever this container needs the cpu, prioritize it over others if there's contention"
[21:19:18] <ada_> qasidy: adjust the cpu-shares value
[21:19:28] <ada_> qasidy: a container with more shares gets more "weight" compared to other contianers
[21:19:34] <qasidy> What do I set it to? 1024?
[21:19:45] <ada_> qasidy: depends on what you have _other_ things set to
[21:19:47] <qasidy> But don't all containers have 1024 by default?
[21:19:56] <ada_> increase the weight to increase the priority of that container
[21:20:01] <ada_> but that only affects other containers
[21:20:13] <ada_> processes on the box that aren't in containers will be prioritized according to their "niceness"
[21:20:19] <ada_> https://www.tecmint.com/set-linux-process-priority-using-nice-and-renice-commands/
[21:21:33] <qasidy> Right, that was what I was wondering about. 
[21:21:57] <qasidy> There's only 1 container anyway. It's competing with other running processes. 
[21:22:49] <ada_> what problem are you trying to solve?
[21:23:45] *** Quits: jonifen (~jonifen@user/jonifen) (Quit: Leaving)
[21:25:51] *** Joins: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net)
[21:25:54] <qasidy> Okay, so I'm running the KIND cluster container which houses all my application container
[21:26:03] <qasidy> So from the host pov, there's only 1 container running
[21:26:28] <qasidy> Now let's say I stress test my application
[21:27:10] <qasidy> My KIND container goes to 100 and even beyond in CPU usage according to kubectl top nodes
[21:28:49] <qasidy> Sorry not according to kubectl top nodes
[21:29:00] <qasidy> According to docker stats it goes to 100% or more.
[21:29:25] <qasidy> Kubectl top nodes just goes to 6% or so (presumably because it's comparing against how much host cpu is available)
[21:29:29] <qasidy> Anyway
[21:29:48] <qasidy> And my request latency goes up dramatically when I stress test
[21:29:59] <qasidy> Meaning the CPU is the bottleneck (memory and network is fine)
[21:30:17] <qasidy> So that means the KIND container is limited in how much CPU it can use?
[21:30:31] <ada_> like programmerq said, the container is only a namespace to run a process in
[21:30:38] <ada_> the linux scheduler is doing all the scheduling
[21:30:39] <qasidy> ada_ and basically I want to set it so that the KIND container can use as much as it wants if the host has CPU available. 
[21:30:44] <qasidy> Which is not happening right now. 
[21:30:53] <ada_> how are you sure that it isn't?  can you share your data and your test procedure?
[21:30:56] <ada_> maybe you made an error somewhere
[21:31:01] <ada_> or maybe you are misinterpreting results?
[21:31:01] <qasidy> Yes that's true but the host itself is not at 100
[21:31:18] <qasidy> So it's not that the host has cpu contention and hence the scheduling kicks in
[21:31:34] <qasidy> ada_ I'm not very sure but I can share results. 
[21:31:44] <qasidy> By results you mean output of docker stats and kubectl top nodes?
[21:31:53] <ada_> all the commands you ran AND their output
[21:31:58] <ada_> and show your stress test
[21:32:04] <ada_> maybe your stress test is only single threaded?
[21:32:09] <ada_> idk im just speculating
[21:32:19] <ada_> but if you want someone to check your math you have to show your work at all stages
[21:32:23] <ada_> otherwise we're just guessing 
[21:32:24] <qasidy> Right. 
[21:32:59] <qasidy> But regardless of how many threads my test is using (it's 8 I believe), the CPU usage of the container is going above 100 right?
[21:33:22] <ada_> I guess I can't say without seeing some data
[21:33:30] <ada_> I would probably look at host-level information and not `kubectl` output
[21:33:45] <ada_> because what I really care about is "what is my host doing" not "what is my pseudo-containerized-kube-node doing"
[21:34:12] *** Joins: SJrX (~sjr@2604:3d08:d180:1f1b::7ea)
[21:34:18] <ada_> maybe your KIND container implicitly has some kind of limit set?  idk
[21:34:23] <ada_> there are a lot of variables here
[21:35:44] <SJrX> I'm trying to use docker-in-docker with an ubuntu container (instead of the alpine based one). When I start docker in docker in ubuntu I notice this in the logs: "failed to mount overlay: invalid argument     storage-driver=overlay2". Eventually docker starts locally with the vfs graph driver. The docker-dind image has no issue starting with overlay2. Thoughts on how to debug that?
[21:36:44] <qasidy> ada_ here's the output of docker and kubectl: https://pastebin.com/uKUxt51m
[21:37:20] *** night_wulfe_ is now known as night_wulfe
[21:39:06] <SJrX> Ooooh the next line might have been my answer, apologies. Let me see "exec: "fuse-overlayfs": executable file not found in $PATH  storage-driver=fuse-overlayfs"
[21:41:22] *** Quits: Lutin (~Lutin@user/lutin) (Ping timeout: 268 seconds)
[21:42:22] *** Quits: arcarius (~ilius@75-169-68-172.slkc.qwest.net) (Remote host closed the connection)
[21:44:05] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[21:44:26] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[21:48:26] *** Joins: Sasazuka (~Sasazuka@user/sasazuka)
[21:48:59] *** Quits: andypandy (~andypandy@h-178-174-148-234.A163.priv.bahnhof.se) (Quit: Bye)
[21:51:21] *** Quits: tyson2 (~user@bras-base-toroon0624w-grc-17-50-101-91-221.dsl.bell.ca) (Ping timeout: 258 seconds)
[21:53:03] *** Joins: andypandy (~andypandy@h-178-174-148-234.A163.priv.bahnhof.se)
[21:54:44] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[21:55:05] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[21:58:47] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[21:59:07] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[21:59:11] <SJrX> Nope that error was a red herring fuse-overlayfs is different than overlay2. Even running with strace I don't see any calls that cause that error. 
[22:04:29] <programmerq> SJrX▸ what filesystem is /var/lib/docker in this host?
[22:04:46] <programmerq> is /var/lib/docker/overlay2 writable? is it xfs or zfs or ex4 or something else?
[22:09:17] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[22:09:38] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[22:11:18] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[22:11:39] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[22:13:03] <SJrX> Host is ext4, as best I can tell /var/lib/docker/overlay2 is writable. If I run this command:  "docker run -it --privileged docker:dind  dockerd --host=unix:///var/run/docker.sock --host=tcp://0.0.0.0:2375"  Docker starts up with the overlay2 fs, so I think the host is fine.
[22:14:40] <SJrX> If I instead try and run in this Dockerfile which is just Ubuntu 20.10 following the instructions for installing docker. http://pastie.org/p/5GeJsjFt0ZXZGnl1sKk1wg it complains about "failed to mount overlay: invalid argument storage-driver=overlay2"
[22:15:10] <SJrX> So I imagine it's _something_ with the ubuntu container. Even running with -l debug doesn't shed light.
[22:23:38] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[22:23:59] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[22:24:34] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-17-50-101-91-221.dsl.bell.ca)
[22:26:46] *** Quits: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net) (Remote host closed the connection)
[22:28:28] *** Joins: cloaked1 (~cloaked1@174-126-226-132.cpe.sparklight.net)
[22:30:16] *** Quits: Crassus (~Crassus@user/crassus) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[22:36:58] *** Quits: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net) (Quit: andydude)
[22:40:05] *** Quits: SomeAwayAnon (~shwn@2a02:8109:abf:ffb4:6906:756f:b2f4:2084) ()
[22:47:45] *** Joins: andydude (~arobbins@c-76-111-99-194.hsd1.md.comcast.net)
[22:48:17] *** Joins: iffraff (~quassel@2605:a601:aae0:4a00:a311:7319:6b90:a24e)
[22:51:55] *** Quits: n9nes (~n9nes@user/n9nes) (Ping timeout: 258 seconds)
[22:56:19] *** Quits: andrzejv (~andrzejv@78-56-77-187.static.zebra.lt) (Read error: Connection reset by peer)
[22:56:53] *** Quits: fabienwang (~fabienwan@user/fabienwang) (Quit: fabienwang)
[22:57:15] *** Joins: andrzejv (~andrzejv@78-56-77-187.static.zebra.lt)
[22:58:02] *** Joins: fabienwang (~fabienwan@user/fabienwang)
[23:00:14] *** Joins: pycurious (~Adium@user/pycurious)
[23:00:26] <pycurious> docker volume ls | wc -l —> Gives me 2971 - how do i fix this? 
[23:00:27] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[23:00:51] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[23:00:51] *** Quits: qasidy (~rodybig@103.134.135.42) (Quit: Client closed)
[23:00:52] *** Quits: klmr (~rodybig@103.134.135.42) (Quit: Client closed)
[23:01:03] <pycurious> Gitlab is telling me this -> "Check and remove all unused containers (both dangling and unreferenced) including volumes" on install - what is the correct way to fix this using docker? 
[23:02:35] <pycurious> is "docker volume prune" the right command to fix > docker volume ls | wc -l —> Gives me 2971
[23:02:56] *** Joins: oxum (~oxum@106.203.210.162)
[23:07:15] *** Quits: oxum (~oxum@106.203.210.162) (Ping timeout: 258 seconds)
[23:15:24] *** Joins: clf59 (~clf59@user/clf59)
[23:17:48] *** Quits: lithium (~lithium@user/lithium) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[23:26:33] *** Parts: Bugz000 (~Bugz000@telf-15-b2-v4wan-161011-cust323.vm31.cable.virginm.net) (a̅ͨŕ͔̦e̬ y͕o̡u͠ t̯͠͝a͍̓̎l̠͈͉̿k͎i̡͛ͧn̦͈͉̉g͇͞ t̄o̭͘҉ ỳ̞͡óû̝rͬ͟s̴̵̴̵e̳͎l͗͑͘f͓҉҉ ạg̮̍a̿i̜ň?)
[23:27:57] *** Quits: Maxattax (~max@50-195-160-193-static.hfc.comcastbusiness.net) (Quit: WeeChat 3.2)
[23:28:12] *** Joins: Maxattax (~max@50-195-160-193-static.hfc.comcastbusiness.net)
[23:29:16] *** Joins: tex (~super@user/dix)
[23:33:51] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[23:33:56] *** Quits: onizu (uid373383@id-373383.highgate.irccloud.com) (Quit: Connection closed for inactivity)
[23:34:22] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[23:39:43] *** Joins: Batzy (~Batzy@user/batzy)
[23:40:05] <Batzy> 1. docker is extremely confusing and 2. is it possible to have a docker container establish an ssh tunnel to a remote host that is running my db?
[23:49:41] <programmerq> a docker container is just a fancy way to run a process on a host. It uses some kernel features to isolate that containerized process from other processes on the host. If you can do something with a process (like establish a tunnel), you can do it in a container.
[23:50:51] *** Joins: lrpe (lrpe@188-182-61-227-cable.dk.customer.tdc.net)
[23:52:41] *** Joins: thiras (~thiras@user/thiras)
[23:53:45] <Batzy> hm
[23:53:57] *** Joins: Steeve (~steve@user/steeve)
[23:54:07] <Batzy> im trying to do a django + nginx + uwsgi setup
[23:54:12] <Batzy> and docker is just too confusing
[23:55:29] <programmerq> that's a pretty standard setup. it does take some time to absorb the docker concepts.
[23:55:35] <Batzy> oh ye it does
[23:58:12] *** Quits: superuser (~superuser@9.red-83-40-153.dynamicip.rima-tde.net) (Ping timeout: 250 seconds)
