[01:00:10] *** Joins: georgios (~georgios@user/georgios)
[01:01:17] *** Quits: miique (~miique@181.46.139.166) (Ping timeout: 240 seconds)
[01:06:44] *** Quits: Jong (~Jong@2620:10d:c090:400::5:f0a6) (Read error: Connection reset by peer)
[01:17:06] *** Joins: miique (~miique@181.46.139.166)
[02:34:35] *** Joins: brand0 (~brandon@user/brand0)
[02:35:17] *** Quits: miique (~miique@181.46.139.166) (Ping timeout: 240 seconds)
[02:40:09] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[02:45:24] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[03:10:25] *** Quits: palasso (~palasso@user/palasso) (Read error: Connection reset by peer)
[03:11:23] *** Quits: marcello42 (~mp@2001:1a81:12be:e200:d599:14bd:bbbe:8542) (Quit: WeeChat 3.4)
[04:34:32] *** Joins: georgios (~georgios@user/georgios)
[04:56:11] *** Joins: spinningCat_ (~spinningC@94.54.210.37)
[04:56:16] *** Joins: zvijezda2 (~Foobar@c-98-203-220-45.hsd1.wa.comcast.net)
[04:59:01] *** Quits: spinningCat (~spinningC@about/web/muscles) (Ping timeout: 240 seconds)
[04:59:25] *** Quits: zvijezda (~Foobar@c-98-203-220-45.hsd1.wa.comcast.net) (Ping timeout: 240 seconds)
[05:02:27] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[05:03:56] *** Joins: Codaraxis (~Codaraxis@user/codaraxis)
[05:51:33] *** Quits: shoky (~uuuggg@141.226.193.67) ()
[05:55:11] *** Joins: shoky (uuuggg@141.226.193.67)
[06:20:43] *** Quits: spinningCat_ (~spinningC@94.54.210.37) (Ping timeout: 256 seconds)
[07:53:21] *** zvijezda2 is now known as zvijezda
[08:45:06] *** Quits: `Tim` (~zenguin@user/zenguin) (Quit: Leaving)
[09:12:50] *** Joins: manti7 (~manti7@176.10.104.94)
[09:30:05] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[09:39:57] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 240 seconds)
[09:40:25] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[09:46:05] *** Joins: jlrnick (~josephler@2a01cb040a159400e89369dfc959fe4f.ipv6.abo.wanadoo.fr)
[10:04:59] *** Quits: _flood (flooded@gateway/vpn/protonvpn/flood/x-43489060) (Ping timeout: 256 seconds)
[10:09:14] *** Joins: flooded (flooded@gateway/vpn/protonvpn/flood/x-43489060)
[10:09:49] *** Quits: jlrnick (~josephler@2a01cb040a159400e89369dfc959fe4f.ipv6.abo.wanadoo.fr) (Ping timeout: 240 seconds)
[10:24:35] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[10:25:35] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[10:46:11] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[10:47:33] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[10:47:49] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Ping timeout: 240 seconds)
[10:48:11] *** Quits: sinaowolabi (~SinaOwola@169.159.67.85) (Ping timeout: 256 seconds)
[10:51:57] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[10:53:05] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[10:56:17] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[10:57:32] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[11:01:10] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[11:02:23] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[11:06:16] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[11:07:01] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[11:25:27] *** Joins: jlrnick (~josephler@2a01cb040a159400e89369dfc959fe4f.ipv6.abo.wanadoo.fr)
[11:31:28] *** Joins: spaceseller (~spacesell@31.147.205.13)
[11:32:45] *** Quits: spaceseller (~spacesell@31.147.205.13) (Remote host closed the connection)
[11:39:59] *** Joins: palasso (~palasso@user/palasso)
[11:44:19] *** Quits: jinsun (~quassel@user/jinsun) (Read error: Connection reset by peer)
[11:45:39] *** Joins: jinsun (~quassel@user/jinsun)
[13:57:10] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[14:18:37] *** Quits: jinsun (~quassel@user/jinsun) (Ping timeout: 240 seconds)
[14:20:10] *** Joins: jinsun (~quassel@user/jinsun)
[15:45:01] *** Quits: jlrnick (~josephler@2a01cb040a159400e89369dfc959fe4f.ipv6.abo.wanadoo.fr) (Remote host closed the connection)
[15:46:39] *** Joins: jlrnick (~josephler@2a01cb040a159400e89369dfc959fe4f.ipv6.abo.wanadoo.fr)
[15:52:20] *** flooded is now known as _flood
[16:10:21] *** Joins: sinaowolabi (~SinaOwola@169.159.67.85)
[16:26:55] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[16:44:02] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Read error: No route to host)
[16:45:20] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[16:49:24] *** Joins: marcello42 (~mp@2001:1a81:12d6:fe00:339f:db3c:2449:7044)
[17:54:40] *** Joins: georgios (~georgios@user/georgios)
[18:07:44] *** dostoyev1ky is now known as dostoyevsky
[18:45:49] *** Quits: sinaowolabi (~SinaOwola@169.159.67.85) (Read error: No route to host)
[18:46:24] *** Joins: sinaowolabi (~SinaOwola@169.159.67.85)
[18:46:52] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Read error: No route to host)
[18:51:18] *** Joins: trace987 (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de)
[18:58:47] *** Quits: sinaowolabi (~SinaOwola@169.159.67.85) (Ping timeout: 256 seconds)
[19:00:13] *** Joins: sinaowolabi__ (~SinaOwola@169.159.67.85)
[19:24:29] *** Joins: cln (sid336875@id-336875.ilkley.irccloud.com)
[19:31:30] <dostoyevsky2> https://www.youtube.com/watch?v=laJX0txJc6M
[19:31:31] <SigmoidFroid> ⇒  'The Writer' Automaton - YouTube, 00:04:48
[19:33:57] *** Quits: HuntsMan (~hunts@p5494d423.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[19:39:27] *** Joins: spaceseller (~spacesell@31.147.205.13)
[19:42:10] <dostoyevsky2> https://www.youtube.com/watch?v=WofWNcMHcl0
[19:42:11] <SigmoidFroid> ⇒  JAQUET DROZ CORPORATE MOVIE - YouTube, 00:17:49
[19:42:37] *** Quits: trace987 (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de) (Ping timeout: 240 seconds)
[19:43:14] *** Joins: trace987 (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de)
[20:15:51] *** Quits: trace987 (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de) (Ping timeout: 256 seconds)
[20:18:21] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[20:19:17] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[20:19:35] <[itchyjunk]> Can we discuss what an encoder and a decoder is?
[20:21:48] <mefistofeles> sure
[20:21:58] <mefistofeles> I have to go for a run, but will read the logs when I come back
[20:22:08] *** Joins: trace987 (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de)
[20:22:47] <[itchyjunk]> I am starting with this :
[20:22:48] <[itchyjunk]> https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a?gi=2f2d39fe28bc
[20:22:50] <SigmoidFroid> ⇒  What is an encoder decoder model? | by Nechu BM | Towards Data Science
[20:23:04] <[itchyjunk]> My goal is to be able to understand this a bit more :
[20:23:05] <[itchyjunk]> https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[20:23:07] <SigmoidFroid> ⇒  Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs (569K)
[20:23:20] <hodapp_> [itchyjunk]: that is a good paper to understand for sure
[20:23:54] <hodapp_> do you understand autoencoders reasonably well?
[20:25:40] <[itchyjunk]> Hm, i skimmed through that stuff a few years back :(
[20:25:48] <[itchyjunk]> i'd have to relearn that too, probably
[20:26:02] <[itchyjunk]> any good place to look up definitions of these type of ML concepts?
[20:26:28] <hodapp_> been awhile since I've looked anything up on autoencoders but the high-level principles should be the same wherever you look
[20:27:53] <[itchyjunk]> great
[20:28:17] <mefistofeles> https://www.deeplearningbook.org/contents/autoencoders.html
[20:28:18] *** Joins: trace (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de)
[20:28:18] <SigmoidFroid> ⇒  (HTML: No title)
[20:28:29] <mefistofeles> might help
[20:28:30] *** Quits: trace987 (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de) (Remote host closed the connection)
[20:29:55] <hodapp_> i.e. you have one part of the net (the encoder) which just learns to convert an input into some kind of embedded space; you have typically some kind of constraint in that embedded space (e.g. sparsity, lower dimensionality); you have another part after that (the decoder) which learns how to turn from the embedded space, back to an output that should equal input
[20:30:00] <[itchyjunk]> So it was something like, data -> map it to some high dimension vector space -> encoder tries to take this and find a lower dimensional representation and autoencoder is something that does this encoding and validation by trying to generate original output?
[20:30:27] <[itchyjunk]> ah that decoder is part of autoencoder, right
[20:31:01] <hodapp_> yeah
[20:35:07] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 256 seconds)
[20:35:07] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[20:35:43] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[20:43:01] *** Quits: trace (~trace@dynamic-002-247-248-171.2.247.pool.telefonica.de) (Quit: Leaving)
[20:43:55] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[20:44:12] <[itchyjunk]> Computer froze
[20:44:14] <[itchyjunk]> what did i miss?
[20:44:58] *** Joins: sonny (~sonny@bras-base-london1483w-grc-32-70-52-175-166.dsl.bell.ca)
[20:45:26] <[itchyjunk]> sonny, hi. so far, i'm reading up on encoder and decoders. I just read up on RNN
[20:45:34] <[itchyjunk]> https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630
[20:45:36] <SigmoidFroid> ⇒  What is a Recurrent Neural Network (RNN)? | by Nechu BM | The Startup | Medium
[20:46:45] <[itchyjunk]> What does "sequence transduction model" mean?
[20:47:32] <[itchyjunk]> https://machinelearningmastery.com/transduction-in-machine-learning/
[20:47:33] <SigmoidFroid> ⇒  Gentle Introduction to Transduction in Machine Learning
[20:47:49] <[itchyjunk]> Ah apparently it just means to map?
[20:48:16] <sonny> heh, a NN is a graph
[20:49:26] <sonny> I'm just reading wikipedia on NN
[20:49:35] <sonny> maybe I can catch up to you
[20:50:07] <[itchyjunk]> NN is a graph?
[20:50:19] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[20:50:21] <[itchyjunk]> i thought of an ANN as a function (at least approximately)
[20:51:10] <sonny> probably, I might just be getting caught up in representation
[20:51:15] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[20:51:22] <sonny> "The connections of the biological neuron are modeled in artificial neural networks as weights between nodes."
[20:51:54] <sonny> <https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/330px-Neural_network_example.svg.png>
[20:52:05] <SigmoidFroid>  ⇒  (PNG) No title 23K
[20:52:39] <[itchyjunk]> I can't fully grasp what transduction is
[20:53:01] <[itchyjunk]> Is it where specific input is mapped to specific output without learning happening or something?
[20:53:48] <sonny> "transduction or transductive inference is reasoning from observed, specific (training) cases to specific (test) cases."
[20:54:13] <sonny> not sure why they use parens there lol
[20:54:24] <[itchyjunk]> I read that but I don't understand what it really mean
[20:54:28] <[itchyjunk]> Give me an easy example
[20:56:00] <sonny> hmm, transduction is reasoning from observed events to specific events, induction is from observed events to general principles
[20:56:10] <sonny> as for an example, let me think
[20:56:52] <[itchyjunk]> oh like if i slip and fall and hit my head on the counter in the bathroom
[20:56:58] <sonny> maybe like seeing a storm and noting lightning strikes metal spires
[20:57:00] <[itchyjunk]> "all bathrooms are slippary" is induction
[20:57:08] <[itchyjunk]> and "this bathroom is slippary" is transduction?
[20:57:08] <sonny> yeah
[20:57:13] <sonny> yeah
[20:57:26] <[itchyjunk]> oh interesting, i was blissfully unaware of "transduction" in logic then
[20:57:32] <sonny> me too
[20:58:07] <sonny> well, induction is more useful to us :P
[20:58:34] <[itchyjunk]> Is a convolutional neural network a feed forward neural network?
[21:00:35] <sonny> yes
[21:01:16] <sonny> well, as long as there is no cycle in the graph
[21:01:30] <sonny> the encoder/decoder stuff sounds like an autoendcoder
[21:04:25] <[itchyjunk]> autoencoder is an example of it, apparently
[21:04:35] <[itchyjunk]> since autoencoder has both and encoder and a decoder
[21:05:28] <[itchyjunk]> "The best
[21:05:28] <[itchyjunk]> performing models also connect the encoder and decoder through an attention mechanism"
[21:05:35] <[itchyjunk]> Is RNN considered an attention mechanism?
[21:05:46] <[itchyjunk]> it does have memory of previous input so i would think yes, no?
[21:06:23] <sonny> how does a RNN have memory?
[21:06:51] <[itchyjunk]> the hidden layers are connected
[21:06:57] <sonny> unless it's actually a state machine :O
[21:07:11] <[itchyjunk]> https://medium.com/swlh/introduction-to-recurrent-neural-networks-rnn-c2374305a630
[21:07:12] <SigmoidFroid> ⇒  What is a Recurrent Neural Network (RNN)? | by Nechu BM | The Startup | Medium
[21:07:16] <[itchyjunk]> picture there explains it well
[21:07:22] <[itchyjunk]> look at the arrow in hiddel layer
[21:07:42] <[itchyjunk]> when the next word is being "computer", the computed value from the previous word is accounted for
[21:07:51] <[itchyjunk]> so the green circles are "memory" cells
[21:07:54] <sonny> first picture?
[21:08:09] <[itchyjunk]> no 3rd maybe
[21:08:16] <[itchyjunk]> first 2 is FFNN
[21:08:19] <[itchyjunk]> which doesn't have memory
[21:08:35] <[itchyjunk]> you norice how there is no arrows between green circles in the first 2
[21:08:42] <[itchyjunk]> but there is in 3rd one
[21:10:14] <sonny> "Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step."
[21:10:49] *** Quits: Codaraxis (~Codaraxis@user/codaraxis) (Ping timeout: 256 seconds)
[21:11:06] <sonny> I wonder if the previous step is from the recurrent nature, but either way ya it has memory
[21:12:18] <[itchyjunk]> so is that thin in RNN an attention mechanism?
[21:12:30] <[itchyjunk]> It is, i think
[21:13:31] <sonny> "Its flexibility comes from its role as "soft weights" that can change during runtime, in contrast to standard weights that must remain fixed at runtime."
[21:13:46] <sonny> " Uses of Attention include memory in Neural Turing Machines"
[21:13:57] <sonny> yeah, looks like attention
[21:14:06] <[itchyjunk]> wtf is a "neural turing machien" ?
[21:14:07] <hodapp_> uses of attention include...... literally all of the current models setting records in NLP and now in vision
[21:14:21] *** Joins: miique (~miique@181.46.139.166)
[21:14:36] *** Quits: kir0ul5 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[21:14:44] <[itchyjunk]> if i recall, transformer is where you take some sentence and make a matrix out of cartisian product of it's element right"
[21:15:26] <[itchyjunk]> "hi, how are you" -> (hi,how), (hi,are), (hi,you), .., (are,you) or somesuch
[21:15:28] <hodapp_> that's sorta right but it misses a whole lot of very important steps
[21:15:38] *** Joins: kir0ul5 (~kir0ul@89.234.141.147)
[21:15:39] <sonny> all this stuff just makes me wonder with NNs seems to have such complex representation
[21:15:43] <[itchyjunk]> and you assign 0's for no relationship and 1's for relationships
[21:21:11] <hodapp_> [itchyjunk]: transformers make a lot more sense when you see them in terms of the actual equations for scaled dot-product attention and ignore the analogies the guides keep trying to use
[21:22:57] <[itchyjunk]> is there a guide that shows that? :P
[21:23:38] <hodapp_> the 'Attention Is All You Need' paper has it, I believe
[21:23:56] <hodapp_> but sometimes they mangle it a little when they try to explain self-attention
[21:24:46] <[itchyjunk]> ah
[21:29:32] *** Joins: ynnos (~sonny@bras-base-london1483w-grc-32-70-52-175-166.dsl.bell.ca)
[21:33:29] *** Quits: sonny (~sonny@bras-base-london1483w-grc-32-70-52-175-166.dsl.bell.ca) (Ping timeout: 256 seconds)
[21:33:30] *** ynnos is now known as sonny
[21:39:37] *** Joins: trace987 (~trace@ip5f5ad0ee.dynamic.kabel-deutschland.de)
[21:52:37] *** Quits: jlrnick (~josephler@2a01cb040a159400e89369dfc959fe4f.ipv6.abo.wanadoo.fr) (Ping timeout: 240 seconds)
[22:04:47] *** Quits: trace987 (~trace@ip5f5ad0ee.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
[22:19:30] *** Joins: trace987 (~trace@ip5f5ad0ee.dynamic.kabel-deutschland.de)
[22:31:51] *** Joins: SiegeLord (~sl@user/siegelord)
[22:45:39] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[22:46:25] *** Joins: Codaraxis (~Codaraxis@user/codaraxis)
[22:48:01] *** Quits: trace987 (~trace@ip5f5ad0ee.dynamic.kabel-deutschland.de) (Remote host closed the connection)
[22:57:55] *** Quits: sonny (~sonny@bras-base-london1483w-grc-32-70-52-175-166.dsl.bell.ca) (Ping timeout: 256 seconds)
[23:29:35] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[23:44:49] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
