[00:00:18] *** Joins: SiegeLord (~sl@user/siegelord)
[00:05:17] *** Quits: Klinda (~superleag@user/klinda) (Ping timeout: 240 seconds)
[00:14:39] <mefistofeles> welcome to ML :P
[00:14:46] <mefistofeles> stirring the pile!
[00:26:08] *** Joins: Klinda (~superleag@user/klinda)
[01:04:43] *** Joins: Guest36 (~Guest36@64.224.81.27)
[01:04:48] *** Parts: Guest36 (~Guest36@64.224.81.27) ()
[01:18:57] *** Quits: immibis (~hexchat@62.156.144.218) (Remote host closed the connection)
[01:22:51] *** Joins: immibis (~hexchat@62.156.144.218)
[01:51:28] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[01:58:18] *** Quits: shoky (uuuggg@141.226.193.67) (Read error: Connection reset by peer)
[01:59:21] *** Joins: shoky (uuuggg@141.226.193.67)
[02:26:46] *** Joins: miique (~miique@181.46.139.166)
[02:58:56] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[03:16:47] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:28:54] *** Quits: neilthereildeil (~neilthere@151.200.23.246) (Quit: Client closed)
[03:29:06] *** Joins: neilthereildeil (~neilthere@151.200.23.246)
[03:41:17] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:e16d:5825:4b3:d575) (Quit: My Mac Mini has gone to sleep. ZZZzzz…)
[03:57:02] *** Quits: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de) (Ping timeout: 240 seconds)
[04:43:07] *** Quits: neilthereildeil (~neilthere@151.200.23.246) (Ping timeout: 256 seconds)
[05:23:57] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 240 seconds)
[05:31:22] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[06:16:22] *** Joins: Jong (~Jong@2620:10d:c090:400::5:a269)
[06:19:05] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[06:20:40] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[06:30:54] <Jong> Hi. Can anyone explain how backprop works through Relu?   I know that without any activation function, the partial derivative of w_c in x*w_a*w_b*w_c*w_d*w_e is simply the same thing but with w_c set to 1, like this:  x*w_a*w_b* 1 *w_d*w_e.   Do backprop just pretend relu doesn't exist?
[06:33:13] <Jong> I think for activation functions that have derivates like tan and log, the values backpropogated pass through the derivative of the activation function. Right?
[06:35:34] <Jong> nevermind. The chain rule shows me how it's done
[06:50:33] <SiegeLord> That's the answer to every backprop question :)
[06:56:37] <Jong> Is backprop just a dynamic programming optimization for the chain rule ?
[06:58:27] <Jong> creating temporary arrays and avoiding redundant computations is considered dynamic programming. That's basically what forward prop and backprop do
[06:59:50] <Jong> Someone asked my question: https://www.quora.com/Can-the-back-propagation-algorithm-for-NN-be-viewed-as-a-Dynamic-Program-that-is-usually-implemented-in-a-bottom-up-fashion
[06:59:51] <SigmoidFroid> ⇒  Can the back-propagation algorithm for NN be viewed as a Dynamic Program that is usually implemented in a bottom up fashion? - Quora
[07:03:01] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:c8c6:e8a3:3411:7416)
[07:09:56] <Jong> Thanks SiegeLord
[08:22:10] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:c8c6:e8a3:3411:7416) (Quit: My Mac Mini has gone to sleep. ZZZzzz…)
[08:46:16] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[08:58:13] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[09:48:37] *** Quits: Jong (~Jong@2620:10d:c090:400::5:a269) (Ping timeout: 240 seconds)
[09:50:22] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 256 seconds)
[10:10:14] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[10:12:44] *** Joins: SiegeLord (~sl@user/siegelord)
[10:26:52] *** Joins: manti7 (~manti7@176.10.104.94)
[11:29:23] *** Joins: palasso (~palasso@user/palasso)
[11:50:01] *** Joins: Klinda (~superleag@user/klinda)
[11:57:37] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[12:32:26] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 245 seconds)
[12:46:48] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df08b-217.dhcp.inet.fi)
[12:46:59] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[13:23:34] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 260 seconds)
[13:25:04] *** Joins: manti7 (~manti7@176.10.104.94)
[13:31:55] *** Quits: manti7 (~manti7@176.10.104.94) (Read error: Connection reset by peer)
[13:37:17] *** Joins: manti7 (~manti7@185.107.94.249)
[13:45:32] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[14:27:25] *** Joins: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de)
[14:49:07] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[14:57:43] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df08b-217.dhcp.inet.fi) (Read error: Connection reset by peer)
[15:09:07] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df08b-217.dhcp.inet.fi)
[16:11:12] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df08b-217.dhcp.inet.fi) (Read error: Connection reset by peer)
[16:18:13] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df08b-217.dhcp.inet.fi)
[16:21:50] *** Quits: wolfshappen (~waff@irc.furworks.de) (Quit: later)
[16:44:00] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[16:48:17] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 240 seconds)
[16:52:54] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[16:59:41] *** Quits: immibis (~hexchat@62.156.144.218) (Read error: Connection reset by peer)
[16:59:43] *** Joins: immibis_ (~hexchat@62.156.144.218)
[17:00:46] *** Joins: sinaowolabi (~SinaOwola@41.58.218.49)
[17:07:05] *** Joins: Jong (~Jong@163.114.132.6)
[17:19:31] *** Quits: miique (~miique@181.46.139.166) (Ping timeout: 245 seconds)
[17:29:22] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 256 seconds)
[17:35:41] *** Joins: wolfshappen (~waff@irc.furworks.de)
[17:36:08] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[17:44:21] *** Joins: tomeaton17 (~tomeaton1@92.234.2.175)
[18:29:12] *** Quits: wolfshappen (~waff@irc.furworks.de) (Quit: later)
[18:29:41] *** Joins: wolfshappen (~waff@irc.furworks.de)
[18:33:37] *** Quits: wolfshappen (~waff@irc.furworks.de) (Ping timeout: 240 seconds)
[18:34:18] *** Joins: wolfshappen (~waff@irc.furworks.de)
[18:42:17] *** Quits: sinaowolabi (~SinaOwola@41.58.218.49) (Read error: No route to host)
[18:47:48] *** Joins: `Tim (~zenguin@user/zenguin)
[18:53:05] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df08b-217.dhcp.inet.fi) (Read error: Connection reset by peer)
[18:54:41] *** Joins: sinaowolabi (~SinaOwola@41.58.218.49)
[19:02:24] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:b58d:8599:673d:813)
[19:02:43] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:b58d:8599:673d:813) (Client Quit)
[19:30:12] *** Quits: arconte[m] (~arcontema@2001:470:69fc:105::1:2fe4) (Quit: You have been kicked for being idle)
[20:45:42] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 268 seconds)
[20:59:51] *** Joins: acresearch (~acresearc@user/acresearch)
[21:01:31] <acresearch> i have a dataset of 5 classes, each with 300 images. they are not enough for training (causes over fitting after 1,000 epochs), i am thinking i would augment the training set, but split the original dataset (without augmentation) into 80% valid and 20% test     does this make sense? it is a good idea?
[21:01:56] <acresearch> that way i do not have an augmented valid not test set,,, this is the correct way to evaluate a model right?
[21:03:08] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: What are some good ways to deny the politicization of SCIENCE!brb)
[21:09:25] *** Joins: Malvolio (~Malvolio@user/malvolio)
[21:23:02] *** Joins: `Tim (~zenguin@user/zenguin)
[21:25:46] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[21:32:56] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[21:41:48] *** Quits: tomeaton17 (~tomeaton1@92.234.2.175) (Quit: Client closed)
[21:43:01] <hodapp> fixed a bug in my Performers implementation, now it fine-tunes even worse
[21:43:04] * hodapp flips table
[21:45:09] * mefistofeles unflips the table
[21:45:15] <mefistofeles> every time...
[21:46:01] <hodapp> thanks, I'm probably gonna need to flip that again soon
[21:46:57] <acresearch> hodapp: mefistofeles i know how you feel all too well  :-(   ask HuntsMan and dostoyevsky   they have seen my frustrations
[21:49:05] <mefistofeles> acresearch: hey, btw, I didn't get your question earlier, tbh
[22:10:03] <acresearch> so i am augmenting my dataset (becuase it is small) on the fly (so augment as training occurs),,, now in keras i cannot find examples of augmenting images and training them, only example of augmenting images and saving them, or training on non-augmented images,
[22:11:09] <acresearch> so my question is: i augment the training set... i should not augment the valid set nor the test set.... rather i use 80% of the original non-augmented dataset as a valid set   and the remaining 20% as a test set      is this ideal?
[22:11:29] <mefistofeles> there's no optimal case for it
[22:11:36] <mefistofeles> but the 80-20 is pretty common
[22:11:57] *** Quits: Jong (~Jong@163.114.132.6) (Ping timeout: 240 seconds)
[22:12:08] <acresearch> reason for my confusion is that the ImageDataGenerator()   has a validation split for training and validation,,, that means it augments both training and validation set,,, which soem people advise against
[22:13:49] <acresearch> not augmenting the test is clear and logical   (my confusion in the validation set)
[22:20:03] *** Quits: sinaowolabi (~SinaOwola@41.58.218.49) (Ping timeout: 268 seconds)
[22:20:17] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 240 seconds)
[22:25:58] *** Joins: `Tim (~zenguin@user/zenguin)
[22:40:24] *** Joins: SiegeLord (~sl@user/siegelord)
[22:42:25] <HuntsMan> acresearch: no, ImageDataGenerator will not augment the validation set
[22:50:14] <acresearch> oh ok good, this makes it easier to code
[22:50:49] <HuntsMan> acresearch: are you sure your model is overfitting?
[22:53:03] <acresearch> yes unfortunatly, i get 0.99 at the train set and 0.6 at the valid   after 2,000 epochs
[22:53:37] <acresearch> HuntsMan: also the loss remains the same for the valid set (does not go down as in the trainin set)
[22:55:12] <acresearch> this is on VGG16   ai m trying a simpler CNN now with drop=0.5 at the last layer, but so far it seems i might get the same results
[22:55:51] <HuntsMan> VGG16 is huge, why? use something like ResNet
[22:56:07] <acresearch> isn't resnet even larger?
[22:56:41] <HuntsMan> no, it has much less parameters
[22:57:18] <acresearch> HuntsMan: resnet 50 ?   not another version?
[22:57:21] <hodapp> VGG16 has I think ~140M parameters, R50 has ~25M
[22:58:04] <HuntsMan> acresearch: ResNet20, start small
[22:58:11] <HuntsMan> acresearch: and what is the data exactly?
[22:58:36] <acresearch> nematode images   (5 classes - 300 images each)
[23:08:50] <HuntsMan> and data augmentation?
[23:47:19] *** Joins: CaCode (~CaCode@user/cacode)
[23:48:31] <hodapp> does "cosine decay", given without anything like a period, just max # epochs, tend to refer to just decay over the whole training process, rather than with warm restarts at some period?
[23:49:30] <hodapp> they link to the SGDR paper but don't say much else
