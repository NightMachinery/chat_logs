[00:05:04] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[00:14:17] *** Quits: rodrigaes (~trace@user/trace) (Quit: Leaving)
[00:40:22] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[00:49:31] *** Joins: mmv (uid549280@id-549280.hampstead.irccloud.com)
[01:22:57] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[01:51:33] *** Joins: bitkiller (~bitkiller@user/bitkiller)
[02:05:15] *** Joins: dstein64- (~dstein64@dannyadam.com)
[02:06:12] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.4)
[02:07:17] *** Quits: dstein64 (~dstein64@dannyadam.com) (Ping timeout: 240 seconds)
[02:07:17] *** dstein64- is now known as dstein64
[02:35:38] *** Quits: blackpawn (~blackpawn@c-73-73-53-23.hsd1.il.comcast.net) (Ping timeout: 272 seconds)
[02:48:05] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Ping timeout: 246 seconds)
[02:49:14] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[02:51:26] *** Joins: blackpawn (~blackpawn@c-73-73-53-23.hsd1.il.comcast.net)
[03:08:43] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: too many headaches)
[03:25:13] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[03:43:59] *** Quits: bitkiller (~bitkiller@user/bitkiller) (Ping timeout: 252 seconds)
[03:58:36] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[04:06:31] *** Quits: diogenese (~diogenese@diogenese.velotech.net) (Quit: Has anybody seen the bridge?)
[04:06:51] *** Joins: diogenese (~diogenese@diogenese.velotech.net)
[04:08:09] *** Quits: Donitz (~Donitz4@88-115-149-215.elisa-laajakaista.fi) (Read error: Connection reset by peer)
[04:08:50] *** Joins: Donitz (~Donitz4@88-115-149-215.elisa-laajakaista.fi)
[04:09:53] *** Quits: theseb (~theseb@47-220-214-54.cnrocmta03.res.dyn.suddenlink.net) (Ping timeout: 256 seconds)
[04:49:03] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[05:28:25] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[05:55:51] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Ping timeout: 256 seconds)
[06:00:42] *** Joins: Coldberg (~C-Man@78.31.190.82)
[06:01:19] *** Quits: C-Man (~C-Man@static.88-198-34-209.clients.your-server.de) (Ping timeout: 260 seconds)
[06:02:18] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[07:08:26] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[07:25:33] *** Quits: trace987 (~trace@user/trace) (Remote host closed the connection)
[07:30:21] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Remote host closed the connection)
[07:32:14] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[07:38:40] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[08:07:45] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[08:17:59] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[08:23:01] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[08:29:58] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[08:35:19] *** Joins: Malvolio (~Malvolio@user/malvolio)
[08:40:12] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[09:03:01] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Remote host closed the connection)
[09:17:07] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[09:56:34] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[09:59:19] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 260 seconds)
[10:01:01] *** Joins: trace987 (~trace@user/trace)
[10:02:35] *** Joins: manti7 (~manti7@176.10.104.94)
[10:24:25] *** Joins: Jong (~Jong@2620:10d:c090:400::5:6efd)
[10:32:08] <Jong> toulene still there?
[10:32:47] <Jong> I can help you get started, but I'd need some context on what you already know to give you good informaiton
[10:58:11] *** Joins: palasso (~palasso@user/palasso)
[11:02:55] *** Quits: [_] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[11:09:27] *** Quits: con3 (~con3@2604:a880:4:1d0::be:d000) (*.net *.split)
[11:09:27] *** Quits: kristjansson (sid126207@id-126207.tinside.irccloud.com) (*.net *.split)
[11:09:28] *** Quits: dostoyevsky (~sck@user/dostoyevsky) (*.net *.split)
[11:09:28] *** Quits: folinoid (~pi@23-233-105-168.cpe.pppoe.ca) (*.net *.split)
[11:10:04] *** Joins: folinoid (~pi@23-233-105-168.cpe.pppoe.ca)
[11:10:49] *** Joins: kristjansson (sid126207@id-126207.tinside.irccloud.com)
[11:11:06] *** Joins: con3 (~con3@2604:a880:4:1d0::be:d000)
[11:13:12] *** Quits: Celelibi (celelibi@user/celelibi) (*.net *.split)
[11:13:12] *** Quits: cln (sid336875@id-336875.ilkley.irccloud.com) (*.net *.split)
[11:13:12] *** Quits: octav1a (~quassel@173.195.145.98) (*.net *.split)
[11:13:18] *** Joins: octav1a (~quassel@173.195.145.98)
[11:13:53] *** Joins: cln (sid336875@id-336875.ilkley.irccloud.com)
[11:14:34] *** Joins: dostoyevsky (~sck@user/dostoyevsky)
[11:15:29] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[11:18:18] *** Joins: Celelibi (celelibi@user/celelibi)
[11:25:44] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[11:33:35] *** Quits: palasso (~palasso@user/palasso) (Ping timeout: 260 seconds)
[12:09:42] *** Joins: palasso (~palasso@user/palasso)
[12:25:30] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[12:36:13] *** Joins: Klinda (~superleag@user/klinda)
[12:43:11] *** Joins: sinaowolabi_ (~SinaOwola@160.152.171.207)
[13:17:06] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[13:21:26] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[13:33:25] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Ping timeout: 240 seconds)
[13:48:18] *** Quits: Coldberg (~C-Man@78.31.190.82) (Ping timeout: 260 seconds)
[13:56:43] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[14:34:02] *** Joins: C-Man (~C-Man@static.88-198-34-209.clients.your-server.de)
[14:46:01] *** Quits: sinaowolabi_ (~SinaOwola@160.152.171.207) (Ping timeout: 268 seconds)
[14:46:17] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 246 seconds)
[14:49:43] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[14:59:43] *** Joins: sinaowolabi_ (~SinaOwola@102.67.1.4)
[15:02:02] *** Joins: sinaowolabi (~SinaOwola@102.67.1.4)
[15:03:47] *** Joins: marcello42 (~mp@ip5f5aca17.dynamic.kabel-deutschland.de)
[15:04:18] *** Joins: flooded (flooded@gateway/vpn/protonvpn/flood/x-43489060)
[15:09:23] *** flooded is now known as _flood
[15:28:45] *** Joins: jinsun (~jinsun@user/jinsun)
[16:05:07] *** Joins: bitkiller (~bitkiller@user/bitkiller)
[16:53:50] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[17:06:00] *** Quits: sinaowolabi_ (~SinaOwola@102.67.1.4) (Ping timeout: 268 seconds)
[17:14:41] *** Quits: sinaowolabi (~SinaOwola@102.67.1.4) (Ping timeout: 246 seconds)
[17:16:47] *** Joins: sinaowolabi (~SinaOwola@102.67.1.37)
[17:19:35] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[17:28:59] *** Quits: HuntsMan (~hunts@2a02-a44b-361b-1-f383-e647-75b7-fa52.fixed6.kpn.net) (Ping timeout: 252 seconds)
[17:32:47] *** Joins: theseb (~theseb@47-220-214-54.cnrocmta03.res.dyn.suddenlink.net)
[17:47:00] *** Quits: theseb (~theseb@47-220-214-54.cnrocmta03.res.dyn.suddenlink.net) (Quit: Leaving)
[18:02:02] *** Joins: `Tim (~zenguin@user/zenguin)
[18:20:54] *** Quits: platta (~platta@pool-98-110-52-37.cmdnnj.fios.verizon.net) (Quit: The Lounge - https://thelounge.chat)
[18:21:49] *** Joins: yelhamer (~yelhamer@105.235.139.243)
[18:23:04] *** Joins: platta (~platta@pool-98-110-52-37.cmdnnj.fios.verizon.net)
[18:24:11] <yelhamer> hello, i'm trying to create ground truth segementation binary masks for some medical images, does anyone know of a service which provides an automation of this? as in i tell them how to segment it and they create the binary masks for me
[18:49:39] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[18:54:37] *** Joins: revolve (~u0_a227@82-132-236-17.dab.02.net)
[19:11:41] *** Quits: yelhamer (~yelhamer@105.235.139.243) (Remote host closed the connection)
[19:12:03] *** Joins: yelhamer (~yelhamer@41.97.189.24)
[19:16:55] *** Joins: HuntsMan (~hunts@145.90.132.109)
[19:36:26] *** Quits: HuntsMan (~hunts@145.90.132.109) (Ping timeout: 246 seconds)
[19:39:47] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[19:41:01] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[20:42:08] *** Joins: HuntsMan (~hunts@2a02-a44b-361b-1-f383-e647-75b7-fa52.fixed6.kpn.net)
[21:05:25] *** Quits: bitkiller (~bitkiller@user/bitkiller) (Quit: bitkiller)
[21:20:15] *** Joins: neilthereildeil (~neilthere@pool-71-191-164-234.washdc.fios.verizon.net)
[21:20:19] <neilthereildeil> hey guys
[21:20:39] <neilthereildeil> im implementing a cost function for a neural network
[21:20:50] <neilthereildeil> ‚àí1ùëö‚àëùëñ=1ùëö(ùë¶(ùëñ)log(ùëé[ùêø](ùëñ))+(1‚àíùë¶(ùëñ))log(1‚àíùëé[ùêø](ùëñ))
[21:20:56] <mefistofeles> wow, nice unicode :P
[21:21:12] <neilthereildeil> :) ¬†im surprised it pasted also!
[21:21:38] <mefistofeles> it's kinda funky, but I think it's readable, looks like a cross entropy kind of thing
[21:21:41] <neilthereildeil> why do i have to flip the matrix multiplication of y and log(a)?
[21:22:45] <neilthereildeil> my code has "b = np.matmul(Y.T, a)"
[21:23:15] <neilthereildeil> oops, its this: b = np.matmul(Y.T, np.log(AL))
[21:23:38] <since_> neilthereildeil: is one of them a vector, and one of them a matrix?
[21:23:55] <neilthereildeil> oops i pasted the wrong one again!
[21:23:56] <neilthereildeil> SRY
[21:24:14] <since_> i think i know what you mean
[21:24:23] <neilthereildeil> heres the code that works: np.matmul(np.log(AL), Y.T)
[21:24:29] <mefistofeles> yeah, seems like the "common" differentiation of columnd and row vectors
[21:24:40] <mefistofeles> *column
[21:25:03] <mefistofeles> but in any case, what are the shapes of AL and Y
[21:25:05] <mefistofeles> ?
[21:25:21] <neilthereildeil> since matrix multiplication is not commutitative, why is it not np.matmul(Y.T, np.log(AL))
[21:25:32] <neilthereildeil> which is basically just he order of multipleication flipped
[21:25:40] <since_> if i have many x vectors i can do A*x often for each vector, or i write all x vectors in a matrix, one per row, and calculate x * t(A), then i get a new matrix where each row is a resulting vector
[21:25:52] <neilthereildeil> both are (1,3)
[21:26:18] <neilthereildeil> i tried flipping the code
[21:26:20] <mefistofeles> oh, in this case it's just the inner product, it seems, so it is commutative, isn't it?
[21:26:39] <neilthereildeil> tho this: np.matmul(Y.T, np.log(AL))
[21:26:41] <neilthereildeil> but i didnt work
[21:26:53] <mefistofeles> hmm ok, not sure what matmul does, one sec
[21:26:59] <neilthereildeil> matrix multiplication
[21:27:12] <neilthereildeil> in both cases, the operation worked because the inner dimensions matched
[21:27:21] <neilthereildeil> but the answer was not correct
[21:27:51] <neilthereildeil> in general, is it expected that if I have XY, the code would be matmul(Y, X)?
[21:28:04] <neilthereildeil> shouldnt the code be matmul(X, Y) also?
[21:28:51] <since_> neilthereildeil: print the results and look at whats different
[21:29:14] <neilthereildeil> this is more of a math question
[21:29:42] <neilthereildeil> in general shouldnt XY=matmul(X, Y) rather than matmul(Y, X)?
[21:29:59] <since_> where is the math? i see a python code
[21:30:55] <mefistofeles> I'd say, they ought to preserve the order, as in XY = matmul(X,Y)
[21:31:14] <neilthereildeil> when i multiply 2 matricies like in the equation i pasted, should the order of operations be the same as in the formula?
[21:31:18] <mefistofeles> and if these are vectors (1D "matrices") then it's just the inner product, imho, so it should be commutative
[21:31:25] <neilthereildeil> hmmm
[21:31:29] <mefistofeles> if they are 2D or more, then it's not commutative
[21:31:40] <neilthereildeil> they are 1D
[21:31:42] <neilthereildeil> (1,3)
[21:33:32] <since_> nope
[21:33:35] <mefistofeles> that's technically 2D, though
[21:33:43] <mefistofeles> 1D would be something like (3,)
[21:33:55] <since_> neilthereildeil: do you know graphical matrix multiplication?
[21:34:05] <neilthereildeil> yea, like by hand
[21:34:12] <neilthereildeil> i took lonear algebra in college
[21:34:15] <neilthereildeil> linear*
[21:35:20] <since_> yes, then you can see that | * _ = [] and _ * |  = .
[21:36:03] <neilthereildeil> i dont understand what u wrote. whats the | and *?
[21:37:20] <mefistofeles> so the issue here is basically differentiating between row and column vectors, as in https://bpa.st/GADQ
[21:37:21] <SigmoidFroid> ‚áí  View paste GADQ
[21:37:22] <since_> are they exactly the same shape, or is one transposed?
[21:38:18] <since_> neilthereildeil: | is column vector, _ row vector, [] matrix and . a scalar in my bullshit notation :D
[21:38:46] <neilthereildeil> ok
[21:39:37] <neilthereildeil> yea u can see in the code i pasted that Y is transposed
[21:40:17] <neilthereildeil> as i mentioned, the mathematical matrix multiplication mechanics work. im not getting an issue multiplying the matricies. the issue is the answer im getting is wrong
[21:41:16] <mefistofeles> so, you have to do the correct multiplication to get the correct result, so it depends on what you actually need to multiply, in terms of row and column vectors
[21:41:33] <mefistofeles> because in general, matrix multiplication is not commutative
[21:42:06] <mefistofeles> if you have the math formulation and the python representations maybe we can see directly what the correct multiplication should be
[21:43:50] <since_> neilthereildeil: when a and b are column vectors of the same shape, then a*b = b*a but transpose(a) * b IS NOT b * transpose(a)
[21:45:27] <neilthereildeil> https://pastebin.com/0e93SaYZ
[21:45:27] <SigmoidFroid> ‚áí  a = np.log(AL) b = np.matmul(a, Y.T) Y_ones = np.ones(Y.T.shape) - Pastebin.com
[21:45:31] <neilthereildeil> heres my code
[21:45:44] <mefistofeles> since_: hmm, that's actually not true, only when the first is row and the second is column, then it translates to inner/dot product, and that one is commutative
[21:45:49] <mefistofeles> check my pasted code
[21:45:54] <neilthereildeil> this works, but has the multiplications backwards and im trynna figure out why it gices me the wrong answer if i flip the multiplication
[21:48:10] <mefistofeles> neilthereildeil: well, the reason is that, matrix multiplication is not commutative, so you just cannot flip it
[21:48:36] <mefistofeles> rather, "so you cannot just flip it", in general
[21:48:46] <since_> mefistofeles: if both are R^n column, then the result is R^n column aswell. then they are commutative. then transpose one: t(a)*b gives the inner product and b*t(a) gives a matrix R^{n*n}
[21:49:05] <neilthereildeil> but in trying to do yLog(a), i had to do np.matmul(np.log(AL), Y.T). THat doesnt make any sense
[21:49:21] <neilthereildeil> look at line 2
[21:49:28] <since_> neilthereildeil: please print them
[21:49:33] <mefistofeles> since_: no, again, check the code, because it's the "external" product (also called matrix product)
[21:50:25] <mefistofeles> for both columnd (or both row), the external/matrix product is not well-defined
[21:50:29] <mefistofeles> *column
[21:55:04] <since_> mefistofeles: thanks. and what is the normal matrix multiplication in Python (like the R %*%) ?
[21:57:59] <since_> https://pastebin.com/raw/QAzcKCHa that has that behavior
[21:58:00] <SigmoidFroid> ‚áí  (: No title)
[21:58:57] <mefistofeles> since_: sorry, my ISP/company doesn't allow pastebin (for some reason), can you use bpa.st ?
[22:00:22] <since_> https://bpa.st/REWQ
[22:00:23] <SigmoidFroid> ‚áí  View paste REWQ
[22:00:25] <mefistofeles> thanks
[22:00:46] <mefistofeles> since_: right, so it's the same
[22:00:48] <mefistofeles> one sec
[22:02:19] <mefistofeles> oh, I see the difference, that's interesting hmm
[22:03:18] <neilthereildeil> im rewriting the function to see if i find a bug somewhere in there, and doing the intermediate steps by hand
[22:04:41] <mefistofeles> since_: so, it's similar, but it's a technical difference between how vectors and arrays are defined in R and numpy
[22:06:53] <mefistofeles> since_: numpy is about arrays, vectors (row or column) are not really a "thing" in numpy... you just explicitely have to use the dimensions correctly if you want to differentiate the two
[22:07:41] <since_> ah that explains it. i thought for a moment my last 10 years linalg where a lie :D
[22:08:04] <mefistofeles> since_: so, the issue is that np.array([1,2,3]) is just an array, doesn't say anything about row/column vector
[22:08:18] <mefistofeles> whereas in R c(1,2,3) is a row vector, as far as I can see
[22:09:03] <mefistofeles> if you want row vector with numpy, you have to do something like np.array([[1,2,3]]) and colum is np.array([[1],[2],[3]])
[22:10:36] <since_> but then i wonder why neilthereildeil transposes them at all - if they are just arrays
[22:10:44] <mefistofeles> since_: or, I guess this is the common solution, you can use np.matrix function, instead of np.array
[22:11:12] <mefistofeles> since_: because they need a "special" row-column combination
[22:11:42] *** Quits: diogenese (~diogenese@diogenese.velotech.net) (Quit: Has anybody seen the bridge?)
[22:12:35] *** Joins: diogenese (~diogenese@diogenese.velotech.net)
[22:14:01] <since_> mh ok, thanks, learned something new :D
[22:17:54] <neilthereildeil> looking at this equation, i need to end up with a [1, 1] matrix
[22:17:58] <neilthereildeil> essentially a scalar
[22:18:02] <neilthereildeil> for this to make sense
[22:18:14] <neilthereildeil> the cost should just be 1 number
[22:19:07] <neilthereildeil> does that necessitate the order of multiplcations? is that a good reason to "massage" the matrices by transposing them to get [1, 1]?
[22:23:02] <neilthereildeil> in other words, is it valid to transpose stuff and switch around the order of matmul in order to make the matrix shape "fit" my desired output shape of [1, 1]? it doesnt see m right to flip order of arguments to matmul just to get the output to fit a shape due to lact of commutative property
[22:28:26] <since_> i have no idea of python, but as i understood it from before "matmul" is not "matrix multiplication" as you have arrays and not vectors. and you might need to look into np.inner() [ or use np.matrix, but that seems to be deprecated ]
[22:30:03] <mefistofeles> since_: matmul IS matrix multiplication, the problem is how you interpret arrays (not vectors) when you pass them to matmul
[22:30:17] <mefistofeles> basically, arrays are not vectors, but you can make them look like ones if you use the tricks I said up there
[22:30:44] <mefistofeles> the easiest solution is to use the matrix() function instead of the array(), since you really want matrices/vectors. This could also potentially increase the performance
[22:31:00] <mefistofeles> since arrays accept any kind of data and check for it, whereas matrices are numerical
[22:32:19] <mefistofeles> my guess is that they allow to use arrays here because many people are using them in the wild, but the better way is to use matrix() instead of array()
[22:38:17] <since_> ok i correct myself: the textbook vector_a * vector_b is not matmul(np.array(a), np.array(b)) in 3 of 4 cases regarding column/row vector cases
[22:39:11] <since_> the numpy doc also states "It is no longer recommended to use this class, even for linear algebra. Instead use regular arrays. The class may be removed in the future."
[22:39:32] <since_> for numpy.matrix
[22:47:50] <mefistofeles> since_: ha! I see, well, I guess the disctintion between row-column is then left to the user and assuming they know what they are doing, but I'm not a fan of that, tbh
[22:48:17] <mefistofeles> this is also one of the examples where R is closer to "real math" and python closer to computation
[22:51:43] <mefistofeles> I guess the "convention" is to use arrays and reshape them , as in https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html#Reshaping-of-Arrays
[22:51:43] <SigmoidFroid> ‚áí  The Basics of NumPy Arrays | Python Data Science Handbook
[22:53:32] *** Quits: sinaowolabi (~SinaOwola@102.67.1.37) (Ping timeout: 272 seconds)
[23:15:35] <neilthereildeil> heres a different way of asking the same question:
[23:15:44] <neilthereildeil> in this equation: ‚àí1ùëö‚àëùëñ=1ùëö(ùë¶(ùëñ)log(ùëé[ùêø](ùëñ))+(1‚àíùë¶(ùëñ))log(1‚àíùëé[ùêø](ùëñ)))
[23:15:59] *** Joins: sinaowolabi (~SinaOwola@102.67.1.37)
[23:16:02] <neilthereildeil> how do i assure I get 1 scalar for the cost, rather than a matrix?
[23:16:11] <neilthereildeil> ensure*
[23:17:57] <mefistofeles> neilthereildeil: you would have to show your implementation as well, I guess
[23:19:08] <neilthereildeil> i pasted it here: https://pastebin.com/0e93SaYZ
[23:19:09] <SigmoidFroid> ‚áí  a = np.log(AL) b = np.matmul(a, Y.T) Y_ones = np.ones(Y.T.shape) - Pastebin.com
[23:19:14] <neilthereildeil> but it might be wrong
[23:19:43] <neilthereildeil> im asking what control do i have in amnipulating the matricies to ensure i get a scalar result for cost?
[23:20:06] <neilthereildeil> am i allowed to switch order of multiplication just to get a [1, 1] matrix?
[23:21:18] <since_> neilthereildeil: https://numpy.org/doc/stable/reference/generated/numpy.inner.html
[23:21:19] <SigmoidFroid> ‚áí  numpy.inner ‚Äî NumPy v1.22 Manual
[23:24:07] <neilthereildeil> isnt inner product just a special case of a matrix multiplication? my question is about matrix multiplication in general
[23:24:29] *** Quits: revolve (~u0_a227@82-132-236-17.dab.02.net) (Read error: Connection reset by peer)
[23:26:07] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[23:26:56] <since_> so you have a vector result, but want a scalar as a metric?
[23:28:33] <neilthereildeil> no, lets say i have AL and Y as 2 [1, 3] matrices. if i multiply them in the wrong order after transposing one of them, ill get [3, 3]. therefore i have to be strategic about transposing and flipping the order of multiplication of the right ones so i get a [1, 1] result
[23:29:15] <neilthereildeil> but is it valid to change the order of ,multiplication just so I get a certain shape in the output? seems like I am assuming too much freedom in a non-commutative operation
[23:31:59] <since_> in general: no you can not just swap things and assume it still is useful afterwards
[23:32:49] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[23:33:20] <since_> but if you think after an algorithm you need a scalar, but end up with a 3x3 matrix, you might have done something wrong
[23:33:43] <neilthereildeil> since_: thats¬† helpful
[23:35:56] <neilthereildeil> how would you implement that function to ensure you get a scalar?
[23:37:01] <since_> neilthereildeil: i would need a screenshot of the page. normally authors define what the result is
[23:37:23] <since_> e.g. c \in R^n
[23:37:40] <neilthereildeil> this is Andrew Ng's deep learning course on coursera that im working on
[23:39:34] <since_> neilthereildeil: what is: ùë¶(ùëñ), what is ùëé[ùêø], what is ùëé[ùêø](ùëñ), what is ‚àí1ùëö‚àëùëñ
[23:40:11] *** Quits: trace987 (~trace@user/trace) (Remote host closed the connection)
[23:40:14] <since_> neilthereildeil: can you do a screenshot?
[23:40:35] *** Joins: trace987 (~trace@user/trace)
[23:40:59] <neilthereildeil> https://pastebin.com/6iQd1T22
[23:41:00] <SigmoidFroid> ‚áí  # GRADED FUNCTION: compute_costdef compute_cost(AL, Y): """ Implem - Pastebin.com
[23:42:53] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[23:48:53] <since_> neilthereildeil: also it might help: if people end up with a vector, but need a scalar for an optimizer - it can make sense to use the vector length, or some other vector norm. as applying a strictly monotonous function wont confuse your solver (9 > 3 and 9**2 > 3**2 aswell), you can calculate the square of the euclidean norm. that is just the inner product: vector^T * vector. so
[23:48:55] <since_> sometimes people do transpose and swapt things. but only if it makes sense i.e. here the effect would be the same as looking how long my vector is (in case of errors, how far am i off)
[23:56:08] *** Quits: sinaowolabi (~SinaOwola@102.67.1.37) (Ping timeout: 246 seconds)
[23:56:43] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 260 seconds)
