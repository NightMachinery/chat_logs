[00:08:25] *** Quits: arrowd (~arr@2.93.163.158) ()
[00:42:33] *** Quits: coot (~coot@213.134.190.95) (Quit: coot)
[00:52:13] *** Joins: emad (~emad@102.45.236.212)
[00:55:06] *** Quits: emad (~emad@102.45.236.212) (Client Quit)
[01:13:39] *** Joins: coot (~coot@213.134.190.95)
[01:50:45] *** Quits: coot (~coot@213.134.190.95) (Quit: coot)
[02:41:52] *** Quits: shapr (~user@pool-173-73-44-186.washdc.fios.verizon.net) (Remote host closed the connection)
[02:42:05] *** Joins: shapr (~user@pool-173-73-44-186.washdc.fios.verizon.net)
[09:13:42] *** Quits: hololeap (~hololeap@user/hololeap) (Read error: Connection reset by peer)
[09:16:11] *** Joins: hololeap (~hololeap@user/hololeap)
[09:50:37] *** Quits: hololeap (~hololeap@user/hololeap) (Remote host closed the connection)
[09:52:05] *** Joins: hololeap (~hololeap@user/hololeap)
[10:35:01] *** Joins: komikat (~komikat@27.57.144.183)
[10:39:06] *** Joins: coot (~coot@213.134.190.95)
[11:02:27] *** Joins: yauhsienhuangtw (~Yau-Hsien@61-231-39-29.dynamic-ip.hinet.net)
[11:04:10] *** Joins: yauhsien (~Yau-Hsien@61-231-39-29.dynamic-ip.hinet.net)
[11:04:35] *** Quits: yauhsien (~Yau-Hsien@61-231-39-29.dynamic-ip.hinet.net) (Read error: Connection reset by peer)
[11:05:03] *** Quits: yauhsien_ (~Yau-Hsien@61-231-39-29.dynamic-ip.hinet.net) (Ping timeout: 256 seconds)
[11:06:22] *** Joins: yauhsien (~Yau-Hsien@61-231-37-5.dynamic-ip.hinet.net)
[11:07:00] *** Quits: yauhsienhuangtw (~Yau-Hsien@61-231-39-29.dynamic-ip.hinet.net) (Ping timeout: 240 seconds)
[11:08:15] *** Joins: yauhsienhuangtw (~Yau-Hsien@61-231-37-5.dynamic-ip.hinet.net)
[11:10:57] *** Quits: yauhsien (~Yau-Hsien@61-231-37-5.dynamic-ip.hinet.net) (Ping timeout: 240 seconds)
[11:53:15] *** Joins: fendor (~fendor@77.119.204.89.wireless.dyn.drei.com)
[12:21:41] *** Joins: cptwunderlich (~Benjamin@212-186-57-158.cable.dynamic.surfer.at)
[12:29:50] *** Quits: coot (~coot@213.134.190.95) (Quit: coot)
[12:30:20] *** Joins: coot (~coot@213.134.190.95)
[12:33:55] *** Quits: komikat (~komikat@27.57.144.183) (Remote host closed the connection)
[12:55:28] *** Quits: coot (~coot@213.134.190.95) (Quit: coot)
[13:12:07] *** Joins: komikat (~komikat@27.57.144.183)
[13:12:25] *** Quits: komikat (~komikat@27.57.144.183) (Read error: Connection reset by peer)
[13:12:54] *** Joins: komikat (~komikat@27.57.144.183)
[13:13:03] *** Quits: komikat (~komikat@27.57.144.183) (Remote host closed the connection)
[13:14:18] *** Joins: komikat (~komikat@27.57.144.183)
[13:30:55] *** Joins: coot (~coot@2a02:a310:e241:1b00:ec1a:e9df:79ac:66ba)
[13:55:24] *** Joins: July541 (~July@218.70.255.195)
[14:06:31] *** Quits: komikat (~komikat@27.57.144.183) (Remote host closed the connection)
[14:07:10] *** Joins: komikat (~komikat@27.57.144.183)
[14:36:43] *** Quits: komikat (~komikat@27.57.144.183) (Remote host closed the connection)
[14:37:25] *** Joins: komikat (~komikat@27.57.144.183)
[14:47:25] *** Quits: komikat (~komikat@27.57.144.183) (Remote host closed the connection)
[14:47:57] *** Joins: fendor_ (~fendor@178.165.161.199.wireless.dyn.drei.com)
[14:48:26] *** Joins: komikat (~komikat@27.57.144.183)
[14:50:12] *** Quits: fendor (~fendor@77.119.204.89.wireless.dyn.drei.com) (Ping timeout: 240 seconds)
[14:53:07] *** Quits: komikat (~komikat@27.57.144.183) (Ping timeout: 256 seconds)
[14:53:41] *** Joins: komikat (~komikat@27.57.144.183)
[15:59:13] *** Joins: yauhsien_ (~Yau-Hsien@61-231-37-5.dynamic-ip.hinet.net)
[16:02:17] *** Quits: yauhsienhuangtw (~Yau-Hsien@61-231-37-5.dynamic-ip.hinet.net) (Ping timeout: 240 seconds)
[16:28:38] *** Joins: July541_ (~July@218.70.255.195)
[16:31:00] *** Quits: July541 (~July@218.70.255.195) (Ping timeout: 240 seconds)
[16:44:36] *** yauhsien_ is now known as yauhsien
[17:06:52] *** Quits: komikat (~komikat@27.57.144.183) (Remote host closed the connection)
[17:20:14] <wz1000> fendor[m], maerwarld: I'm getting this after upgrading hie-bios
[17:20:15] <wz1000> Error when calling cabal --builddir=/home/zubin/.cache/hie-bios/dist-mercury-web-backend-c6e10991a03c6f9b23f211723d634d75 v2-exec --with-compiler /home/zubin/.cache/hie-bios/wrapper-b54f81dea4c0e6d1626911c526bc4e36 --with-hc-pkg /home/zubin/.cache/hie-bios/ghc-pkg-ea9d393ac47910ea27682ac4572c4230 ghc -v0 -- --print-libdir","","cabal: The program 'ghc' version >=7.0.1 is required
[17:20:18] <wz1000> but the version of\n/home/zubin/.cache/hie-bios/wrapper-b54f81dea4c0e6d1626911c526bc4e36 could not\nbe determined.\
[17:25:25] <wz1000> running --version on the --with-compiler argument works
[17:35:25] <maerwald> wz1000: can you try this
[17:35:30] <maerwald> cabal --builddir=/home/zubin/.cache/hie-bios/dist-mercury-web-backend-c6e10991a03c6f9b23f211723d634d75 v2-exec --with-compiler /home/zubin/.cache/hie-bios/wrapper-b54f81dea4c0e6d1626911c526bc4e36 --with-hc-pkg /home/zubin/.cache/hie-bios/ghc-pkg-ea9d393ac47910ea27682ac4572c4230 sh -- -c 'ghc -v0 --print-libdir'
[17:35:49] <maerwald> cabal treats `ghc` specially, there may be issues with that
[17:39:12] <fendor[m]> wz1000, what's the content of the wrapper?
[17:40:27] <fendor[m]> does it work if you remove the `--with-hc-pkg` from the cabal invocation?
[17:42:46] *** Joins: komikat (~komikat@27.57.144.183)
[17:47:02] *** Quits: komikat (~komikat@27.57.144.183) (Ping timeout: 240 seconds)
[17:49:53] <michaelpj> historical question: is there a reason why `lsp` doesn't provide a reactor-based server by default? It seems like ~everyone is going to want to use a design like that, since without it you can't e.g. wait for a message response inside a message handler (I think this explains https://github.com/haskell/lsp/blob/master/lsp/src/Language/LSP/Server/Core.hs#L605).
[17:49:57] <michaelpj> maybe wz1000 knows?
[17:50:51] <michaelpj> I would buy the argument that it's complexity you don't necessarily need, but arguably that core progress-reporting code is just broken currently, and I don't think it can be fixed without being able to wait for a message response inside a handler
[17:51:01] <wz1000> michaelpj: possibly, but we want to give a lot of control to the user of the library for scheduling and forking policy
[17:51:03] <michaelpj> that's basically mandated by how progress reporting is supposed to work
[17:52:12] <wz1000> ghcide for instance does a lot of special things, forking for requests but not notifications, a bunch of logic for cancellation and so on
[17:52:47] <michaelpj> I also think that's a bit weird tbh, since `lsp` also has code for cancellation, but I haven't understood it all properly yet
[17:53:41] <wz1000> lsp just defines the data types I think?
[17:54:24] <michaelpj> it provides lots of server glue code too
[17:54:42] <wz1000> really?
[17:55:43] <michaelpj> yes, the entry point is this thing, which handles the IO and runs an IO loop, feeding events into your handlers as defined in the `ServerDefinition`: https://github.com/haskell/lsp/blob/master/lsp/src/Language/LSP/Server/Control.hs#L96
[17:56:08] <michaelpj> in order to get around that you have to do a funny dance with defining a bunch of handlers that just put requests into your reactor queue, and then manually farm them out yourself to the real handlers
[17:56:15] <michaelpj> so we do provide a "server framework"
[17:56:47] <michaelpj> but it's one where (AFAICT) if you just use it naively it's almost certainly not what you want
[17:58:28] <michaelpj> and that given e.g. the requirements of progress reporting, I think it may be literally impossible to do it correctly with a non-concurrent server implementation
[18:01:31] <michaelpj> wait, maybe it's even weirder.  if you run something with `withProgressBase` it *does* get run asynchronously, so we can cancel it
[18:01:49] <michaelpj> I am officially confused :D
[18:02:08] <wz1000> I think the solution may be to move the progress stuff out of lsp :)
[18:05:38] <michaelpj> to be clear, this is `lsp` not `lsp-types`. I thought the whole point of `lsp` was to provide most of a server implementation for  you
[18:06:26] <wz1000> I'm not sure, because I think a lot of the scheduling decisions really belong in ghcide
[18:08:39] <wz1000> fendor[m], maerwald: It works when I run the cabal command manually, but not via `hie-bios check` or ghcide
[18:08:55] <michaelpj> then maybe the question is "how can `lsp` provide something useful while also letting the user make most of the scheduling decisions?"
[18:08:59] <michaelpj> ðŸ¤”
[18:09:23] <fendor[m]> that sucks... Ill give it a try to, maybe it fails for me, too
[18:09:39] <fendor[m]> Any specific package or on all packageS?
[18:09:45] <wz1000> all packages
[18:12:32] <fendor[m]> Hrmpf, building from source
[18:15:30] <fendor[m]> maerwald, regarding vscode, maybe the 60s download timeout is just too short?
[18:15:41] <maerwald> I tried increasing to 120, didn't help
[18:16:15] <maerwald> I would be surprised if my internet locally is faster than the CI runners
[18:22:21] <fendor[m]> that is indeed true
[18:25:14] <fendor[m]> how about the paths? are these correct?
[18:25:31] <fendor[m]> at least for me: https://paste.tomsmeding.com/NiRalf1J
[18:25:33] <fendor[m]> no bin
[18:25:47] <fendor[m]> ah forget it, hidden directory
[18:25:59] <fendor[m]> still
[18:26:29] <fendor[m]> https://paste.tomsmeding.com/eF3Qv6PN
[18:26:53] <fendor[m]> maybe the first `bin` is wrong?
[18:26:54] <maerwald> yes, they are correct
[18:27:09] <fendor[m]> from `bin/.ghcup/bin/haskell-language-server-wrapper${exeExt}`
[18:27:14] <maerwald> that's correct
[18:27:42] <maerwald> the first bin is just the store dir
[18:28:04] <fendor[m]> what store-dir?
[18:28:16] <maerwald> vscode
[18:28:54] <fendor[m]> is that for testing only, or should I find that, too?
[18:30:39] <fendor[m]> I guess it is for testing only
[18:31:10] <maerwald> the store dir is /home/hugin/.config/Code - OSS/User/globalStorage/haskell.haskell
[18:31:15] <maerwald> for the tests it's bin/
[18:33:20] <fendor[m]> yeah, I am debugging it right now
[18:53:31] <fendor[m]> tbh, that test fails locally for me too
[18:53:38] <fendor[m]> (but I have bad internet, tbf)
[18:54:02] <maerwald> works well for me
[18:54:21] <maerwald> I think the test is flaky
[18:54:39] <maerwald> and I don't have enough interest in vscode/TS nightmares
[18:54:56] <fendor[m]> yeah that's fair
[18:55:14] <maerwald> maybe the tests run async even and interfere somehow
[18:55:57] <fendor[m]> iirc, the vscode plugin only downloaded wrapper and the exact version it needed. Does ghcup install in this setting hls for all supported ghc's for some hls version?
[18:56:08] <maerwald> yes
[18:57:29] <fendor[m]> then since the download takes much longer, maybe that's indeed the issue? We download now 4 times as many executables (ignoring compression for no)
[18:58:47] <maerwald> we can download it manually before the test
[18:59:01] <maerwald> via `ghcup prefetch`
[19:01:26] <fendor[m]> goodness, ghcup is prepared for everything ðŸ˜€
[19:01:38] <maerwald> lol
[19:02:02] <maerwald> it's a pretty common feature of package managers to prefetch, so you can have offline installation
[19:02:36] <fendor[m]> true, but it just amazes me how many tasks ghcup now takes care of already
[19:06:23] <fendor[m]> wz1000, I can't reproduce it... I assume you are on NixOS and built it via cabal?
[19:06:41] <wz1000> I'm on archlinux and built hie-bios via cabal
[19:07:36] <fendor[m]> ok then we have the same environment, cabal is installed via ghcup?
[19:07:49] <maerwald> you also have to make sure you don't have a global HLS before running the tests
[19:08:43] <fendor[m]> yeah, I know, I removed all hls's (ghcup unset) before running the tests
[19:18:22] <wz1000> fendor[m]: yes
[19:18:38] <wz1000> cabal-install version 3.6.2.0
[19:18:57] <fendor[m]> yeah same. You have a ghc selected?
[19:19:13] <fendor[m]> the issue can be that we set `HIE_BIOS_GHC` wrong
[19:19:41] <fendor[m]> you should add something like `echo $HIE_BIOS_GHC` in the wrapper in ~/.cache/hie-bios/wrapper-* and see what's reported 
[19:22:08] <fendor[m]> while you are at it, add the contents of `$HIE_BIOS_GHC_ARGS` as well
[19:22:20] <fendor[m]> we should include the information of the environment variables to the error message...
[19:22:56] <wz1000> ok, I'll do that
[19:32:46] *** Joins: shapr` (~user@pool-173-73-44-186.washdc.fios.verizon.net)
[19:33:55] *** Quits: shapr (~user@pool-173-73-44-186.washdc.fios.verizon.net) (Read error: Connection reset by peer)
[19:43:01] *** Quits: coot (~coot@2a02:a310:e241:1b00:ec1a:e9df:79ac:66ba) (Quit: coot)
[19:47:26] *** shapr` is now known as shapr
[20:08:58] *** fendor_ is now known as fendor
[20:36:17] *** Joins: coot (~coot@213.134.190.95)
[21:07:58] <michaelpj> pepeiborra: are you attached to the ghcide test suite being in one ginormous file? I find it a bit of a pain to work with, would you be okay with splitting it up into multiple modules?
[21:10:44] <pepeiborra> Not attached to it, but splitting loses history, and we have IDEs now so the pain is manageable. I have split some things out myself, but never sent PRs for them. 
[21:12:54] <michaelpj> when I edit the file in vim I can't save it without crashing the very basic syntax checker I have running
[21:12:59] <michaelpj> it's quite painful
[21:14:05] <michaelpj> perhaps the better way forward is just moving more bits of ghcide (and their tests) into standalone plugins
[21:14:45] <pepeiborra> or fixing your vim syntax checker :)
[21:15:19] <michaelpj> I don't dare open it with HLS running, but maybe it would be fine
[21:16:05] <pepeiborra> one thing that I really do want is a workflow for running the test suite in GHCi with working :r
[21:16:05] <pepeiborra> But that is much harder, cabal real is in the way, and also the way we run the tests with an external ghcide process
[21:16:19] <pepeiborra> cabal repl (autocorrect)
[21:17:11] <michaelpj> well, one thing that might work with a module split would be at least being able to load the modules that do work that way
[21:17:34] <michaelpj> anyway, I'm not going to do it unless people are keen
[21:17:58] <pepeiborra> the cabal repl problem is loading both the library and the testsuite in a single ghci session
[21:18:42] <michaelpj> oh right, that one
[21:18:43] <michaelpj> yeah
[21:18:51] <pepeiborra> and the :r problem is that the tests need to be converted to use ghcide in-process (using lsp-test runWithHandles)
[21:18:55] <michaelpj> multiple home modules when
[21:19:23] <michaelpj> (I'm joking, I know Matt is working on it)
[21:19:32] <fendor[m]> the initial pr has been merged
[21:19:57] <fendor[m]> while cabal will not benefit from it in the near future, at least hls will probably ðŸ™‚
[21:20:38] <michaelpj> is that just because modifying cabal is a pain?
[21:26:53] <fendor[m]> that too, but implementing can be slightly tricky, since Cabal has been designed for single packages
[21:27:28] <fendor[m]> for that reason the POC implementation happened in cabal-install, which breaks the separation of cabal-install and Cabal
[21:28:18] <fendor[m]> it opens the question, do we still need or want this separation? 
[21:29:02] <michaelpj> I'm out of my depth here, I had naively just assumed that `cabal repl component1 component2` wouldn't be too hard, and would just be a matter of combining the flags for each in some appropriate way...
[21:29:59] <fendor[m]> ah well, kind of 
[21:30:16] <fendor[m]> that's basically the idea of the POC
[21:30:36] <fendor[m]> but cabal wasn't designed with such as use-case in mind
[21:31:16] <fendor[m]> if you have followed the show-build-info discussion, it is not so trivial to actually retrieve the compilation flags for cabal units
[21:31:19] <fendor[m]> in cabal-install
[21:31:26] <fendor[m]> due to build-hooks
[21:31:35] <michaelpj> ðŸ™ˆ
[21:33:59] <fendor[m]> so the idea is trivial and just ignoring this abstraction works rather well. However, implementing it in a non-hacky way... is not straight forward in my opinion
[21:47:24] *** Quits: coot (~coot@213.134.190.95) (Quit: coot)
[22:25:11] *** Joins: komikat (~komikat@27.57.144.183)
[23:25:00] *** Quits: komikat (~komikat@27.57.144.183) (Remote host closed the connection)
[23:33:06] *** Joins: komikat (~komikat@27.57.144.183)
[23:35:25] *** Quits: fendor (~fendor@178.165.161.199.wireless.dyn.drei.com) (Remote host closed the connection)
[23:46:52] <maerwald> fendor[m]: any progress on the test?
[23:48:12] <fendor[m]> maerwald, sorry got distracted but Ill continue tomorrow on it. I currently feel confident that this is the issue
