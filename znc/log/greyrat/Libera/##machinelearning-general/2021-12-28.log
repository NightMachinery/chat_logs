[00:00:04] *** Quits: Colt (~Colt@user/colt) (Read error: Connection reset by peer)
[00:01:34] *** Joins: SiegeLord (~sl@user/siegelord)
[00:04:58] *** Joins: miique (~miique@181.46.139.166)
[00:08:59] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[00:30:14] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[00:38:47] *** Joins: sinaowolabi (~SinaOwola@160.152.36.12)
[00:42:44] *** Quits: rito_ (~rito_gh@45.112.243.219) (Quit: Leaving)
[00:56:31] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[01:21:19] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:b1ec:c200:85e1:4f7d) (Quit: My Mac Mini has gone to sleep. ZZZzzz…)
[01:53:31] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[01:54:32] *** Joins: henistein (~henistein@2001:8a0:c18a:f601:e0b9:9d3b:2bec:5f70)
[01:55:51] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:fdb9:e1eb:f99b:dc41)
[02:23:07] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:fdb9:e1eb:f99b:dc41) (Quit: My Mac Mini has gone to sleep. ZZZzzz…)
[02:25:57] *** Quits: miique (~miique@181.46.139.166) (Ping timeout: 240 seconds)
[02:31:00] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:29ad:19a:fa15:e17c)
[02:44:10] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:56:51] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:29ad:19a:fa15:e17c) (Quit: My Mac Mini has gone to sleep. ZZZzzz…)
[03:44:38] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:52:17] *** Quits: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de) (Ping timeout: 240 seconds)
[04:03:09] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[04:07:53] *** Quits: henistein (~henistein@2001:8a0:c18a:f601:e0b9:9d3b:2bec:5f70) (Quit: Client closed)
[06:09:37] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 240 seconds)
[06:10:09] *** Quits: sobobobobo (~sobobobob@85-76-74-73-nat.elisa-mobile.fi) (Ping timeout: 256 seconds)
[06:10:19] *** Joins: shoky (uuuggg@141.226.193.67)
[06:40:30] <Jong> I thought of an idea of creating AI through simulating how life came about in the universe.   Here is the question. If we created a virtual universe where the same natural forces that exist in our universe exist within the simulation, and we set the virtual universe on fast forward so 1 sec is 1 billion secs in the virtual universe, would you expect life to develop within the virtual universe?
[06:41:37] <Jong> Probably simulating such a virtual universe would take so much computational crunching that it would be impossible.
[06:41:50] *** Joins: black_13 (~jjosb@2600:1700:1100:61c0:c9b8:d709:9ae6:53d8)
[06:41:57] <SiegeLord> And wasteful... evolution doesn't use gradients, so it's pretty inefficient
[06:44:14] <black_13> what was CAFE?
[06:51:32] <Jong> Never heard of CAFE. What's it sand for?
[07:03:47] <black_13> https://caffe.berkeleyvision.org/
[07:03:48] <SigmoidFroid> ⇒  (HTML: No title)
[07:04:21] <black_13> predecessor off torch i think
[07:18:06] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:d06f:74a7:1060:5672)
[07:41:17] *** Quits: Jong (~Jong@2620:10d:c090:400::5:da53) (Ping timeout: 240 seconds)
[07:59:57] *** Quits: black_13 (~jjosb@2600:1700:1100:61c0:c9b8:d709:9ae6:53d8) (Ping timeout: 240 seconds)
[08:32:20] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Remote host closed the connection)
[09:16:01] *** Joins: coraxx (~coraxx@mail.8-pm.com)
[09:18:11] *** Joins: manti7 (~manti7@176.10.104.94)
[09:31:28] *** Joins: black_13 (~jjosb@2600:1700:1100:61c0:88b8:68cb:a904:2bce)
[09:38:17] *** Quits: sinaowolabi (~SinaOwola@160.152.36.12) (Ping timeout: 240 seconds)
[09:50:53] *** Quits: coraxx (~coraxx@mail.8-pm.com) (Quit: Leaving)
[09:55:08] *** Joins: spaceseller (~spacesell@31.147.205.13)
[10:04:13] *** Quits: jinsun (~quassel@user/jinsun) (Ping timeout: 240 seconds)
[10:04:21] *** Joins: ns12 (~ns12@user/ns12)
[10:05:36] *** Joins: 038AASGNZ (~josephler@42.118.113.78.rev.sfr.net)
[10:17:29] <spaceseller> I have a question about PCA then seq2seq LSTM. Where is the best place to ask?
[10:30:32] *** Joins: sinaowolabi (~SinaOwola@41.58.253.32)
[10:37:45] *** Quits: sinaowolabi (~SinaOwola@41.58.253.32) (Ping timeout: 256 seconds)
[10:40:13] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[10:47:39] *** Joins: spaceseller (~spacesell@31.147.205.13)
[10:48:54] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[10:49:13] *** Quits: spaceseller (~spacesell@31.147.205.13) (Client Quit)
[10:49:38] *** Joins: spaceseller (~spacesell@31.147.205.13)
[10:51:20] *** Quits: spaceseller (~spacesell@31.147.205.13) (Client Quit)
[10:56:47] *** Joins: spaceseller (~spacesell@31.147.205.13)
[10:57:20] *** Quits: spaceseller (~spacesell@31.147.205.13) (Remote host closed the connection)
[11:01:16] *** Joins: sinaowolabi (~SinaOwola@160.152.36.12)
[11:01:36] *** Joins: spaceseller (~spacesell@31.147.205.13)
[11:09:07] *** Joins: Jong (~Jong@2620:10d:c090:400::5:f0a6)
[11:23:05] *** Quits: 038AASGNZ (~josephler@42.118.113.78.rev.sfr.net) (Ping timeout: 256 seconds)
[11:26:35] *** Joins: `Tim (~zenguin@user/zenguin)
[11:27:01] *** Joins: sinaowolabi_ (~SinaOwola@160.152.36.12)
[11:28:30] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:d06f:74a7:1060:5672) (Quit: My Mac Mini has gone to sleep. ZZZzzz…)
[11:34:50] *** Joins: palasso (~palasso@user/palasso)
[11:54:33] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[12:35:06] *** Joins: jinsun (~quassel@user/jinsun)
[12:35:46] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:35:56] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:37:25] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:37:28] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:41:19] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:41:27] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:42:30] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:42:38] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:43:40] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:43:42] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:44:22] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[12:44:36] *** Joins: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de)
[12:45:22] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:45:28] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:46:05] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:51:06] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:53:38] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:54:36] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:55:14] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:55:23] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:56:03] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:56:21] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:57:01] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:57:19] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:58:38] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:59:05] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[12:59:42] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[12:59:49] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:00:28] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:00:37] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:01:19] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:01:46] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:02:22] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:03:44] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:04:25] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:04:57] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:05:35] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:06:32] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:07:13] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:07:34] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:08:38] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:08:55] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:10:06] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:10:24] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:11:33] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:11:52] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:13:39] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:13:55] *** Joins: georgios (~georgios@user/georgios)
[13:14:21] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:15:22] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:15:29] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:17:54] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:18:54] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:20:06] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:20:46] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:21:23] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:21:50] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:22:30] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:23:56] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:24:33] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:25:55] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:26:38] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:27:01] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:27:40] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:27:56] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:44:17] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:44:22] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[13:55:28] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[13:56:46] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[13:56:52] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[14:01:32] *** Quits: sinaowolabi (~SinaOwola@160.152.36.12) (Ping timeout: 240 seconds)
[14:02:21] *** Quits: sinaowolabi_ (~SinaOwola@160.152.36.12) (Ping timeout: 268 seconds)
[14:11:28] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[14:14:21] *** Joins: sinaowolabi_ (~SinaOwola@160.152.140.189)
[14:14:40] *** Joins: sinaowolabi (~SinaOwola@160.152.140.189)
[14:45:38] *** Joins: miique (~miique@181.46.139.166)
[14:48:57] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[14:49:00] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Killed (NickServ (GHOST command used by jlrnick-)))
[14:50:38] <hodapp> Jong: for one, simulating even a fraction of a second with current known physics is something that exhausts the resources of supercomputers, is nowhere even close to real-time, and still requires cutting some corners on what they're even simulating
[14:51:23] <Jong> that's depressing to hear
[14:51:53] <hodapp> and it's not clear what corners can even be cut without impacting something relevant to life developing
[15:00:34] <hodapp> so at once you run into a problem: if you can't do an 'approximate' simulation (because you don't know what can safely be approximated) then you are talking about simulating basically the entire universe, using a tiny part of the universe
[15:03:57] *** Quits: sinaowolabi_ (~SinaOwola@160.152.140.189) (Ping timeout: 240 seconds)
[15:05:38] *** Joins: rito_ (~rito_gh@45.112.243.151)
[15:10:26] <Jong> hodapp  in all fairness, what's happening in all the other solar systems probably didn't impact life on earth.  Neither do the other planets
[15:11:01] <Jong> In fact, the sun can be reduced. No need to simulate every little thing happening on the sun.  Just treat it as a single light source giving energy to earth
[15:11:49] <Jong> but still, simulating earth is not happening any time soon
[15:11:52] * Jong cries
[15:28:24] *** Quits: trace987 (~trace@ip5b42963a.dynamic.kabel-deutschland.de) (Remote host closed the connection)
[15:39:24] *** Joins: henistein (~henistein@2001:8a0:c18a:f601:35cf:5152:f3f7:8def)
[15:48:15] <ns12> Has machine learning always been such a big part of artificial intelligence? When did machine learning become so big?
[15:48:19] *** Joins: sinaowolabi_ (~SinaOwola@160.152.140.189)
[15:50:19] <HuntsMan> ns12: around 2012
[15:50:41] <ns12> HuntsMan: What happened around 2012?
[15:50:47] <HuntsMan> ns12: CNNs won ImageNet
[15:51:56] <ns12> HuntsMan: That caused a sudden spike in interest in neural networks?
[15:52:39] <HuntsMan> yes!
[15:53:54] <ns12> What was the largest AI field before the resurgence of interest in neural networks (which caused machine learning to become popular)?
[15:54:06] <HuntsMan> symbolic IA
[15:54:09] <HuntsMan> feature engineering
[15:54:26] <HuntsMan> we still used ML but on engineered features, and feature learning just works better than that
[15:55:11] <ns12> Did neural networks have a poor reputation at that time?
[15:55:15] <HuntsMan> yes
[15:55:24] <HuntsMan> very
[15:55:28] <ns12> Why?
[15:56:27] <HuntsMan> they didn't work very well for many tasks
[15:56:38] <HuntsMan> but we found out they just needed more data and compute
[15:56:56] <HuntsMan> ML methods scale differently with more data
[15:57:06] <ns12> What were the most popular ML algorithms at the time?
[15:58:40] <HuntsMan> SVMs maybe? or gradient boosting? there are a lot
[16:01:27] <ns12> HuntsMan: Thank you for the information. I have read "Introduction to Statistical Learning" (ISLR). Where does statistical learning fit into all this? Was it ever popular?
[16:08:41] *** Quits: sinaowolabi (~SinaOwola@160.152.140.189) (Ping timeout: 256 seconds)
[16:20:17] *** Quits: henistein (~henistein@2001:8a0:c18a:f601:35cf:5152:f3f7:8def) (Quit: Client closed)
[16:30:50] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[16:31:35] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Remote host closed the connection)
[16:33:16] *** Quits: georgios (~georgios@user/georgios) (Ping timeout: 245 seconds)
[16:34:57] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[16:34:58] *** Joins: georgios (~georgios@user/georgios)
[16:40:30] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[16:41:23] *** Joins: sinaowolabi (~SinaOwola@41.58.243.117)
[17:05:03] *** Joins: trace987 (~trace@ip5b42963a.dynamic.kabel-deutschland.de)
[17:19:38] <hodapp> Jong: these are all approximations that are anything but certain; it'd need better than "probably"
[17:20:19] <hodapp> ns12: statistical learning has indeed a lot of overlap with machine learning
[17:21:16] <hodapp> but AFAIK, it focuses a lot more on methods with a meaningful statistical derivation and interpretation, whereas in most "modern" ML that either won't be there or will be done largely after-the-fact
[17:37:41] <hodapp> I guess it's "popular" the same way statistics is; it's more of a niche thing
[17:39:08] <hodapp> but HuntsMan is likely right; prior to convnets being huge, the popular methods were things like SVMs, gradient boosting... maybe things like naive Bayes and Markov models (e.g. for spam detection and speech recognition, respectively)
[17:40:28] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[17:41:38] <HuntsMan> statistical learning theory also covers modern ML methods, its just that the theory is not precise (like VC dimension)
[17:43:19] *** Quits: Guest1145 (~somewhere@202-63-76-134.ip4.superloop.com) (Ping timeout: 256 seconds)
[17:43:55] *** Joins: etolier (~somewhere@125-63-6-35.ip4.superloop.com)
[17:47:54] <ns12> hodapp: "I guess it's "popular" the same way statistics is" - What is "popular"? Statistical learning?
[17:51:06] <hodapp> yes
[17:51:53] <hodapp> in the sense that - statistics has never been all that popular and has never received nearly the hype machine learning has, but it remains an area of importance unto itself
[17:51:57] <hodapp> and statistical learning is sort of the same
[17:52:31] *** Joins: Klinda (~superleag@user/klinda)
[17:53:02] *** Quits: jinsun (~quassel@user/jinsun) (Ping timeout: 240 seconds)
[17:54:45] <ns12> hodapp: Why aren't the "modern" ML practitioners interested in the mathematical foundations that statistical learning brings? (By "modern" ML, I'm assuming that you mean neural networks, deep learning, etc.).
[17:55:16] <hodapp> they're more interested in results
[17:55:32] <HuntsMan> ns12: what mathematical foundations?
[17:56:24] <hodapp> and the results from ML methods that are more firmly-rooted in theory, versus ones rooted almost completely in experimentation, have tended to be much more lackluster
[17:56:26] <ns12> HuntsMan: Statistical foundations.
[17:56:37] <HuntsMan> ns12: yes, but what exactly?
[17:57:43] <hodapp> which is why anytime you look around research papers you'll find papers trying to explain a theoretical basis for the empirical performance seen in some aspect of deep learning (e.g. why a certain architectural change produces better empirical results)
[18:01:03] <ns12> Does this mean that the algorithms used in machine learning are still not well understood?
[18:01:40] <hodapp> a lot is still really not
[18:01:57] <hodapp> it's not so much the algorithms themselves, as why they work
[18:02:57] <hodapp> but look up the manifold hypothesis, for instance
[18:05:12] <hodapp> another big question that still isn't satisfactorily answered: why it works to pick a model architecture that is very overparametrized (i.e. has so many learnable parameters that it could very trivially overfit by simply memorizing the entire dataset), and then extensively use regularization in order to redeuce its "effective" capacity at training
[18:05:44] <hodapp> and why it *doesn't* seemingly work to just use a model that isn't overparametrized in the first place
[18:06:27] <hodapp> whereas statistical learning will focus a lot more on the number of parameters in the model and in setting this appropriately
[18:06:33] <HuntsMan> the overfitting predictions of statistical learning theory have not happened, which means part of the theory is wrong
[18:08:01] <hodapp> well... I don't think statistical learning holds that it will *always* overfit if it's overparametrized; the whole notion of regularization methods to reduce overfitting also came out of statistical learning, I thought
[18:12:37] <dostoyevsky2> > In a nutshell, the BOV works as fol- lows. Interest points are detected in the image and local invariant descriptors are extracted. Each descriptor is as- signed to its closest visual word in a “visual vocabulary”: a codebook obtained offline by clustering a large set of de- scriptors with k-means. This results in a typically high- dimensional sparse histogram representation.
[18:12:43] <dostoyevsky2> Similarly to text retrieval, an inverted list structure is employed for ef- ficient indexing and tf-idf scoring is used to discount the influence of visual-words which occur in many images.
[18:13:04] <dostoyevsky2> This is from the 2011 ImageNet winner...  I have seen these architectures described as Hypercolum architectures
[18:13:55] <hodapp> BOV = bag of visual words?
[18:14:17] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[18:14:31] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[18:15:24] <dostoyevsky2> bag-of-visual-words (BOV)
[18:15:46] <hodapp> ah, yeah, I remember hearing about this one in class
[18:19:05] <HuntsMan> hodapp: it is kind of predictions that do not hold true in reality
[18:20:50] <dostoyevsky2> I guess with the GPT3 type of models overfitting matters even less as they just already contain most things everyone has ever written
[18:22:41] *** Joins: jailop (~jailop@ool-944bb915.dyn.optonline.net)
[18:24:08] <hodapp> HuntsMan: things like sample-complexity bounds don't really give false predictions here, just useless ones
[18:25:39] <HuntsMan> I meant the error bounds, which generally are quite useless
[18:27:28] <hodapp> performance remains within the predicted error bounds though; it's a case of the theory just not having anything useful to say, not of the theory being wrong
[18:28:37] <HuntsMan> if the bound is almost to infinity, I would say the theory is kind of failed :)
[18:29:26] <HuntsMan> take a look at the paper "Deep Learning required Rethinking Generalization", where they show some actual failures (like not considering the labels in the theoretical part)
[18:29:52] <HuntsMan> and I am talking about bounds with VC-dimension, rademacher complexity, etc
[18:30:05] <HuntsMan> why people not use them? Because they are not useful at all, and in many cases, plain wrong
[18:32:42] <hodapp> I see them saying, " This is a trivial upper bound on the Rademacher complexity that does not lead to useful generalization bounds in realistic settings." but I don't see anything yet here about theory actually producing anything that's wrong
[18:47:17] *** Joins: spaceseller (~spacesell@31.147.205.13)
[18:50:52] *** Quits: spaceseller (~spacesell@31.147.205.13) (Client Quit)
[18:51:12] <ns12> HuntsMan, hodapp: Thank you for the explanations.
[18:53:10] *** Quits: sinaowolabi_ (~SinaOwola@160.152.140.189) (Ping timeout: 260 seconds)
[18:53:25] *** Quits: sinaowolabi (~SinaOwola@41.58.243.117) (Ping timeout: 268 seconds)
[18:53:45] <hodapp> sure
[18:54:12] <hodapp> "Machine Learning" by Mitchell is a good pre-deep-learning text on machine learning, though it predates SVM and boosting
[19:01:44] *** Joins: spaceseller (~spacesell@31.147.205.13)
[19:06:54] <spaceseller> PCA then seq2seq LSTM is my plan. Where is the place for that kind discussion?
[19:21:47] <ns12> hodapp: Thank you for the book recommendation.
[19:22:08] <ns12> I have another question: Where does genetic algorithms and genetic programming fit into the history of machine learning?
[19:24:56] <mefistofeles> spaceseller: plan for what? Maybe I missed the context, but this channel sounds like a good place for what you mention
[19:25:21] <hodapp> ns12: hah, in my grad classes they described genetic algorithms as "the second best solution to everything"
[19:25:42] <ns12> What is that supposed to mean?
[19:26:22] <hodapp> that for any given thing they are good at doing, there is probably a better model out there
[19:27:28] <ns12> Is genetic programming part of machine learning too? From my understanding, it is definitely not part of statistical learning, since genetic programming is "soft".
[19:28:26] <mefistofeles> ns12: hmm, i don't see why it wouldn't, but maybe it's not one of the most relevant, which is a different story
[19:35:21] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Ping timeout: 268 seconds)
[19:41:15] <hodapp> ns12: it's borderline; I'd call it more AI
[19:41:45] <hodapp> it's not data-driven/inductive the way machine learning is
[19:43:25] <mefistofeles> ns12: why is the distinction that important? If I may ask
[19:43:28] <hodapp> being data-driven and being based around optimization are pretty central to ML, and genetic programming/algorithms (I use them sort of interchangeably) include the optimization aspect but not the data-driven part
[19:43:40] <hodapp> similar to something like simulated annealing
[19:44:01] <mefistofeles> yeah, that's true
[19:45:44] <hodapp> and if memory serves me right: if your objective function is differentiable, a neural network will probably end up solving things much better, and if it's not, the brute-force of simulated annealing will probably end up finding a similar solution as fast
[20:13:25] <dostoyevsky2> ns12: The genetic algorithms I have used did not use gradients on differentiable datatypes... they just did random guesses and adjusted if it worked better, very slow in practice
[20:14:36] <hodapp> though, along those lines: https://arxiv.org/abs/1712.06567
[20:14:45] <SigmoidFroid> ⇒  [1712.06567] Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning  -> We also showed that interesting algorithms developed in the neuroevolution community can now immediately be tested with deep neural networks, by showing that a Deep GA-powered novelty search can solve a deceptive Atari-scale game
[20:14:59] <hodapp> there was some buzz on this years ago but I don't know if anything came of it
[20:15:24] <dostoyevsky2> biz for buzz
[20:16:37] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 240 seconds)
[20:17:12] *** Joins: shoky (uuuggg@141.226.193.67)
[20:28:39] <xs> GAs were developed by a largely separate community to most of ML. the most high profile exception i'm aware of is the work from schmidhuber's lab. there's historically a degree of skepticism about GAs from the ML perspective: c.f., David MacKay's book chapter titled "Why have sex?" (or something like this) which gives an MCMC perspective on crossover mutation.
[20:29:54] <xs> that said, stuff like population based training show how useful this stuff can be.
[20:31:33] <xs> neural architecture search is also a popular application of GAs in the modern ML space. (e.g., the work of quoc le from brain)
[20:37:07] *** Joins: georgios (~georgios@user/georgios)
[20:37:25] <hodapp> huh, interesting how this channel went from fairly dead to having pretty regular chatter
[20:51:35] *** Joins: sinaowolabi (~SinaOwola@41.58.232.102)
[20:53:12] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[20:54:49] *** Joins: sinaowolabi_ (~SinaOwola@160.152.140.189)
[20:59:14] *** Quits: jailop (~jailop@ool-944bb915.dyn.optonline.net) (Ping timeout: 268 seconds)
[21:00:11] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[21:08:55] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[21:09:15] *** Joins: Klinda (~superleag@user/klinda)
[21:29:08] <black_13> hodapp, holidays?
[21:35:13] <hodapp> hmm, looking over the fine-tuning configuration from 'Masked Autoencoders Are Scalable Vision Learners' in table 9 and maybe someone else will have a clue on these questions
[21:35:33] <Klinda> wow when I don't ask questions you write a lot
[21:37:13] <hodapp> they say 0.1 for drop-path (for ViT-B/L), and they cite 'Deep Networks with Stochastic Depth' - so it looks like this means they are randomly dropping certain groups of layers (wherever there is a residual connection). Each transformer block has two sub-layers (one for attention, one for feedforward), each of which has a residual connection bypassing it too
[21:37:55] <hodapp> but are they dropping the entire transformer block (note that in the paper they bypass an entire ResBlock), or are they dropping just the sublayer?
[21:38:41] <hodapp> also: is layer-wise learning rate decay done based on the layers *after* drop-path, or before?
[21:42:31] <Klinda> happy holidays to all btw
[21:42:54] <Klinda> I sent to my teacher the project at 26th December but nevermind
[21:42:56] <Klinda> all ok right?
[21:45:29] *** Joins: Arrinao (~Arrinao@102.61.broadband3.iol.cz)
[21:46:35] <Arrinao> Do I have anyone here with proficiency in pandas?
[21:47:31] <Klinda> state your question is better
[21:50:07] <Arrinao> true
[21:50:18] <Arrinao> How do I specify the output for datetime in pandas to_datetime()? I know that format='%Y-%m-%d' specifies the input, but how do I make the output not be YYYY-MM-DD ?
[21:50:51] <Klinda> maybe #python is better
[21:51:06] <Arrinao> ok
[22:08:44] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[22:17:11] *** Joins: SiegeLord (~sl@user/siegelord)
[22:27:36] <hodapp> Arrinao: it is just a pd.Timestamp object and you can use something like .apply to turn it to something else
[22:34:35] *** Quits: sinaowolabi (~SinaOwola@41.58.232.102) (Ping timeout: 256 seconds)
[22:34:48] *** Quits: sinaowolabi_ (~SinaOwola@160.152.140.189) (Ping timeout: 268 seconds)
[22:41:14] *** Joins: jlrnick (~josephler@42.118.113.78.rev.sfr.net)
[22:47:44] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: brb)
[22:48:26] *** Joins: sinaowolabi (~SinaOwola@41.190.2.118)
[22:49:31] *** Joins: Malvolio (~Malvolio@user/malvolio)
[23:00:42] *** Quits: sinaowolabi (~SinaOwola@41.190.2.118) (Ping timeout: 268 seconds)
[23:01:55] *** Joins: henistein (~henistein@2001:8a0:c18a:f601:1861:a984:7f7c:5a12)
[23:11:29] *** Joins: sinaowolabi (~SinaOwola@41.58.49.11)
[23:15:22] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:78d0:dbf8:2bc2:cf2c)
[23:24:27] *** Quits: jlrnick (~josephler@42.118.113.78.rev.sfr.net) (Ping timeout: 256 seconds)
