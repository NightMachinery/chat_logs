[00:03:03] *** Quits: mro (~mro@port-92-195-129-79.dynamic.as20676.net) (Remote host closed the connection)
[00:03:13] *** Joins: mro (~mro@port-92-195-129-79.dynamic.as20676.net)
[00:09:29] *** Quits: tomku (~tomku@user/tomku) (Ping timeout: 250 seconds)
[00:16:23] *** Joins: tomku (~tomku@user/tomku)
[00:30:45] *** Joins: rgrinberg (~textual@2806:101e:7:6946:301f:4093:a99:4144)
[00:41:10] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Quit: WeeChat 3.3)
[00:41:32] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[00:41:49] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Client Quit)
[00:52:15] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[00:53:30] *** Quits: mro (~mro@port-92-195-129-79.dynamic.as20676.net) (Remote host closed the connection)
[00:55:34] *** Joins: mro (~mro@2002:5cc3:814f:1:96e:aafe:2c6d:b3ee)
[00:56:19] *** Joins: mro_ (~mro@2002:5cc3:814f:0:fd0a:e80b:3eff:f5be)
[00:58:02] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Quit: WeeChat 3.3)
[00:58:15] *** Quits: mro_ (~mro@2002:5cc3:814f:0:fd0a:e80b:3eff:f5be) (Remote host closed the connection)
[00:59:50] *** Joins: mro_ (~mro@port-92-195-129-79.dynamic.as20676.net)
[01:00:02] *** Quits: mro (~mro@2002:5cc3:814f:1:96e:aafe:2c6d:b3ee) (Ping timeout: 260 seconds)
[01:03:44] *** Joins: namkeleser (~namkelese@101.179.128.103)
[01:20:26] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[01:25:28] <rgrinberg> thizanne \o/
[01:25:41] <thizanne> rgrinberg: o/
[01:26:12] <companion_cube> thizanne \o/
[01:26:13] *** Quits: Anarchos (~Anarchos@88.168.112.216) (Quit: Vision[]: i've been blurred!)
[01:26:24] <thizanne> companion_cube: \o
[01:26:25] <rgrinberg> Shall we re-org the interface a little and get 1.0 out the door?
[01:26:26] *** Quits: average (uid473595@user/average) (Quit: Connection closed for inactivity)
[01:26:41] <thizanne> ok now you're frightening me :)
[01:26:49] <companion_cube> 1.0 of what? :D
[01:26:54] <thizanne> hamt
[01:27:05] <rgrinberg> I'm thinking Hamt.Stdlib for a drop in replacement to Stdlib's map
[01:27:11] <rgrinberg> and a slightly saner api by default
[01:27:23] <thizanne> yeah I'd vote for that too
[01:27:53] <rgrinberg> Also, wdyt of asking users to provide compare so that we can use a good old map for collision nodes?
[01:28:08] <rgrinberg> It seems like a better way to degrade the perf in that case
[01:28:35] <thizanne> it seems reasonable to ask for compare anyway
[01:29:27] <thizanne> and using a map seems legit too
[01:29:34] <companion_cube> waaaaaaaaaaaa
[01:29:38] <thizanne> what does hashtbl do nowadays ?
[01:29:45] <companion_cube> omg omg
[01:29:51] <companion_cube> rgrinberg: does that imply popcnt as well?
[01:30:15] <rgrinberg> companion_cube maybe post 1.0 since it's just an optimization. I'm always working on the CHAMP optimizations rn
[01:30:18] <companion_cube> ah wait, misread.  thought you were proposing to upstream HAMT into ocaml
[01:30:20] <thizanne> yeah don't be too excited yet, the core code is mostly the same bad one that I wrote so many years ago
[01:30:21] <rgrinberg> and i think those are a higher priority
[01:30:25] <companion_cube> "just"
[01:30:37] <thizanne> but rgrinberg has been putting a lot of work into making it usable these days
[01:30:47] <rgrinberg> thizanne stdlib's hash table uses a list. core's hash table uses a map like a suggested
[01:30:55] <rgrinberg> *i suggested*
[01:31:01] <thizanne> let's use a hash table then
[01:31:11] <rgrinberg> you mean a map?
[01:31:20] <thizanne> yes
[01:31:34] <companion_cube> weirdly it's hard to make a hashtbl that's faster than the stdlib's, in my experience
[01:31:49] <companion_cube> at least, trying to make a flat one, without lists :/
[01:32:32] <thizanne> regarding the CHAMP stuff: that's something I've wished to do since quite a long time
[01:32:48] <thizanne> but never did
[01:33:08] <thizanne> before that I believe that it could be worth reading the paper again, and look at the code a bit more carefully
[01:33:10] <rgrinberg> thizanne should give the library a little extra oomph
[01:33:10] <companion_cube> what's the CHAMP stuff anyway
[01:33:22] <thizanne> I think I made some things a little differently, which may actually have been bad ideas
[01:33:45] <thizanne> canonical hamt and I don't know what P stands for
[01:33:51] <companion_cube> thizanne: I wrote a kinda-sorta-hamt like at the same time roughly, and wlel
[01:33:53] <rgrinberg> companion_cube standard optimizations to hamt
[01:33:55] <companion_cube> it was similar perf I think
[01:34:07] <companion_cube> rgrinberg: is there a short explanation?
[01:34:32] <thizanne> companion_cube: someone also wrote an implementation that was consistently faster than mine, also around the same time
[01:34:36] <thizanne> that was bob atkey I believe
[01:34:45] <rgrinberg> yeah, if your nodes are type t = Emtpy | Leaf of .. | Bitmap of t array
[01:34:50] <thizanne> I didn't look at it in depth though
[01:35:12] <companion_cube> https://github.com/c-cube/ocaml-containers/blob/master/src/data/CCHashTrie.ml#L348-L352 <-- kinda
[01:35:14] <rgrinberg> you're kind of killing perf with all the Leaf's in the array
[01:35:25] <rgrinberg> so you store the Leaf's in a separate bitmap
[01:35:43] <rgrinberg> thizanne do you have a link?
[01:35:56] <rgrinberg> i read the champ paper btw. any other paper you have in mind?
[01:36:23] <companion_cube> bitmap + vec, you mean?
[01:36:37] <rgrinberg> yes
[01:36:40] <thizanne> https://github.com/bobatkey/array-mapped-trie
[01:36:43] <companion_cube> interesting
[01:37:10] <companion_cube> wow thizanne that's full of Obj
[01:38:24] *** Quits: Guest9603 (~bobo@2a01:e0a:8d3:a300:ca60:ff:fe98:ce57) (Quit: Konversation terminated!)
[01:38:40] <rgrinberg> thizanne yes that paper skips the whole non bitmap nodes stuff
[01:38:50] <rgrinberg> i'm also skeptical of how helpful that stuff is
[01:38:51] <thizanne> yeah, in a conversation with gasche, he said (bob):
[01:38:52] <thizanne> For the array mapped trie library, I'd also started it just before I saw the HAMT announcement. At the moment, my library is not ready for real use: it only works on 64bit machines (it assumes that there are at least 32 bits available in the 'int' type), and the interface is not yet complete. I have experimented with a 'packed' version that attempts to reduce memory usage by adhering more closely to the memory
[01:38:55] <thizanne> layout described in Bagwell's 'Ideal Hash trees' paper -- at the cost of using the Obj module to do type coercions.
[01:39:35] <thizanne> rgrinberg: I think I tried to move the thresholds to never use bitmaps, and the opposite
[01:39:51] <thizanne> and both extremes were performing worst than the default
[01:39:51] <d_bot> <dinosaure> for instance, `art` is faster than `hashtbl` if you want
[01:40:00] <companion_cube> for me Obj is a no no
[01:40:04] <thizanne> I'm not sure how that plays with champs though
[01:40:08] <companion_cube> @dinosaure it's not generalistic, is it?
[01:40:22] <thizanne> yeah companion_cube if that's why he's faster, then I'd be fine with being slower
[01:40:28] <d_bot> <dinosaure> it's not :/ must be a string as a key
[01:40:42] <companion_cube> heh, then it doesn't count
[01:40:57] <d_bot> <dinosaure> you can serialize your key to a string :p
[01:41:03] <companion_cube> in some ways, OCaml comes short when it comes to implement a good vector or a good hashmap
[01:41:17] <companion_cube> you simply can't do it cleanly if you can't have a partially uninitialized array
[01:42:28] <rgrinberg> thizanne i think hamt's bitmap hot path can be optimized enough so that the array path will be useless
[01:42:32] <rgrinberg> i'll experiment with that
[01:42:49] <rgrinberg> dinosaure what's good about art again?
[01:43:13] <companion_cube> better memory usage for strings, i think?
[01:43:22] <companion_cube> it's a trie specialized on strings so it got to be good
[01:43:25] <rgrinberg> how good are we talking?
[01:43:31] <rgrinberg> i actually want something like that for dune
[01:43:41] <rgrinberg> we have a ton of path maps and sets
[01:43:58] <rgrinberg> and obviously those share long prefixes
[01:44:07] <d_bot> <NULL> companion_cube: I still have the idea of an unboxed option type (where the sanity check for unboxing is bypassed) for partially uninitialised structures, but I'm too much of a noob to implement it
[01:44:23] <companion_cube> well rgrinberg, ask @dinosaure :p
[01:44:25] <d_bot> <dinosaure> rgrinberg: as companion_cube, it's like a light raidx tree
[01:44:28] <companion_cube> it's also a good concurrent structure iirc
[01:44:30] <d_bot> <dinosaure> radix*
[01:45:46] <d_bot> <dinosaure> you can see benchmark here:
[01:45:47] <d_bot> <dinosaure> https://dinosaure.github.io/art/bench/find.html
[01:45:48] <d_bot> <dinosaure> https://dinosaure.github.io/art/bench/insert.html
[01:47:08] <d_bot> <dinosaure> so you have kind of compression and it's not a naive radix tree, it wants to be lighter than a radix tree (and offers 4 kind of nodes)
[01:47:51] <companion_cube> the graphs are nice, really
[01:47:52] <rgrinberg> dinosaure how many elements in that tree in each benchmark?
[01:48:51] <d_bot> <dinosaure> 1000 elements
[01:48:55] *** Joins: Anarchos (~Anarchos@88.168.112.216)
[01:49:39] <rgrinberg> hmm, in dune we have plenty of small sets
[01:50:05] <d_bot> <dinosaure> in my case, I mainly test that the given distribution of strings are "normal"
[01:50:16] <d_bot> <dinosaure> (and you can check that via the histogram)
[01:51:25] <d_bot> <dinosaure> after yeah, we should check more elements but we can easily say that, via the path compression, the result should be more interesting than an hashtbl in anyway
[01:52:37] <companion_cube> rgrinberg: you could have one big string set, and a bitmap for each small set :p
[01:52:43] <companion_cube> I hear about these "roaring bitmaps" regularly
[01:53:04] <rgrinberg> companion_cube rawr. not a bad idea
[01:53:10] <d_bot> <dinosaure> (this is the big diff between `art` and `hashtbl`, `art` can jump in some cases some bytes where it saw a common parT)
[01:53:25] <rgrinberg> there's a few more requirements, such as being able to do Filename.relative efficiently to every element.
[01:53:36] <companion_cube> oof
[01:53:49] *** Quits: bartholin (~bartholin@158.110.70.215) (Quit: Leaving)
[01:54:29] <rgrinberg> we need unboxed arrays badly in ocaml
[01:54:30] <companion_cube> seems like a trie might help for that too, tho
[01:54:33] <companion_cube> yes please
[01:54:52] <rgrinberg> companion_cube yes, something trie like. but optimizing tries is a headache
[01:54:57] <rgrinberg> and the naive ones are quite poor
[01:55:19] <companion_cube> that's what art is for, indeed
[01:55:22] <companion_cube> ART*
[01:56:52] *** Quits: Serpent7776 (~Serpent77@90-156-31-193.internetia.net.pl) (Quit: leaving)
[01:57:15] <rgrinberg> thizanne there's a few more pr's up :)
[01:57:35] <thizanne> lalala not hearing you
[01:57:40] <companion_cube> :D
[02:01:54] <d_bot> <Shon> Could someone help me prove out my understanding of the advantage/tradeoffs using Seq, vs. Lazy lists, vs. just reading everything into memory say as a list? In particular, I want to be sure I understand the tradeoffs when reading in data from an in_channel, so statefulness is key.
[02:02:02] <d_bot> <Shon> My current understanding is this: Seq.t  is nice and can save the amount of memory you need to allocate at any given time IF you can deal with each item one at a time or small batches AND (critically) you won't traverse the Seq.t more than once. Otherwise, the statefulness of the channel while break your logic (been bit by this twice this week).
[02:02:54] <d_bot> <Shon> Proper lazy lists allow for building up collections incrementally, and traversing them multiple times, but unless you need to do expensive computations to construct each element of the list, there's no advantage if you're gonna end up needing all the lines eventually, because the memoization will mean you end up keeping everything in memory.
[02:03:09] <companion_cube> lazy is a bit more expensive than `fun() -> ‚Ä¶`
[02:03:13] <companion_cube> that's it basically
[02:03:27] <d_bot> <Shon> Right, and it's also more expensive than just a thunk.
[02:03:50] <rgrinberg> tbh Seq.t is also kind of a pig and should be avoided anywhere where performance is needed
[02:04:17] <companion_cube> it's‚Ä¶ the status quo that was able to make it in
[02:04:58] <d_bot> <Et7f3> Seq.t isn't like generator in other languages ?
[02:05:49] <d_bot> <Shon> I thought there was memory savings if you read, e.g., a file line by line using Seq.t, I assume because you can free the memory for each line after processing it?
[02:05:51] <companion_cube> it's one that's functional in some cases
[02:05:59] <companion_cube> the imperative version was never accepted
[02:06:09] <companion_cube> yeah, the GC will do that
[02:06:17] <d_bot> <Shon> Whereas, I assume this doesn't happen for Lazy.t, since it needs to keep the data in memory for memoization?
[02:06:17] <companion_cube> not *immediately* after, but it can
[02:06:33] <companion_cube> lazy can also be GC'd if it's not referenced anywhere
[02:06:36] <d_bot> <Shon> (Assuming you still have a handle to the lazy list in scope)
[02:06:52] <Anarchos> i made a version of ppxlib, assuming that 5.00 has same AST as 4.14, should i do a pull request ?
[02:07:37] <d_bot> <Shon> Hmm, ok, so if it's true that Seq.t should be avoided (I have found it very dangerous when trying to use with in_channes, cause you get to thinking in functional terms, but the state is actually critical)
[02:07:38] <companion_cube> if you have a handle, neither will be collected
[02:07:59] <companion_cube> Seq + in_channel is ok if you consume the seq inside the `with_file_in` or whatever.
[02:08:08] <d_bot> <Shon> is there anything better then either reading the while file into a list, or just doing it imperatively?
[02:08:13] <companion_cube> Anarchos: maybe to ppxlib
[02:08:36] <Anarchos> companion_cube not sure if i did things right
[02:09:01] <rgrinberg> doing it imperatively or using a list both sound a lot better than Seq to me
[02:09:05] <companion_cube> I think it's premature, Anarchos
[02:09:11] <companion_cube> rgrinberg: unless you don't need the list
[02:09:13] <Anarchos> companion_cube but i was amazed to see nobody began work on it
[02:09:46] <companion_cube> because 5.0 is not out yet I guess
[02:09:52] <companion_cube> but maybe they have a branch
[02:09:55] <Anarchos> companion_cube i understood that 5.00 was 'mostly' multicore, so not much ast tweeaking
[02:09:58] <rgrinberg> Anarchos why do it early? the parsetree might change later anyway
[02:10:11] <rgrinberg> it's done quite quickly anyway
[02:10:50] <Anarchos> rgrinberg sure, but as i need a trunk version of the compiler, i needed it already.
[02:12:18] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Quit: WeeChat 3.3)
[02:12:57] <rgrinberg> might be good to open a PR then. I'm not sure if it will be accepted in the near future though
[02:13:17] <Anarchos> ok
[02:16:07] *** Joins: oriba (~oriba@dynamic-089-012-016-065.89.12.pool.telefonica.de)
[02:16:34] *** Quits: Techcable (~Techcable@168.235.93.147) (Ping timeout: 260 seconds)
[02:21:24] <d_bot> <Shon> Thanks for the POVs, rgrinberg and companion_cube. I'm still not convinced to be fully anti Seq, as having something that allows a mostly functional idiom over a stream of lines seems nice, except I've also been gunning my foot with it üòê -- maybe use of something like `with_file_in` will help provide the logical supports I need. Thanks!
[02:22:14] <rgrinberg> i'm not anti Seq, I just wouldn't mix it with IO or performance sensitive code
[02:23:02] *** Quits: namkeleser (~namkelese@101.179.128.103) (Quit: Client closed)
[02:25:07] *** Joins: namkeleser (~namkelese@101.179.128.103)
[02:34:12] <thizanne> damn rgrinberg we've used all my travis credits with the last PRs
[02:34:18] <thizanne> which is why it doesn't trigger anymore
[02:34:26] <rgrinberg> we need to switch to actions anwyay
[02:35:14] <thizanne> yeah I was looking at it
[02:35:20] <companion_cube> rgrinberg: depends how performance sensitive I'd say
[02:35:34] <thizanne> (re. travis, that really doesn't matter for me, I wasn't using it for anything else anyway)
[02:35:41] <companion_cube> In a core loop of a cpu intensive thing, maybe not, but it's not *that * slow
[02:37:33] <rgrinberg> I think the problem is that a lot of people use it for "optimization"
[02:37:41] <rgrinberg> to avoid having to allocate some intermediate lists for example
[02:38:15] <rgrinberg> not realizing the overhead this introduces is usually much greater than throwing away a couple of lists
[02:44:37] <companion_cube> ah, that
[02:44:50] <companion_cube> I mean, you should allocate the same amount, more or less
[02:44:58] <companion_cube> except it's closures instead of cons nodes
[02:45:06] <companion_cube> (so perhaps slightly bigger, but same number of allocations)
[02:45:25] <companion_cube> for me the "optimisation" part is that you only produce stuff when you need to.
[02:46:44] <d_bot> <Shon> Yeah, I think what rgrinberg described is my scenario actually. Premature optimization, and then reaching for something of which I didn't have a clear understanding of the tradeoffs. I think the new preamble introduced with the expansion of the Seq library should help with this a lot! (https://github.com/ocaml/ocaml/blob/trunk/stdlib/seq.mli)
[02:47:13] *** Joins: average (uid473595@user/average)
[02:47:41] <rgrinberg> in general, laziness and IO don't mix well. I suppose this applies to Seq as well
[02:48:53] *** Quits: rgrinberg (~textual@2806:101e:7:6946:301f:4093:a99:4144) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[02:49:06] <d_bot> <Shon> But a couple days ago I was producing a Seq from lines of input, and tried to then convert it into an array (using Containers), and didn't consult the docs first, and the array conversion traverses the whole seq to get the length before allocating the array, initializing with the first element, so I was getting very surprising behavior üôÇ -- good reminder to RTFM, also perfect illustration of your warning about mixing laziness and
[02:53:47] <companion_cube> Hmm right
[02:53:58] <companion_cube> This particular part could be improved tbh
[02:54:23] <companion_cube> But then it requires a vector or similar
[02:57:20] <d_bot> <Shon> I was thinking about opening an issue/pr. the best idea I came up with was constructing a list and counting length in one go, then traverse the reversed list, using the length index to store each item into the array in reverse order?
[02:58:09] <d_bot> <Shon> Would require allocation of the list, but I think would only take the same amount of traversals of the seq? If you think that'd be helpful, I'd be happy to make note in an issue or open a PR with the suggestion.
[03:02:58] <companion_cube> you can do to_rev_list followed by an array.of_list (but reversed)
[03:03:03] <companion_cube> it's just also a lot of allocations
[03:03:10] <companion_cube> it's only worth it if you do a lot of intermediate work imho
[03:03:42] <companion_cube> for years I also used an imperative version (`unit -> 'a option`) which is actually somewhat faster
[03:03:48] <companion_cube> but it's less general as it can't be used twice, ever
[03:11:15] *** Quits: Tuplanolla (~Tuplanoll@91-159-69-50.elisa-laajakaista.fi) (Quit: Leaving.)
[03:16:51] *** Quits: oriba (~oriba@dynamic-089-012-016-065.89.12.pool.telefonica.de) (Read error: Connection reset by peer)
[03:32:35] <d_bot> <Shon> Yeah, I can see the downside of that for sure. I guess this exactly the kind of place where having linear types, or something else that could track the effects at play, would really be helpful.
[03:32:36] <d_bot> <Shon>
[03:32:37] <d_bot> <Shon> I know there's nothing novel about the insight, but I'm finding it interesting to think about the impedance mismatch between trying to get input from inherently stateful sources like in channels and trying to express the program logic in a maximally declarative, and FP-friendly way (without just giving up on predictability or performance).
[03:33:28] <d_bot> <Shon> What about something like `fold_over_in_channel: ('acc -> string -> 'acc) -> 'acc -> in_channel`
[03:33:33] *** Joins: Techcable (~Techcable@168.235.93.147)
[03:41:29] <companion_cube> sure
[03:41:40] <companion_cube> I also have this `iter` library just for that :p
[03:41:47] <companion_cube> but the downside is, you lose expressiveness even more
[03:42:02] <companion_cube> you can't do stuff like, say, read 2 sorted files and sort-merge them
[03:46:41] <d_bot> <Shon> Ah yeah, I see the downside there.
[03:49:00] *** Quits: namkeleser (~namkelese@101.179.128.103) (Quit: Client closed)
[03:58:07] *** Joins: namkeleser (~namkelese@101.179.128.103)
[04:05:30] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[04:05:37] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Client Quit)
[04:10:59] <companion_cube> Otoh it seems to optimise very well.
[04:22:45] *** Quits: Anarchos (~Anarchos@88.168.112.216) (Quit: Vision[]: i've been blurred!)
[04:35:29] *** Quits: mro_ (~mro@port-92-195-129-79.dynamic.as20676.net) (Quit: Leaving...)
[04:46:46] <d_bot> <Shon> Yeah. I guess more residue from the impedance mismatch!
[05:16:03] *** Joins: rgrinberg (~textual@2806:101e:7:6946:301f:4093:a99:4144)
[05:17:31] *** Joins: Anarchos (~Anarchos@88.168.112.216)
[05:26:04] <companion_cube> on flambda it was sometimes the case that a `range n |> flat_map |> filter |> map |> iter` (or sth like that) would become a proper loop üòç
[05:27:19] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[05:31:41] <d_bot> <Shon> Oh wow. that's very nice!
[05:31:57] *** Quits: rgrinberg (~textual@2806:101e:7:6946:301f:4093:a99:4144) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[05:34:11] <companion_cube> (iter is: `'a iter = ('a -> unit) -> unit`)
[05:35:07] *** Quits: Anarchos (~Anarchos@88.168.112.216) (Quit: Vision[]: i've been blurred!)
[05:40:04] *** Joins: rgrinberg (~textual@2806:101e:7:6946:301f:4093:a99:4144)
[05:42:41] <rgrinberg> relying on the optimization gods is not the ocaml way
[05:44:35] <companion_cube> not like we have any choice.
[05:44:57] <companion_cube> this would all be reliable if we had staging or compile time evaluation‚Ä¶
[05:45:44] <rgrinberg> those would be nice, but i think we're suffering from the lack of control over memory layout much more
[05:47:20] <rgrinberg> companion_cube ocamlopt seems awfully conservative with inlining. have you noticed?
[05:47:32] <companion_cube> the vanilla one? yeah
[05:47:42] <companion_cube> well, I think it's both
[05:47:55] <companion_cube> memory layout, _and_ lack of good inlining which means a lot of things will produce tons of closures
[05:47:59] <d_bot> <Shon> I do find it nice to be able to write things in the most clear and declarative way you can work out, since it's often easier to reason about, and then go back through and try to optimize if needed.
[05:47:59] <companion_cube> (including iterators)
[05:49:00] <d_bot> <Shon> But flambda helps make that need less frequent, that is nice! Tho I of course also agree that I'm not eager to be in a situation like Haskell, where it seems you have to be an advanced expert to have any hope of reasoning about what optimizations are making your code feasible to run.
[05:49:33] <rgrinberg> worse than that, they've introduced a whole new class of crippling performance problems - space leaks
[05:50:28] <companion_cube> in the compiler, you mean, right?
[05:50:37] <companion_cube> ah sorry, haskell.
[05:50:45] <companion_cube> flambda is nice but some packages fail to compile with it
[05:50:50] <companion_cube> so flambda2 is where hope lies
[05:59:19] <companion_cube> alright Shon, you might have speedrun the PR
[06:27:53] <d_bot> <Shon> lol, well it was a nice, well defined, tiny problem, and I had the benefit of the feedback here in advance üôÇ Thanks for the quick review!
[06:35:12] *** Joins: zebrag (~chris@user/zebrag)
[07:07:58] <companion_cube> :)
[07:18:23] *** Joins: xd1le (~xd1le@user/xd1le)
[07:40:07] *** Joins: Corbin (~Corbin@c-73-67-140-116.hsd1.or.comcast.net)
[07:42:09] <d_bot> <Et7f3> What is space leak ?
[07:45:55] <d_bot> <Shon> > A space leak occurs when a computer program uses more memory than necessary. In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected
[07:47:04] <d_bot> <Shon> So basically you end up allocating more memory (e.g., perhaps you're constructing a bunch of intermediary lists you don't actually) for operations that they should need.
[07:47:36] <d_bot> <Shon> But it's not that those things are slipping out of view of the garbage collector or anything (or that you're failing to deallocate).
[07:47:43] <d_bot> <Et7f3> Oh so program that use free list and free all at end is space leak ?
[07:49:52] <d_bot> <Shon> I wouldn't think that'd also be a space leak.
[07:49:56] <d_bot> <Shon> More details here: https://stackoverflow.com/a/46009036/1187277
[07:50:47] <d_bot> <Shon> iiuc, basically just where your language/program/compiler whatever uses a bunch of allocations that wouldn't expect, and shouldn't actually need, to perform your computations.
[07:53:52] *** Joins: mbuf (~Shakthi@122.174.253.173)
[07:58:57] <rgrinberg> more concretely, in haskell every computation no matter how cheap it is allocates a cell due to lazy evaluation
[07:59:00] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Quit: WeeChat 3.3)
[07:59:11] <rgrinberg> So 2 + 3 is more like lazy (2 + 3) in OCaml
[07:59:17] <companion_cube> except for strict computations, but it's opt-in, yeah
[07:59:44] <rgrinberg> it turns out that there's a lot of things that are cheaper to compute immediately than to delay
[07:59:52] <rgrinberg> so laziness is a huge waste
[08:00:03] <companion_cube> I'd tend to agree yep
[08:01:44] <companion_cube> unsurprisingly, successor languages like Idris are strict
[08:02:32] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[08:13:56] <d_bot> <darrenldl> do people opt for slightly more restrictive licenses these days? say <https://commonsclause.com/>
[08:14:21] <d_bot> <darrenldl> licenses which make predatory commercialisation more(?) difficult
[08:36:37] <Corbin> Sure; I like AGPL.
[08:40:07] <d_bot> <darrenldl> do a lot of opam packages use agpl?
[08:46:38] *** Quits: zebrag (~chris@user/zebrag) (Quit: Konversation terminated!)
[08:48:37] *** Quits: rgrinberg (~textual@2806:101e:7:6946:301f:4093:a99:4144) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[09:02:30] <d_bot> <darrenldl> hm, janestreet just uses mit it seems
[10:02:58] *** Joins: rgrinberg (~textual@187.223.209.106)
[10:41:40] *** Quits: namkeleser (~namkelese@101.179.128.103) (Quit: Client closed)
[10:58:03] *** Joins: namkeleser (~namkelese@101.179.128.103)
[11:34:32] *** Quits: Techcable (~Techcable@168.235.93.147) (Quit: ZNC - https://znc.in)
[11:35:52] *** Joins: Techcable (~Techcable@168.235.93.147)
[11:47:19] *** Joins: kadir (~kadir@95.14.12.204)
[11:56:44] *** Joins: Serpent7776 (~Serpent77@90-156-31-193.internetia.net.pl)
[12:00:35] *** Quits: chrisz (uwhr36a55a@55d4021a.access.ecotel.net) (Ping timeout: 250 seconds)
[12:02:21] *** Joins: chrisz (gk0qpcz3q1@55d4682c.access.ecotel.net)
[12:31:20] *** Joins: haesbaert (~haesbaert@195.95.187.184)
[12:38:42] *** Joins: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4)
[12:38:59] *** Joins: bartholin (~bartholin@109.53.2.52)
[13:02:50] *** Parts: kadir (~kadir@95.14.12.204) (WeeChat 3.3)
[13:36:52] *** Quits: mbuf (~Shakthi@122.174.253.173) (Quit: Leaving)
[13:41:43] *** Quits: bartholin (~bartholin@109.53.2.52) (Ping timeout: 252 seconds)
[13:54:55] *** Quits: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4) (Ping timeout: 252 seconds)
[13:55:51] *** Joins: bartholin (~bartholin@109.53.2.52)
[13:56:58] *** Quits: rgrinberg (~textual@187.223.209.106) (Quit: My MacBook has gone to sleep. ZZZzzz‚Ä¶)
[14:31:13] *** Quits: bartholin (~bartholin@109.53.2.52) (Ping timeout: 252 seconds)
[14:34:28] <Leonidas> why wouldn't they? they're not in the software commercialization business
[14:35:10] <d_bot> <darrenldl> true
[14:35:39] <Leonidas> commons clause has the problem that it cannot be included e.g. in Debian
[14:35:53] <Leonidas> so, any software that would use your project, couldn't
[14:36:01] <d_bot> <darrenldl> oh huh
[14:36:16] <d_bot> <darrenldl> good to know : o
[14:37:03] <Leonidas> (notably, AGPL doesn't have this problem)
[14:40:24] <d_bot> <darrenldl> ah a fair few big ocaml packages are in mit or bsd 2, not much point in overthinking it i guess
[14:40:36] <d_bot> <darrenldl> thanks for the info~
[14:43:11] *** Joins: bartholin (~bartholin@109.53.4.48)
[14:47:26] *** Joins: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4)
[14:48:36] *** Joins: Tuplanolla (~Tuplanoll@91-159-69-50.elisa-laajakaista.fi)
[14:53:02] *** Quits: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4) (Ping timeout: 260 seconds)
[15:02:48] *** Quits: hackinghorn (~hackingho@user/hackinghorn) (Ping timeout: 256 seconds)
[15:11:22] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[15:15:04] *** Joins: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4)
[15:20:55] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Quit: Leaving...)
[15:32:43] <aru> Hi, I'm pretty new to ocaml and I've hit a bit of behavior that I can't really grasp. I'm putting together a solution for one of the advent of code challenges and I have something which works just fine, but breaks as soon as I open core. Could anyone point me in the right direction with this https://pastebin.com/X8UQd5UX ?
[15:33:40] <aru> I wanted to add core to have access to fold_until, but the same typechecking error hits me there
[15:35:13] <aru> I mean I'm destructuring two lists of chars and comparing their elements, how can it assume of the items is an int?
[15:36:26] *** Quits: chrisz (gk0qpcz3q1@55d4682c.access.ecotel.net) (Ping timeout: 260 seconds)
[15:38:16] *** Joins: chrisz (wxka6yh33z@55d4a46a.access.ecotel.net)
[15:45:37] <d_bot> <NULL> Core's (=) is not polymorphic, it is only equality for ints
[15:53:01] <aru> oh
[15:53:08] <aru> I didn't expect that one
[15:53:33] *** Joins: jlrnick (~josephler@2a01cb040a159400c5072f162e4c1f90.ipv6.abo.wanadoo.fr)
[15:53:34] <aru> so Base.Char.equal?
[15:54:31] * qwr would say that deciding to use Core is matter of taste, it has some decisions that not everybody agrees - but you can do Stdlib.(foo = blaah) :)
[16:01:34] <aru> cool, thanks a bunch
[16:31:12] *** Quits: bartholin (~bartholin@109.53.4.48) (Ping timeout: 256 seconds)
[16:38:03] *** Joins: hackinghorn (~hackingho@user/hackinghorn)
[16:43:26] <d_bot> <Shon> You can also do `let (=) = Int.equal` to set your definition of equality within the scope you need it be for into.
[16:43:34] *** Joins: bartholin (~bartholin@109.53.6.27)
[16:45:26] *** Joins: gravicappa (~gravicapp@178.214.248.83)
[16:54:22] *** Quits: tomku (~tomku@user/tomku) (Ping timeout: 260 seconds)
[17:03:08] *** Quits: jlrnick (~josephler@2a01cb040a159400c5072f162e4c1f90.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[17:04:38] *** Quits: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4) (Ping timeout: 260 seconds)
[17:14:14] *** Joins: tomku (~tomku@user/tomku)
[17:20:08] *** Quits: namkeleser (~namkelese@101.179.128.103) (Quit: Client closed)
[17:21:04] *** Quits: bartholin (~bartholin@109.53.6.27) (Ping timeout: 256 seconds)
[17:37:16] *** Joins: bartholin (~bartholin@109.53.6.27)
[17:45:31] <Soni> dmbaturin: poke?
[17:56:46] *** Joins: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4)
[18:02:49] <dmbaturin> Hi Soni!
[18:06:58] <d_bot> <Et7f3> Sone: ball
[18:07:13] *** Joins: SpiceGuid (~SpiceGuid@alyon-659-1-75-206.w109-213.abo.wanadoo.fr)
[18:21:27] *** Quits: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4) (Ping timeout: 268 seconds)
[18:22:52] *** Joins: jlrnick (~josephler@2a01cb040a159400c5072f162e4c1f90.ipv6.abo.wanadoo.fr)
[18:23:30] *** Joins: waleee (~waleee@h-98-128-229-110.NA.cust.bahnhof.se)
[18:31:01] *** Quits: bartholin (~bartholin@109.53.6.27) (Ping timeout: 252 seconds)
[18:35:10] *** Quits: SpiceGuid (~SpiceGuid@alyon-659-1-75-206.w109-213.abo.wanadoo.fr) (Quit: Client closed)
[18:36:43] *** Quits: xd1le (~xd1le@user/xd1le) (Quit: xd1le)
[18:45:05] *** Joins: bartholin (~bartholin@109.53.6.102)
[18:48:53] *** Joins: spip (~bobo@37.166.77.115)
[18:49:07] *** spip is now known as Guest4123
[19:41:54] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Ping timeout: 260 seconds)
[19:42:58] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[20:24:03] <d_bot> <zakkor> aru: hahaha, what the hell man. I literally hit this exact same problem, on the exact same AOC problem, and asked about it here earlier
[20:24:12] <d_bot> <zakkor> not sure if you can see this through the bridge
[20:24:12] <d_bot> <zakkor> https://cdn.discordapp.com/attachments/436568060288172044/919633245140844544/unknown.png
[20:27:31] <aru> heh
[20:31:22] *** Quits: bartholin (~bartholin@109.53.6.102) (Ping timeout: 260 seconds)
[20:32:27] <aru> well, now we know
[20:33:11] <d_bot> <RegularSpatula> Does anyone know of any examples repos fuzzing do fuzzing in CI?
[20:35:35] *** Joins: thizanne` (~thibault@176.170.195.127)
[20:38:59] *** Quits: aru (~aru@ip-86-49-27-241.net.upcbroadband.cz) (Quit: WeeChat 3.0)
[20:39:34] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[20:43:33] <Soni> dmbaturin: what happened to you? haven't seen you in #soupault in forever
[20:44:02] *** Joins: aru (~aru@ip-86-49-27-241.net.upcbroadband.cz)
[20:44:48] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Remote host closed the connection)
[20:45:00] *** Joins: bartholin (~bartholin@109.53.2.125)
[20:45:04] *** Quits: thizanne` (~thibault@176.170.195.127) (Quit: WeeChat 3.3)
[20:45:30] *** Joins: thizanne` (~thibault@176.170.195.127)
[20:45:42] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[20:47:11] *** Quits: thizanne (~thibault@2001:41d0:a:f682::1) (Quit: WeeChat 2.3)
[20:47:15] *** thizanne` is now known as thizanne
[20:48:11] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Remote host closed the connection)
[20:48:29] *** Joins: mro (~mro@2002:5cc3:98da:0:4d14:b5e6:e67:af4c)
[20:51:03] *** Quits: mro (~mro@2002:5cc3:98da:0:4d14:b5e6:e67:af4c) (Remote host closed the connection)
[21:02:58] <d_bot> <undu> @RegularSpatula I'd like to see one as well, I'm curious about how to automate it, how to avoid limits in the CI runners, periodicity, etc
[21:11:24] <d_bot> <RegularSpatula> Yeah i was planning on working on it at some point this week, but I‚Äôm hoping to find an example first to avoid reinventing the wheel
[21:27:23] *** Quits: gdd (~gdd@129.199.146.230) (Ping timeout: 252 seconds)
[21:30:56] *** Joins: gdd (~gdd@129.199.146.230)
[21:42:07] *** Quits: waleee (~waleee@h-98-128-229-110.NA.cust.bahnhof.se) (Ping timeout: 250 seconds)
[21:42:32] *** Joins: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4)
[21:48:06] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[21:51:49] *** Joins: namkeleser (~namkelese@101.179.128.103)
[21:56:42] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Remote host closed the connection)
[21:59:09] *** Quits: waleee (~waleee@2001:9b0:21d:fc00:398f:b003:b90d:acf4) (Ping timeout: 268 seconds)
[22:00:23] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[22:01:18] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Remote host closed the connection)
[22:01:42] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[22:20:36] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Remote host closed the connection)
[22:31:05] *** Quits: bartholin (~bartholin@109.53.2.125) (Ping timeout: 250 seconds)
[22:44:56] *** Joins: bartholin (~bartholin@109.53.5.12)
[23:06:10] *** Quits: namkeleser (~namkelese@101.179.128.103) (Quit: Client closed)
[23:08:02] *** Joins: rgrinberg (~textual@187.223.209.106)
[23:11:29] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[23:18:34] <rgrinberg> thizanne using a native popcnt does absolutely nothing for performance ^_^
[23:39:01] <thizanne> rgrinberg: does it?
[23:39:11] <rgrinberg> Yes, I just tried it
[23:39:29] <thizanne> I remember gasche doing these experiments and finding that it wasn't zero
[23:40:06] <d_bot> <rgrinberg> I can put up a PR if you want to experiment measuring it
[23:40:44] <thizanne> nah I trust you
[23:41:22] <rgrinberg> I think the issue is that C calls aren't free
[23:41:55] <rgrinberg> I used no [@@noalloc] but still the call cannot be inlined
[23:43:32] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Remote host closed the connection)
[23:44:12] <thizanne> yeah I read the old mails again and it seems consistent with your result
[23:44:43] <thizanne> what he did was actually implementing popcount as a compiler primitive, but with C code that does the bit shifting stuff
[23:45:26] <thizanne> which seems to be the opposite of what you did, which is having a function that's not a primitive but uses the native instruction
[23:45:49] <thizanne> and then on some benchmark the time spent in popcnt went from 17% to 11% if I can read correctly
[23:45:50] <rgrinberg> i wonder why he didn't upstream. having a popcount primitive in the compiler seems quite useful
[23:46:08] *** Joins: mro (~mro@port-92-195-152-218.dynamic.as20676.net)
[23:47:04] <thizanne> probably because it was 2013, so "upstreaming in the compiler" was a dark path
[23:48:05] <thizanne> anyway it's nice to see that, apart from a builtin, we don't really gain something on optimised popcount
[23:48:28] <thizanne> I was wondering if we should try some implementations that maybe could be a little faster than all the shifting
[23:48:52] *** Quits: sagax (~sagax_nb@user/sagax) (Excess Flood)
[23:49:22] <rgrinberg> at key does the shifting a little faster
[23:49:27] <rgrinberg> let bottom5 = skey land 0x1f in
[23:49:29] <rgrinberg> *atkey
[23:49:56] <rgrinberg> doesn't have that masking like we do
[23:50:06] <rgrinberg> (h asr shift) land mask
[23:51:32] <thizanne> wikipedia (hamming weight) has several implementations that seem to have different tradeoffs (eg. the last one is faster if one has fast multiplications)
[23:51:48] *** Quits: mro (~mro@port-92-195-152-218.dynamic.as20676.net) (Remote host closed the connection)
[23:52:02] <thizanne> but if a native operation has no observable effect, then there is no point in trying non-native-slightly-optimised ones
[23:54:20] *** Quits: gravicappa (~gravicapp@178.214.248.83) (Ping timeout: 256 seconds)
[23:57:15] <rgrinberg> just to give a rough idea where we're at:
[23:57:40] <rgrinberg> fetching from a 10 element map is roughly twice as as slow than from an stdlib map
[23:57:46] <rgrinberg> we catch up when there's around 100 elements
[23:58:01] <rgrinberg> and at around 1k, we're about twice as fast
[23:58:16] <rgrinberg> this is on integer keys, which of course favor the stdlib map
[23:58:29] <rgrinberg> but it's good to have a pessimistic baseline
