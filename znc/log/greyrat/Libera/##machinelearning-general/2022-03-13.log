[00:20:48] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[01:10:17] *** Quits: bitkiller (~bitkiller@user/bitkiller) (Ping timeout: 240 seconds)
[01:20:53] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[01:34:14] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[01:50:12] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.4)
[02:12:08] *** Joins: CaCode (~CaCode@user/cacode)
[02:22:29] *** Quits: CaCode (~CaCode@user/cacode) (Quit: Leaving)
[03:37:33] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:41:26] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[03:43:22] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 250 seconds)
[03:46:32] *** Quits: OverCoder (~OverCoder@user/overcoder) (Quit: Quitting? That's odd.)
[03:47:39] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[05:16:07] *** Joins: sinaowolabi (~SinaOwola@160.152.118.7)
[05:17:26] *** Joins: namkeleser (~namkelese@101.175.104.107)
[05:19:13] *** Quits: medium_cool (~medium_co@136.36.128.48) (Quit: I have gone to sleep. ZZZzzz…)
[05:21:08] *** Joins: medium_cool (~medium_co@136.36.128.48)
[05:22:14] *** Quits: medium_cool (~medium_co@136.36.128.48) (Client Quit)
[05:22:18] *** Joins: ymwm_ (~yingmanwu@27.153.166.227)
[05:28:06] *** Quits: sinaowolabi (~SinaOwola@160.152.118.7) (Ping timeout: 272 seconds)
[05:41:25] *** Joins: sinaowolabi_ (~SinaOwola@102.67.1.24)
[05:41:58] *** Joins: sinaowolabi (~SinaOwola@102.67.1.24)
[05:53:03] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[06:29:30] *** Quits: sinaowolabi (~SinaOwola@102.67.1.24) (Ping timeout: 252 seconds)
[06:36:16] *** Joins: blei (~blei@user/blei)
[06:42:56] *** Joins: sinaowolabi (~SinaOwola@160.152.118.7)
[06:56:38] *** Quits: sinaowolabi_ (~SinaOwola@102.67.1.24) (Ping timeout: 250 seconds)
[06:56:46] *** Quits: sinaowolabi (~SinaOwola@160.152.118.7) (Ping timeout: 272 seconds)
[07:10:45] *** Joins: sinaowolabi_ (~SinaOwola@160.152.118.7)
[07:18:23] *** Quits: namkeleser (~namkelese@101.175.104.107) (Quit: Client closed)
[07:22:44] *** Quits: sinaowolabi_ (~SinaOwola@160.152.118.7) (Ping timeout: 272 seconds)
[07:26:59] <Jong> Is there a difference between a perceptron and PyTorch's torch.nn.Linear layer?
[07:41:40] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Quit: Hay te huacho!)
[08:58:21] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[09:33:40] *** Joins: sinaowolabi (~SinaOwola@102.67.1.24)
[09:49:59] *** Joins: sinaowolabi_ (~SinaOwola@160.152.118.7)
[10:46:26] *** Joins: spaceseller (~spacesell@31.147.205.13)
[11:03:09] *** Joins: manti7 (~manti7@176.10.104.94)
[11:12:18] *** Quits: sinaowolabi (~SinaOwola@102.67.1.24) (Ping timeout: 250 seconds)
[11:24:23] *** Joins: sinaowolabi (~SinaOwola@160.152.118.7)
[12:03:25] *** Joins: namkeleser (~namkelese@101.175.104.107)
[12:07:48] *** Quits: C-Man (~C-Man@78.31.190.82) (Ping timeout: 240 seconds)
[12:28:38] *** Joins: palasso (~palasso@user/palasso)
[12:38:49] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[13:17:14] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[13:23:41] *** Quits: diogenese (~diogenese@diogenese.velotech.net) (Quit: Has anybody seen the bridge?)
[13:24:11] *** Joins: diogenese (~diogenese@diogenese.velotech.net)
[13:32:03] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[13:49:21] *** Quits: namkeleser (~namkelese@101.175.104.107) (Quit: Client closed)
[14:25:02] *** Quits: stefan-_ (~cri@42dots.de) (Ping timeout: 240 seconds)
[14:25:51] *** Joins: C-Man (~C-Man@78.31.190.82)
[14:29:07] *** Joins: stefan-_ (~cri@42dots.de)
[14:40:11] *** Joins: namkeleser (~namkelese@101.175.104.107)
[14:58:48] *** Quits: sinaowolabi (~SinaOwola@160.152.118.7) (Ping timeout: 252 seconds)
[15:00:00] *** Quits: sinaowolabi_ (~SinaOwola@160.152.118.7) (Ping timeout: 272 seconds)
[15:02:35] <HuntsMan> Jong: yes
[15:02:55] <Jong> What's the difference?
[15:03:12] <Jong> I tried Google searching for an answer but couldn't find one
[15:03:57] *** Quits: HuntsMan (~hunts@2a02-a44b-361b-1-5f8-874b-a485-d79e.fixed6.kpn.net) (Quit: Konversation terminated!)
[15:04:46] <Jong> PyTorch's linear layer is the same as Tensorflow's dense layer. Just different names for the exact same thing
[15:18:24] *** Joins: HuntsMan (~hunts@2a02-a44b-361b-1-5f8-874b-a485-d79e.fixed6.kpn.net)
[15:19:22] <HuntsMan> Jong: the learning algorithm
[15:20:33] <Jong> All the diagrams I see show that a perceptron is just a tensorflow dense layer or pytorch linear layer in which the layer has multiple inputs and 1 output, and an activation function
[15:21:47] <Jong> activation(features matrix * weights matrix + bias value)
[15:31:10] <HuntsMan> Jong: which learning algorithm is used in the perceptron?
[15:33:59] <Jong> From all I see, the perceptron refers just to the topology / connectivity / architecture of the inference.  The learning algorithm is about finding the values to use for the weights and biases. I'm not finding anything suggesting that a certain learning algo must be used for a perceptron
[15:34:17] <Jong> HuntsMan have any link you could refer me to?
[15:38:47] <HuntsMan> Jong: https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm
[15:38:48] <SigmoidFroid> ⇒  Perceptron - Wikipedia
[15:59:33] *** Joins: Klinda (~superleag@user/klinda)
[16:02:50] <HuntsMan> Jong: do you see the difference? its not just the architecture
[16:05:49] <Jong> The output must be a 0 or 1 ?
[16:06:05] <Jong> It must be a binary classifier?
[16:06:26] <Jong> There isn't such a thing as a perceptron with a regression / continuous output?
[16:06:32] <HuntsMan> the learning algorithm is not gradient descent
[16:06:42] <HuntsMan> and yeah it is a classification algorithm
[16:16:32] <Jong> What about in multi-layer perceptrons? That wiki link states backprop is used for MLPs. But aren't the components of the MLPs called perceptrons too?
[16:22:32] <HuntsMan> no, not really
[16:22:34] <HuntsMan> not conceptually
[16:23:32] <Jong> This is very equivocal: https://en.wikipedia.org/wiki/Multilayer_perceptron#:~:text=The%20term%20%22multilayer%20perceptron%22%20does,in%20the%20strictest%20possible%20sense.
[16:23:33] <SigmoidFroid> ⇒  Multilayer perceptron - Wikipedia
[16:25:01] <Jong> More names need to be made to avoid the same name being used to refer to different things, making the name unfit for understandable communication
[16:25:46] <HuntsMan> hahaha
[16:25:50] <HuntsMan> you are taking the concepts too literally
[16:26:55] <Jong> Article states: Moreover, MLP "perceptrons" are not perceptrons in the strictest possible sense. True perceptrons are formally a special case of artificial neurons that use a threshold activation function such as the Heaviside step function. MLP perceptrons can employ arbitrary activation functions. A true perceptron performs binary classification, an MLP neuron is free to either perform classification or regression, depending
[16:26:55] <Jong>  upon its activation function.
[16:27:16] <HuntsMan> yup
[16:27:22] <Jong> so there are "MLP perceptrons" and "true perceptrons"
[16:31:16] <Jong> Is there a point to learning about perceptrons? It seems to be trivia information. Everyone is using backprop these days, not old other ways
[16:31:27] <HuntsMan> its just historical details
[16:35:51] *** Joins: ymwm__ (~yingmanwu@27.153.167.53)
[16:38:21] *** Quits: ymwm_ (~yingmanwu@27.153.166.227) (Ping timeout: 252 seconds)
[16:57:17] *** Joins: bitkiller (~bitkiller@user/bitkiller)
[17:56:38] *** Joins: `Tim (~zenguin@user/zenguin)
[18:42:02] <Jong> HuntsMan  I have some ideas I'm wondering if they've been thought of before. (1) Parameters in the activation function, like if the activation function were an exponent.  (weighted sum + bias)^n for example. Or a relu version would be Max(0, (weighted sum + bias))^n
[18:45:19] <Jong> The purpose of this would be to enlarge or decrease numbers in a few steps that otherwise would take many steps through a chain of weighted sums.  For example, capturing x^2 through a series of weighted sums is more inefficient than a single activation of squaring the input.
[18:48:40] *** Joins: marcello1 (~mp@p200300dfaf10fe015baf6eed7f4ef68f.dip0.t-ipconnect.de)
[18:54:28] <Jong> (2) branching a layer near the final output; concatenating this branch with the input ;  training this branch to all ones.   The purpose would be for better generalization by forcing the creation of features that cause the inputs to form clusters of points close to one another in a latent space.  For example suppose we wanted a net that simply squared the input.  We could train input - output pairs (2,4); (3,9); (4, 16);
[18:54:28] <Jong>  (10,100).  Likely the neural net wouldn't morph into an input^2 implementation.  But if we concatenated the output to the input there is a formula--I hash key in a sort-- that outputs numbers that would be identical:  sqrt(4)/2 = 1;  sqrt(9)/3 = 1;  sqrt(16)/4 = 1;  sqrt(100)/10 == 1.  If the branch could imitate this math by training to 1, it's likely a common pattern was found in the main neural net.
[18:55:09] <Jong> Either of these ideas sound interesting and new?
[19:38:07] <HuntsMan> Jong: you just described a residual network (except the part of power activations, but activations with learnable parameters already exist)
[19:38:54] *** Joins: sinaowolabi (~SinaOwola@160.152.118.7)
[19:40:27] <Jong> what about training to 1?
[19:48:37] *** Joins: sinaowolabi_ (~SinaOwola@160.152.118.7)
[20:00:11] *** Quits: sinaowolabi (~SinaOwola@160.152.118.7) (Ping timeout: 252 seconds)
[20:05:49] <Jong> residual blocks don't do that
[20:14:01] *** Joins: sinaowolabi (~SinaOwola@102.67.1.24)
[20:37:08] <HuntsMan> there are residual blocks with concatenation
[20:46:31] <Jong> I mean residual blocks don't train to 1.   Residual blocks in and of themselves don't create a separate/additional output. I'm proposing an additional output that is always trained with a target of 1.
[20:56:53] <HuntsMan> that's not really different from mean squared error
[20:57:11] <HuntsMan> but in any case, proposing something is worthless until you prove it is useful
[20:57:35] <Jong> Thanks HuntsMan  for the feedback
[21:00:46] *** Quits: ymwm__ (~yingmanwu@27.153.167.53) (Ping timeout: 250 seconds)
[21:09:56] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[21:11:36] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:115a:d879:b217:1367)
[21:12:24] *** Quits: sinaowolabi (~SinaOwola@102.67.1.24) (Ping timeout: 272 seconds)
[21:14:25] *** Quits: blei (~blei@user/blei) (Ping timeout: 256 seconds)
[21:15:32] *** Quits: sinaowolabi_ (~SinaOwola@160.152.118.7) (Ping timeout: 240 seconds)
[21:17:33] *** Joins: jlrnick (~josephler@2a01cb040a159400d5f1734ba665bbe6.ipv6.abo.wanadoo.fr)
[21:43:31] *** Quits: namkeleser (~namkelese@101.175.104.107) (Quit: Client closed)
[21:47:26] *** Joins: sinaowolabi_ (~SinaOwola@160.152.118.7)
[22:18:18] *** Joins: sinaowolabi (~SinaOwola@160.152.118.7)
[22:36:30] <hodapp> Jong: if you're going to throw an exponential in there you have to contend with its gradient having a logarithm in it, and logarithms are going to be ill-behaved as soon as you get near 0 and undefined as soon as they are <= 0
[22:38:09] *** Joins: spaceseller (~spacesell@31.147.205.13)
[22:38:21] <hodapp> also, #2 sounds identical in purpose to sparsity constraints, which have been in wide use for decades
[22:39:09] <hodapp> which L1/L2 effectively get you (similar to what HuntsMan just said about MSE)
[22:39:51] <hodapp> if you have some fluency in something like Pytorch, most of these ideas could be implemented in under an hour and tested
[22:41:17] <hodapp> and ultimately, the only people who receive any attention for proposing something that "should" work, but is untested, is people who have a track record of proposing things that actually do work
[22:45:03] *** Joins: blei (~blei@user/blei)
[22:58:06] *** Quits: blei (~blei@user/blei) (Quit: Client closed)
[23:19:56] *** Quits: rvalue (~rvalue@user/rvalue) (Remote host closed the connection)
[23:20:36] *** Joins: rvalue (~rvalue@user/rvalue)
[23:23:13] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[23:44:34] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:115a:d879:b217:1367) (Quit: I have gone to sleep. ZZZzzz…)
