[00:01:02] *** Quits: sinaowolabi (~SinaOwola@160.152.109.220) (Ping timeout: 240 seconds)
[00:01:33] <dostoyevsky2> ah, Julia has this catalyst.jl https://catalyst.sciml.ai/dev/
[00:01:34] <SigmoidFroid> ⇒  Home · Catalyst.jl
[00:02:37] <dostoyevsky2> mefistofeles: could actually run Julia notebooks on the HPC system?
[00:02:44] <mefistofeles> dostoyevsky2: I could, yes
[00:03:03] <dostoyevsky2> I guess it might be a bit annoying if your notebook session is blocking the hpc system for others
[00:03:48] <mefistofeles> catalyst sounds like very theoretical/symbolic, don't think it would require much computational power, but not sure
[00:03:54] <mefistofeles> I guess it depends on the amount if inputs/data
[00:04:00] <mefistofeles> *of
[00:08:55] *** Quits: trace987 (~trace@user/trace) (Ping timeout: 256 seconds)
[00:10:16] <dostoyevsky2> I guess a lot of the jl libs do some shiny thing but miss depth/breadth
[00:13:35] *** Joins: tracey (~trace@user/trace)
[00:18:06] *** Quits: raffaem (~raffaem@45.91.20.167) (Quit: Leaving)
[00:19:49] <mefistofeles> dostoyevsky2: yeah, the ecosystem is pretty new and in development still
[00:23:05] <dostoyevsky2> idk, Julia has been in development since 2012... Golang has made quite some progress and is only 3 years older
[00:23:52] <mefistofeles> hmm depends, maybe in web frameworks, also Julia doesn't have Google behind it
[00:24:03] <mefistofeles> I don't even see them as competitors
[00:24:16] <hodapp> they both can have some ugly bare spots though, e.g. the available numerical/symbolic libraries in Go were just garbage, though that may change with generics now being available
[00:24:20] <hodapp> or, soon being available
[00:27:27] <dostoyevsky2> mefistofeles: Go has gotten the cs part right, they have a stable API, you can find programs on stackoverflow from ten years ago, and they still work today flawlessly... if that were the case with Julia I am pretty sure someone would have written a more substantial chemistry package... but if your code breaks every three weeks, no one is going to try
[00:29:33] *** Joins: Hunts (~hunts@84-82-208-103.fixed.kpn.net)
[00:30:36] *** Quits: HuntsMan (~hunts@84-82-208-103.fixed.kpn.net) (Ping timeout: 240 seconds)
[00:31:06] <mefistofeles> dostoyevsky2: I don't really think that's much of an issue, and as a counter example, there's python. Go can afford this by being very minimalistic, but that also hinders the possibilities/flexibilities, that's why I see it as a different thing, just another tool for the toolkit
[00:32:45] <hodapp> python retains pretty stable APIs though
[00:35:33] <mefistofeles> you think? For me python is one of the "worst" on that regards, not that it's that much of an issue, imo
[00:37:25] <hodapp> I have Python code I wrote in 2004 that still runs
[00:38:01] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[00:38:27] <mefistofeles> hodapp: if you don't change the major version? Sure
[00:38:38] <hodapp> this is with a major version change
[00:38:39] <mefistofeles> but the backward compatibility is pretty poor, imho
[00:40:50] <mefistofeles> I mean, it is not a mystery that the py2 py3 transition was a painful one for many
[00:48:47] <mefistofeles> maybe it will be better now, it seems to be, but we'll see, python is very pragmatic. Pragamatism vs correctness is sometimes a tradeoff
[00:59:39] <dostoyevsky2> mefistofeles: after a while your language just dies though, because no one trusts the language anymore to become stable... R might be ugly but they never break code... so that old CRAN package some phd candicate wrote 25 years ago, still works today
[01:08:41] <hodapp> this is one of the reasons it was significant when Rust reached a major release where they'd keep APIs stable
[01:09:17] <hodapp> people that are going to invest time and money into a language very often do care whether or not that investment is going to be worth anything down the road
[01:21:20] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[01:39:29] *** Quits: rvalue (~rvalue@user/rvalue) (*.net *.split)
[01:42:14] *** Joins: rvalue (~rvalue@user/rvalue)
[01:43:23] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[01:45:26] *** Joins: bitkiller (~bitkiller@user/bitkiller)
[01:48:34] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: beware the living)
[01:50:29] *** Joins: Malvolio (~Malvolio@user/malvolio)
[02:03:54] *** Quits: marloi (~mark@108.175.227.116) (Ping timeout: 256 seconds)
[02:13:36] *** Joins: trace987 (~trace@user/trace)
[02:16:22] *** Quits: tracey (~trace@user/trace) (Ping timeout: 256 seconds)
[02:23:29] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[02:24:18] *** Quits: sinaowolabi_ (~SinaOwola@160.152.109.220) (Ping timeout: 256 seconds)
[02:31:15] *** Quits: trace987 (~trace@user/trace) (Remote host closed the connection)
[02:39:42] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[03:04:29] *** Joins: ohyeahprivacy (~ohyeahpri@193.32.127.240)
[03:05:12] *** Joins: trace987 (~trace@user/trace)
[03:41:26] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[03:52:17] *** Quits: bitkiller (~bitkiller@user/bitkiller) (Ping timeout: 240 seconds)
[05:03:51] *** Quits: Starfoxxes (~Starfoxxe@2a02:8070:5390:d00:12bf:48ff:feb8:38c8) (Ping timeout: 252 seconds)
[05:04:23] *** Joins: Starfoxxes (~Starfoxxe@2a02:8070:5390:d00:12bf:48ff:feb8:38c8)
[05:50:42] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 260 seconds)
[05:50:55] *** Joins: naeui (~cpp@user/naeui)
[05:50:57] *** Quits: derelict (derelict@user/derelict) (Quit: bye)
[05:51:59] *** Joins: derelict (derelict@user/derelict)
[05:54:35] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[06:00:22] *** Joins: marloi (~mark@108.175.227.116)
[06:10:36] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[06:12:17] *** Quits: marloi (~mark@108.175.227.116) (Ping timeout: 240 seconds)
[06:12:46] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[06:30:47] *** Joins: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net)
[06:30:50] <blei> i'm looking for a gpu accelerated linear svm that supports incremental learning
[06:31:40] *** Joins: marloi (~mark@108.175.227.116)
[06:36:11] *** Quits: marloi (~mark@108.175.227.116) (Ping timeout: 256 seconds)
[06:36:49] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[06:43:37] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[07:11:38] *** Quits: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net) (Quit: Client closed)
[07:20:14] *** Joins: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net)
[07:32:49] *** Quits: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net) (Quit: Client closed)
[07:39:26] *** Quits: naeui (~cpp@user/naeui) (Read error: Connection reset by peer)
[07:39:32] *** Joins: naeui_ (~cpp@user/naeui)
[07:43:05] *** Joins: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net)
[08:19:25] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[08:44:13] *** Joins: Gimble (~G1mble2@2a01:4b00:8e07:7900:cf47:59fb:2875:5d41)
[08:47:31] *** Gimble is now known as Huutha
[09:07:35] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[09:11:36] *** Quits: Huutha (~G1mble2@2a01:4b00:8e07:7900:cf47:59fb:2875:5d41) (Quit: Leaving)
[09:11:53] *** Joins: Gimble (~G1mble2@2a01:4b00:8e07:7900:cf47:59fb:2875:5d41)
[09:13:04] *** Quits: Gimble (~G1mble2@2a01:4b00:8e07:7900:cf47:59fb:2875:5d41) (Remote host closed the connection)
[09:27:52] *** Quits: ohyeahprivacy (~ohyeahpri@193.32.127.240) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[09:35:17] *** Joins: blei54 (~blei@c-73-153-189-5.hsd1.co.comcast.net)
[09:35:22] *** Quits: blei54 (~blei@c-73-153-189-5.hsd1.co.comcast.net) (Client Quit)
[09:37:31] *** Quits: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net) (Ping timeout: 256 seconds)
[10:23:58] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[10:29:45] *** Joins: manti7 (~manti7@176.10.104.94)
[11:20:12] *** Joins: palasso (~palasso@user/palasso)
[11:21:44] *** Joins: shoky_ (uuuggg@141.226.193.67)
[11:25:30] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 272 seconds)
[11:27:41] *** Joins: Klinda (~superleag@user/klinda)
[11:32:02] *** Joins: sinaowolabi_ (~SinaOwola@160.152.109.220)
[11:34:25] *** Quits: yauhsien (~Yau-Hsien@61-231-19-150.dynamic-ip.hinet.net) (Read error: Connection reset by peer)
[11:53:17] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: EVOLUTION DOES NOT LOVE YOU)
[11:54:47] *** Joins: Malvolio (~Malvolio@user/malvolio)
[11:57:19] *** Joins: sinaowolabi (~SinaOwola@160.152.109.220)
[12:22:04] <cloudcell> Hello everyone, I am trying to teach a neural network a series of step functions. the results so far are shown in a graph below. I am using sine activation function. So in effect I am trying to "imitate" a Fourier transform. At the same time this article at stack overflow https://stackoverflow.com/questions/65753737/can-neural-networks-learn-to-do-fourier-transform tells me that it's a useless exercise. My question is, what is
[12:22:04] <cloudcell> the most efficient alternative approach to teach a NN such a function?
[12:22:04] <SigmoidFroid> ⇒  python - Can Neural Networks learn to do Fourier Transform? - Stack Overflow
[12:22:06] <cloudcell> https://usercontent.irccloud-cdn.com/file/Wdo41C5O/image.png
[12:22:12] <SigmoidFroid>  ⇒  (PNG) No title 21K
[12:28:18] <cloudcell> (as a side note, in the past, I used radial basis functions and genetic algorithms that learned such functions very well, but GAs are too slow and I want to know the most efficient alternative way.) Thanks in advance.
[12:36:05] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[12:40:14] *** Quits: sinaowolabi (~SinaOwola@160.152.109.220) (Ping timeout: 272 seconds)
[12:44:40] *** Quits: sinaowolabi_ (~SinaOwola@160.152.109.220) (Ping timeout: 272 seconds)
[13:07:41] *** Joins: sinaowolabi (~SinaOwola@160.152.109.220)
[13:15:04] *** Quits: Hunts (~hunts@84-82-208-103.fixed.kpn.net) (Ping timeout: 272 seconds)
[13:30:04] *** Joins: B0B1 (~quassel@201.130.21.6)
[13:37:13] *** Quits: sinaowolabi (~SinaOwola@160.152.109.220) (Ping timeout: 256 seconds)
[13:54:04] *** Quits: B0B1 (~quassel@201.130.21.6) (Quit: https://quassel-irc.org - Chat comfortably. Anywhere.)
[13:59:23] *** Joins: sinaowolabi (~SinaOwola@160.152.109.220)
[14:04:02] *** Quits: sinaowolabi (~SinaOwola@160.152.109.220) (Ping timeout: 240 seconds)
[14:08:50] *** Joins: sinaowolabi (~SinaOwola@160.152.109.220)
[14:17:17] *** Quits: Klinda (~superleag@user/klinda) (Ping timeout: 240 seconds)
[14:21:40] *** Joins: marcello42 (~mp@p200300dfaf10fe015baf6eed7f4ef68f.dip0.t-ipconnect.de)
[14:24:18] *** Joins: yauhsien (~Yau-Hsien@61-231-19-150.dynamic-ip.hinet.net)
[14:25:46] *** Malvolio is now known as Malvolio|haunted
[14:30:19] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[14:31:32] *** Quits: naeui_ (~cpp@user/naeui) (Read error: Connection reset by peer)
[14:32:41] *** Joins: naeui_ (~cpp@user/naeui)
[14:33:52] *** Joins: immibis_ (~hexchat@62.156.144.218)
[14:34:35] *** Joins: Klinda (~superleag@user/klinda)
[14:35:48] *** Quits: marcello42 (~mp@p200300dfaf10fe015baf6eed7f4ef68f.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[14:37:17] *** Quits: immibis (~hexchat@62.156.144.218) (Ping timeout: 256 seconds)
[14:38:40] *** Quits: sinaowolabi (~SinaOwola@160.152.109.220) (Ping timeout: 272 seconds)
[14:40:26] *** Joins: sinaowolabi (~SinaOwola@160.152.109.220)
[14:56:24] *** Quits: naeui_ (~cpp@user/naeui) (Read error: Connection reset by peer)
[14:56:30] *** Joins: naeui (~cpp@user/naeui)
[14:57:19] *** Joins: bitkiller (~bitkiller@user/bitkiller)
[15:17:57] *** Quits: diogenese (~diogenese@diogenese.velotech.net) (Ping timeout: 240 seconds)
[15:20:07] *** Quits: naeui (~cpp@user/naeui) (Read error: Connection reset by peer)
[15:20:13] *** Joins: naeui_ (~cpp@user/naeui)
[15:34:26] *** Quits: naeui_ (~cpp@user/naeui) (Read error: Connection reset by peer)
[15:34:32] *** Joins: naeui (~cpp@user/naeui)
[15:43:58] *** Malvolio|haunted is now known as UUUV
[15:43:58] *** Quits: naeui (~cpp@user/naeui) (Read error: Connection reset by peer)
[15:44:18] *** Joins: naeui (~cpp@user/naeui)
[15:45:38] *** Quits: naeui (~cpp@user/naeui) (Read error: Connection reset by peer)
[15:45:44] *** Joins: naeui_ (~cpp@user/naeui)
[15:48:07] *** Quits: naeui_ (~cpp@user/naeui) (Read error: Connection reset by peer)
[15:49:54] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[15:59:24] *** Quits: sinaowolabi (~SinaOwola@160.152.109.220) (Ping timeout: 240 seconds)
[16:00:11] *** Joins: HuntsMan (~hunts@2a02-a44b-361b-1-5f8-874b-a485-d79e.fixed6.kpn.net)
[16:11:00] *** Quits: HuntsMan (~hunts@2a02-a44b-361b-1-5f8-874b-a485-d79e.fixed6.kpn.net) (Quit: Konversation terminated!)
[16:18:17] <dostoyevsky2> cloudcell: NNs can't really learn a function, merely approximate it... Why would you want a crappy version of a FFT that sometimes will be horribly wrong?  It doesn't make any sense.  NNs make sense when your only alternative is to just use a random guess...
[16:23:18] *** Joins: HuntsMan (~hunts@2a02-a44b-361b-1-5f8-874b-a485-d79e.fixed6.kpn.net)
[16:53:52] *** Joins: diogenese (~diogenese@diogenese.velotech.net)
[17:31:40] *** Joins: naeui (~cpp@user/naeui)
[17:36:55] *** Quits: naeui (~cpp@user/naeui) (Read error: Connection reset by peer)
[17:41:53] *** Joins: sinaowolabi (~SinaOwola@160.152.109.220)
[17:50:27] *** Joins: georgios (~georgios@user/georgios)
[17:50:59] *** Joins: sinaowolabi_ (~SinaOwola@160.152.109.220)
[18:01:30] *** Joins: marloi (~mark@108.175.227.116)
[18:11:30] *** Joins: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net)
[18:13:08] <blei> i'm looking for a gpu accelerated implementation of incremental learning for a linear svm
[18:19:08] *** Quits: yauhsien (~Yau-Hsien@61-231-19-150.dynamic-ip.hinet.net) (Quit: Leaving)
[18:26:22] *** Quits: blei (~blei@c-73-153-189-5.hsd1.co.comcast.net) (Quit: Client closed)
[18:34:24] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[18:35:32] *** Quits: sinaowolabi (~SinaOwola@160.152.109.220) (Ping timeout: 272 seconds)
[18:40:06] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[18:51:08] *** Joins: `Tim (~zenguin@user/zenguin)
[19:00:44] *** Joins: naeui (~cpp@user/naeui)
[19:55:21] *** Quits: sinaowolabi_ (~SinaOwola@160.152.109.220) (Read error: Connection reset by peer)
[20:13:33] *** Quits: naeui (~cpp@user/naeui) (Read error: Connection reset by peer)
[20:13:39] *** Joins: naeui_ (~cpp@user/naeui)
[20:19:50] *** Quits: naeui_ (~cpp@user/naeui) (Read error: Connection reset by peer)
[20:19:56] *** Joins: naeui (~cpp@user/naeui)
[20:23:05] *** Quits: naeui (~cpp@user/naeui) (Client Quit)
[20:37:37] *** Joins: toma (~RubbishPu@node-1w7jr9yhpl489qar15qh6gx6a.ipv6.telus.net)
[20:42:44] *** Joins: georgios (~georgios@user/georgios)
[20:42:56] *** Joins: SiegeLord (~sl@user/siegelord)
[21:12:05] *** Quits: toma (~RubbishPu@node-1w7jr9yhpl489qar15qh6gx6a.ipv6.telus.net) (Remote host closed the connection)
[21:18:39] *** Quits: diogenese (~diogenese@diogenese.velotech.net) (Quit: Has anybody seen the bridge?)
[21:19:01] *** Joins: diogenese (~diogenese@diogenese.velotech.net)
[21:42:21] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[21:45:05] *** Joins: glestrade (~glestrade@wsip-174-79-241-94.sd.sd.cox.net)
[21:45:41] <glestrade> Hi again all - it's been a while since I've been here
[21:46:08] <glestrade> So, I definitely have tensorflow-gpu installed on my machine
[21:46:54] <glestrade> But the load on the gpu is pretty small most of the time, and it's sort of crawling along... it's as if tensorflow is deciding to be cpu bound
[21:47:06] <glestrade> what can I do to fix this/tell what's going on
[21:47:26] <glestrade> using nvidia-smi to tell the gpu load
[21:54:30] <dostoyevsky2> glestrade: increase the batch size?
[21:54:56] <glestrade> hmm, maybe
[21:55:52] <dostoyevsky2> glestrade: how much % of the ram is your kernel using in nvidia-smi
[21:55:58] <dostoyevsky2> +?
[21:59:00] *** Quits: rvalue (~rvalue@user/rvalue) (Quit: ZNC - https://znc.in)
[21:59:46] <glestrade> well, it has 8gb of its own ram, is that correct?
[22:00:34] *** Joins: rvalue (~rvalue@user/rvalue)
[22:00:55] <glestrade> it reports that python 3 is using ~7.5 GB
[22:01:06] <glestrade> I think I was reading the volatility...
[22:01:42] <glestrade> But the reason I asked is because it's acting slower than a different machine that only has a CPU
[22:02:06] <glestrade> But with this different machine, I only have access to it periodically
[22:02:49] <cloudcell> thanks dostoyevsky2
[22:03:26] <glestrade> https://gist.github.com/erick016/171fc6df05b8dbc27ba4f81a8cb7e4da
[22:03:26] <SigmoidFroid> ⇒  nvidia-smi-02-23.txt · GitHub
[22:16:45] <dostoyevsky2> glestrade: I can only imagine that the kernel on the gpu is rather quick and so it's mostly waiting for new data to arrive... so maybe the model doesn't perform well on the gpu because of that
[22:17:55] <glestrade> So, probably bottlenecking? Maybe I can stop sending it to the GPU :/
[22:18:13] <glestrade> Or buy a really expensive GPU lol
[22:19:03] <glestrade> Now that I look, not much of the non-GPU ram is being used...
[22:19:08] <dostoyevsky2> glestrade: I don't think a more expensive gpu will help, has cpu<->gpu always has some latency... if you don't do complicated computations, then the cpu is faster
[22:19:23] <Klinda> hello if you have to reconstruct a black hole, which dataset are you gonna use... the one related to black holes (maybe imagined and painted), the one that represent astronomy things, or things related to everyday life?
[22:20:24] <glestrade> well the typical out of context wisdom about GPUs is that they're good at doing non-complicated operations in parallel
[22:20:53] <glestrade> is that an accurate assessment?
[22:20:59] <Klinda> why the first black holes is a biased dataset? biased or high bias doesn't mean like that the model doess't express well as it potentially do ?
[22:21:20] <glestrade> So neural networks training is just adds and multiplies... are GPUs good at that in parallel?
[22:23:28] <dostoyevsky2> glestrade: a dot product between too large matrices are typical for NNs, this gets quite exahustive for larger matrices... so a gpu would benefit from that... if you merely multiply a matrix with a vector, the cpu could do it probably faster, as it saves having to transfer all the data to the gpu
[22:25:31] <glestrade> Yes, and I think I have large matrices here
[22:25:43] <glestrade> So maybe there's a gpu that just has a larger ram bank
[22:25:52] <glestrade> and isn't much more expensive?
[22:26:14] <glestrade> Or maybe I can find a generous gamer somewhere who will let me test on their machine xD
[22:37:04] *** immibis_ is now known as immibis
[22:38:13] *** Joins: toma (~RubbishPu@node-1w7jr9yhpl4889z4y5ner5h4v.ipv6.telus.net)
[22:42:06] *** Quits: bitkiller (~bitkiller@user/bitkiller) (Quit: bitkiller)
[22:50:31] *** Quits: glestrade (~glestrade@wsip-174-79-241-94.sd.sd.cox.net) (Quit: Client closed)
[22:55:33] *** Quits: HuntsMan (~hunts@2a02-a44b-361b-1-5f8-874b-a485-d79e.fixed6.kpn.net) (Quit: Konversation terminated!)
[22:55:50] *** Joins: HuntsMan (~hunts@2a02-a44b-361b-1-5f8-874b-a485-d79e.fixed6.kpn.net)
[22:56:44] *** Joins: glestrade (~glestrade@wsip-174-79-241-94.sd.sd.cox.net)
[23:51:17] *** Quits: trace987 (~trace@user/trace) (Remote host closed the connection)
[23:56:40] *** Joins: georgios (~georgios@user/georgios)
