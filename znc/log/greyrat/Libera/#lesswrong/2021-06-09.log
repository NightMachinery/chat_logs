[00:00:37] <two2thehead> kuudes, s0ph1a feep : Lord Porky is apparently on a diet : https://twitter.com/nktpnd/status/1402335913691127809
[00:00:38] <|dbotdan> Ankit Panda (@nktpnd, 2021-06-08 18:45): ‘It matters that Kim Jong Un is looking noticeably thinner (nice work by @ColinZwirko) https://www.nknews.org/2021/06/kim-jong-un-looks-thinner-and-intelligence-agencies-are-likely-paying-attention/’ Images: https://nitter.42l.fr/pic/media%2FE3YY9nzVIAI0gMr.jpg%3Fname%3Dorig
[00:02:29] <Betawolf> or he got a new watch
[00:03:52] <saturn2> isn't this fatphobic?
[00:04:02] <dbohdan> gwern: Doom on the Sansa Clip Zip mentioned in article is less than on the older Sansa Clip+ with a ~monochrome OLED screen.  https://www.youtube.com/watch?v=FbmnePiXycw
[00:04:03] <Robomot> Doom on Sansa clip+ - YouTube
[00:04:43] <dbohdan> gwern: *the article  *less impressive
[00:04:55] * dbohdan should sleep
[00:06:29] <dbohdan> https://old.reddit.com/r/itrunsdoom/top/
[00:06:30] <Robomot> top scoring links : itrunsdoom (This subreddit focuses on odd hardware that runs Doom. Calculators, ATMs, fridges, old video game systems if it has a computer in it, it can...)
[00:07:48] <gwern> https://en.wikipedia.org/wiki/GPS_week_number_rollover surprisingly short
[00:07:49] <Robomot> GPS week number rollover - Wikipedia (The Global Positioning System (GPS) week number rollover is a phenomenon that happens every 1024 weeks, which is about 19.6 years. The Global Positioning system broadcasts a date, including a weekly counter that is stored in only ten binary digits. The range is therefore 0–1023. After 1023 an integer overflow causes the internal value to roll over, changing to zero again. …)
[00:10:02] *** Joins: oxide (~lambda@user/oxide)
[00:11:47] <gwern> https://codegolf.stackexchange.com/questions/42510/reverse-indentation
[00:11:47] <Robomot> code golf - Reverse indentation - Code Golf Stack Exchange (I've heard that your code can run faster if you indent it in reverse, so that the compiler can process it like a tree design pattern from the very top of the "branches" down. This helps because gra...)
[00:12:47] <dbohdan> https://www.arcade-museum.com/game_detail.php?game_id=9188  Trackball!
[00:12:49] <Robomot> Quake - Arcade Tournament Edition - Videogame by ID Software/LBE Systems/Lazer-Tron (The Quake - Arcade Tournament Edition coin-operated Videogame by ID Software/LBE Systems/Lazer-Tron (circa 1998), and it's history and background, photos, repair help, manuals, for sale and wanted lists, and census survey is brought to you by The International Arcade Museum at the Museum of the Game.)
[00:16:29] <saturn2> this one's amusing https://gfycat.com/unhealthyphonyamphiuma
[00:16:30] <Robomot> RealisticDoom GIF by mustardbucket1234 | Gfycat (Watch and share RealisticDoom GIFs by mustardbucket1234 on Gfycat)
[00:16:31] <gwern> https://suricrasia.online/no-knowledge.html seems like a convoluted way of generating a random number
[00:16:33] <Robomot> --this is a no-knowledge proof--
[00:17:04] *** Joins: kanzure (~kanzure@user/kanzure)
[00:18:05] <gwern> https://vaibhavsagar.com/blog/2019/09/08/popcount/ speaking of popcount
[00:18:05] <Robomot> You Won’t Believe This One Weird CPU Instruction! - Vaibhav Sagar
[00:20:57] <gwern> 'We used the fact that altering a small number of bits can give you any checksum value to our advantage. By inserting the correct value into the middle of the data, the checksum could be made to equal any predetermined value. This meant the checksum value could be hard-coded and therefore become part of the data being checksummed. This is bewildering to even think about, let alone try to...
[00:21:03] <gwern> ...crack.' https://www.gamasutra.com/view/feature/131439/keeping_the_pirates_at_bay.php ouch
[00:21:04] <Robomot> Gamasutra - Keeping the Pirates at Bay (Beyond giving developers a feeling of violation over having their hard work pillaged and disturbed illegally, piracy exacts a steep toll on the entire)
[00:22:15] <gwern> https://twitter.com/shafikyaghmour/status/991913614977060866
[00:22:15] <|dbotdan> Shafik Yaghmour (@shafikyaghmour, 2018-05-03 05:33): ‘Pre ANSI C was wild! | 8 and 9 were valid octal digits, they had the octal values 10 and 11  | Found this in the C99 rationale: http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf | old C reference manual: https://www.bell-labs.com/usr/dmr/www/cman.pdf | Wish we had a live version to try | #cprogramming #programming
[00:22:15] <|dbotdan> #PreANSICWasWild’ Images: https://nitter.pussthecat.org/pic/media%2FDcP7s_1V4AAHpIO.jpg%3Fname%3Dorig https://nitter.pussthecat.org/pic/media%2FDcP7s_7U8AAo79E.jpg%3Fname%3Dorig
[00:23:49] <two2thehead> kuudes, s0ph1a feep : https://www.youtube.com/watch?v=3eDnKct5XAw
[00:23:49] <Robomot> Money-Laundering Expert Rates 8 Money-Laundering Scams In Movies and TV | How Real Is It? - YouTube (Retired FBI investigator Jerri Williams looks at eight money-laundering scams from popular TV shows and movies and rates them based on realism. She looks at ...)
[00:24:03] <two2thehead> since we haven't talked about money laundering for a few months
[00:25:50] *** Quits: thoros (~thoros@193-154-185-59.adsl.highway.telekom.at) (Quit: WeeChat 3.0.1)
[00:27:44] <gwern> hm. kimg jong pork does look thinner. stress? shortages?
[00:28:14] <adiabatic> horizontal cardio?
[00:28:25] <two2thehead> he is showing Juche! by not eating five meals a day :V
[00:28:38] <two2thehead> also what adiabatic said re his pleasure squad
[00:30:45] <gwern> http://www.amirrorclear.net/academic/ideas/simulation/index.html
[00:30:46] <Robomot> How to simulate everything (all at once)
[00:37:35] *** Quits: LeoTal (~LeoTal@182.170.86.79.rev.sfr.net) (Quit: Leaving.)
[00:43:02] <SDr> ..
[00:53:53] *** Quits: Mark_ (~mkosmatri@2001:470:69fc:105::625) (Changing host)
[00:53:53] *** Joins: Mark_ (~mkosmatri@user/mark/x-9597255)
[01:11:54] *** Joins: drethelin (~drethelin@096-042-035-082.res.spectrum.com)
[01:12:42] * gwern uploads https://www.gwern.net/docs/cs/2012-jarvisalo.pdf
[01:12:46] <Robomot> PDF (6 pages; 125 KB) - The International SAT Solver Competitions (Matti Järvisalo) - AI Magazine Volume 33 Number 1 (Spring 2012 …
[01:19:06] <SDr> how come startup pitches to potential cofounders, or employees are never litigated as securities fraud?
[01:20:16] <SDr> they're massively misrepresenting the opportunity, skip the risks section entirely, and have every hallmark of boiler room operator coldcalls; except instead of paying with cash for it, one pays in the form of opportunity cost / lower salary. but substantially, it's the same fucking thing
[01:20:30] <gwern> https://www.medrxiv.org/content/10.1101/2021.04.12.21255236v2 *Taiwan* Biobank?
[01:20:31] <Robomot> Analysis across Taiwan Biobank, Biobank Japan and UK Biobank identifies hundreds of novel loci for 36 quantitative traits | medRxiv (Genome-wide association studies (GWAS) have identified tens of thousands of genetic loci associated with human complex traits and diseases. However, the majority of GWAS were conducted in individuals of European ancestry. …)
[01:23:53] <kuudes> what's odd about taiwan having a biobank?
[01:25:31] <gwern> I hadn't heard about them starting one, or I've forgotten. so n=100k out of nowhere is a little surprising
[01:50:54] <gwern> https://www.reddit.com/r/rootsofprogress/comments/mn2533/highlights_from_the_autobiography_of_andrew/
[01:50:56] <Robomot> Highlights from The Autobiography of Andrew Carnegie : rootsofprogress (I’ve been reading , published late in his life, in the early 1900s. Here are some interesting themes and quotes. (Emphasis added in all block quotes …)
[02:04:42] <gwern> https://twitter.com/theshawwn/status/1402377126121181186 kinky
[02:04:43] <|dbotdan> Shawn Presser (@theshawwn, 2021-06-08 21:28): ‘My 3yo niece was bouncing around on the couch, then she was like come here! I have a secret! Can I tell you? | I come over. She pulls my ear close, puts her hands around it so no one else can hear, and whispers really quietly: | “I want to play Minecraft with you”’
[02:07:36] <gwern> https://www.nytimes.com/2021/06/08/technology/farewell-millennial-lifestyle-subsidy.html incredible. no matter what it is about tech, the media can always find a negative angle
[02:07:37] <Robomot> Farewell, Millennial Lifestyle Subsidy - The New York Times (The price for Ubers, scooters and Airbnb rentals is going up as tech companies aim for profitability.)
[02:11:14] *** Joins: tex (~super@user/dix)
[02:13:27] <Betawolf> "You'll Live Forever: Here's Why That's A Bad Thing"
[02:16:47] <Obormot\Arcturus> Good news: immortality
[02:17:00] <Obormot\Arcturus> Bad news: have to endure dumbshit takes from the N Y fucking Times forever
[02:17:53] <gwern> Betawolf: "Because R>G, The Wealthy Will Keep Getting Wealthier Without The Hand of the Grim Reaper to Stop Them; And Who Will Pay For Social Security?"
[02:17:58] *** dTal is now known as Guest37
[02:18:09] *** Guest37 is now known as dTal
[02:18:22] <gwern> "Immortality Predicted To Exaceberate Inequality, Experts Warn"
[02:25:50] <Obormot\Arcturus> gwern: cf. Altered Carbon, that one episode in S2 of LD&R, etc., etc., etc.
[02:26:08] <Obormot\Arcturus> The "in the future we will all be even more stratified" is just an irresistible dystopian form
[02:28:21] *** Quits: oxide (~lambda@user/oxide) (Ping timeout: 258 seconds)
[02:29:18] *** Joins: oxide (~lambda@user/oxide)
[02:30:30] <saturn2> every rational person knew uber would have to jack up prices sooner or later, and now it's happening, and the nyt wrote an article saying it's happening... what's the nyt doing wrong?
[02:33:26] <kuudes> is uber raising prices?
[02:34:43] <saturn2> yes
[02:35:26] <two2thehead> Apparently hemorrhaging cash wasn't a valid business model. Who knew?
[02:37:53] <kuudes> I guess a pandemic is a good time to alter prices heavily, because you can blame everything on the disease
[02:38:33] <two2thehead> https://archive.is/FqRCi
[02:38:35] <Robomot> Farewell, Millennial Lifestyle Subsidy - The New York Times
[02:40:48] <two2thehead> "Uber lost only $108 million in the first quarter of 2021 — a change partly attributable to the sale of its autonomous driving unit,"
[02:41:01] <two2thehead> Uber got out of the driverless car business? TIL
[02:41:53] <saturn2> uber was never really in the driverless car business, it was mostly a scam
[02:42:43] <two2thehead> my prior is that most ai car people thought uber was cutting corners, so you and I are in agreement I think
[02:46:40] *** Quits: two2thehead (~AVX0@209.212.217.186) (Quit: Leaving)
[03:07:51] *** Joins: shawwwn (uid6132@id-6132.brockwell.irccloud.com)
[03:07:54] <shawwwn> mood today: https://usercontent.irccloud-cdn.com/file/rdJZ9j9N/image.png
[03:08:45] <shawwwn> linear: odd, irc won't let me message you. anyway, it's just my own thing
[03:09:01] <shawwwn> I doubt anyone else would be interested in training the most offensive GPT-2 bot possible
[03:10:59] <shawwwn> RiversHaveWings: "out-46" looks like a failed run, to be honest
[03:11:28] <RiversHaveWings> how come?
[03:11:49] <shawwwn> well, was it? That was some time ago
[03:11:53] <RiversHaveWings> I eventually got this https://usercontent.irccloud-cdn.com/file/NDMHTLlB/out_0195452.jpg
[03:11:54] <Robomot> image/jpeg (320x320; 34 KB)
[03:11:54] <shawwwn> or was it another ctrlc
[03:12:05] <shawwwn> ah, I was wrong then.
[03:12:30] <shawwwn> nice work.
[03:13:10] <shawwwn> guess it's lack of experience staring at fixed latents
[03:13:22] <RiversHaveWings> clipgan experimentation continues
[03:13:29] <shawwwn> empirically, the texture patterns seem to be frozen
[03:13:36] <shawwwn> even as training evolves it
[03:13:47] <shawwwn> rather unexpected. I've only seen random latents, so I never saw a correlation
[03:13:49] <RiversHaveWings> yes, that's what this design does
[03:13:56] <shawwwn> you tweet one of your training videos yet?
[03:14:12] <RiversHaveWings> it derives stylegan-type noise inputs to each feature map deterministically and smoothly from the latent
[03:14:29] <RiversHaveWings> but the noise is really different for different sampled latents
[03:15:45] <shawwwn> that's not stylegan-style then
[03:16:01] <RiversHaveWings> yes, i was unhappy with how they did it
[03:16:02] <shawwwn> stylegan-style is to randomize the noise, always
[03:16:24] <shawwwn> making it deterministic from the latents defeats the purpose, which is to give the model a source of noise so that it doesn't have to figure out how to generate it
[03:16:35] <shawwwn> it can already do that
[03:16:35] <RiversHaveWings> it just makes it from the latent
[03:16:53] <RiversHaveWings> no, GANs are bad at it
[03:17:03] <RiversHaveWings> and it uses a lot of network capacity
[03:17:09] <shawwwn> ok boomer
[03:17:18] <RiversHaveWings> this is why stylegan started adding the noise in the first place
[03:17:42] <RiversHaveWings> i just have a dedicated noise function that is deterministic based on the latent
[03:18:06] <shawwwn> "I have a thing that works completely differently and subverts the original design, but I shall call it the same style"
[03:18:15] <shawwwn> in fairness, if it's cryptographically RNG, it shouldn't matter
[03:18:22] <shawwwn> i.e. if one bit of latent is different -> entire noise pattern is different
[03:18:30] <shawwwn> at that point I agree
[03:18:37] <RiversHaveWings> it isn't, the noise changes smoothly as the latent changes
[03:18:39] <shawwwn> but if the noise is similar for similar latents, forget about it. totally wrong
[03:18:42] <shawwwn> yeah.
[03:18:57] <shawwwn> but, I like when people try new things. maybe you'll turn out to be right. hard to say
[03:19:10] <RiversHaveWings> my latents are 128 dim so i have 128 independent noise constants per feature map
[03:19:12] <shawwwn> model definitely isn't learning noise patterns though. it just treats it as an additional texture input pattern.
[03:19:33] <RiversHaveWings> they are not learned, i init them at model creation and load them back in from the checkpoint
[03:19:34] <shawwwn> shoudn't matter. the derivative of the noise is still continuous wrt the latents
[03:20:00] <shawwwn> if they're not learned, but they vary per latent, do you set them before every sample?
[03:20:08] <RiversHaveWings> no
[03:20:22] <RiversHaveWings> during training i just sample random latents
[03:20:26] <RiversHaveWings> and so i get different noise
[03:20:48] <shawwwn> you get my point that if the noise is related to the latent, it's not noise, right?
[03:21:13] <shawwwn> for the same reason that if a tv channel is showing a picture, and the noise is related to that picture (and doesn't vary at all), then it's not noise
[03:21:37] <RiversHaveWings> it's supposed to be good enough to trick the discriminator and not look visually weird
[03:21:53] <shawwwn> it's not a pedantic distinction. it's the essence of the benefit
[03:22:24] <shawwwn> I urge you to try completely random RNG rather than latent-based RNG, if nothing else. I was wrong about the failed run, and could be wrong about this. ¯\_(ツ)_/¯
[03:22:38] <RiversHaveWings> "the derivative of the noise is still continuous wrt the latents" yes but i don't optimize the latents
[03:23:24] <shawwwn> I wasn't saying that for that reason. I was saying that as a precise way of communicating the problem I'm talking about
[03:23:36] <shawwwn> it's easy to be imprecise. the precise problem is that a slightly different latent gives a slightly different noise pattern
[03:23:43] <shawwwn> it needs to be completely different, in the cryptographic RNG sense
[03:24:02] <shawwwn> (1 bit different input = completely random bit-different output)
[03:24:13] <RiversHaveWings> i could do that but i wanted smooth interpolation videos
[03:24:24] <shawwwn> that's fine. you still can, just like stylegan
[03:24:36] <RiversHaveWings> by picking some noise and holding it fixed
[03:24:40] <shawwwn> just hold the noise constant at sampling time
[03:24:53] <shawwwn> are you saying that you already randomize it during training time? completely random?
[03:25:15] <RiversHaveWings> like this has been a problem for my stylegan clip methods, they can't capture the actual diversity of stylegan output because i only optimize the latent and hold the noise constant
[03:27:29] <shawwwn> I was going to make a lawyer meme with a speech bubble containing my previous question
[03:27:34] <shawwwn> but in seriousness, you seem pretty evasive on that point
[03:27:37] <shawwwn> it's a pretty simple yes or no
[03:27:43] <RiversHaveWings> i don't randomize it during training time
[03:27:46] <shawwwn> ok.
[03:27:47] <RiversHaveWings> i randomize the latents
[03:27:51] <shawwwn> doesn't matter
[03:28:00] <shawwwn> definitely does not at all do the same thing
[03:28:05] <shawwwn> model learns from the latents
[03:28:29] <shawwwn> it won't be able to sense that the noise is derived from the latents. it'll just treat it as a different topology space entirely
[03:28:41] <shawwwn> this weird bubbly nebulous topology space (fractal noise based on latent)
[03:29:19] <shawwwn> I'm not saying that the random noise technique is superior. I'm only saying, I wanted to try it with biggan. so don't fool yourself that you tried it here. You miiight be right, but it's equally possible you might not
[03:29:45] <RiversHaveWings> biggan could seriously benefit from either imo
[03:30:04] <gwern> shawwwn: I'm reminded of that rumor about likcing the slice of einstein's brain as a university hazing rite
[03:30:15] <shawwwn> gwern: what the fuck
[03:30:24] <shawwwn> props for pointing out an actual wtf
[03:30:27] <shawwwn> where was that?
[03:30:44] <gwern> 'i herd you liek brains and einstein so now u have to like einstein's brain'
[03:30:50] <shawwwn> RiversHaveWings: it's unclear there's any benefit at all from your current design, is my point
[03:30:52] <gwern> it was on twitter
[03:31:25] <shawwwn> my hypothesis is that a constant derived from a latent has no effect, for the same reason that the derivative of a constant is 0.
[03:31:39] <shawwwn> again, I'm using math to communicate, not describing your technique with math.
[03:32:11] <shawwwn> the topology is being perturbed by some delta, entirely determined by the latent
[03:32:15] <shawwwn> (fractal noise)
[03:32:32] <shawwwn> that perturbation is smooth as the latent changes. a small change in the latent = a small change in the wiggle
[03:32:53] <shawwwn> that's a key difference from completely-random noise. it's not even in the same category of thing, really.
[03:33:12] <shawwwn> it's an N-dimensional texture being sampled by the latent
[03:34:03] <gwern> https://twitter.com/BrendanBordelon/status/1402393709522702336
[03:34:08] <|dbotdan> Brendan Bordelon (@BrendanBordelon, 2021-06-08 22:34): ‘🚨The U.S. Innovation and Competition Act, formerly know as Endless Frontier, a bill authorizing tens of billions of dollars for the U.S. R&D ecosystem and *appropriating* $52 billion for semiconductor subsidies (and a bunch of other stuff too), passes the Senate 68-32.’ Videos:
[03:34:08] <|dbotdan> https://nitter.moomoo.me/pic/video.twimg.com%2Ftweet_video%2FE3ZNofnWEAESzM-.mp4
[03:35:04] <RiversHaveWings> shawwwn: no effect how? the outputs are visually very different, especially in the early stages of training, than not using noise at all
[03:35:20] <RiversHaveWings> but nonetheless, i'm gonna go take that same design and make the noise random
[03:35:23] <RiversHaveWings> and run it again overnight
[03:35:45] <RiversHaveWings> (random except during making the demo grids)
[03:35:55] <RiversHaveWings> actually i will make it random and just fork the rng state for the demo grids
[03:35:59] <RiversHaveWings> to make impl easy
[03:36:36] <shawwwn> I mean... I'm kind of in a pickle, because I don't want to dismay you, but I also want to give honest feedback. I see a potential problem. My options are, either say nothing (or stop trying to convince you it might be a problem, which is kind of equivalent), or to keep yammering about it and insisting it's not the same
[03:36:45] <shawwwn> I try not to do the latter if I think it's a pedantic point
[03:36:51] <RiversHaveWings> shawwwn: well i am gonna actually try it, i should have results by tomorrow morning
[03:36:55] <shawwwn> i.e. I only try to be annoying when i'm pretty sure it's a big deal.
[03:37:12] *** Quits: tex (~super@user/dix) (Ping timeout: 252 seconds)
[03:37:19] <shawwwn> but like, yes, that's the situation. you could be completely right. and if you're right, that's a very interesting thing, at least to me
[03:37:33] <gwern> https://thereader.mitpress.mit.edu/blaming-our-genes-the-heritability-of-behavior/
[03:37:34] <Robomot> Blaming Our Genes: The Heritability of Behavior | The MIT Press Reader (When it comes to behavior, genetics may play a larger role than you think.)
[03:38:04] <shawwwn> but, to answer your question directly: of course the outputs are visually very different, especially in the early stages of training. They'd be different if you were sampling from an N-dimensional texture, too.
[03:38:19] <RiversHaveWings> it's not quite sampling
[03:38:22] <shawwwn> doesn't change the fact that the N-dimensional texture is still static. the model will learn from the patterns in it
[03:38:56] <shawwwn> it's basically perlin noise, which is sampling. if the noise is smooth relative to the input, then it's sampling, because you can zoom in by changing the latents more slowly
[03:39:26] <shawwwn> or slow down time, at least. just like perlin.
[03:39:31] <RiversHaveWings> time?
[03:39:59] <shawwwn> yeah, with perlin noise there's usually a time input. you vary it so that you don't have to vary the x and y input (pixel location) just to get a different pattern
[03:40:11] <shawwwn> but it's just an additional dimension
[03:40:41] <gwern> shawwwn: could the offset matter? in the same way that biggan could learn to undo the gamma bug and ramp it up to where it should've been, but it took a lot of iterations and screwed up the training dynamics from the start?
[03:40:45] <shawwwn> so it's the same thing. and perlin noise is certainly a (parametric) texture, even if it's noise.
[03:40:53] <RiversHaveWings> gwern: oh no, what gamma bug
[03:41:03] <RiversHaveWings> what's gamma again
[03:41:08] <shawwwn> that was the initialization bug
[03:41:11] <RiversHaveWings> oh
[03:41:18] <shawwwn> (it was the gamma variable)
[03:41:21] <RiversHaveWings> right
[03:41:33] <RiversHaveWings> i was afraid for a moment it was the init scale of the nonlocal block
[03:42:07] <gwern> so yeah, maybe an offset is in a sense irrelevant because it is a constant which doesn't change the gradient and can be exactly offset by biases or parameters scaling back down, but in the long run we're all dead etc
[03:42:16] <shawwwn> gwern: the offset could help. you're right about that -- RHW's technique might give additional stability / generalization. But it's equally possible that it might not. After all, the loss landscape should be more or less identical, just perturbed by a certain offset 
[03:42:48] <shawwwn> and if the loss landscape is identical, in the sense that there are still the same number of local minima (and similar shapes), then it doesn't really help
[03:43:02] <shawwwn> or if it helps, it only helps a little. not the same thing as having totally random noise.
[03:43:15] <gwern> in RL, reward and gradient scaling are also very important. it's conventional wisdom to try to scale your rewards down to ~N(0,1) or thereabouts. it shouldn't matter in theory - in theory, it doesn't matter if your rewards have mean 0 or mean million, but for whatever reason, unit-scale rewards are much much stabler
[03:43:33] <shawwwn> are they? interesting.
[03:44:14] <shawwwn> RiversHaveWings: the good news is, once pytorch is up on a TPU, you'll be able to run a bunch of training attempts in parallel. I see you're reluctant to try the randomness because it means having to restart training. I agree that's painful :(
[03:44:29] <gwern> (yeah. in theory, your reward function is invariant to any kind of shift up or down. if you have reward mean million, then it tries to get million+1. if you have reward 0, it tries to get 0+1. and so on. in practice, you'd damn well better scale your rewards and clip your gradients if you ever want to finish your thesis)
[03:44:33] <shawwwn> hopefully someone'll have libtpu bindings within, uh, two decades.
[03:48:32] <shawwwn> gwern: not sure what to tell this fellow https://usercontent.irccloud-cdn.com/file/QpXfScYn/image.png
[03:48:46] <shawwwn> he's certainly seen https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/
[03:48:59] <feepbot> A Very Unlikely Chess Game | Slate Star Codex (Almost 25 years after Kasparov vs. Deep Blue, another seminal man vs. machine matchup: Neither competitor has much to be proud of here. White has a poor opening. Black screws up and loses his queen…)
[03:49:01] <shawwwn> and he seems to be an academic, which means he's considered and rejected the idea of using that link as a citation, which was your original suggestion
[03:49:06] <gwern> shawwwn: why not ask if SSC works?
[03:49:20] <shawwwn> I mean, he wouldn't be asking now if it did work
[03:50:26] <RiversHaveWings> shawwwn: these outputs look really different
[03:50:29] <RiversHaveWings> already
[03:50:31] <spxtr> SSC doesn't have a DOI
[03:50:45] <gwern> most stuff doesn't have a DOI
[03:50:51] <gwern> all of arxiv is sans doi
[03:51:01] <shawwwn> most stuff isn't dead on arrival, either. oh wait, that's DOA
[03:51:07] <shawwwn> RiversHaveWings: with the random noise?
[03:51:13] <RiversHaveWings> yeah, i think
[03:51:17] <RiversHaveWings> gonna run it a bit more
[03:51:17] <shawwwn> yis.
[03:51:17] <gwern> 'a DOI' is the sound homer makes when he realizes he made a mistake
[03:52:43] <gwern> shawwwn: why not link him https://www.gwern.net/GPT-2-preference-learning#decision-transformers-preference-learning-as-simple-as-possible https://www.gwern.net/CYOA while you're at it? he'd probably find them interesting
[03:52:47] <Robomot> Choose-Your-Own-Adventure AI Dungeon Games · Gwern.net (Neural networks like GPT-2 power text adventure games where you can do anything; but they are too expensive. I propose that if we turn them into Choose Your Own Adventure hypertext games, they become feasible and enable entirely new gameplay.)
[03:52:47] <Robomot> GPT-2 Preference Learning for Music Generation · Gwern.net (Experiments with OpenAI's 'preference learning' approach, which trains a NN to predict global quality of datapoints, and then uses reinforcement learning to optimize that directly, rather than proxies. I am unable to improve quality, perhaps due to too-few ratings.)
[03:54:16] <shawwwn> 'cause it's not GPT chess. guess you're right, and I should write something up.
[03:54:27] <shawwwn> will I? well...
[03:54:40] <gwern> 'That’s not to say they are all photogenic winners. Take the city of Chongqing’s People’s Liberation Monument, for example. Made of brick and mortar, it’s a respectable piece of architecture. As a tubular, bulbous, peachy-red ice pop, however, it’s uncomfortably phallic. Cue the social media free-for-all. But at least some effort was obviously put into it. Woe betide the park or...
[03:54:42] <shawwwn> if I could predict that, I'd have the superpower of "being predictable", which society seems to like
[03:54:46] <gwern> ...museum that phones it in with their cultural and creative frozen treats. Li, after our interview, emailed me an article titled “I Ate a Stomachful of ‘Cultural and Creative’ Ice Creams, and Here is the Ugliest One.” It featured a truly hideous ice-cream bar sold by a park in Beijing. The anthropomorphic, misshapen, buck-toothed maple leaf was meant to honor the park’s famous...
[03:54:52] <gwern> ...foliage.' https://www.atlasobscura.com/articles/chinese-ice-cream
[03:54:52] <Robomot> Every Landmark in China Now Makes Gorgeous Themed Ice Cream - Gastro Obscura (Temples, towers, and famous figures, all as frozen treats.)
[03:55:08] <shawwwn> but yes, I'll forward those links to him. he might add those as related work
[03:59:48] *** Quits: srhm (~ascii@user/srhm) (Quit: Konversation terminated!)
[04:04:57] *** Quits: Gurkenglas_ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de) (Ping timeout: 258 seconds)
[04:05:44] <spxtr> any reason in particular you don't want to write up something for arxiv?
[04:06:33] <gwern> neither of us has an arxiv account or latex skills, there's not that much to say, and it's unnecessary
[04:06:55] <gwern> a gwern.net or shawwn.net page would be perfectly adequate for a writeup if shawwwn wanted to do one
[04:07:00] <Betawolf> it perpetuates the idea that science happens only if you dress it correctly
[04:07:17] <spxtr> those are all good reasons
[04:07:40] <shawwwn> gwern: your subtle negs have no effect on me, villain
[04:07:49] <shawwwn> my latex skills are perfectly superior to yours
[04:07:57] <shawwwn> which is to say, shitty, but not non-existent 
[04:07:59] <spxtr> assuming you have some way of preventing link rot, as that person said. does using the web archive count?
[04:08:01] <shawwwn> (https://www.docdroid.net/faDq8Bu/swarm-training-v01a.pdf)
[04:08:05] *** Joins: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com)
[04:08:12] <feepbot> Swarm Training v0.1a.pdf | DocDroid (Swarm Training Shawn Presser January 2020 Abstract We present a method for training large machine learning models using dozens of independent TPUs. We are able to fine-tune GPT-2 1.5B using 80 preemptible TPUv3-8’s on a 10GB dataset in less than a week, a spee [snip])
[04:08:22] <shawwwn> sadly, I no longer have the tex source to that :) It's on my old laptop, which I haven't turned on in months now.
[04:08:28] <Betawolf> gwern.net is probably less linkrotted than most online journals
[04:08:32] <RiversHaveWings> shawwwn: also changed clipgan to use actual stylegan type noise
[04:08:41] <gwern> Betawolf: oh yes. but then, that's a low bar
[04:08:47] <RiversHaveWings> shawwwn: ty :)
[04:08:48] <spxtr> Really?
[04:09:05] <spxtr> I guess I read different journals :P
[04:09:29] <gwern> yeah, journals, supreme court decisions, that sort of thing, have as bad linkrot as anyone else. heck, they themselves linkrot badly
[04:09:42] <gwern> not too rare for obscurer journals to just disappear from the internet
[04:10:09] <shawwwn> RiversHaveWings: I mean, I feel bad when pointing out design flaws, because of the huge training time investment. An overnight run means you only get 365 attempts at maximally-productive output, and no one is maximally-productive. And again, to underscore, it's entirely possible I was wrong and that this will turn out to be the same thing you already had
[04:10:10] <RiversHaveWings> shawwwn: i'll figure out how i want to handle the noise at sampling time later, right now i'm just forking and reseeding the rng before sampling demo grids
[04:10:14] <Betawolf> (I think I've mentioned before the case when I found a link in one paper about linkrot to another paper about linkrot, and that link was broken.)
[04:10:32] <shawwwn> but it's better than saying nothing, probably
[04:10:38] <gwern> the european journal of parapsychology, first use of registered reports - all gone. there was some anthropology journal I was looking up about hippies 'levitating the pentagon', iirc - all gone, had to find the PDFs in IA
[04:11:13] <RiversHaveWings> mb i will let the user pass in a generator for the noise
[04:11:28] <RiversHaveWings> or like... idk, interpolate between two sets of noise buffers
[04:15:33] <spxtr> I have some strong feelings about why journal papers and the like are not a good format for scientific research.
[04:16:22] <spxtr> Wondering how to go about actually improving the situation though.
[04:23:27] <ggreer> https://mobile.twitter.com/SamoBurja/status/1402370141518585861 the more I read his tweets the more I think he doesn’t have a deep understanding of many things he claims to have a deep understanding of
[04:23:28] <|dbotdan> Samo Burja (@SamoBurja, 2021-06-08 21:01): ‘And lost knowledge is not just ancient. In 2013, Jeff Bezos recovered two Apollo 11 rockets from the bottom of the ocean. Do you believe that he did so simply to donate them to the Smithsonian—without having the team at Blue Origin reverse-engineer them first? | 6/n’
[04:35:01] *** Quits: Urchin[emacs] (~user@user/urchin) (Read error: No route to host)
[04:35:23] <spxtr> Betawolf: "It perpetuates the idea that science happens only if you dress it correctly." can I quote you on that?
[04:40:03] <Betawolf> sure
[04:41:12] *** Quits: Dyo (~dyo@user/dyo) (Quit: Dyo)
[04:53:24] * shawwwn releases https://twitter.com/theshawwn/status/1402420962360954883
[04:53:25] <|dbotdan> Shawn Presser (@theshawwn, 2021-06-09 00:23): ‘Hacker News recently changed comments. Large threads hide subtrees of comments, like Reddit. If too many comments, then the "reply" link is replaced with "more". | But it says "more". It doesn't show how many replies there are. | Here's how to see the count: https://gist.github.com/shawwn/3c1b71d69482afde15efb163925f2c7a’ Images:
[04:53:25] <|dbotdan> https://nitter.fdn.fr/pic/media%2FE3Zl9NZXIAIkFVu.jpg%3Fname%3Dorig
[04:55:53] <Robomot> [Astral Codex Ten] I Will Not Eat The Bugs - https://astralcodexten.substack.com/p/i-will-not-eat-the-bugs
[04:57:06] *** mst_ is now known as mst
[05:05:54] <Robomot> [Overcoming Bias] On UFO As Aliens Priors - https://www.overcomingbias.com/2021/06/on-ufo-as-aliens-priors.html
[05:25:50] <wilrnh> anyone has any experience using an app to learn a language?
[05:26:10] <wilrnh> thinking of trying duolingo - heard good things about it
[05:28:52] <ggreer> I tried duolingo and it's definitely more useful than tapes or books, but you have to have a good reason to learn the language if you're going to commit to spending years doing it
[05:29:33] <wilrnh> the good thing i heard about it was that i get a lot of ROI in a few weeks/mths
[05:30:58] <wilrnh> i guess ill give it a shot as i havent heard anything terrible about it
[05:31:24] <spxtr> I took a class for chinese, I think that's the only good way to go.
[05:33:09] <wilrnh> yea im not too enticed by the idea of a class
[05:33:23] <wilrnh> since nursery i think
[05:34:14] <Mark_> wow these more buttons on hn are terrible
[05:34:43] <spxtr> If you have the money, pay a tutor.
[05:36:02] <wilrnh> hmm that sounds interesting
[05:36:35] <spxtr> I had some success with a language exchange friend. Helped them with their english, they helped me with chinese.
[05:36:52] <wilrnh> back in the day i challenged the sanity of many of my teachers, poor critters
[05:37:36] <ggreer> you have to have a reason to use the language. I cannot stress this enough
[05:38:53] <wilrnh> uhoh, i dont really, except that "its cool"
[05:45:02] <gwern> 'Stark contrast: 10 years earlier, the patient's sibling had a similar neurologic presentation that rapidly progressed to epileptic encephalopathy; the child died at 11 months of age without a diagnosis, despite extensive workup (no sequencing though).' https://twitter.com/ProfMakris/status/1402376554068459520
[05:45:03] <|dbotdan> Michael Makris (@ProfMakris, 2021-06-08 21:26): ‘Amazing case report. The medicine of tomorrow. 5 week old boy admitted to NICU. Within 37 hours he had his whole genome sequenced, gene defect identified, treatment ordered and received by patient. Within 6 hr symptoms resolved. https://tinyurl.com/4emtz2p4’ Images: https://nitter.domain.glass/pic/media%2FE3Y9ovhXIAAfN9f.jpg%3Fname%3Dorig
[05:46:18] <gwern> '("Wigglesworth" is obviously a great name, but it's even better than that "wiggle" is cognate with "wigga", the Old English word for "bug", which survives in modern "earwig". So someone named Wigglesworth studying bugs' [eg wigga]'s moral value [eg worth] is kabbalistically exactly correct. Also, this is related to my pet theory that Orson Scott Card knew exactly what he was doing and the...
[05:46:24] <gwern> ...name "Ender Wiggin" was meant to convey "ender of bugs".)'
[05:46:42] <gwern> 'What about the other limit? Plausibly the most morally correct action, short of becoming vegetarian, would be to eat the largest animal there is. And according to the Talmud - Baba Bathra 74b- the righteous in Heaven dine on the flesh of Leviathan, which suffices to feed all of them forever. Hypothesis confirmed!'
[05:59:19] <gwern> https://www.khq.com/news/dog-ejected-from-car-during-sunday-crash-found-on-sheep-farm-herding-sheep/article_fc15bfe6-c805-11eb-ae68-7fb69c0af0d6.html born to herd
[05:59:19] <Robomot> Dog ejected from car during Sunday crash found on sheep farm, herding sheep | News | khq.com (UPDATE: JUNE 8 AT 10:28 A.M.)
[05:59:37] <gwern> 'Feedback? :: That is so cool thx this helped with my project' <-- you're welcome. good luck with... whatever
[06:00:58] *** Quits: vrs (~vrs@user/vrs) (Ping timeout: 268 seconds)
[06:17:19] <gwern> https://arxiv.org/abs/2106.04283 oh my
[06:17:20] <Robomot> [2106.04283] NWT: Towards natural audio-to-video generation with representation learning (In this work we introduce NWT, an expressive speech-to-video model. Unlike)
[06:22:46] <ggreer> > Mitch Hedberg used to say that “escalators can’t break, they can only become stairs”; in the same way, prediction markets can’t break, they can only give you free money.
[06:23:04] <gwern> 'It's like being a vtuber but with ray tracing and project milestone discussions' https://twitter.com/ModDoesStuff/status/1402193180670533637?s=19
[06:23:04] <|dbotdan> Mod (@ModDoesStuff, 2021-06-08 09:18): ‘Decided to be a VR dragon in my work's daily stand up this morning because I'm a professional’ Images: https://nitter.namazso.eu/pic/media%2FE3WW6kLXwAAwyn2.jpg%3Fname%3Dorig
[06:25:17] *** Joins: vrs (~vrs@user/vrs)
[06:28:21] <gwern> https://next-week-tonight.github.io/NWT/ hands are hard :(
[06:28:21] <Robomot> Video samples from "NWT: Towards natural audio-to-video generation with representation learning"
[06:34:20] <Obormot\Arcturus> !latest acx
[06:34:20] <Robomot> [Astral Codex Ten] I Will Not Eat The Bugs - https://astralcodexten.substack.com/p/i-will-not-eat-the-bugs
[06:37:38] <Obormot\Arcturus> "I will be honest - I swat flies and only feel a little bad about it. That's fine." ... when I swat flies - well, we don't get a lot of flies here, but mosquitos, same deal - I feel bad that they aren't conscious and don't feel pain
[06:38:00] <Obormot\Arcturus> They deserve to suffer for their intrusion, and to know that it is I who has destroyed them, as is just and proper
[06:38:02] <spxtr> haha
[06:38:42] <spxtr> .22 blanks kill flies and so on from up to several feet away.
[06:39:01] <rmmh> they'll kill people if shot against the temple, too
[06:39:12] <spxtr> yeah, I think that happened to some actor at some point
[06:39:22] <Obormot\Arcturus> "It's not a problem _unless you're factory-farming ten trillion insects_, at which point it really starts to add up." ... no, because moral value isn't additive in this way so this entire line of reasoning is complete nonsense
[06:39:47] <Obormot\Arcturus> spxtr: Wasn't that what killed Brandon Lee
[06:39:56] <spxtr> Obormot\Arcturus: I agree with you on this, and predicted that you would respond that way when I read the article :0
[06:40:38] <feep> I also agree!
[06:40:42] <spxtr> Obormot\Arcturus: No, https://en.wikipedia.org/wiki/Brandon_Lee#Death
[06:40:43] <Robomot> Brandon Lee - Wikipedia [Death] (Brandon Bruce Lee (February 1, 1965 – March 31, 1993) was an American actor and martial artist. Lee was also known for being the only son of Bruce Lee and was accidentally killed during production of his breakthrough film The Crow (1994). Lee's father, who died in 1973, was iconic in the field of martial arts both as a practitioner and leading man in their films. …)
[06:40:45] <Obormot\Arcturus> Hm no his firearm-related death was slightly different
[06:40:46] <Obormot\Arcturus> Yeah
[06:40:46] <Robomot> [Less Wrong [frontpage]] “Decision Transformer” (Tool AIs are secret Agent AIs) by gwern - https://www.greaterwrong.com/posts/PcL5FmnjmvsArYeNk/decision-transformer-tool-ais-are-secret-agent-ais
[06:41:19] <gwern> RiversHaveWings: https://arxiv.org/pdf/2106.04283.pdf#page=3 what do you think? of any general interest or just a minor tweak on gumbel?
[06:41:23] <spxtr> https://en.wikipedia.org/wiki/Blank_(cartridge)#Fatal_accidents maybe I was thinking of Brandon lee, though
[06:41:23] <Robomot> PDF (30 pages; 7.9 MB) - ￼NWT: Towards natural audio-to-video generation with representation learning …
[06:41:23] <Robomot> Blank (cartridge) - Wikipedia [Fatal accidents] (A blank is a firearm cartridge that generates a muzzle flash and an explosive sound (muzzle report) like any normal gunshots, and the firearm experiences a recoil capable of cycling its action, but without shooting a projectile (e.g. bullet or shot). …)
[06:41:48] <Obormot\Arcturus> "But Brian Tomasik, who analyzes this issue heroically thoroughly, says that if you're trying to prevent insect suffering, crop farming is probably net positive, because cropland has fewer insects than wild land, and most insects that exist suffer most of the time, with pesticide-related suffering being swiftly over and not very relevant." ... see, I just don't get how this sort of thing isn't a reductio of this approach to
[06:41:48] <Obormot\Arcturus>  ethics
[06:42:49] <nshepperd> I'm pretty sure it does add up. see, an insect's value is zero, and ten trillion zeros add up to zero
[06:42:54] <rmmh> won't someone think of the ants
[06:43:09] <Obormot\Arcturus> Like, maybe you can sort of nod along at the basically reasonable-sounding math (without thinking to yourself "aren't the conclusions of this argument actually dumb though"), but then you get to "oh by the way, insects just existing in their natural state, without humans ever interfering in any way? also horrific, terrible, no good, horrible, etc."
[06:43:20] <Obormot\Arcturus> And you just *gotta* think "wait a sec".
[06:43:26] <rmmh> Global Neutron Bombs As Compassionate Cure
[06:43:35] <Obormot\Arcturus> "We've reasoned ourselves into total absurdity"
[06:43:47] <rmmh> Obormot\Arcturus: Actually Silent Spring Is Good
[06:44:17] <rmmh> those birdcalls and insect chirps that used to be heard? that's right, SCREAMING
[06:45:31] <Obormot\Arcturus> "Second, it's good for me to remember that there are people like Brian Tomasik and Vincent Wigglesworth who care a lot about insect suffering and try to prevent it. Most people are only about as moral as the average of the other people they hear about and interact with - if the rest of your society feels okay with slavery, probably you will too. But it can be hard to cultivate a community full of people exactly as moral as you
[06:45:32] <Obormot\Arcturus>  want to be. One helpful trick is knowing a couple of people who are much _more_ moral than you're aiming for, so that even when you average them out with all the jerks you know and inevitably fail to live up to their example, you land somewhere close to where you want to be. People who care a lot about insect suffering fulfill that role for me. I am probably not able to care that much, but if they can work that hard to help
[06:45:32] <Obormot\Arcturus>  insects, I can at least remember not to kick a dog or something." ... I'm sorry, but no. See, this part actually offends me
[06:45:50] <nshepperd> that doesn't seem that absurd. besides the part where you assume insects can suffer at all
[06:45:51] <Obormot\Arcturus> Brian Tomasik is NOT more moral than other people by virtue of caring about insects or electrons
[06:45:59] <Obormot\Arcturus> Doing that is not more moral than not doing it
[06:46:01] <Obormot\Arcturus> It's a *mistake*
[06:46:09] <Obormot\Arcturus> And it's not a mistake in any correct direction, either
[06:46:17] <gwern> 'Trained on 400B tokens with TPU v3-256 for five weeks GPT-J performs much closer to GPT-3 of similar size than GPT-Neo does' https://twitter.com/arankomatsuzaki/status/1402446954550874116
[06:46:18] <rmmh> insufferability and morality are basically the same thing, right?
[06:46:20] <Obormot\Arcturus> It's actually a dramatic distortion of moral intuitions and ethical reasoning
[06:46:22] <|dbotdan> Aran Komatsuzaki (@arankomatsuzaki, 2021-06-09 02:06): ‘Ben and I have released GPT-J, 6B JAX-based Transformer LM 🥳 | - Performs on par with 6.7B GPT-3 | - Performs better and decodes faster than GPT-Neo | - repo + colab + free web demo  | article: https://bit.ly/2TH8yl0 | repo: https://bit.ly/3eszQ6C’ Images: https://nitter.database.red/pic/media%2FE3ZjuzFVgAMvwxB.png%3Fname%3Dorig
[06:46:28] <Obormot\Arcturus> It's substantially *worse* than ordinary-person morality
[06:47:31] <rmmh> Obormot\Arcturus:that meme about the ethical circle with "just do the right thing lol" being at the top
[06:48:03] <Obormot\Arcturus> Heh
[06:49:16] <PlanckWalk> something something breeding swarms of heroin-doped rats
[06:49:28] <PlanckWalk> Except now with insects
[06:49:45] <rmmh> let's breed insects incapable of suffering
[06:50:05] <RiversHaveWings> gwern: this is just straight-through gumbel-softmax, it's from the gumbel-softmax paper but they don't seem to recognize it?
[06:51:14] <feep> let's breed insects capable of suffering >:3
[06:51:41] <rmmh> has someone made an art installation that involves torturing insects and/or releasing CFLs
[06:51:56] <rmmh> every time you press the button, 1000kg of CO2e gases are released
[06:51:57] <adiabatic> …compact fluorescent lightbulbs?
[06:52:02] <rmmh> er, CFCs
[06:52:56] <adiabatic> too edgy
[06:53:08] <rmmh> too on the nose?
[06:53:16] <rmmh> what if it dispensed a candy too
[06:53:38] <rmmh> it dispenses a morsel of food and also emits as much smoke as it takes to produce the food
[06:53:47] <nshepperd> let's breed insects capable of suffering, and insects incapable of suffering, and make them go to war against each other
[06:54:10] <adiabatic> what if you need to make them smart in order to make them capable of suffering? I don't want smart bugs
[06:54:27] <adiabatic> flies are hard enough to swat and they're as dumb as bricks
[06:54:32] <rmmh> we already have smart bugs, haven't you heard of Alexa?
[06:54:48] <rmmh> australian flies are dumber because they didn't evolve to be fast
[06:55:13] <shawwwn> for fuck's sake. saturn2/nshepperd/anyone, halp. I'm a sad panda. All I'm trying to do is `cat` a file with syntax highlighting. I want the defaults to be pleasant. The closest thing I've found is `highlight`. The default is fucking this https://usercontent.irccloud-cdn.com/file/EOyd05rN/image.png
[06:55:26] <shawwwn> for the goddamn life of me, I can't figure out how to change that ugly green shit. And googling doesn't help. Good luck googling for "unix highlight"
[06:55:44] <shawwwn> In fact, that's not the default; The default is to be completely green
[06:55:56] <shawwwn> this is markdown default, which still fucking sucks, but it appears to be my best option
[06:56:08] <shawwwn> maybe feep would know some obscurity like this
[06:56:19] <nshepperd> you can use pygments on the command line
[06:56:24] <shawwwn> also, I'm pissed because we lost the dream apartment today and everyone's freaking out, so sorry
[06:56:28] <shawwwn> ah, pygments. thank you.
[06:56:40] <nshepperd> that's a python package for syntax highlighting
[06:57:46] <shawwwn> rofl https://usercontent.irccloud-cdn.com/file/iwRSjy9v/image.png
[06:57:59] <shawwwn> okay. apparently this is somehow a me-problem, not a unix problem.
[06:58:23] <gwern> 'def LESSOPEN="| /usr/share/source-highlight/src-hilite-lesspipe.sh %s"'
[06:58:40] <nshepperd> is it your terminal? is your terminal fucked
[06:58:47] <PlanckWalk> The Machine has deemed that you like green, and aims to give you all the green.
[06:59:05] <gwern> it knows shawwwn is a true hacker, and they only like green, sometimes amber, but usually green
[06:59:56] <shawwwn> gwern: what's that for? I'm sorry
[07:00:13] <gwern> I'm not sure. but it seems to be how, many years ago, I got my `less` to automatically syntax highlight everything nicely
[07:00:18] <shawwwn> ah
[07:00:24] * gwern did not document it in his config and is not sure what it means ;_;
[07:00:25] <shawwwn> unfortunately I don't seem to have that file on MacOS M1
[07:00:32] <shawwwn> but! thank you for trying. it meant a lot.
[07:00:35] <shawwwn> didn't really expect it.
[07:00:49] <PlanckWalk> pygmentize here doesn't make nearly everything green
[07:01:24] <shawwwn> anyway, people are yelling at me so I guess I'll go get yelled at for awhile. thanks for trying everyone. I guess my terminal is fucked. https://usercontent.irccloud-cdn.com/file/Yn6uXkqB/image.png
[07:01:29] <shawwwn> (iterm2)
[07:02:19] <enterprisey> <gwern "I'm not sure. but it seems to be"> there's an argument that makes it pass control codes through, might be -R, not at a computer
[07:02:46] <gwern> '...The scientists decided to ask one of the creatures to show them what English sounds like. The creature was then transported in a private jet to a real-life, windy city, where it was finally placed on the roof of a building, wearing a set of headphones. The scientist proceeded to ask the creature to demonstrate the letter “R”, which turned out to be an “R”-shaped unicorn. At this...
[07:02:52] <gwern> ...point, the scientists expected the creature to start singing, or making a horselike sound. Instead, the unicorn imitated a car horn, roaring “Raaaaaarrrr!”. The scientist chuckled at the unplanned execution. The unicorn finished the demonstration by shouting “Ahhhhhhhhhhhhhhhhh!”, sounding like an escaped convict having an orgasm. Then the creature, after spending about an hour on...
[07:02:58] <gwern> ...the rooftop, as it was exhausted, went to sleep. By chance, the creature’s body was mown down by a steamroller a few hours later, and its body was sent to the scientists.' <-- DAMN YOU TRUCK-KUN, YOU UNICORN-MURDERER
[07:04:04] <PlanckWalk> Is that machine-generated text?
[07:04:56] <gwern> yes, gpt-J
[07:05:04] <nshepperd> 'The scientists chuckled at the unplanned execution.'
[07:06:15] <nshepperd> truck-kun's executions are always unplanned, as a matter of pride
[07:06:54] <PlanckWalk> It was unplanned, but the scientists somehow chuckled at it before it happened.
[07:07:11] <PlanckWalk> (I was thinking of a different meaning of "execution")
[07:08:02] <saturn2> does this mean... they planned the unplanned execution all along? dun dun dun
[07:08:08] <PlanckWalk> I wonder in what sense it would be reasonable to ask which meaning the 'writer' intended here?
[07:08:11] <adiabatic> shawwwn: try sharkdp/bat
[07:08:33] <PlanckWalk> Does the model have the ability to separate meanings like that?
[07:09:31] <gwern> I think it means 'unplanned execution of [the requested demonstration task]'
[07:09:40] <PlanckWalk> That's how I read it, yes
[07:11:00] <adiabatic> shawwwn: random tip: whenever you run brew update && brew upgrade, do `brew info` on all the updated formulae (and casks?) of all the formulae that have cool-sounding names
[07:11:21] <adiabatic> literally a modern-day Freshmeat right in your terminal
[07:11:54] <adiabatic> hard mode: take a drink every time you run brew info on a formula you've already run brew info on
[07:13:39] <saturn2> shawwwn: this vim script will let you dump a whole file with vim syntax highlighting https://github.com/rkitover/vimpager
[07:13:40] <Robomot> GitHub - rkitover/vimpager: Use Vim as PAGER (Use Vim as PAGER. Contribute to rkitover/vimpager development by creating an account on GitHub.)
[07:15:10] <gwern> https://cdn.discordapp.com/attachments/838133794204155914/852014988876185610/97d4986a-f175-4b60-b974-93403f057173.png more foxos
[07:15:11] <Robomot> image/png (1448x1448; 5.5 MB)
[07:24:27] <gwern> https://cdn.discordapp.com/attachments/733347369847881838/852017531904720986/E3X6HltXIAA86j6.png uh oh
[07:24:28] <Robomot> image/png (1920x1080; 476 KB)
[07:26:52] <nshepperd> amazing
[07:33:27] <gwern> 'I would begin interviews with sources who seemed lucid and prudent and who insisted, like Kean, that they were interested only in vetted data, and that they used the term “U.F.O.” in the strictly literal sense—whether the objects were spaceships or drones or clouds, we just didn’t know. An hour later, they would reveal to me that the aliens had been living in secret bases under the...
[07:33:33] <gwern> ...ocean for millions of years, had genetically altered primates to become our ancestors, and had taught accounting to the Sumerians.' https://www.reddit.com/r/slatestarcodex/comments/nvai98/theres_been_a_lot_of_ufo_news_recently_heres_why/
[07:33:35] <Robomot> There's been a lot of UFO news recently. Here's why I think it's overhyped. : slatestarcodex (37 votes, 63 comments. In a couple of weeks, the Pentagon will release a much-anticipated report on UFOs. This stems from those videos that were …)
[07:33:44] <gwern> that's a heck of a motte and bailey
[07:35:48] <saturn2> or it takes an hour for the drugs to kick in
[07:37:06] <gwern> "
[07:37:38] <nshepperd> or to wear off
[07:37:50] <gwern> We were past Barlow when the drugs started kicking in. 'Holy shit', my lawyer screamed. 'The accounts payable - the air is full of them! We've got to get out of here before the fiscal quarter ends and we're billed for all the jars of barley!'"
[07:38:19] <spxtr> Sounds like an excellent GPT-n creation
[07:44:43] <gwern> 'Hunter: ‘Its big penis George. They always find it. And I only love you because you’re black.’ Mesires: ‘It’s so annoying when you interject with frivolity.’ Hunter: ‘True dat nigga. But I’m done my rant.’ Along with his naughty words, Hunter also describes the God his father believes in as ‘a fictional character from the imagination of the collective frightened’. The...
[07:44:49] <gwern> ...exchange also suggests that Hunter may have accidentally sent George a sext, attempting to reach a woman named Georgia.' https://spectatorworld.com/topic/hunter-biden-n-word-daily-mail-president/ practically the same thing, you'll concede
[07:44:50] <Robomot> No one will care about the Hunter Biden N-word scandal - The Spectator World (Newly published text messages from Hunter Biden reveal that Hunter Biden...is an American with actual freedom of speech! How lucky for him!)
[07:46:50] <Robomot> [Less Wrong [frontpage]] The Inside View #3: Evan Hubinger— homogeneity in takeoff speeds, learned optimization and interpretability by Michaël Trazzi - https://www.greaterwrong.com/posts/NFfZsWrzALPdw54NL/the-inside-view-3-evan-hubinger-homogeneity-in-takeoff
[07:46:56] <Robomot> [Less Wrong [frontpage]] Should we vaccinate against PGBD5 which codes for a transposase? by ChristianKl - https://www.greaterwrong.com/posts/Js95rcWGufPWjbECd/should-we-vaccinate-against-pgbd5-which-codes-for-a
[07:47:01] <Robomot> [Less Wrong [frontpage]] AXRP Episode 8 - Assistance Games with Dylan Hadfield-Menell by DanielFilan - https://www.greaterwrong.com/posts/fzFyCJ6gB9kBL9RqW/axrp-episode-8-assistance-games-with-dylan-hadfield-menell
[07:47:06] <gwern> I sometimes wonder how many things have happened because of tab/autocomplete slipping up. the human error rate, an industrial psych textbook tells me, never goes much below 1/100,000, and for anything complex is more like 1/10,000, so if on the other of billions of convos a day happen, then there must be millions of mistakes
[07:47:06] <Robomot> [Less Wrong [frontpage]] ...and then sometimes, for no clear reason, they innately become good. by Andrew Vlahos - https://www.greaterwrong.com/posts/QWjxu3hj59oMzDyp3/and-then-sometimes-for-no-clear-reason-they-innately-become
[07:51:33] <Mark_> if you want cat with syntax highlighting, bat is exactly that
[07:53:20] <shawwwn> okay, the yelling has ceased
[07:53:56] <shawwwn> adiabatic: cool! uh, what do you mean? sorry, I have no idea what you're saying :)
[07:54:05] <shawwwn> saturn2: thank you, that might be perfect. I'll try it out
[07:54:21] <shawwwn> adiabatic: i'll try those too if vimpager doesn't do it
[07:54:35] <adiabatic> https://github.com/sharkdp/bat
[07:54:36] <Robomot> GitHub - sharkdp/bat: A cat(1) clone with wings. (A cat(1) clone with wings. Contribute to sharkdp/bat development by creating an account on GitHub.)
[07:55:48] <adiabatic> sharkdp has some other cool things, especially pastel if you like colors and have a terminal that supports 24-bit (iTerm, VS Code, but not Terminal.app)
[07:56:37] <gwern> (speaking of truck-kun, after like 10 years of it sitting on my hard drive, I've been trying to work through _Kara no Kyoukai_. the female protagonist, a serial killer upset about loving the male protagonist, is stopped from murdering him when she hesitates and... is hit by a truck, apparently. she gets better and is *somewhat* less stabby after the long nap)
[07:58:11] <shawwwn> I'm not sure what's going on here https://usercontent.irccloud-cdn.com/file/RKtPAdS4/image.png
[07:58:18] <shawwwn> vimpager seems to ... uh ....
[07:58:25] <shawwwn> hm.
[07:58:35] <shawwwn> I've never seen cat fail to cat stdin
[07:59:08] <shawwwn> ??? https://usercontent.irccloud-cdn.com/file/UiOeaXnP/image.png
[07:59:23] *** Joins: Lord_of_Life_ (~Lord@user/lord-of-life/x-2819915)
[07:59:40] <adiabatic> 🦇
[08:00:24] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (Ping timeout: 250 seconds)
[08:00:37] *** Lord_of_Life_ is now known as Lord_of_Life
[08:10:49] <nshepperd2> shawwwn: https://github.com/rkitover/vimpager/blob/master/vimpager#L11
[08:10:49] <Robomot> vimpager/vimpager at master · rkitover/vimpager · GitHub (Use Vim as PAGER. Contribute to rkitover/vimpager development by creating an account on GitHub.)
[08:11:12] <shawwwn> lmao
[08:11:16] <shawwwn> what the fuck vimpager. okay
[08:11:59] <shawwwn> yes, all the commands work now
[08:12:02] <shawwwn> thank you very much.
[08:13:21] <shawwwn> sigh. okay, so, it works on commandline
[08:13:25] <shawwwn> https://usercontent.irccloud-cdn.com/file/yEKr2v6x/image.png
[08:14:01] * nshepperd2 doesn't see any line numbers
[08:14:06] <shawwwn> but when I try it in my `f` script:exec fzf --preview "cat {} | vimpager --force-passthrough --LINE-NUMBERS" -q "$*"I get https://usercontent.irccloud-cdn.com/file/MlPV1JrL/image.png
[08:14:14] <shawwwn> well, fuck the line numbers, I was just hoping it'd work at all
[08:14:21] <shawwwn> but NnNNnooOope
[08:14:26] <shawwwn> guess I'll settle for pygments
[08:14:36] <nshepperd2> maybe edit vimpager and remove that whole tty check
[08:14:40] <saturn2> i have no clue what's going on here lol
[08:14:41] <nshepperd2> it's retarded
[08:15:17] <shawwwn> basically... if you've never used fzf with preview, it's pretty cool
[08:15:26] <Mark_> just saying, bat is available through homebrew if you feel like using a tool that’s actually fit for purpose
[08:15:26] <pie_bnc> does this mean if i start talking^Wscreaming in a high pitched voice i become a cute anime? <Robomot> [2106.04283] NWT: Towards natural audio-to-video generation with representation learning (In this work we introduce NWT, an expressive speech-to-video model. Unlike)
[08:15:30] <saturn2> i've never used fzf
[08:15:47] <shawwwn> try sticking this in a script called f:
[08:15:47] <shawwwn> exec fzf --preview "highlight --force -O xterm256 {} | nl -w2 -s' '" -q "$*"
[08:15:57] <gwern> pie_bnc: of course you would have to ask that
[08:16:04] <saturn2> removing the tty check sounds like a smart idea
[08:16:04] <shawwwn> if you cd into a directory, you get a nice fuzzy completion thing with preview on the right
[08:16:11] <shawwwn> (I tried removing the tty check)
[08:16:22] <pie_bnc> gwern: in the current social context? unavoidable
[08:16:30] <shawwwn> (it resulted in the "Loading..." thing when used as fzf preview, so abandoning ship on vimpager)
[08:16:46] <nshepperd2> pie_bnc: it takes years of training to perfect the anime voice
[08:16:46] <gwern> pie_bnc: but hm... I think you *could* do that, if you did your own 'dubbing' of anime to match the lipflaps and acting. after all, the model doesn't care.
[08:16:56] <saturn2> vim probably isn't happy about being given an non-tty though
[08:17:03] <gwern> pie_bnc: you could just use your own normal voice when dubbing, no need for high-pitched
[08:17:19] <pie_bnc> nshepperd2: youre saying they purposefully mutilate themselves
[08:17:21] <gwern> then it's just pie-voice -> pixels
[08:17:32] <nshepperd2> pie_bnc: who doesn't
[08:17:47] <pie_bnc> jayy music stops
[08:18:10] <gwern> (this is trained on like hundreds of hours of john oliver, you probably don't want to make hundreds of hours of anime dubs. but nothing a bunch of pretraining & transfer-learning can't fix)
[08:18:17] <pie_bnc> (In reality my cpu fans just got a bit quieter)
[08:18:56] <pie_bnc> gwern: sadly I was just shitposting about audio -> video :c
[08:19:10] <gwern> someone will do it
[08:19:23] <gwern> VTubers alone are an adequate reason
[08:19:27] <pie_bnc> mm
[08:19:31] <shawwwn> sighs with relief. no more green. https://usercontent.irccloud-cdn.com/file/HCrLTZ0j/image.png
[08:19:50] <shawwwn> the magic incantation turned out to be: fzf --preview "highlight --force -O xterm256 {} | replace '\\033[38;5;28m' '\\033[38;m' | nl -w2 -s' '" -q "$*"
[08:19:56] <pie_bnc> yall should license some vtuber tech and get rich
[08:20:13] <saturn2> wow i had no idea how complicated vimpager was
[08:20:14] <shawwwn> i.e. I'm manually replacing \033[38;5;28m with \033[38;m
[08:20:20] <shawwwn> yeah, it was a good idea
[08:20:23] <shawwwn> and it almost worked
[08:20:32] <shawwwn> no idea why it failed as an fzf preview though
[08:20:40] <shawwwn> (and the background color was kinda crummy... a bit of a relief tbh)
[08:21:20] <shawwwn> so like, check out how badass fzf is: https://usercontent.irccloud-cdn.com/file/LwrbScoW/image.png
[08:21:24] <shawwwn> I'm cd'ed into my home dir
[08:21:38] <shawwwn> I just type f<enter>ml tensorflow server_lib, and it pops up exactly the file I wanted
[08:21:54] <pie_bnc> huh
[08:21:55] <shawwwn> so it's kind of a global search engine almost (from your current dir downwards)
[08:22:01] <nshepperd2> removing the tty check in vimpager didn't work
[08:22:15] <shawwwn> yeah, didn't seem to be perfect for me either. but, it tried
[08:22:18] <gwern> https://twitter.com/Thinkwert/status/1402440887276347393
[08:22:18] <|dbotdan> Thinkwert (@Thinkwert, 2021-06-09 01:42): ‘“There was a terrible accident. I lost consciousness and woke up like this. They saved my life…but at what cost? I am now more plastic than cat.”’ Images: https://nitter.1d4.us/pic/media%2FE3Z4jcTWEAUTZ5K.jpg%3Fname%3Dorig
[08:22:51] <nshepperd2> there's at least three different places in its code where they have the genius idea of delegating to 'cat' without even parsing flags first, and i can't find the third
[08:24:26] <shawwwn> lel.
[08:24:41] <shawwwn> well, I mean, I'm grateful they made the code available at all
[08:24:44] <shawwwn> they didn't have to make it
[08:25:00] <shawwwn> more open source = better imo, even if it's sometimes a little derpy. up to us to file a PR, etc.
[08:29:12] <saturn2> i also didn't know wei dai worked on vimpager...
[08:29:16] <nshepperd2> oh, it worked, i just had to rebuild from source, because vimcat has a tty check and vimpager includes an encoded version of vimcat.sh inside it
[08:30:14] <nshepperd2> vim throws a 'Warning: Output is not to a terminal' but who cares
[08:34:27] <gwern> every day, we stray further from god's light
[08:53:55] *** Joins: quanticle (~quanticle@user/quanticle)
[08:54:21] *** Joins: adiabatty (~adiabatic@user/adiabatic)
[08:54:49] *** Joins: Obormot_\Gaia (~obormot@user/obormot)
[08:55:17] *** Joins: dreth (~drethelin@096-042-035-082.res.spectrum.com)
[08:56:45] *** Joins: gbear605_ (~gbear605@user/gbear605)
[08:58:50] *** Quits: Obormot\Arcturus (~obormot@user/obormot) (Read error: Connection reset by peer)
[08:59:41] *** Joins: Obormot\Arcturus (~obormot@user/obormot)
[09:00:14] <quanticle> Obormot\Acturus: A bodega cat encounter?
[09:00:54] *** Quits: Robomot (~Robomot@user/robomot) (Ping timeout: 245 seconds)
[09:03:58] *** Quits: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com) (*.net *.split)
[09:03:58] *** Quits: drethelin (~drethelin@096-042-035-082.res.spectrum.com) (*.net *.split)
[09:03:58] *** Quits: adiabatic (~adiabatic@user/adiabatic) (*.net *.split)
[09:03:58] *** Quits: gbear605 (~gbear605@user/gbear605) (*.net *.split)
[09:03:58] *** Quits: inky (~inky@141.136.79.172) (*.net *.split)
[09:03:58] *** Quits: Obormot\Gaia (~obormot@user/obormot) (*.net *.split)
[09:05:26] <shawwwn> saturn2: dei wai? or, what's wei dai?
[09:07:06] *** Joins: Robomot (~Robomot@user/robomot)
[09:07:06] *** ChanServ sets mode: +v Robomot
[09:07:18] <shawwwn> https://www.youtube.com/watch?v=iQO9yRutpso
[09:07:30] <feepbot> DO YOU KNOW DA WAE - YouTube
[09:11:34] <otoburb> shawwwn: http://www.weidai.com/
[09:11:35] <Robomot> Wei Dai's Home Page
[09:11:51] <shawwwn> apparently I didn't know wei dai, even if I knew da wae
[09:11:58] <adiabatty> he's, uh, some guy? he posts on LW occasionally
[09:12:21] * shawwwn thought that wei dai was some tool that happened to work when piped through vimpager :)
[09:12:39] <saturn2> shawwwn: he's one of the people commonly accused of being satoshi nakamoto, and he also does decision theory/alignment stuff on LW
[09:12:45] <shawwwn> aha.
[09:12:48] <shawwwn> is he?
[09:12:54] *** Joins: inky (~inky@141.136.79.172)
[09:12:56] <adiabatty> didn't know about the satoshi thing
[09:13:01] <saturn2> probably not
[09:13:32] <saturn2> he has a wikipedia article https://en.wikipedia.org/wiki/Wei_Dai
[09:13:32] <Robomot> Wei Dai - Wikipedia (Wei Dai (Chinese: 戴维) is a computer engineer known for contributions to cryptography and cryptocurrencies. He developed the Crypto++ cryptographic library, created the b-money cryptocurrency system, and co-proposed the VMAC message authentication algorithm. The smallest subunit of Ether, the wei, is named after him.[1] …)
[09:17:02] *** Joins: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com)
[09:17:06] *** Quits: dreth (~drethelin@096-042-035-082.res.spectrum.com) (Ping timeout: 250 seconds)
[09:25:00] <Mark_> https://redesign.metaculus.com/
[09:25:01] <Robomot> Metaculus (Metaculus is a community dedicated to generating accurate predictions about future real-world events by aggregating the collective wisdom, insight, and intelligence of its participants.)
[09:25:52] <Mark_> 12px font size everywhere, letter spacing and small click targets in header navigation, no visible hover or active state at all on any link or button...
[09:26:18] <Mark_> (okay, some buttons do have a hover state. but many don’t, and links don’t)
[09:28:39] <saturn2> wow, sounds perfect
[09:31:22] <quanticle> saturn2: w.r.t. https://extropian.net/notice/A7v5oLhPxJquPkDDMW That just sounds like normal namespace doomsaying to me
[09:31:22] <Robomot> Extropy (John David Pressman (@jdp@extropian.net): “The promising young people in my life need to understand that in these last few years the winds have changed. Old optimistic timelines have compressed into decisions to be made here and now, proble...”)
[09:32:30] <saturn2> quanticle: what are the decisions that need to be made though?
[09:33:07] <nshepperd2> who to kill to prevent ufai i guess
[09:33:08] <quanticle> I have no idea; he's vaguetweeting again
[09:33:15] <quanticle> Maybe you should ask him
[09:34:46] <quanticle> nshepperd2: Honestly? My take on that is that it's too late. Maybe if you went back to 2015 or so and took out some of the key people working on NNs then you might have been able to stop it, but now? I think the cat is out of the bag. No matter who you kill, the basic technology is available enough and widespread enough that someone else will pick up the torch.
[09:35:50] <saturn2> if you got really good you could probably manage to kill enough people
[09:36:49] <quanticle> I mean, a series of nuclear strikes on the Bay Area could still probably do it... depending on how far along you think the Chinese are
[09:37:15] <saturn2> https://en.wikipedia.org/wiki/List_of_serial_killers_by_number_of_victims
[09:37:17] <Robomot> List of serial killers by number of victims - Wikipedia (A serial killer is typically a person who murders three or more people, in two or more separate events over a period of time, for primarily psychological reasons.[1][2] There are gaps of time between the killings, which may range from a few days to months, or many years.[ …)
[09:37:35] <quanticle> ITT we recast Terminator as the story of a deeply misunderstood Friendly AI trying to prevent humanity from unleashing a greater evil
[09:40:31] <shawwwn> nshepperd / saturn2: uh, funny story.  Turns out pygmentize works fabulously well. https://usercontent.irccloud-cdn.com/file/VLIwn0tr/image.png
[09:40:47] <nshepperd2> nice
[09:40:59] <shawwwn> the reason it didn't work for https://usercontent.irccloud-cdn.com/file/iwRSjy9v/image.png is because it was just bad luck of the draw. I happened to choose a file that pygmentize happened to color exactly the same as the other tool
[09:41:19] <shawwwn> so yes, thanks for the pygmentize suggestion
[09:41:40] * shawwwn now has a `killgreen` script that strips out that damn green escape code so he never sees it again, though
[09:41:41] <adiabatty> better than bat, eh?
[09:42:09] <quanticle> I mean, if you think about it, Skynet was pretty friendly. It didn't totally wipe out humanity. It let them have Australia
[09:42:14] *** Quits: feep[cx] (~feepcx]@ppp-93-104-188-93.dynamic.mnet-online.de) (Ping timeout: 250 seconds)
[09:42:28] <adiabatty> let them have Australia, like how the Moors let the Spaniards eat pork?
[09:42:57] *** Quits: feepbot (~feepbot@ppp-93-104-188-93.dynamic.mnet-online.de) (Ping timeout: 252 seconds)
[09:43:56] *** Joins: feep[work] (~mathis@217.64.163.97)
[09:44:20] *** Quits: feep (~feep@2001:a61:34c2:601:5ddf:222:36e5:1a99) (Ping timeout: 272 seconds)
[09:45:11] <nshepperd2> skynet's mission was to protect america. it only launched the nukes because they tried to turn it off
[09:48:41] <nshepperd2> i guess it wasn't that smart otherwise it would have just mind controlled everyone instead of letting a whole complicated time loop happen
[09:48:44] *** Quits: dove (~jordan@li1158-85.members.linode.com) (Quit: bye o/)
[09:48:54] *** Joins: dove (~jordan@li1158-85.members.linode.com)
[09:55:29] <nshepperd2> i don't like the dark blue pygmentize uses, it's pretty low contrast against my black terminal
[10:06:46] <quanticle> No way to customize the colors or choose alternate themes?
[10:09:25] <nshepperd2> pygmentize -O style=emacs looks alright
[10:12:48] <shawwwn> Oooh
[10:12:56] <shawwwn> didn't know about -O style=emacs. *tries it*
[10:14:36] <nshepperd2> "pygmentize -L style" lists a bunch of different styles
[10:14:37] <shawwwn> emacs style https://usercontent.irccloud-cdn.com/file/QIKhkLoV/image.png
[10:14:42] <shawwwn> default https://usercontent.irccloud-cdn.com/file/kgfotNer/image.png
[10:15:26] <shawwwn> emacs style is much nicer. I wish it wouldn't do the imports in blue, but, sweet! thanks.
[10:16:18] <nshepperd2> lol, style=xcode makes half the text disappear because black on black
[10:18:10] <quanticle> Obormot\Arcturus: https://i.imgur.com/2DAACre.jpg
[10:20:28] <shawwwn> nshepperd2: a week or two ago I ended up setting my background to dark blue rather than total black, for exactly that reason https://usercontent.irccloud-cdn.com/file/NxuvcB8h/image.png
[10:20:32] <shawwwn> but yes, not exactly comfy to read :)
[10:20:47] <nshepperd2> feels bad
[10:22:22] <Obormot\Arcturus> quanticle: Omissions: what about non-damaging offensive spells/abilities? What about Armor Class - are we doing this fully clad in combat gear, or in pajamas? (If the latter, the fighter-types are greatly disadvantaged)
[10:25:40] <quanticle> I assume that armor class doesn't apply; this is not explicitly mentioned in the text, but I'm assuming it isn't because they said that strength bonuses do not apply
[10:25:42] <Obormot\Arcturus> (Also, I guess this is for low-level parties, because at high levels you get stuff like "the wizard casts Reverse Gravity and now all opponents, along with their pillows, are on the ceiling and can't come down and the rules do not permit them to throw the pillows - melee attacks only, after all - while the wizard uses Telekinesis to smack people around with his pillow at leisure, thus winning easily while remaining at full
[10:25:42] <Obormot\Arcturus>  Pillow Points himself")
[10:25:59] <quanticle> I feel like that's entirely valid
[10:26:05] <quanticle> But also kind of a dick move
[10:27:06] <Obormot\Arcturus> Alternatively, summon an air elemental and command it to assume whirlwind form, hoovering up all opponents+pillows; your hapless buddies whirl around in the whirlwind, being smacked by the pillows, until victory
[10:27:16] <nshepperd2> cleric summons a horde of pillow wielding angels
[10:27:19] *** Joins: feep (~feep@ppp-93-104-168-97.dynamic.mnet-online.de)
[10:27:25] <Obormot\Arcturus> Indeed
[10:27:32] <quanticle> nshepperd2: I think wishing for more pillows is beyond the pale
[10:27:37] <quanticle> Abusing existing pillows is okay
[10:27:49] <quanticle> And w.r.t whirlwind: https://www.youtube.com/watch?v=Ju88tEm4SOI
[10:27:49] <Robomot> Bud Light - Helga Pillow Fight - YouTube (Hilarious Bud Light commercial.)
[10:27:50] <Obormot\Arcturus> No no, you summon the angels and then hand them the existing pillows
[10:28:03] <quanticle> From where? Are there more pillows than PCs?
[10:28:18] <nshepperd2> if necessary, the angels can pop down to ikea
[10:28:24] <shawwwn> fzf -O paraiso-dark is my favorite https://usercontent.irccloud-cdn.com/file/us63drW3/image.png
[10:28:35] <Obormot\Arcturus> a) Why not, but also b) the angels take the pillows from the other PCs and/or your confiscate the pillows via Telekinesis
[10:28:36] <shawwwn> er, pygmentize
[10:29:02] <shawwwn> fzf --preview "pygmentize -O style=${STYLE:-paraiso-dark} {} -g"
[10:29:04] <shawwwn> that thing.
[10:29:32] <shawwwn> mildly annoying that the #includes are dark grey, but whatever. https://usercontent.irccloud-cdn.com/file/AvuevNFO/image.png
[10:29:39] <quanticle> If your pillow gets confiscated, are you out of PP?
[10:29:44] <Obormot\Arcturus> Good question
[10:30:04] <shawwwn> the other contenders were stata-dark and solarized-light
[10:30:53] <Obormot\Arcturus> Another good Telekinesis trick is that if you have a confederate (e.g. an NPC child), you can use TK to hurl the (pillow-wielding) kid at someone, along with your own pillow, and then the kid also attacks the target on his own turn - sort of a pillow barrage
[10:31:00] <nshepperd2> pp represents your spiritual connection to the pillow god
[10:32:11] <Obormot\Arcturus> http://www.ganesh.com/wp-content/uploads/2017/01/mockup-f3adcbcc.jpg related, I guess
[10:32:11] <Robomot> image/jpeg (600x600; 104 KB)
[10:33:30] <shawwwn> STYLE=colorful is certainly...colorful https://usercontent.irccloud-cdn.com/file/d5WN9amx/image.png
[10:37:03] <shawwwn> STYLE=material is close to ideal, I think. Comments are slightly too dark, but nothing too terrible. https://usercontent.irccloud-cdn.com/file/UChiAIUu/image.png
[10:41:39] <shawwwn> ah. perfect. -O native is clearly the superior style. bright comments and strings; class names / module names are underscore+bold, distinct from function names https://usercontent.irccloud-cdn.com/file/cR6pl3JM/image.png
[10:42:11] <shawwwn> and doesn't do anything completely stupid for dark backgrounds. every other style has at least one
[10:46:48] <quanticle> Really?
[10:46:55] <quanticle> He chose *ganesha* for that?
[10:47:13] <quanticle> Not uh, any one of the numerous characters in Hindu mythology who *literally* have armor from God?
[10:47:47] <quanticle> Like Indrajit, or Karna?
[10:57:04] <adiabatty> you'd think these people would know better
[10:57:56] <adiabatty> they have typos/grammos on the front page. they're probably not white hippies with a degree in asian studies
[11:02:41] <SDr> heh, playing with site summarization via gpt-3, ran in on a recruiting site, which was summarized as thus: "The best way to get a job is to get a job."
[11:02:58] <SDr> I mean, it's not _wrong_ :D
[11:04:31] <saturn2> have you tried being a better person?
[11:05:19] *** Joins: Gurkenglas_ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de)
[11:08:04] *** Joins: tex (~super@user/dix)
[11:17:34] *** Quits: Fusxfaranto (~Fusxfaran@cpe-75-85-179-208.san.res.rr.com) (Ping timeout: 250 seconds)
[11:42:52] *** o is now known as niko
[11:45:19] *** Quits: feep[work] (~mathis@217.64.163.97) (Ping timeout: 264 seconds)
[11:47:01] *** Joins: feep[work] (~mathis@217.64.163.97)
[11:55:11] *** Quits: Thurl (~Nivim@174-24-87-60.clsp.qwest.net) (Quit: Probably due to necessity, but of the planned kind.)
[11:58:44] *** Quits: Gurkenglas_ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de) (Ping timeout: 250 seconds)
[12:06:23] <brigand> to be better i would need to get into activism, i would say
[12:07:33] <nshepperd2> most activists are worse than average imo
[12:08:28] <brigand> then i don't become a bad activist
[12:08:47] *** Quits: jrayhawk (~jrayhawk@user/jrayhawk) (Ping timeout: 272 seconds)
[12:09:17] <brigand> but most people are already on edge if you give out information at something they might do wrong
[12:10:54] *** Quits: kanzure (~kanzure@user/kanzure) (Ping timeout: 252 seconds)
[12:11:38] *** Joins: kanzure (~kanzure@user/kanzure)
[12:11:56] <brigand> the mere presence of a vegan for example triggers some people
[12:15:14] *** Joins: jrayhawk (~jrayhawk@user/jrayhawk)
[12:17:05] *** Quits: dove (~jordan@li1158-85.members.linode.com) (Quit: bye o/)
[12:22:22] *** Joins: dove (~jordan@li1158-85.members.linode.com)
[12:37:54] <Robomot> [Less Wrong [frontpage]] Bad names make you open the box  by adamzerner - https://www.greaterwrong.com/posts/NYaLudjSqsYtZDB2t/bad-names-make-you-open-the-box
[12:41:11] <Robomot> [Less Wrong [frontpage]] Bad names make you open the box  by adamzerner - https://www.greaterwrong.com/posts/NYaLudjSqsYtZDB2t/bad-names-make-you-open-the-box
[12:48:25] <saturn2> i didn't mean better at following the latest moral fads
[12:50:27] <dbohdan2> quanticle: '[...] will pick up the torch.'  Nice stealth pun
[12:51:23] <saturn2> i meant more excellent, surpassing your limitations
[12:51:50] <dbohdan2> gwern: My several attempts to get through _Kara no Kyoukai_ have all failed, and I am not exactly sure why.  I guess it isn't gorgeous enough to compensate for whatever makes it not very fun to watch for me.
[12:51:56] *** Joins: Gurkenglas_ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de)
[12:52:55] <feep[work]> :-(((
[12:53:15] <brigand> see? getting called out for just mentioning it xD
[12:54:56] <dbohdan2> feep[work]: Eh?
[12:55:10] <brigand> saturn2: how?
[12:57:44] <brigand> going into activism might be "surpassing your limitations" if you're socially awkward for example
[12:59:18] <dbohdan2> <@idlebot> Paperclip, the Maximizer, has attained level 31! Next level in 0 days, 16:35:51.
[13:02:58] *** Quits: Robomot (~Robomot@user/robomot) (Quit: Bye)
[13:03:11] *** Joins: Robomot (~Robomot@user/robomot)
[13:03:11] *** ChanServ sets mode: +v Robomot
[13:04:01] <Obormot\Arcturus> "the mere presence of a vegan" triggers people because most vegans are annoying and preachy
[13:04:24] <Obormot\Arcturus> (At least, most vegans who are, shall we say, perceptibly vegan)
[13:04:43] <Obormot\Arcturus> Here's the thing about telling people they're doing something wrong
[13:05:20] *** Quits: Ralith (~ralith@2001:470:69fc:105::776) (Read error: Connection reset by peer)
[13:05:20] *** Quits: Mark_ (~mkosmatri@user/mark/x-9597255) (Read error: Connection reset by peer)
[13:05:20] *** Quits: enterprisey (~enterpris@user/enterprisey) (Read error: Connection reset by peer)
[13:05:20] *** Quits: Guest7494 (~lucentmat@2001:470:69fc:105::dec) (Read error: Connection reset by peer)
[13:05:39] <Obormot\Arcturus> It's certainly possible that a person might "actually know" (or counterfactually "actually know" in the case where they're informed by you) that they really are doing something wrong, and just don't want to think about it
[13:06:04] <Obormot\Arcturus> But it's also possible that they, on consideration, disagree with you that they're doing anything wrong, and are insulted by the claim
[13:06:12] *** Joins: Ralith (~ralith@2001:470:69fc:105::776)
[13:07:07] <Obormot\Arcturus> Note that the latter possibility becomes more likely in the case where the information you're attempting to provide is more widely available
[13:07:08] *** Joins: Mark_ (~mkosmatri@2001:470:69fc:105::625)
[13:07:08] *** Joins: enterprisey (~enterpris@user/enterprisey)
[13:07:08] *** Joins: Guest1911 (~lucentmat@2001:470:69fc:105::dec)
[13:08:06] <Obormot\Arcturus> i.e., the likelier it is that any given victim^W target has already encountered some moral claim, the likelier it therefore is that their indignation is a result of disagreement and offense rather that cognitive dissonance
[13:09:19] <brigand> i wasn't even talking about this room
[13:09:24] <Obormot\Arcturus> And in today's age of pervasive "activism", it is very likely indeed that any given person has already encountered any of the sort of moral claims that someone has in mind given that they believe becoming a better person means activism
[13:09:30] <Obormot\Arcturus> brigand: I know you weren't
[13:10:34] <Obormot\Arcturus> What I say of course applies here, that is a no-brainer, my point rather was that it also applies in the wider world
[13:11:07] <brigand> so activism bad. got it
[13:11:30] <Obormot\Arcturus> That's not what I said and I think you know that
[13:11:53] <Obormot\Arcturus> Although most activism is, indeed, bad (but primarily for reasons other than what I've said just now)
[13:14:06] <brigand> what does "more excellent, surpassing your limitations" mean then? coding more?
[13:14:26] <Obormot\Arcturus> I am not saturn2 sa I can't say what he meant by that
[13:15:04] <Obormot\Arcturus> (But surely doing more of something bad can hardly constitute becoming better or more excellent etc.)
[13:15:14] <brigand> what does "becoming a better person" mean then?
[13:15:47] <Obormot\Arcturus> Again, I couldn't say what saturn2 meant by it...
[13:16:56] <Obormot\Arcturus> If you just want to know what I think becoming a better person means, then I'd say it's pretty straightforward - becoming more conscientious, more courageous, more loyal, more honest, more capable, more erudite...
[13:17:59] <Obormot\Arcturus> We need hardly reinvent the concept of virtue, we've had literally thousands of years to invent it in the first place, and it needs little refurbishing
[13:18:37] *** Quits: Mark_ (~mkosmatri@2001:470:69fc:105::625) (Changing host)
[13:18:37] *** Joins: Mark_ (~mkosmatri@user/mark/x-9597255)
[13:19:24] <brigand> we don't need to work on virtue. got it
[13:19:44] <Obormot\Arcturus> ... are you intentionally caricaturing what I'm saying or what
[13:20:45] <brigand> i guess i misunderstood. what does "reinvent the concept of virtue" mean? who wants to "reinvent the concept of virtue"?
[13:21:41] <quanticle> Oh god, I can't sleep
[13:21:56] <quanticle> dbohdan: Hah, that pun was so stealthy, even I didn't know I made it
[13:22:01] <Obormot\Arcturus> What I'm saying is that "what does it mean to become a better person" is not a new question. It's the central question of virtue ethics. And virtue ethics is very old, and very well-trod ground, philosophically speaking
[13:22:14] <feep[work]> dbohdan2: but it's so good!
[13:22:47] <brigand> Obormot\Arcturus: ah, i see
[13:22:53] <Obormot\Arcturus> So when you ask "what does it mean to become a better person", my answer is basically "the usual" (with the caveat that it's "what *was* the usual before the successor ideology started reinventing it")
[13:23:44] <brigand> you speak in riddles
[13:24:41] <feep[work]> dbohdan2: to be fair, I'm a total sucker for stylish fight scenes and good music
[13:25:02] <feep[work]> also to be fair, maybe 20% the reason I love it is lio's amazing omake rant in Maybe I'm A Lion, hang on, lemme find a link
[13:25:38] <feep[work]> dbohdan2: https://forums.nrvnqsr.com/showthread.php/1870-Maybe-I-m-a-Lion-%28KnK-Prototype-Crossover%29?p=1190321&viewfull=1#post1190321
[13:25:40] <Robomot> Maybe I'm a Lion (KnK/Prototype Crossover) - Page 48 (The video game, not Fate/Prototype. …)
[13:25:41] <Obormot\Arcturus> brigan: Hardly... it's just impractical to recapitulate long chains of reasoning in response to casual chatroom comments. One can offer pointers, at best...
[13:25:54] <Obormot\Arcturus> *brigand
[13:25:57] <kuudes> just my 2c: "<saturn2> have you tried being a better person?" was satire
[13:26:49] <Obormot\Arcturus> (Of course, it's not clear that we would *want* that to be practical; cf. https://wiki.obormot.net/Archive/StarDiariesTwentyFirstVoyage )
[13:26:50] <Robomot> The Star Diaries: The Twenty-First Voyage — OborWiki
[13:28:07] *** Joins: Dyo (~dyo@user/dyo)
[13:29:26] <brigand> Obormot\Arcturus: does that page answer what the "successor ideology" is? how it reinvented the concept of virtue? and what the new meaning of what it means to become better means now?
[13:29:37] <Obormot\Arcturus> brigand: Not even slightly
[13:31:19] <feep[work]> I subscribe only to failuror ideologies
[13:31:23] <brigand> so more riddles?
[13:31:45] <feep[work]> <brigand> No more riddles. No more jests...
[13:33:03] <adiabatty> brigand: do you not know what Wesley Yang refers to when he talks about the successor ideology?
[13:33:36] <Obormot\Arcturus> brigand: "Behind our efforts, let there be found our efforts."
[13:33:48] <brigand> adiabatty: oh, i see https://en.wikipedia.org/wiki/Successor_ideology thanks
[13:33:48] <Robomot> Successor ideology - Wikipedia
[13:34:38] <shawwwn> it finally happened. I have a GPU on my laptop, in tensorflow
[13:34:46] <shawwwn> Apple made Christmas finally come
[13:34:52] <kuudes> adiabatty, I did not know that either
[13:34:53] <shawwwn> (along with me, almost.)
[13:34:54] <kuudes> who is yang
[13:35:09] <kuudes> some american writer, ok
[13:36:27] <adiabatty> other people will summarize him way differently, but like 10 years ago he was some whiny asian guy who (I assume) couldn't get laid and was way better at writing than at anything math or science, so he got his first 15 minutes of fame for basically whining about being an asian guy
[13:36:49] <kuudes> ok
[13:37:21] <adiabatty> somewhere along the line, he started getting into Jordan B. Peterson and more time passed and now he's married with a kid and writes for Tablet (some Jewish magazine, oddly enough) and he's more well-adjusted now and he's anti-woke left
[13:37:23] <Obormot\Arcturus> (I know even less about this Yang fellow than adiabatty, fwiw)
[13:37:34] <kuudes> yeah, he sounds peterson adjacent
[13:37:52] <adiabatty> his Twitter account is an absolute gem: https://twitter.com/wesyang
[13:38:16] <adiabatty> and he posts frequently
[13:38:40] <kuudes> I guess people like to read different things
[13:39:25] <quanticle> My Twitter is literally: 1. Cats 2. Russian military equipment 3. Robin Hanson
[13:39:32] <quanticle> I have a *great* Twitter
[13:39:37] <brigand> getting well-adjusted is also part of becoming better, i would say
[13:39:37] <quanticle> If I say so myself
[13:41:29] <Obormot\Arcturus> quanticle: Hanson is Hanson, but surely the first two of those must intersect at times? Have you any cats on or in or otherwise interacting with Russian military equipment?
[13:41:53] <adiabatty> are there any catgirls in Girls und Panzer?
[13:42:42] <feep[work]> asking the hard questions
[13:43:02] <quanticle> Obormot\Arcturus: Good question. I haven't seen any yet, but I will be sure to keep an eye out.
[13:43:02] <adiabatty> What, you don't know the answer to that one off the top of your head?
[13:43:17] <feep[work]> I don't watch the show
[13:45:07] *** Quits: adiabatty (~adiabatic@user/adiabatic) (Remote host closed the connection)
[13:45:27] <Obormot\Arcturus> brigand: I would say that "well-adjusted" is "better" to the extent that it is in the service of something else, something worthy in itself... and not otherwise. One may adjust to all sorts of things, after all...
[13:45:42] <quanticle> Obormot\Arcturus: https://twitter.com/RALee85/status/1300534438024151045 Here you go
[13:45:44] <|dbotdan> Rob Lee (@RALee85, 2020-08-31 20:42): ‘The always dangerous rocket-propelled cat.’ Images: https://nitter.exonip.de/pic/media%2FEgxtAfQXYAI2EWv.jpg%3Fname%3Dorig
[13:45:46] <kuudes> adiabatty melted down on someone not watching girls und panzer
[13:46:25] <Obormot\Arcturus> quanticle: Excellent
[13:48:16] <quanticle> Obormot\Arcturus: I'm sure you already have this calendar: https://twitter.com/RALee85/status/1103696478772568064
[13:48:16] <|dbotdan> Rob Lee (@RALee85, 2019-03-07 16:38): ‘Russian spetsnaz, in this case from Rosgvardia, also hold a dominant position among international SOF in cat photos.’ Images: https://twitr.gq/pic/media%2FD1EeGBmX0AAok6t.jpg%3Fname%3Dorig https://twitr.gq/pic/media%2FD1EeGBqWoAE72n4.jpg%3Fname%3Dorig https://twitr.gq/pic/media%2FD1EeGBlW0Ag44r7.jpg%3Fname%3Dorig
[13:48:16] <|dbotdan> https://twitr.gq/pic/media%2FD1EeGBoX4AA3jYc.jpg%3Fname%3Dorig
[13:49:00] <quanticle> 10/10 propaganda, would be propagandized again
[13:49:30] <Obormot\Arcturus> That is indeed excellent
[13:51:40] <quanticle> Spetznaz? More like Spetznyas, amirite
[13:53:26] <Betawolf> If only one of those guys was Hanson.
[13:53:59] <quanticle> Somehow, I don't think that Robin Hanson will be joining the Russian special forces any time soon.
[13:54:06] <quanticle> But you never know!
[13:54:25] <quanticle> Maybe one day one of his ems will be in charge of conducting psyops or something
[14:01:05] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[14:05:40] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 268 seconds)
[14:06:38] <dbohdan> feep[work]: I agree it *looks* very good.  I don't know, maybe it's because I'd read the whole of _Fate/stay night Realta Nua_ (every Tiger Dojo, 100% completion) before watching it and had had enough of Nasu's storytelling.
[14:06:52] <dbohdan> The music is pretty nice, too
[14:07:04] <feep[work]> like
[14:07:13] <feep[work]> if you have enough of nasu then knk is ... yeah :p
[14:07:25] <quanticle> KNK?
[14:07:34] <feep[work]> kara no kyoukai
[14:07:56] <quanticle> Ah. The only K?K show I'm familiar with is KLK (Kill La Kill)
[14:08:04] <quanticle> Which is very good, but also very ridiculous
[14:08:12] <feep[work]> it's prototypical nasuverse
[14:08:31] <feep[work]> bullshit high concept plot with an incredibly op main character
[14:08:54] <quanticle> The main character in Fate Stay/Night isn't that OP, at least not at the beginning...
[14:09:05] <dbohdan> feep[work]: I do want to give the _Fate_ LNs a chance some time: _Fate/Zero_ because it's written by Gen Urobuchi, _Fate/strange fake_ because the plot summary sounds like it mixes up the elements in an interesting way, and _Fate/Apocrypha_ because Astolfo is cute
[14:09:39] <feep[work]> good taste :) note that I haven't read them tho
[14:09:59] <feep[work]> quanticle: the mc in knk has kill: yes
[14:10:27] <feep[work]> if you put her in worm, I'd give her passable chances against scion.
[14:10:30] <quanticle> Fate/Zero is cool because of Iskander (a.k.a. Broskander). Everyone else is all serious, all the time, meanwhile Broskander is like, "Fuck yeah, let's get smashed and take over the world!"
[14:11:13] <quanticle> Literally the first thing he does when he's summoned is ask for a world map. He's like, "Huh, the world is a lot bigger than I thought it was. Great! More conquest fun!"
[14:11:43] <brigand> iscandar?
[14:12:29] *** Quits: superz (~superegg@user/superegg) (Ping timeout: 272 seconds)
[14:13:08] <quanticle> I guess? He's better known in the West as Alexander (The Great)
[14:13:26] <quanticle> Iskander is his name in Persia (and points east, such as Afghanistan)
[14:13:34] <brigand> nah, iscandar is from another anime
[14:17:15] <quanticle> Oh
[14:24:14] *** Quits: inky (~inky@141.136.79.172) (Ping timeout: 245 seconds)
[14:27:29] <PapuaHardyNet> Fate/Zero is very enjoyable because of Waver, yes. 
[14:27:43] <PapuaHardyNet> (and Iskandar too)
[14:28:22] <PapuaHardyNet> But I really enjoyed mage killer's strategies against the mages. That was my favorite bit
[14:28:49] <quanticle> Yeah, I liked Waver because he started out as an arrogant whiny little bitch... and then became progressively less and less of one as the series went on. 
[14:49:35] <dbohdan2> feep[work]: https://github.com/cosmos72/twin/  Looks kinda like your IRC client
[14:49:36] <Robomot> GitHub - cosmos72/twin: Text mode window environment. A "retro" program for embedded or remote systems, that doubles as X11 terminal and text-mode equivalent of VNC server (Text mode window environment. A "retro" program for embedded or remote systems, that doubles as X11 terminal and text-mode equivalent of VNC server - cosmos72/twin)
[14:56:26] <Betawolf> I also thought Iksander was Alexander.
[14:57:49] <quanticle> It is
[14:59:57] *** Joins: badsektor (~badsektor@user/badsektor)
[15:02:55] <Betawolf> a brigand and a liar
[15:06:32] <kuudes> ?
[15:07:31] *** Quits: brigand (~brigand@user/brigand) (Read error: Connection reset by peer)
[15:10:04] *** Quits: niko (~niko@libera/staff/niko) (Remote host closed the connection)
[15:31:21] *** Joins: inky (~inky@5.77.129.235)
[15:50:28] *** Joins: brigand (~brigand@user/brigand)
[16:02:05] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[16:06:58] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[16:38:04] <Gurkenglas_> How literal is working memory? Does each of the slots carry the address of a neuron? Can one count the slots? Can one write their contents on a piece of paper?
[16:38:07] *** Gurkenglas_ is now known as Gurkenglas
[16:39:56] *** Quits: inky (~inky@5.77.129.235) (Ping timeout: 264 seconds)
[16:41:22] *** Quits: V (~v@anomalous.eu) (Quit: We're here. We're queer. Connection reset by peer)
[16:41:52] *** Joins: V (~v@anomalous.eu)
[16:56:15] *** Joins: inky (~inky@141.136.79.172)
[17:07:54] *** Joins: two2thehead (~AVX0@209.212.217.186)
[17:16:35] *** Joins: superz (~superegg@user/superegg)
[17:27:40] <wrycode> I'm doing a review of the best essays on SRS with personal focus on practical advice. So far I've looked at Gwern's essay and "Augmenting Long-term Memory" by Micheal Nielsen. I know there are a TON of other blog posts, does anyone recommend one or two in particular?
[17:29:21] *** Joins: LeoTal (~LeoTal@166.170.86.79.rev.sfr.net)
[17:34:04] <wrycode> Oh I guess Piotr Woźniak's articles are pretty great for this
[17:38:01] *** Joins: paul473838 (~AndChat72@2607:fb90:c230:cbac:2efd:5b6d:f98d:2027)
[17:38:13] *** Quits: paul473838 (~AndChat72@2607:fb90:c230:cbac:2efd:5b6d:f98d:2027) (Client Quit)
[17:38:15] *** Quits: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com) (Read error: No route to host)
[17:38:31] *** Joins: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com)
[17:39:56] <capisce> wrycode: maybe https://www.jackkinsella.ie/articles/janki-method-refined
[17:39:59] <Robomot> Janki Method Refined | Jack Kinsella (Tips, shortcuts and revisions to the original method)
[17:48:04] <wrycode> capisce: thanks
[18:01:07] <gwern> https://www.reddit.com/r/evangelion/comments/nvrzzg/self_my_asuka_langley_cosplay/ more evidence all you need to be a cosplayer is to be (a) thin and (b) have a triangle-shaped face with pointy chin
[18:01:09] <Robomot> [Self] My Asuka Langley cosplay : evangelion (192k members in the evangelion community. God's in his heaven. All's right with the world.)
[18:02:45] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[18:04:40] <gwern> https://twitter.com/MasterTimBlais/status/1402540650545958913 [2 astronaut meme]
[18:04:42] <|dbotdan> Tim � (@MasterTimBlais, 2021-06-09 08:18): ‘currently wondering to what extent "geoengineering cannot substitute for emissions reduction" is the "masks don't work" of climate change’
[18:06:59] <gwern> https://twitter.com/sympathetic_opp/status/1402397093713547268
[18:07:00] <|dbotdan> sympathetic opposition (@sympathetic_opp, 2021-06-08 22:48): ‘my dad lost his wedding ring at the beach a cpl yrs ago & far from being mad my mom was blushing & laughing at the dirty looks she got from other women when she & my dad held hands, went to dinner, etc etc while waiting for the new ring’
[18:07:32] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[18:12:14] *** Joins: Gurkenglas_ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de)
[18:12:59] <gwern> https://twitter.com/nickcammarata/status/1402473982809542659
[18:12:59] <|dbotdan> Nick Cammarata (@nickcammarata, 2021-06-09 03:53): ‘It does feel a bit strange that my main learning source every day is a giant neural network impersonating long-dead scientists who write me custom essays on topics I’m curious about’
[18:15:33] *** Quits: Gurkenglas (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de) (Ping timeout: 252 seconds)
[18:16:42] <nshepperd> 'Solar radiation management is basically a big mask around the world'
[18:17:37] <gwern> https://twitter.com/wrathofgnon/status/1024212222824726528/photo/1 https://twitter.com/alexhern/status/1402559486838071300
[18:17:38] <|dbotdan> Wrath Of Gnon (@wrathofgnon, 2018-07-31 08:36): ‘It turns out modernist architecture even created unlivable habitats for zoo animals. @johndurant in "The Paleo Manifesto" describes Georgian-born architect Berthold Lubetkin's famous 1932 Penguin Pool at London Zoo. One wonders what modernist arhictecture does for human habitats?’ Images:
[18:17:38] <|dbotdan> https://nitter.pussthecat.org/pic/media%2FDja7R4RVsAE26j2.jpg%3Fname%3Dorig https://nitter.pussthecat.org/pic/media%2FDja7UaYUwAAma3M.jpg%3Fname%3Dorig https://nitter.pussthecat.org/pic/media%2FDja7V0mU4AI9edU.jpg%3Fname%3Dorig
[18:17:38] <|dbotdan> alex hern (@alexhern, 2021-06-09 09:33): ‘it is perennially funny to me that the London Zoo penguin pool is grade I listed, meaning that they legally have to maintain it in good upkeep and can't do structural alterations, and also kills penguins, meaning that they can't use it as a penguin pool’
[18:19:28] <gwern> https://twitter.com/minimaxir/status/1402475849819967489
[18:19:28] <|dbotdan> Max Woolf (@minimaxir, 2021-06-09 04:01): ‘PICKLE RICK’ Images: https://nitter.moomoo.me/pic/media%2FE3aYNoyVUAEoU4p.png%3Fname%3Dorig
[18:21:44] <nshepperd2> wonder if i can use 'decision transformer go brr' approach to make danbooru-dalle create images that match the prompt better
[18:22:06] <gwern> possibly. if you have scoring information from somewhere to prefix?
[18:22:11] <dbohdan> 'It's why zoo directors have a long-running joke about the most dangerous animal in the zoo: the architect.'
[18:22:19] <nshepperd2> add various amounts of noise to the clip prompt during training, then sample with extrapolated negative noise?
[18:22:57] <gwern> like you saw wosshisname's architecture dall-e? he has downstream programs which score each architectre blueprint by thermal efficiency, sunlight, etc. he asked how to use that - well, you just prefix them to each random floorplan and retrain!
[18:23:51] <gwern> (generate a million random floorplans, run them through the efficiency raters, prefix the n scores to their token embedding, retrain; now it's the inline metadata trick and you can control decoding of high-efficiency or high-sunlight floor plans)
[18:24:23] <PapuaHardyNet> wrycode: PM me the link when you are done writing it
[18:24:29] <PapuaHardyNet> I'd love to read it
[18:24:38] <gwern> there's also the cogview self-criticizing caption trick. you *maybe* could feed that back in as the score?
[18:24:55] <nshepperd2> well, all i really have is CLIP embeddings
[18:25:18] <gwern> maybe you could generate captions and use gpt-j to score them on likelihood?
[18:25:27] <nshepperd2> the problem i'm trying to solve is that sometimes when you give it a prompt, the sampling seems to 'fall back on priors' and generate basically a random image
[18:25:31] <gwern> low likelihood = gibberish / low quality
[18:25:45] <gwern> or wait, that's just worse than using CLIP for ranking
[18:25:55] <gwern> since CLIP can look at the actual image
[18:26:23] <gwern> why not use the clip rankings as scores?
[18:26:57] <nshepperd2> clip rankings?
[18:28:48] *** Joins: srhm (~ascii@user/srhm)
[18:29:13] <nshepperd2> basically i want to do contrastive learning to force the generated image to have a clip embedding close to the prompt, but that's impossible on a LM
[18:29:14] <gwern> yeah, can't you do what OA did for dall-e?
[18:29:43] <gwern> like, generate 50 samples for a prompt; rank with CLIP; prefix 1-50 as the 'score'; retrain
[18:29:49] <gwern> then decode with '1|'
[18:30:13] <gwern> * the prefixing/retraining/decoding is not what OA did
[18:31:08] <RiversHaveWings> gwern: isn't that going to take forever to train though
[18:32:46] <RiversHaveWings> this sort of consideration is why i went with the clipgan design :/
[18:33:00] <nshepperd2> ...i suppose maybe i should start by just attempting to train with image embeddings only again
[18:33:20] <nshepperd2> now that i've got the normalization right
[18:34:28] <RiversHaveWings> what if you put both the text and image embeddings
[18:34:36] <nshepperd2> my dataset is currently (clip(image), tokens(image)) as well as (average(clip(tags)), tokens(image))
[18:34:37] <RiversHaveWings> and then to sample you put the same text embedding twice
[18:34:54] <nshepperd2> the tag embeddings are presumably much worse quality
[18:35:08] <nshepperd2> that's an interesting idea
[18:35:27] <RiversHaveWings> you could also use it to sample outputs that had both a text and an image prompt
[18:35:55] *** Joins: inara` (~inara@static.38.6.217.95.clients.your-server.de)
[18:36:51] <RiversHaveWings> i'm trying a non-contrastive method rn on a 2-gpu box on the side but i don't expect much to come of it really
[18:37:03] <RiversHaveWings> (it still requires you to get clip embeddings for the outputs)
[18:37:14] <RiversHaveWings> well, it's sort of contrastive
[18:37:40] <RiversHaveWings> it matches an input text embedding to an output image embedding
[18:37:46] <RiversHaveWings> and it also has a repulsion targeting term
[18:38:54] *** Joins: ptrcmd_ (~ptrcmd@user/ptrcmd)
[18:39:35] <RiversHaveWings> that computes the pairwise squared spherical distances between everything in the input text embedding batch
[18:39:42] <RiversHaveWings> and between everything in the output image embedding batch
[18:39:57] <RiversHaveWings> and tries to match their means
[18:40:34] *** Quits: brigand (~brigand@user/brigand) (*.net *.split)
[18:40:34] *** Quits: wilrnh (~wilrnh@235.83.74.34.bc.googleusercontent.com) (*.net *.split)
[18:40:35] *** Quits: ptrcmd (~ptrcmd@user/ptrcmd) (*.net *.split)
[18:40:35] *** Quits: lucerne (~thelounge@ip202.ip-51-178-215.eu) (*.net *.split)
[18:40:35] *** Quits: inara (~inara@static.38.6.217.95.clients.your-server.de) (*.net *.split)
[18:40:40] <Gurkenglas_> Is there a futures market for GPUs? Ideally it would somehow refer to the consumer-grade model of median price
[18:40:46] <gwern> RiversHaveWings: I suppose. it is embarassingly parallel, however
[18:40:55] <Gurkenglas_> (like three years hence)
[18:41:00] *** Gurkenglas_ is now known as Gurkenglas
[18:41:14] <gwern> Gurkenglas: I've never heard of one
[18:41:29] <RiversHaveWings> gwern: actually i managed to work out an embarassingly parallel contrastive method
[18:41:49] <RiversHaveWings> I think this is worse but I'm trying it anyway
[18:41:51] <gwern> RiversHaveWings: you may also have the information already. like, OA got the ranking info. it just then was thrown away
[18:42:07] <RiversHaveWings> oh, you mean training with scores?
[18:42:38] <Gurkenglas> hmm. i hear gpu production is bottlenecked on raw materials, right? maybe a futures market for those?
[18:43:15] <RiversHaveWings> i mean. i have a method to produce grids of vqgan tokens optimized for a particular clip prompt but it's really slow if you want to make a training set with it
[18:43:31] <RiversHaveWings> two methods
[18:43:55] <Gurkenglas> what bottlenecks the emergence of new gpu producers? economies of scale? trade secrets?
[18:44:12] <RiversHaveWings> mb you could filter your dataset only to include pairs where the text and image encodings match particularly well, actually
[18:46:39] <gwern> I thought the bottleneck was the chip fabs themselves just being very time & capital intensive to setup and the shortages being 'everything'. I've never heard about, say, 'oh the raw silicon is out for the year'
[18:49:38] <gwern> https://www.biorxiv.org/content/10.1101/2020.12.02.403477v3
[18:49:40] <Robomot> Thinking ahead: spontaneous prediction in context as a keystone of language in humans and machines | bioRxiv (Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). These models are trained to generate appropriate linguistic responses in a given context using a self-supervised prediction task. …)
[18:49:57] *** Joins: brigand (~brigand@user/brigand)
[18:50:30] <nshepperd2> maybe instead of just generating one clip embedding for each dataset image i could apply clip on different translations then save the variance of the results as a measure of 'robustness'
[18:50:49] <nshepperd2> the rank images by that
[18:51:24] *** Joins: wilrnh (~wilrnh@235.83.74.34.bc.googleusercontent.com)
[18:51:37] <nshepperd2> dunno if that would actually translate to the generations matching better
[18:51:49] <TheWhisper> https://fuse.wikichip.org/news/5531/amd-3d-stacks-sram-bumplessly/ and colorless green ideas sleep furiously too!
[18:51:50] <Robomot> AMD 3D Stacks SRAM Bumplessly – WikiChip Fuse (AMD recently unveiled 3D V-Cache, their first 3D-stacked technology-based product.  Leapfrogging contemporary 3D bonding technologies, AMD jumped directly into advanced packaging with direct bonding and an order of magnitude higher wire density.)
[18:54:27] <feep[work]> coprocessors are back boyys
[19:00:06] <RiversHaveWings> nshepperd2: hm https://twitter.com/tehqin17/status/1402427333328334848
[19:00:07] <|dbotdan> Matt Fontaine (@tehqin17, 2021-06-09 00:48): ‘Excited to share some recent work with @snikolaidis19 on differentiable quality diversity (DQD). | pdf: https://arxiv.org/pdf/2106.03894.pdf  | abs: https://arxiv.org/abs/2106.03894  | code: https://github.com/icaros-usc/dqd’ Images: https://nitter.mailstation.de/pic/media%2FE3ZkN03VIAMnfjA.jpg%3Fname%3Dorig
[19:00:39] *** Quits: feep[work] (~mathis@217.64.163.97) (Ping timeout: 252 seconds)
[19:00:55] <RiversHaveWings> I don't like that their CMA-ES type method requires you to store a covariance matrix whose size is quadratic in the number of dimensions
[19:01:09] <RiversHaveWings> I think everyone forgot about LM-CMA
[19:01:27] <RiversHaveWings> (Which is to CMA-ES what L-BFGS is to BFGS)
[19:02:03] <RiversHaveWings> (i.e. it stores a low rank approximation to the big covariance matrix so you can actually use it for really high dimensional problems)
[19:13:49] <two2thehead> kuudes, s0ph1a feep https://twitter.com/pattymo/status/1402015828535562248
[19:13:50] <|dbotdan> Patrick Monahan (@pattymo, 2021-06-07 21:33): ‘Every mega rich guy has been or is becoming super interested in ways to get off of the planet. Seems fine’
[19:14:54] *** Quits: srhm (~ascii@user/srhm) (Quit: Konversation terminated!)
[19:15:35] *** Joins: srhm (~ascii@user/srhm)
[19:16:50] <gwern> https://twitter.com/ID_AA_Carmack/status/1402496694940536834 plus angel investment. he had the cashflow and costs were still low back then
[19:16:51] <|dbotdan> John Carmack (@ID_AA_Carmack, 2021-06-09 05:24): ‘Much is made of the serendipity of proximity in Silicon Valley. A while back there was a loose collection of companies sometimes referred to as the "Dallas Gaming Mafia". In hindsight, I could have been a more positive force if I got out of hermit mode and socialized some.’
[19:20:47] <feep> mew~
[19:20:53] <feep> okay yeah, ~20m for the ride home
[19:35:53] * gwern uploads https://www.gwern.net/docs/iq/2021-vonstumm.pdf
[19:35:57] <Robomot> PDF (5 pages; 199 KB) - Using DNA to predict intelligence (Sophie von Stumm) - behavioral genetics
[19:37:22] <Robomot> [Less Wrong [frontpage]] Book review: “Feeling Great” by David Burns by Steven Byrnes - https://www.greaterwrong.com/posts/jqTeghCJ2anMHPPjG/book-review-feeling-great-by-david-burns
[19:39:21] <ggreer> http://welcometosweden.blogspot.com/2014/10/moving-to-sweden-laundry-room.html Sweden doesn’t have in unit laundry? You have to schedule your laundry times? Wtf
[19:39:21] <Robomot> A Swedish American in Sweden: Moving to Sweden – The Laundry Room (Bruce Springsteen woke me up this morning at 6:55.  The screen door slams  Mary's dress waves  Like a vision she dances across the porch  As...)
[19:42:04] <gwern> 'Today, we’re announcing Firmina, an open subsea cable being built by Google that will run from the East Coast of the United States to Las Toninas, Argentina, with additional landings in Praia Grande, Brazil, and Punta del Este, Uruguay. Firmina will be the longest cable in the world capable of running entirely from a single power source at one end of the cable if its other power source(s)...
[19:42:10] <gwern> ...become temporarily unavailable—a resilience boost at a time when reliable connectivity is more important than ever.' https://cloud.google.com/blog/products/infrastructure/announcing-the-firmina-subsea-cable subtle commentary about latin america there. it can be powered by 1 landing... and there's exactly 1 landing outside latin america, in the USA ie "we won't let latin america fuck up...
[19:42:11] <Robomot> Google Cloud Blog - News, Features and Announcements
[19:42:16] <gwern> ...the cable with rolling outages or whatever dumb third world shit their tinpot dictators get up to"
[19:42:40] <dbohdan> feep: I didn't notice until now Zack M. Davis got so mad at you in a later comment on the longpost.  Getting him to use the word "motherfucker" for the first time on the blog is some kind of achievement.  http://unremediatedgender.space/2021/May/sexual-dimorphism-in-the-sequences-in-relation-to-my-gender-problems/#isso-212
[19:42:40] <Robomot> Sexual Dimorphism in Yudkowsky's Sequences, in Relation to My Gender Problems ★ The Scintillating But Ultimately Untrue Thought (I'll write my way out Write everything down, far as I can see I'll write my way out Overwhelm them with honesty This is the eye of the hurricane This is the only way I can protect my legacy —)
[19:43:37] *** Joins: galambo_ (galambo@user/galambo)
[19:44:11] *** Joins: Gurkenglas_ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de)
[19:44:13] *** Quits: two2thehead (~AVX0@209.212.217.186) (Read error: Connection reset by peer)
[19:45:35] *** Joins: Gurkenglas__ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de)
[19:46:40] *** Joins: badsektur (~badsektor@user/badsektor)
[19:47:11] *** Joins: nshepperd24 (~nshepperd@li364-218.members.linode.com)
[19:48:45] *** Joins: superz_ (~superegg@user/superegg)
[19:49:55] *** Joins: Lord_of_Life_ (~Lord@user/lord-of-life/x-2819915)
[19:49:55] *** Joins: oxide_ (~lambda@user/oxide)
[19:50:17] *** Joins: kanzure_ (~kanzure@user/kanzure)
[19:50:23] *** Joins: quanticle_ (~quanticle@user/quanticle)
[19:50:28] *** Joins: edef_ (edef@panther.nathan7.eu)
[19:53:15] *** Joins: two2thehead (~AVX0@209.212.217.186)
[19:54:16] *** Joins: plutonic_ (~user@130.44.155.145)
[19:54:58] *** Quits: Gurkenglas_ (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de) (*.net *.split)
[19:55:00] *** Quits: srhm (~ascii@user/srhm) (*.net *.split)
[19:55:07] *** Quits: Gurkenglas (~Gurkengla@dslb-088-075-022-175.088.075.pools.vodafone-ip.de) (*.net *.split)
[19:55:09] *** Quits: superz (~superegg@user/superegg) (*.net *.split)
[19:55:09] *** Quits: inky (~inky@141.136.79.172) (*.net *.split)
[19:55:12] *** Quits: badsektor (~badsektor@user/badsektor) (*.net *.split)
[19:55:15] *** Quits: kanzure (~kanzure@user/kanzure) (*.net *.split)
[19:55:21] *** Quits: Obormot_\Gaia (~obormot@user/obormot) (*.net *.split)
[19:55:21] *** Quits: quanticle (~quanticle@user/quanticle) (*.net *.split)
[19:55:22] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (*.net *.split)
[19:55:25] *** Quits: oxide (~lambda@user/oxide) (*.net *.split)
[19:55:26] *** Quits: PlanckWalk (~Planck@user/planckwalk) (*.net *.split)
[19:55:27] *** Quits: edef (edef@user/edef) (*.net *.split)
[19:55:27] *** Quits: mst (~matthewt@vps-6c71e929.vps.ovh.net) (*.net *.split)
[19:55:27] *** Quits: galambo__ (galambo@user/galambo) (*.net *.split)
[19:55:29] *** Quits: nshepperd2 (~nshepperd@li364-218.members.linode.com) (*.net *.split)
[19:55:33] *** Quits: plutonic (~user@130.44.155.145) (*.net *.split)
[19:55:33] *** nshepperd24 is now known as nshepperd2
[19:55:33] *** Lord_of_Life_ is now known as Lord_of_Life
[19:55:47] *** Joins: PlanckWalk (~Planck@user/planckwalk)
[19:56:26] *** Quits: badsektur (~badsektor@user/badsektor) (Remote host closed the connection)
[19:57:43] * nshepperd2 implements incremental generation for minGPT
[19:58:00] *** Quits: kanzure_ (~kanzure@user/kanzure) (Quit: leaving)
[19:58:19] <nshepperd2> now i can sample four images in 10 seconds, instead of 80 seconds
[19:58:29] *** Joins: kanzure (~kanzure@user/kanzure)
[19:59:10] <feep> dbohdan: oh wow, I didn't even see that
[19:59:48] *** Joins: mst (~matthewt@vps-6c71e929.vps.ovh.net)
[20:02:20] *** Joins: alampridius (~alampridi@user/alampridius)
[20:03:47] *** Joins: inky (~inky@141.136.79.172)
[20:03:48] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[20:08:05] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 244 seconds)
[20:09:07] <RiversHaveWings> nshepperd2: so
[20:09:21] <feep> dbohdan: replied
[20:09:22] <RiversHaveWings> nshepperd2: what if, instead of conditioning on a text embedding and an image embedding
[20:09:36] <RiversHaveWings> you conditioned on a text embedding and the cosine sim between that text embedding and the real image
[20:10:02] <RiversHaveWings> Then when you sample you just explicitly tell it to produce well-matching outputs (:
[20:10:46] <RiversHaveWings> i am unsure if this is exactly what gwern was proposing on EleutherAI (i think it isn't) but it may help
[20:11:00] <nshepperd2> hmm
[20:11:27] <nshepperd2> could work!
[20:11:53] <RiversHaveWings> it's probably better than filtering the dataset to only train on well-matching text/image pairs
[20:12:09] <RiversHaveWings> because this way it still learns what coherent images look like from the negative examples
[20:12:16] <nshepperd2> so that would be the cosim between the image embedding and the summed bag of tags in this case
[20:12:19] <RiversHaveWings> yes
[20:14:35] <feep> dbohdan: important sentence that I hadn't realized before
[20:14:44] <feep> > ... In that sense the movement to get people to pack trans women into the women category is almost equivalent to a kind of redistributive taxation.
[20:15:13] <feep> I think this is a fruitful lens for a lot of unstated leftist thought.
[20:15:20] <ggreer> feep: that's zach? you linked to a comment by the blog author
[20:15:26] <feep> ggreer: no, my reply
[20:16:14] <gwern> https://twitter.com/annadgoldie/status/1402644252320886786 https://twitter.com/Azaliamirh/status/1402644571872432128 ehhhhhh
[20:16:16] <|dbotdan> Anna D Goldie (@annadgoldie, 2021-06-09 15:10): ‘Excited to share that our work has been published in Nature! Our RL agent generates chip layouts in just a few hours, whereas human experts can take months. These superhuman AI-generated layouts were used in Google's latest AI accelerator (TPU-v5)! https://www.nature.com/articles/s41586-021-03544-w’
[20:16:16] <|dbotdan> Azalia Mirhoseini (@Azaliamirh, 2021-06-09 15:11): ‘Thrilled to announce that our work on RL for chip floorplanning was published in Nature & used in production to design next generation Google TPUs, with potential to save thousands of hours of engineering effort for each next generation ASIC: | https://www.nature.com/articles/s41586-021-03544-w (1/7)’ Images:
[20:16:16] <|dbotdan> https://nitter.database.red/pic/media%2FE3aXM5LWYAUD534.jpg%3Fname%3Dorig
[20:16:47] <ggreer> https://www.usconcealedcarry.com/resources/ccw_reciprocity_map/ut-gun-laws/ 21% of utah residents have concealed pistol licenses? that seems... unusually high. in most states the number is around 5%
[20:16:48] <Robomot> Utah Concealed Carry Gun Laws & Permits USCCA CCW Reciprocity Map(Last Updated 05/19/2021) (Last Updated 05/5/2021. Concealed carry is legal in Utah with a CCW Permit. Find Utah Gun Laws, Concealed Carry Permits & Reciprocity Map.)
[20:17:09] <nshepperd2> apparently kde has a maximum trash size, of about 180GB
[20:17:10] <shawwwn> RiversHaveWings: how’d de wei go? I mean de run?
[20:17:13] <gwern> RiversHaveWings: I thought that was how the dall-e ranking worked? you took the original prompt's text-CLIP embedding, the image-CLIP embeddings of all of the DALL-E-generated images, and the 'best' was the one that matched most similarly?
[20:17:21] <feep> hey, how do I initialize a generic struct type to 0 in C?
[20:17:21] <nshepperd2> i was deleting old checkpoints
[20:17:27] <feep> I tried "= {0}" but that warns out sometimes
[20:17:39] <shawwwn> gwern: no, clip ranks images
[20:17:46] <shawwwn> It doesn’t rank embeddings
[20:17:56] <gwern> clip generates embeddings. how does it 'rank images'?
[20:18:27] <shawwwn> I don’t know, I’ve been up all night in order to ensure I’m awake for the dr appt in an hour
[20:18:28] <feep> for instance, https://onlinegdb.com/eln7l32rQ
[20:18:29] <Robomot> GDB online Debugger | Code, Compile, Run, Debug online C, C++ (Online GDB is online ide with compiler and debugger for C/C++. Code, Compiler, Run, Debug Share code nippets.)
[20:18:44] <shawwwn> So it’s likely I’m a randomly initialized model
[20:18:50] <RiversHaveWings> gwern: cosine sim between the text embedding and the image embedding, higher is better
[20:19:12] <gwern> RiversHaveWings: that sounds like what I said
[20:19:50] <RiversHaveWings> i'm confused about whether what you said also included fine-tuning on the model's own sampled outputs
[20:20:14] <gwern> well, you can only finetune after you've reprocessed into or generated a ranked dataset
[20:20:28] <gwern> you need to get the rewards from somewhere first
[20:22:14] <shawwwn> now you’ve got me all interested in how clip ranks images vs a text prompt
[20:22:19] <shawwwn> But I’m on mobile
[20:22:51] <gwern> (note that because this is 'off-policy', this works with any source of images. your original real datapoints, dall-egenerated images, GAN-generated images, doesn't matter too much. they may vary in their usefulness or informativeness, but the model just models it all)
[20:23:15] <RiversHaveWings> shawwwn: it's easy
[20:23:58] <RiversHaveWings> yeah i was worried that fine-tuning on sampled images would confuse the model into thinking that incoherent, jumbled sampled outputs were OK
[20:24:12] <RiversHaveWings> *if CLIP thought they were similar*
[20:24:38] <RiversHaveWings> or well, that it would waste model capacity modeling what its own sampled outputs looked like
[20:24:52] <gwern> well, what would happen there, if the rewards are garbage, is that conditioning on them will give you garbage also. but you can still sample from the original distribution, assuming adequate model capacity
[20:24:53] *** Joins: paul473838 (~AndChat72@2607:fb90:c23a:6b96:1b4d:141f:2e2a:7330)
[20:24:57] *** Joins: lucerne (~lucerne@ip202.ip-51-178-215.eu)
[20:25:09] <gwern> (but you will maximize your CLIP rewards! so CLIP will be happy0
[20:25:16] <RiversHaveWings> eheh
[20:25:25] <gwern> incidentally, note that the source of reward is itself more metadata
[20:25:46] <RiversHaveWings> I already do that with the codebook sampling method actually, and the outputs are OK, but a transformer model should be able to better than imitate that
[20:26:02] <RiversHaveWings> To do better
[20:26:07] <gwern> if you have human and clip rewards, and you are worried that clip rewards are bad compared to human rewards, you can simply include the source. 'HUMAN|+15|vae tokens...' vs 'CLIP|-5|vae tokens...'
[20:26:15] <shawwwn> Can your GAN be named CLIPPY? And have a space pilot frog as a logo?
[20:26:25] <shawwwn> Toad, not frog
[20:26:47] <gwern> if you had large-scale datasets, it wouldn't be a bad idea to include the identity of each human rater. human interrater agreement can be quite low
[20:26:50] <RiversHaveWings> well, we don't have human rewards really
[20:26:56] <gwern> *yet*
[20:27:13] <shawwwn> Perhaps even be among a crew of four anthropomorphic star fighter pilots
[20:27:21] <gwern> but if you built a Ganbooru, say, you could very easily start with mere CLIP rewards, and add in human rewards as they arrived
[20:28:01] <RiversHaveWings> What about including "this sequence of vqgan tokens was a real image" and "this sequence of vqgan tokens was a sampled output" and then when you sampled the outputs, telling it to imitate real images
[20:28:28] <gwern> sure. same point about metadata holds for the datasets. exactly the same way it's useful to include the author or original dataset in bigass text dumps
[20:29:08] *** Quits: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com) (Ping timeout: 264 seconds)
[20:29:08] <gwern> 'this one is biggan, this one is FFHQ, this one is YFC100M, this one is Cogview, this one is tadne, this one is Imagenet...'
[20:29:14] <RiversHaveWings> But yeah, I can probably improve the thing with no fine-tuning, just by conditioning on CLIP similarity as well as the text embedding
[20:30:59] <RiversHaveWings> (at the very least, it could help it deal with the dataset being kind of noisy/bad)
[20:35:16] <RiversHaveWings> ...Wait, can you just *improve a DALL-E* by conditioning on not just the text tokens but the CLIP similarity?
[20:35:43] <shawwwn> RiversHaveWings: did that noise run work out?
[20:36:06] <RiversHaveWings> shawwwn: it was kinda the same i think, my machine went away unexpectedly overnight :/
[20:36:21] <shawwwn> Ahh
[20:36:27] <RiversHaveWings> Like you don't even need to input any CLIP embeddings to the DALL-E, just the similarity.
[20:36:35] <RiversHaveWings> For each data point.
[20:39:52] <nshepperd2> how the danbooru clip 'reconstructions' are going: https://zlkj.in/tmp/vqgan/mingpt-18-dalle-l13/sample-71100.png
[20:39:55] <Robomot> image/png (1024x1024; 1.2 MB)
[20:40:16] <nshepperd2> top left is real, others are fakes generated from the clip embedding embedding with gpt
[20:40:45] <RiversHaveWings> nshepperd2: that's semi-decent
[20:41:04] <RiversHaveWings> did you condition on the image embedding for this?
[20:41:26] <nshepperd2> right after that one it did https://zlkj.in/tmp/vqgan/mingpt-18-dalle-l13/sample-71200.png
[20:41:26] <Robomot> image/png (1024x1024; 1.5 MB)
[20:41:51] <RiversHaveWings> ooh.
[20:42:07] <nshepperd2> the fakes are generated just conditioned on the image embedding of the real
[20:42:11] <RiversHaveWings> ahh.
[20:42:20] <RiversHaveWings> haven't tried substituting a text embedding in yet?
[20:43:52] <two2thehead> kuudes, s0ph1a feep : https://www.nbcnews.com/tech/tech-news/mcdonald-s-french-fries-carrots-onions-all-foods-come-bill-n1270033
[20:43:53] <Robomot> McDonald's french fries, carrots, onions: all of the foods that come from Bill Gates farmland (Microsoft founder Bill Gates invests big in farmland growing potatoes, carrots and onions)
[20:44:02] <nshepperd2> https://zlkj.in/tmp/vqgan/mingpt-18-dalle-l13/prompted-71200.png 'a planet ruled by nightmares'
[20:44:02] <Robomot> image/png (1024x1024; 1.1 MB)
[20:44:10] <two2thehead> "Though the Gateses are major owners of American farmland, the couple’s holdings only represent a fraction of the 283 million acres of farmland that is owned and rented out by nonfarmers. But that could soon change. About 40 percent of farmland is owned by seniors 65 or older, according to 2014 estimates by the U.S. Department of Agriculture, meaning more farmland is expected to come on the market soon."
[20:44:18] <RiversHaveWings> mmm.
[20:44:53] *** Quits: paul473838 (~AndChat72@2607:fb90:c23a:6b96:1b4d:141f:2e2a:7330) (Quit: Bye)
[20:45:10] *** Joins: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com)
[20:46:13] <gwern> nshepperd2: shouldn't that be tag-encoded if it's a danbooru dall-e?
[20:46:28] <dbohdan> feep: What do you mean, "a generic struct type"?  You want to initialize a struct without knowing its exact type?
[20:47:04] <gwern> like 'dystopia landscape fire demon'
[20:48:26] <two2thehead> nshepperd2, if you told me that was concept art made by a person, I'd be fooled
[20:49:57] <nshepperd2> gwern: it could be, but since it's CLIP anything should work really
[20:50:20] <feep> dbohdan: yes
[20:51:43] <nshepperd2> in c++ you go 'mystruct x = {};'
[20:52:03] <feep> sadly I use C.
[20:52:15] <nshepperd2> if that doesn't work in C i assume you just accept your fate as demons crawl out of the monitor and devour you
[20:52:21] <feep> yeah sounds bout right
[20:52:34] <dbohdan> feep: I don't think it's possible in C
[20:52:47] <feep> :-(
[20:53:31] *** Quits: edef_ (edef@panther.nathan7.eu) (Changing host)
[20:53:31] *** Joins: edef_ (edef@user/edef)
[20:53:39] <dbohdan> feep: Although https://gcc.gnu.org/onlinedocs/gcc/Designated-Inits.html says 'Omitted fields are implicitly initialized the same as for objects that have static storage duration.'
[20:53:39] <Robomot> Designated Inits (Using the GNU Compiler Collection (GCC)) (Designated Inits (Using the GNU Compiler Collection (GCC)))
[20:53:44] *** edef_ is now known as edef
[20:53:52] <feep> dbohdan: right, the problem is you can't generically fill the first field
[20:53:53] <dbohdan> So maybe try "x = {};"?
[20:54:28] <feep> gcc people say {{}}
[20:54:29] <feep> trying that
[20:54:36] <gwern> https://slatestarcodex.com/2015/08/17/the-goddess-of-everything-else-2/#comment-229308 salem was the epitome of libertarian stereotypes, but he did a great counter-poem there
[20:54:37] <Robomot> The Goddess of Everything Else | Slate Star Codex [] ([Related to: Specific vs. General Foragers vs. Farmers and War In Heaven, but especially The Gift We Give To Tomorrow] They say only Good can create, whereas Evil is sterile. Think Tolkien, where M…)
[20:57:56] <Gurkenglas__> ...neuralink is gonna give me the ability to save and restore working memory and it's going to end like my number of open tabs
[20:58:00] <RiversHaveWings> so with decision transformers you can just condition on any reward you want?
[20:58:12] <RiversHaveWings> does it still work if you make it too high when you sample?
[20:58:17] <dbohdan> gwern: "mirth"/"first"?!
[20:58:27] <gwern> it's a half-rhyme ok
[21:00:41] <Logos01> That's even a half-rhyme?
[21:01:13] <Logos01> phoneme "urf" vs "erst"
[21:02:50] *** Joins: Obormot\Gaia (~obormot@user/obormot)
[21:04:18] <weyl37355> https://twitter.com/OkieSpaceQueen/status/1402243921560883202?s=19
[21:04:19] <|dbotdan> ☆⋆Okie Space Queen ⋆☆ (@OkieSpaceQueen, 2021-06-08 12:39): ‘I own a portable planetarium and I take it to rural areas to show people the universe. I mostly visit schools but I'm cool going to churches and community events.  | Sometimes (often) I get  flat earth believers who end up in the planetarium. Here's how I handle that. 🧵1/17’
[21:04:28] *** Joins: galambo__ (galambo@user/galambo)
[21:04:37] <dbohdan> gwern: I've always liked the two dialogues Land wrote in response to this post and "Moloch".  https://web.archive.org/web/20140730163616/http://www.xenosystems.net/war-in-heaven/  https://web.archive.org/web/20150819070133/http://www.xenosystems.net/war-in-heaven-ii/
[21:04:39] <Robomot> Outside in - Involvements with reality » Blog Archive   » War in Heaven (Elua: So you saw the Scott Alexander piece? Gnon: Of course. Elua: Almost indescribably fabulous, wasn't it? Gnon: [*Hmmmph*] Elua: Always thought you had some kind of Moloch thing going on. Gnon: ...)
[21:04:39] <Robomot> Outside in - Involvements with reality » Blog Archive   » War in Heaven II (Cank: [Tap, tap] Gnon: I'm having a bath. Cank: The Hypercosmic Ocean of Death will always be there, O Greatness. Scott Alexander has released another egregore. Gnon: Really? Cank: Yes, really. She...)
[21:08:08] *** Quits: galambo_ (galambo@user/galambo) (Ping timeout: 264 seconds)
[21:09:35] *** Joins: galambo (galambo@user/galambo)
[21:10:07] *** Joins: galambo_ (~galambo@user/galambo)
[21:11:05] <nshepperd2> https://zlkj.in/tmp/vqgan/mingpt-18-dalle-l13/prompts/a%20giant%20cube%20made%20of%20water%20floating%20in%20the%20sky/prompted-72300.png cool
[21:11:05] <Robomot> image/png (1024x1024; 1.2 MB)
[21:13:12] *** Quits: galambo__ (galambo@user/galambo) (Ping timeout: 252 seconds)
[21:13:39] <dbohdan> nshepperd: Bottom right is the Rocket Town from Final Fantasy VII after a flood.  https://finalfantasy.fandom.com/wiki/Rocket_Town
[21:13:39] <Robomot> Rocket Town | Final Fantasy Wiki | Fandom (Rocket Town is a town in the Rocket Launch Pad Area in Final Fantasy VII built around the launch site of the Shinra No. 26 rocket, part of Shinra Electric Power Company's space program. Cid Highwind lives and works here. The town also features in Before Crisis -Final Fantasy VII- prequel. 1...)
[21:13:52] <dbohdan> (I keep forgetting the "2")
[21:14:03] <nshepperd2> hah
[21:14:23] *** Quits: galambo (galambo@user/galambo) (Ping timeout: 252 seconds)
[21:15:03] <nshepperd2> i really want to do these at higher resolution
[21:15:42] <nshepperd2> i feel like waifu2x upscaling isn't that good
[21:16:57] <dbohdan> Has there been progress in anime upscaling since waifu2x?  It seems everyone still uses it.
[21:17:29] <nshepperd2> however, doubling the resolution to 1024 will mean 4096 tokens
[21:19:07] <nshepperd2> which means more OOMs unless i do more complicated stuff
[21:20:02] <RiversHaveWings> nshepperd2: what if you trained an f=32 VQGAN
[21:20:36] <RiversHaveWings> Then it would be 1/4 as many tokens for a given resolution
[21:21:36] <RiversHaveWings> (The compvis people actually did train f=32 and f=64 VQGANs, they observed f=16 was the minimum for decent quality results from a transformer and so ended up using those)
[21:22:04] <nshepperd2> the minimum huh
[21:22:10] <RiversHaveWings> ...How did they generate large image actually
[21:22:34] <RiversHaveWings> Yeah they generated locally incoherent output too much with smaller downsampling ratios
[21:22:53] <nshepperd2> interesting
[21:22:54] <RiversHaveWings> They were specifically testing on faces for this
[21:23:04] <RiversHaveWings> So you could really tell when something was incoherent
[21:23:10] <nshepperd2> so f=32 might actually be better in general?
[21:23:28] <RiversHaveWings> maybe
[21:24:33] <nshepperd2> well at least i guess f=32 is unlikely to be worse than downscaled f=16
[21:24:54] <RiversHaveWings> yeah you should train it on 512x512 crops instead of 256x256
[21:24:58] <nshepperd2> if downscaling first was the best option is suppose vqgan would just do that
[21:26:11] <nshepperd2> crops?
[21:26:13] * nshepperd2 doesn't actually know how vqgan works apart from 'it compresses an image as discrete tokens'
[21:26:37] <RiversHaveWings> of the input images
[21:26:41] <RiversHaveWings> of the training set
[21:27:45] <nshepperd2> i just sort of fed 512px image files into the training loop and didn't think about it too hard
[21:29:02] <RiversHaveWings> well
[21:29:16] <RiversHaveWings> vqgan is resolution independent, it's just best if you train it on at least 256x256
[21:29:44] *** Joins: drethelin (~drethelin@096-042-035-082.res.spectrum.com)
[21:30:00] <RiversHaveWings> like gabriel_syme's wikiart vqgan was trained on random size random location crops from the wikiart dataset, downscaled to 256x256
[21:30:08] <RiversHaveWings> so it learned to represent different scales of textures
[21:30:29] <RiversHaveWings> i am not sure of the default behavior, it didn't do this at first and he had to set a flag in the yaml or something
[21:30:49] <RiversHaveWings> "vqgan is resolution independent, it's just best if you train it on at least 256x256" this goes up to 512x512 for an f=32
[21:30:53] <nshepperd2> i couldn't make any sense of the yaml bullshit and wrote my own training loop lol
[21:30:56] <RiversHaveWings> oh
[21:31:03] <Logos01> nshepperd2: Wait vqgan isn't vegan?
[21:31:19] <RiversHaveWings> so you fed in what, 512x512... downscales? center crops then downscales?
[21:31:20] <Logos01> Well maybe it is if it isn't powered by fosil fuels?
[21:31:26] <Logos01> fossil*
[21:31:44] <Logos01> (Also; wouldn't most fossil fuels actually be decayed plant matter? This is confusing.)
[21:32:12] <nshepperd2> RiversHaveWings: literally just whole images from the 512px danbooru2020 dataset
[21:32:15] <RiversHaveWings> oh
[21:32:22] <RiversHaveWings> wait, they were all square?
[21:32:25] <nshepperd2> (which themselves are scaled down images with black bars added
[21:32:28] <RiversHaveWings> oh.
[21:32:30] <RiversHaveWings> right.
[21:32:48] <RiversHaveWings> yeah that'll work, you just have to be careful that it doesn't learn jpeg artifacts
[21:33:13] <RiversHaveWings> which it will, if they're always in the same places in every training set image.
[21:34:21] <RiversHaveWings> if you feed in unaugmented images it'll learn that token patterns represent different things at different positions
[21:34:40] <RiversHaveWings> like, for the faces one, it knows about the locations features generally appear in faces
[21:35:07] <RiversHaveWings> then you may have problems if you encode things at a different resolution later. but maybe not.
[21:35:19] <nshepperd2> interesting
[21:35:28] <nshepperd2> that sounds like it'll make things harder for gpt
[21:35:35] <RiversHaveWings> feed in augmented images and it'll learn more position independent features
[21:35:51] <RiversHaveWings> well, gpt knows about positions too?
[21:35:59] <RiversHaveWings> should be able to handle it
[21:37:19] <nshepperd2> yeah but
[21:37:45] <nshepperd2> it means the relationship between tokens is more complicated? idk
[21:38:39] <RiversHaveWings> maybe
[21:38:50] <RiversHaveWings> compvis trained transformers that could output good faces
[21:40:50] <nshepperd2> well, if i do 1024px, i'll have to download the danbooru2020 originals, and scale them down myself
[21:40:55] <nshepperd2> then i can do all that
[21:41:59] <RiversHaveWings> nshepperd2: so wait, taming transformers does causal 2d sliding window attention/
[21:42:00] <RiversHaveWings> ?
[21:42:53] <nshepperd2> idk, does it?
[21:43:21] <nshepperd2> that sounds like a recipe for globally incoherent images
[21:45:45] <RiversHaveWings> ohhhh
[21:46:12] <Robomot> [Less Wrong [frontpage]] How do you establish a comfort zone in your studies? by AllAmericanBreakfast - https://www.greaterwrong.com/posts/ixeStEMsNABQmkGQp/how-do-you-establish-a-comfort-zone-in-your-studies
[21:48:26] <RiversHaveWings> nshepperd2: they also condition on image coordinates sometimes
[21:48:53] <RiversHaveWings> hm
[21:49:08] <RiversHaveWings> they only do this for high res generations
[21:49:10] <RiversHaveWings> i think
[21:49:48] <RiversHaveWings> "Our VQGAN ensures that the available context is still sufficient to faithfully model images, as long as either the statistics of the dataset are approximately spatially invariant or spatial conditioning information is available."
[21:50:02] <RiversHaveWings> so yeah, it would result in global incoherence i think
[21:50:16] <feep> gwern: https://soundcloud.com/dj-allophonix/dj-allophonix-shibayanrecords
[21:50:16] <Robomot> DJ Allophonix - ShibayanRecords mega mix by Dj Allophonix | Free Listening on SoundCloud (Stream DJ Allophonix - ShibayanRecords mega mix by Dj Allophonix from desktop or your mobile device)
[22:04:33] *** Joins: adiabatic (~adiabatic@user/adiabatic)
[22:04:57] <kuudes> woo hoo! \o/ https://www.statnews.com/2021/06/02/pandemic-upside-flu-virus-became-less-diverse-simplifying-task-of-making-flu-shots/
[22:04:58] <Robomot> Flu virus became less diverse, simplifying task of making flu shots - STAT (Covid suppression measures may have solved the headache of correctly formulating flu shots to match a winter's dominant influenza strain.)
[22:09:18] *** Quits: adiabatic (~adiabatic@user/adiabatic) (Ping timeout: 264 seconds)
[22:09:46] *** Quits: two2thehead (~AVX0@209.212.217.186) (Read error: Connection reset by peer)
[22:18:45] *** Joins: two2thehead (~AVX0@209.212.217.186)
[22:22:15] <pie_bnc> https://news.ycombinator.com/item?id=8536468
[22:22:16] <Robomot> BC: Do you ever look at your own code and think, "What the hell was I doing ... | Hacker News
[22:22:26] <pie_bnc>     BC: Do you ever look at your own code and think, "What the hell was I doing here?"
[22:22:26] <pie_bnc>     AW: No, I guess I don't.
[22:22:53] <pie_bnc> Arthur Whitney, something something APL something something implementor of J which is an APL derivative, something something unreadable C code
[22:28:43] *** Quits: weyl37355 (~AndChat72@cpe-66-69-68-162.satx.res.rr.com) (Remote host closed the connection)
[22:37:36] <kuudes> https://coronavirus.data.gov.uk/ uk has had +66% case growth since last week.
[22:37:39] <Robomot> Daily summary | Coronavirus in the UK (Official Coronavirus (COVID-19) disease situation dashboard with latest data in the UK.)
[22:39:41] <kuudes> vaccinations in uk: 54% having got 2 doses, 77% at least 1 dose
[22:41:18] <kuudes> t-7 to t-14 days 16649 cases, t to t-7 41889 cases
[22:46:05] <lucerne> Is there an ebook 'anthology' of gwern posts?
[22:46:27] <lucerne> (Ala https://www.slatestarcodexabridged.com/ )
[22:46:31] <Robomot> Slate Star Codex Abridged
[22:51:06] *** Quits: drethelin (~drethelin@096-042-035-082.res.spectrum.com) (Ping timeout: 252 seconds)
[22:54:22] <kuudes> should households also have a debt ceiling like governments?
[22:55:08] *** Joins: galambo__ (galambo@user/galambo)
[22:58:35] *** Quits: galambo_ (~galambo@user/galambo) (Ping timeout: 244 seconds)
[22:59:31] <lucerne> kuudes: Govs had a debt ceiling?
[23:02:58] <Logos01> lucerne: I'm not aware of any that don't.
[23:03:08] <Logos01> But then again debt ceilings don't mean much, really.
[23:04:51] <kuudes> many governments have proclaimed they have such
[23:06:04] <Logos01> What would be more interesting is if a government had an *absolute* debt ceiling.
[23:06:34] <Logos01> Whereafter it becomes illegal for the gov't to take out any loans at all until they are back below it.
[23:10:08] <kuudes> given usually the government owns the central bank, things would be somewhat complicated
[23:10:21] <kuudes> and the government can change the laws
[23:10:34] <kuudes> iirc germany may have something like debt ceiling in the constitution
[23:10:37] <Logos01> If it's written into the constitution it can be difficult to do it.
[23:10:43] <Logos01> But yeah, I seem to recall that too.
[23:10:50] <kuudes> but in most countries the government can also alter constitution with high enough majority
[23:11:12] <kuudes> and then we get into definition of words
[23:11:18] <kuudes> "what is debt" etc
[23:11:45] <kuudes> how about if the government guarantees loans to some corporation that gets money and uses it to fund stuff that are basicly government expenses etc
[23:11:51] <Logos01> Yeah ... it's actually super weird to me that changing the US Constitution is now seen as completely impossiburu.
[23:11:58] <Logos01> I don't get why that happened.
[23:12:03] <dbohdan> pie_bnc: 'Of course, maybe he was answering the first 8 words.'
[23:18:36] <kuudes> hmm. EU is proposing regulation for AI
[23:18:52] <kuudes> it seems to have also some good things re UFAI etc
[23:21:29] <two2thehead> kuudes, s0ph1a feep RiversHaveWings : "Trans wizard Harriet Porber is a master spellsmith who's found herself in a bit of a pickle. After finishing wizard college, Harriet made a name for herself by creating a hit viral spell, but has since failed to craft a follow up. Now Harriet’s agent, Minerma, is breathing down her neck, suggesting that Harriet take a trip to an island off the coast of England for inspiration.
[23:21:30] <two2thehead> Hoping for some peace and quiet to clear her head, Harriet Porber arrives to find that her new neighbor, an angsty bard named Snabe from the band Seven Inch Nails, is already there making a racket. This parasaurolophus spellcaster is a bad boy through and through, and with his incredible powers of metamagic, Snabe reveals that this layer of reality is much more than it seems. Could Harriet and Snabe really be characters in a parody romance novel?"
[23:26:59] <feep> okay um why tho.
[23:27:22] <dbohdan> Is this GPT?
[23:27:38] <two2thehead> https://www.amazon.com/Trans-Wizard-Harriet-Porber-Parasaurolophus/dp/B08B386R6J
[23:27:40] <Robomot> Amazon.com: Trans Wizard Harriet Porber And The Bad Boy Parasaurolophus: An Adult Romance Novel (9798653526718): Tingle, Chuck: Books (Amazon.com: Trans Wizard Harriet Porber And The Bad Boy Parasaurolophus: An Adult Romance Novel (9798653526718): Tingle, Chuck: Books)
[23:28:54] <dbohdan> Chunk Tingle is one of the more GPT-like living writers
[23:29:53] <two2thehead> hmm
[23:30:17] <shawwwn> lol.
[23:30:30] <dbohdan> *Chuck
[23:30:41] <dbohdan> lucerne: I haven't seen an ebook of gwern's site.  It would be more difficult to adapt his site to a static format than it was SSC.  If you want a best-of list, try https://www.gwern.net/index#popular and https://www.gwern.net/index#notable.
[23:30:46] <Robomot> Essays · Gwern.net [] (Personal website of Gwern Branwen (writer, self-experimenter, and programmer): topics: psychology, statistics, technology, deep learning, anime. This index page is a categorized list of Gwern.net pages.)
[23:30:46] <Robomot> Essays · Gwern.net [] (Personal website of Gwern Branwen (writer, self-experimenter, and programmer): topics: psychology, statistics, technology, deep learning, anime. This index page is a categorized list of Gwern.net pages.)
[23:31:11] *** Joins: Fusxfaranto (~Fusxfaran@cpe-75-85-179-208.san.res.rr.com)
[23:31:20] <dbohdan> (What got me hooked was <https://www.gwern.net/fiction/The-Ones-Who-Walk-Towards-Acre>)
[23:31:20] <Robomot> The Ones Who Walk Towards Acre · Gwern.net (Short story on assassination markets.)
[23:31:50] <galambo__> am I biased or is this article a poor criticism of the economist? https://www.currentaffairs.org/2017/05/how-the-economist-thinks
[23:31:52] <Robomot> How The Economist Thinks ❧ Current Affairs (<p>Is it fair to trash The Economist? You bet it is. </p>)
[23:34:20] <shawwwn> it's nice to be a developer. I made a map to show the latest houses for rent without having to subject myself to the horrors of the web. Even padmapper doesn't have apartments.com. http://test.tensorfork.com:31338/houses.html
[23:34:31] <shawwwn> and apartments. http://test.tensorfork.com:31338/apts.html
[23:35:44] <shawwwn> a bit of a superpower, once I figure out how to alert me when a new entry pops up...
[23:36:07] <shawwwn> sadly AT&T discontinued their "send an email to your number + magic email address" to send yourself a text message
[23:36:10] <dbohdan> shawwwn: Is 'This page can't load Google Maps correctly.' a bug or unavailable?
[23:36:22] <dbohdan> *unavoidable
[23:36:46] <shawwwn> no idea. seems to work though. for me at least https://usercontent.irccloud-cdn.com/file/fujEaxgU/image.png
[23:37:13] <shawwwn> I suspect I'm supposed to make an API key or something. but screw that for a one-off internal tool. :)
[23:37:28] <dbohdan> It works correctly for me, too, after I close the message
[23:37:39] <shawwwn> more apartment websites should do the "starmap view" though. It's neat seeing everything point to where you live + distance https://usercontent.irccloud-cdn.com/file/LjY5SM4m/image.png
[23:38:43] <shawwwn> it turned out we lost our dream apartment yesterday/whenever because the other applicant applied just slightly earlier than us, and were a great match. So I vow, never again
[23:39:01] <shawwwn> the other applicants won't even get to see the house/apartment. that's how quickly we'll know about it and be there before them
[23:39:47] <shawwwn> fastapi is neat too. it even autogenerates docs http://test.tensorfork.com:31337/docs and my stupid api endpoint is runnable
[23:40:16] <shawwwn> you know, for all the endpoints that have zero different parameters and options. super configurable.
[23:40:37] * shawwwn ended up copy-pasting once it worked rather than bothering to make it a parameter.
[23:41:48] <shawwwn> huh, apparently you have to click "try it out" and then "execute". there it goes.
[23:41:58] *** galambo__ is now known as galambo
[23:49:22] <Robomot> [Astral Codex Ten] Drug Users Use A Lot Of Drugs - https://astralcodexten.substack.com/p/drug-users-use-a-lot-of-drugs
[23:50:01] <feep> oh, my nas restarted randomly
[23:50:04] <feep> that's why feepbot is absent..
[23:51:13] * gwern ran out of food and went shopping, and was struck how many people were not wearing masks and how obvious repressed inflation in the form of worse services & labor shortages were
[23:51:36] <Logos01> "Drug Users Use A Lot Of Drugs" -- AKA "Most drugs are used by people who use drugs."
[23:51:43] <Logos01> Keyword "most"
[23:51:43] *** Joins: feepbot (~feepbot@ppp-93-104-168-97.dynamic.mnet-online.de)
[23:52:12] <LeoTal> A better title might be "Recreational drug doses are much larger than therapeutic ones" 
[23:52:57] <feep> "oh, so that was the device file where I kept feepbot"
[23:53:06] <feep> fun sentences to say about my server setup lol
[23:54:10] <gwern> like, Aldi simply blocked off one cashier aisle with pallets. I guess they know they're not going to have enough cashiers for all the aisles in... a while. at walmart, I walked in, went to get a cart and... there weren't any. the entire bay, which usually has like 20-100 carts, was empty. no one was in the parking lot moving carts, either, that I could see. there were no carts whatsoever near...
[23:54:16] <gwern> ...the entrance, and I had to walk back out to one of the collecting spots to get one.
[23:54:40] <gwern> *everyone* has hiring signs up. even sign-walker people, apparently, are posting signs along the roads to hire people. ironic. they can't even advertise themselves with sign-walkers
[23:54:47] <LeoTal> feep: Relevant classic http://www.bash.org/?5273
[23:54:47] <Robomot> QDB: Quote #5273
[23:54:53] <Logos01> LeoTal: Well that's not *necessarily* true. But they are likely more frequent.
[23:55:00] <Logos01> But.
[23:55:14] <Logos01> LeoTal: Your version is dramatically less tautological and is therefore less true.
[23:55:16] <feep> LeoTal: haha, I was thinking of that one
[23:55:19] <feep> I had literally lost feepbot
[23:55:25] <LeoTal> Logos01: It might not be true but it constitutes the entirety of the article
[23:55:28] <Logos01> Because tautological statements are inherently true because they are tautologically true.
[23:55:28] <feep> I was like, "wait, why is the backup folder four months old"
[23:55:39] <feep> "wait, why is this other backup folder two years old"
[23:56:11] <feep> (I don't make backups anymore ... not much point backing feepbot up to my nas when it's literally running on my nas)
[23:56:47] <Logos01> feep: Depends on your storage scheme.
[23:56:59] <Logos01> Could make migration of NAS instances for the same storage easier.  But meh.
[23:57:02] <LeoTal> I'm reminded of the labour shortage in Europe post-Black plague
[23:57:10] <Logos01> I *REALLY* need to get my UPS batteries replaced.
