[00:04:27] *** Quits: wolfshappen (~waff@irc.furworks.de) (Read error: Connection reset by peer)
[00:45:15] *** Joins: flooded (flooded@gateway/vpn/protonvpn/flood/x-43489060)
[00:45:34] *** flooded is now known as _flood
[00:48:05] *** Joins: bitkiller (~bitkiller@user/bitkiller)
[00:54:46] *** Joins: wolfshappen (~waff@irc.furworks.de)
[00:55:39] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[01:04:55] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[01:05:03] *** Joins: Gimble (~G1mble2@2a01:4b00:8e07:7900:53ff:9449:f6d:aa8c)
[01:09:11] <Gimble> hey just wondering but
[01:09:52] <Gimble> say if I gave you all a task to provide me a machine learning model to solve a problem, do you think you could make a model that cheated?
[01:10:05] <Gimble> sorry using Tensorflow Keras
[01:10:39] <Gimble> so I would want the solution to use machine learning, but do you think you could provide a cheated version that worked like a bunch of if statements
[01:10:57] <Gimble> as an exported Keras model
[01:11:35] <HuntsMan> you mean a decision tree?
[01:13:27] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[01:13:30] <Gimble> yeah
[01:13:42] <Gimble> if i provide x I am expecting an output of y every time
[01:14:01] <HuntsMan> that does not really say anything
[01:14:04] <Gimble> so I want the model to learn a set of inputs and output their corresponding pair
[01:14:28] <Gimble> could be as simple as i pass in "green" and get back "apple"
[01:14:37] <HuntsMan> that is standard ML
[01:14:38] <Gimble> but for an array of pairs like this
[01:14:42] <Gimble> yes
[01:15:00] <Gimble> but is there a way for someone to provide a keras model that has not had back propergation
[01:15:05] <Gimble> and somehow been cheated
[01:15:22] <Gimble> since you can define the layers used in a keras model
[01:15:36] <HuntsMan> I am not sure what you mean "cheated"
[01:15:50] <Gimble> cheated means they spent no time training the model
[01:16:32] <Gimble> say I expect the model to take 1 hour to train using a FNN set of layers
[01:16:50] <Gimble> or 30 MINS using CNN layers
[01:17:18] <Gimble> do you think someone could completely cheat this to < 10 seconds using other types of network configuration
[01:17:38] <Gimble> say it needed to learn 1,000 pairs
[01:19:35] <HuntsMan> so a human set the weights manually? or what?
[01:20:10] <Gimble> sure not just the weights but because the solution would be an exported keras model it gives them control of over the network topology too, such as layers used
[01:20:39] <HuntsMan> a keras model is just weights and the network structure
[01:20:42] <Gimble> i would only be expecting the input and output layers to be standardised
[01:20:57] <Gimble> sure network structure thats what I mean when i said topology
[01:21:31] <HuntsMan> so how would you set the weight values?
[01:21:40] <Gimble> no idea thats why im asking
[01:21:49] <Gimble> im worried that if I give a bunch of students this task
[01:21:54] <Gimble> say 80 students
[01:22:01] <Gimble> i dont have time to check every model submitted
[01:22:17] <Gimble> and when I run the test to ensure the pairs are all working
[01:22:29] <Gimble> im worried there might be a way to submit a cheated keras model
[01:22:54] <HuntsMan> unlikely
[01:23:04] <HuntsMan> if you can't even come up with a way to manually tune the weights
[01:24:20] <Gimble> okay ty yes I cant think of a way either but im not THAT experienced with keras
[01:24:56] <HuntsMan> its not a keras problem!
[01:25:47] *** Quits: blei (~blei@user/blei) (Quit: Client closed)
[01:31:36] *** Joins: NeuroWinter (~neuro@107.172.219.235)
[01:42:58] *** Joins: sinaowolabi (~SinaOwola@160.152.94.96)
[01:43:40] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[02:37:47] <brand0> wooooo i got really good accuracy on my NER model
[02:38:00] <brand0> simpler turned out to be better, just BERT followed by two layers
[02:38:42] <mefistofeles> gotta love those acronyms if you are into ML
[02:38:42] <mefistofeles> :P
[02:38:54] <mefistofeles> the more the merrier
[02:39:02] <brand0> BERT followed by a single layer actually works okay too
[02:42:57] *** Joins: junebug (~junebug@2600:1700:3db0:2540:dbe3:7d60:6f59:546f)
[02:52:47] *** Quits: junebug (~junebug@2600:1700:3db0:2540:dbe3:7d60:6f59:546f) (Remote host closed the connection)
[02:53:03] *** Joins: junebug (~junebug@2600:1700:3db0:2540:dbe3:7d60:6f59:546f)
[02:56:28] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[02:56:54] *** Quits: junebug (~junebug@2600:1700:3db0:2540:dbe3:7d60:6f59:546f) (Remote host closed the connection)
[02:57:16] *** Joins: junebug (~junebug@2600:1700:3db0:2540:dbe3:7d60:6f59:546f)
[03:09:43] <brand0> somehow ...
[03:09:49] <brand0> BERT is very powerful i suppose
[03:12:57] *** Quits: sinaowolabi (~SinaOwola@160.152.94.96) (Read error: No route to host)
[03:46:42] *** Joins: sinaowolabi (~SinaOwola@160.152.94.96)
[04:19:17] *** Quits: junebug (~junebug@2600:1700:3db0:2540:dbe3:7d60:6f59:546f) (Ping timeout: 240 seconds)
[04:33:21] *** Quits: sinaowolabi (~SinaOwola@160.152.94.96) (Read error: No route to host)
[04:37:42] *** Joins: sinaowolabi (~SinaOwola@160.152.94.96)
[04:45:25] <Jong> HuntsMan you can't stay on Keras forever. When will you transition to PyTorch? The fixed computation graph is the downfall of TF. Google's attempt to fix it is horrible says Jeremy Howard of Fast ai
[04:50:14] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[04:53:16] *** Quits: bitkiller (~bitkiller@user/bitkiller) (Ping timeout: 245 seconds)
[05:13:24] <HuntsMan> Jong: X is horrible says Y, so I should just believe other people? :P
[05:13:37] <HuntsMan> Jong: actually I can, that is the whole point of creating interfaces
[05:13:47] <mefistofeles> just use whatever suits you best
[05:13:47] <mefistofeles> :)
[05:14:20] <HuntsMan> precisely, totally agree with that
[05:14:41] <Jong> I'm just start trouble because I'm jealous you live the lavish life
[05:15:23] <Jong> anyone here ever watch two minute papers on youtube?
[05:15:23] <HuntsMan> Jong: what is your background, are you a student, researcher, what?
[05:16:12] <Jong> I graduated years ago. I just have software dev job
[05:18:02] <mefistofeles> Jong: I've seen it, a couple of times
[05:18:03] <Jong> It seems there are few people who like to learn on their own, outside of university. It's unfortunate. I wish there were more people here and we could be friends and pursue together our quest to be the best in the field
[05:18:38] <mefistofeles> aiming for being the best in the field is hardly a good motivation
[05:18:52] <mefistofeles> so, don't count me on that one :P
[05:19:19] <HuntsMan> Jong: graduated with Masters, PhD?
[05:21:28] <Jong> My issue with university is that a portion of my energy is spent on people matters. "Hi can I sit next to you?" and I think to myself "maybe they don't want me to sit next to them. Maybe I'm not liked".   Or /me raises his hand and asks a question, and the professor doesn't understand the question. Now I pretend as if the professor answered the question because following up isn't guaranteed to clarify anything. Now I'm just
[05:21:28] <Jong>  frustrated.  Or the room is too cold and I can't control the temp. etc
[05:22:04] <Jong> HuntsMan I graduated with only a Bachelor's in Computer Science
[05:22:13] <mefistofeles> Jong: sounds like a bit of social anxiety
[05:24:44] <Jong> it certainly is social anxiety. If I was a cocky snob and didn't care whether I upset other people or not, maybe life would be easier. I wouldn't be stressed about not knowing whether I'm offending anyone if I didn't care whether I did or not. But I do care. I want to get along with others. Caring what others think and not knowing what they think brings stress
[05:25:41] <mefistofeles> Jong: sure, the trick is caring enough but not getting anxious about it, so that's why it's a red flag
[05:25:57] <mefistofeles> but enough of the off-topic :P
[05:26:52] <HuntsMan> well going to Uni could make it a bit easier
[05:27:00] <HuntsMan> depending on Uni and program
[05:29:19] <mefistofeles> uni is a great excuse for learning stuff, but it does need a lot more than just going to it
[05:30:15] <HuntsMan> Natürlich
[05:33:03] *** Joins: junebug (~junebug@2600:1700:3db0:2540:6f34:49ad:a0b8:f509)
[05:36:08] <Jong> I'm looking for another book that covers the confusion matrix and AUROC.  Probabilistic Machine Learning is too expensive.   Aren't there books published in the 90s that cover confusion matrix and AUROC? I thought these are old ideas?
[05:37:42] <HuntsMan> Jong: the new book is literally in github
[05:37:49] <HuntsMan> google for kevin murphy's github
[05:38:13] <HuntsMan> Jong: I will tell you a secret, I did not learn confusion matrices and ROC curves from a book
[05:38:25] <HuntsMan> just don't share this secret, keep it shhhh
[05:38:36] <Jong> I don't like reading from a screen. It hurts my eyes.
[05:39:14] <mefistofeles> https://github.com/probml/pml-book/releases/latest/download/book1.pdf
[05:39:24] <SigmoidFroid> ⇒  Probabilistic Machine Learning Adaptive Computation and Machine Learning Thomas Dietterich, Editor Christopher Bishop, David Heckerman, Michael Jordan, and Michael Kearns, Associate Editors Bioinformatics: The Machine Learning Approach, Pierre Baldi and Søren Brunak Reinforcement Learning: An Introduction, Richard S. (91M)
[05:39:33] <HuntsMan> Jong: get a e-reader with a digital ink screen, like the kindle
[05:39:51] <mefistofeles> kindle are not great for PDFs though, I use a remarkable2 for that, and it works fine
[05:40:04] <mefistofeles> (rm2 is expensive, though)
[05:40:29] <HuntsMan> yeah that too
[05:40:44] <HuntsMan> or even a tablet in dark mode works greats
[05:50:57] <HuntsMan> some PDF readers on tables also have a mode that is easy on the eyes
[06:33:57] *** Quits: sinaowolabi (~SinaOwola@160.152.94.96) (Ping timeout: 256 seconds)
[06:34:31] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[06:47:01] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 245 seconds)
[06:51:19] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[06:52:06] <brand0> Jong: sure you can stay on TF/Keras forever
[06:52:27] <brand0> tf/keras vs. pytorch is a matter of opinion nothing more
[07:32:21] <causative_> has anyone ever heard of the following type of optimization method:  for a single weight w and the global error E, plot dE/dw (estimated from SGD) on the y-axis against w on the x-axis. Draw a line of regression through the points, which hopefully has positive slope. Where the line intersects the x-axis, is the best estimate of where dE/dw = 0, so it should be a local minimum or maximum.
[07:33:26] <causative_> if the regression line has positive slope it would be a local error minimum, so the best value of w would be that.
[07:33:38] *** Quits: junebug (~junebug@2600:1700:3db0:2540:6f34:49ad:a0b8:f509) (Remote host closed the connection)
[07:34:03] *** Joins: junebug (~junebug@2600:1700:3db0:2540:5964:5c6c:b427:583c)
[07:37:27] <mefistofeles> causative_: sure, that's the common differential calculus approach
[07:37:58] <mefistofeles> mefistofeles: now, it can't be a single weight w, can it?
[07:38:04] <mefistofeles> err causative_ &
[08:04:08] <causative_> eh? are you sure you are thinking of something that involves plotting dE/dw from stochastic gradient descent and drawing a *regression* line through the plot?
[08:04:19] <causative_> plotting dE/dw against w
[08:05:37] <SiegeLord> Sounds like Newton's method with a finite-difference second derivative
[08:06:13] <causative_> the key element is that it is a *regression* line through a set of empirical observations
[08:06:34] <causative_> the dE/dw plot will be very noisy
[08:08:55] <causative_> the gradient is not analytically available
[08:09:12] <causative_> we only have noisy estimates of it from SGD
[08:13:42] <SiegeLord> That's not really the issue with Newton's method. It's the fact that you need to estimate the Hessian
[08:14:01] <SiegeLord> The noisy second derivative seems quite incidental at that point
[08:18:31] <mefistofeles> yeah, noise is just... well, noise, the method is still a common and well-known optimization method
[08:19:50] <causative_> there's no Hessian, this is for a single weight, i.e. one dimensional
[08:20:53] <mefistofeles> sure, second derivative is the hessian
[08:20:57] <mefistofeles> hessian is just the generalization
[08:23:22] <causative_> the point is, there's no issue of computational complexity like there is when you estimate the Hessian with Newton's method
[08:24:11] <mefistofeles> I don't think that's the point, at all
[08:24:18] <causative_> it's the point I was making
[08:24:40] <mefistofeles> you asked if that was an optimization method we knew, the answer is "yes"
[08:24:49] <mefistofeles> and it's a very common one
[08:25:38] <causative_> usually when Newton's method is applied they compute the Hessian for *all* the weights
[08:25:44] <causative_> this is not doing that
[08:25:50] <causative_> it's working with each weight independently
[08:25:52] <mefistofeles> depends on what you are optimizing
[08:25:59] <mefistofeles> the method is still the same
[08:26:36] <mefistofeles> and yes, the way you put it it CAN'T be a single w, because you want an estimate of dE/dw, so that means many w
[08:28:05] <causative_> there are many weights, and this method would used on each of the many weights, performing one-dimensional linear regression on the dE/dw_i vs w_i plots for each weight w_i
[08:28:45] <mefistofeles> yep, exactly
[08:29:01] <causative_> this is different from any setting of Newton's method that I've heard of
[08:29:04] <mefistofeles> the dimensionality is irrelevant for the method
[08:29:12] <mefistofeles> it's the very basic setup for it
[08:29:43] <causative_> the basic setup computes the entire multidimensional Hessian for all the weights together
[08:29:46] <causative_> that is completely different
[08:29:47] <mefistofeles> no
[08:29:55] <mefistofeles> that's not the basic setup
[08:30:21] <mefistofeles> again, the dimensionality is irrelevant for the method, you just generalize it to use whatever dimensions you need/want
[08:34:59] <causative_> I have also not heard of Newton's method estimating the Hessian by performing a linear regression on the first derivatives
[08:36:37] <causative_> usually the Hessian is calculated in a different way
[08:39:14] <causative_> "usually" = I'm not aware of it ever being calculated this way. Maybe someone has done that, that's why I'm asking.
[08:39:49] <mefistofeles> causative_: there are many ways of numerically estimating the hessian, sure, but that's a technicality and it's the same method
[08:40:19] <mefistofeles> and the numerical stability depends on the problem at hand, so there isn't a unique way
[08:41:24] <causative_> you don't see that part of the question as important but I see that as the most important part of it
[08:41:35] <causative_> let me rephrase my question, then. Do you have any citations of Newton's method being applied, in which the Hessian is estimated by a linear regression on the first derivatives?
[08:42:04] <mefistofeles> ooh, we would have to cite really old papers, because that's like the most basic and straightforward approach
[08:42:26] <causative_> especially if the Hessian in question is in the setting I described
[08:42:35] <mefistofeles> causative_: also, optimizing for the rror here is not really the main problem (I'd pressume), they are probably just going for an automatic estimation of the local minima
[08:42:43] <mefistofeles> and that method is good enough
[08:42:56] <mefistofeles> you are guaranteed to have some kind of quadratic form, which is well-behaved
[08:43:35] <causative_> well, cite a really old paper then, please
[08:44:10] <causative_> the approach I see all the time in descriptions of Newton's method is the one where you use the form of the model to find the second derivatives directly
[08:44:17] <mefistofeles> causative_: I don't have one at hand, but I'm pretty sure you can find some
[08:44:17] <causative_> that's not the same thing
[08:44:23] <mefistofeles> it is
[08:44:34] <causative_> no it obviously is not
[08:44:38] <mefistofeles> :/
[08:46:42] <causative_> there is obviously a difference between calculating second derivatives directly, and estimating them by drawing a regression line through a plot of first derivatives
[08:48:56] <mefistofeles> yes, but that's irrelevant to the method, you just need an estimate for them, the method doesn't force one
[08:48:59] <mefistofeles> again, there are many ways
[08:49:28] <mefistofeles> you may be confusing the method with some common implementation
[08:50:06] <causative_> I'm asking if it is *ever* implemented the way I'm saying, with the regression line, and if you can cite this being done.
[08:50:13] <mefistofeles> yes
[08:50:29] <causative_> then can you give me a cite?
[08:50:33] <mefistofeles> no
[08:50:51] <mefistofeles> I mean, it sounds like a really straightforward and old approach, just to do things a simple way
[08:51:02] <mefistofeles> because, as I said, I'm sure that's not the main issue/problem or concern
[08:51:14] <mefistofeles> they just need a good enough estimation for the local minima of the error
[08:51:23] <causative_> okay so what you're saying is it *sounds* like something that might have been old and common, but you personally haven't read about it being done
[08:51:27] <mefistofeles> which is not really an important thing, at least commonly
[08:52:00] <mefistofeles> no, I'm pretty sure it has been used in research many times
[08:52:18] <causative_> see above
[08:52:18] <mefistofeles> but it's such an old and straightforward approach that I don't have a research paper for it
[08:52:29] <mefistofeles> at hand
[08:52:38] <causative_> and you haven't ever read a research paper for it, either
[08:52:48] <mefistofeles> for it? No, using it... sure, yes
[08:53:03] <causative_> can you point me to one, then?
[08:53:16] <mefistofeles> no, because that's not the main point of the paper, so I'd have to check many
[08:53:29] <mefistofeles> it's not an easy thing to find, it's just in some supplementary information or so
[08:53:41] <mefistofeles> again, that's not the main problem
[08:54:20] <mefistofeles> unless when it was... which I'd pressume it was like 60-70 years ago or so
[08:55:42] <mefistofeles> maybe even more
[08:56:22] <mefistofeles> causative_: now, clearly you hve found someone using it, maybe *you* want to share the source
[08:57:31] <causative_> this is something I was thinking about, I don't have a source other than myself
[08:57:50] <mefistofeles> ok
[08:57:54] <causative_> but there would be problems with this in practice, primarily what do you do if the slope of the regression line happens to be negative instead of positive
[08:58:19] <causative_> so it would be nice if there was some source that addresses that problem
[08:59:11] <causative_> I'm thinking I could switch to some other SGD optimizer in that case, until the slope of the regression line turns positive
[08:59:38] <mefistofeles> sure, I'm pretty sure it will give a similar result, again... that's not the main problem and also not the bottleneck
[08:59:41] <causative_> but what I can think of, for that, is full of ad-hoc tunable parameters
[09:00:00] <causative_> so it would be nice if some source has solved the problem in a better way
[09:00:18] <mefistofeles> it would be better if you share your source so we can have the same information
[09:00:32] <causative_> my source is me
[09:00:38] <mefistofeles> ok, not good enough
[09:05:49] *** Quits: causative_ (~halberd@user/causative) (*.net *.split)
[09:05:49] *** Quits: NeuroWinter (~neuro@107.172.219.235) (*.net *.split)
[09:05:57] *** Joins: NeuroWinter (~neuro@107.172.219.235)
[09:06:55] *** Joins: causative_ (~halberd@user/causative)
[09:08:56] <causative_> I don't even have any good search terms, like I can't search for "linear regression newton's method" or "linear regression of gradient" because those turn up almost entirely unrelated results
[09:09:45] <mefistofeles> you are mixing stuff
[09:09:57] <mefistofeles> one thing is the linear regression as an estimate of some function/behavior
[09:10:12] <mefistofeles> the other is the second derivative method for finding local minima
[09:10:43] <mefistofeles> there isn't a single method mixing those two, since the Newton-type methods are generic enough, that's what methods are... implementations/techniques are specific
[09:11:12] <mefistofeles> and as I said, yes, I'm pretty sure many people have used the latter with data from the former, for decades
[09:11:16] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[09:11:17] <causative_> well, if I could find *any* method mixing the two, that would be useful
[09:11:37] <mefistofeles> now, to cite some reference, is hard, because it's never the objective of the paper to show that, that's just secondary
[09:11:37] <causative_> finding each by itself is not really useful because the issues of implementation have to do with the combination of the two
[09:12:15] <causative_> like, if I want to know how to make a good key lime pie, it's not really helpful to investigate articles written about the individual ingredients of the pie
[09:12:27] <causative_> I need something about the ingredients when used in combination
[09:13:15] <mefistofeles> causative_: you could just cite both independently, if you really want to
[09:13:30] <mefistofeles> but I'm pretty sure you don't need to, since it's already a "trivial" thing
[09:14:29] <causative_> I'm not looking for citations to pad a paper, I'm looking for guidance on how this particular combination of methods was done before, if it ever has
[09:15:12] <mefistofeles> yes it has
[09:15:26] <causative_> prove it
[09:15:41] <mefistofeles> functions have been fitted using least squares and local minima have been detected using newton-type method
[09:15:49] <causative_> thumbs down
[09:15:52] <mefistofeles> lol
[09:16:20] <mefistofeles> I mean, if that's not good enough for you I think you need to check your calculus or real analysis background
[09:23:07] <causative_> I ask you about when this particular combination of methods was done before, and you tell me instead about how each individual component was done on its own
[09:23:21] <causative_> I told you some of the issues that arise when trying to apply this combination of methods naively, and you say my problem must be basic calculus
[09:25:09] *** Quits: Jong (~Jong@184-99-74-148.boid.qwest.net) (Ping timeout: 252 seconds)
[09:25:35] <mefistofeles> I'm pretty sure I said many more things, but sure, you do you... I'm just saying, I have seen done before, yes, in papers... I cannot cite one because who remembers these supplementary info anyways. And again, if you think this is an "odd" combination, I'd suggest you just check basic calculus and numerical approximations to know why this makes a good numerical and reliable
[09:25:41] <mefistofeles> approximation
[09:26:27] <causative_> I don't know if it's "odd," it could even have been standard 60 years ago like you speculated
[09:26:43] <causative_> but I don't know how to search for it
[09:27:35] <mefistofeles> yeah, it's not easy to search, these methods are almost as old as calculus itself
[09:28:02] <mefistofeles> so nobody actually cite these or make a big deal out of them, at least not commonly
[09:28:37] <causative_> so you keep claiming
[09:28:57] <mefistofeles> sure, I suggest you read on them and verify yourself
[09:29:17] <mefistofeles> any popular textbook should have something on them
[09:29:28] <causative_> bullshit
[09:29:37] <mefistofeles> lol
[09:30:56] <causative_> What a popular textbook would have is an explanation of Newton's method, or an explanation of linear regression, *separately*. It would not have anything about using these things in combination in the way I'm describing.
[09:31:41] <mefistofeles> you don't need them to be explained in combination, that's the whole point, if you don't see why maybe you need to review your real analysis/calculus
[09:33:45] <causative_> sure, just like if I want to know about SGD with momentum all I need is a basic physics textbook
[09:34:46] <causative_> and if I don't see from the physics textbook what are good parameters for SGD+momentum, the problem must be my basic physics knowledge, yes?
[09:35:41] * mefistofeles bails, too late for this
[09:35:51] <mefistofeles> take care
[09:36:03] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Remote host closed the connection)
[09:55:44] *** Joins: manti7 (~manti7@176.10.104.94)
[10:30:16] *** Joins: Jong (~Jong@2620:10d:c090:400::5:bdbd)
[10:36:25] *** Joins: sinaowolabi (~SinaOwola@160.152.94.96)
[10:53:24] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[10:59:28] *** Joins: spaceseller (~mario@31.147.205.13)
[11:03:57] *** Quits: junebug (~junebug@2600:1700:3db0:2540:5964:5c6c:b427:583c) (Ping timeout: 240 seconds)
[11:39:42] *** Joins: palasso (~palasso@user/palasso)
[11:47:13] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[11:49:47] *** Quits: mugli[m] (~muglimatr@2001:470:69fc:105::1:3bde) (Quit: Bridge terminating on SIGTERM)
[11:49:50] *** Quits: andrew[m]123 (~andrewmtx@2001:470:69fc:105::3af2) (Quit: Bridge terminating on SIGTERM)
[11:49:51] *** Quits: jinsun_ (~jinsun@user/jinsun) (Quit: Bridge terminating on SIGTERM)
[11:49:53] *** Quits: psydroid (~psydroid@user/psydroid) (Quit: Bridge terminating on SIGTERM)
[11:50:10] *** Joins: SiegeLord (~sl@user/siegelord)
[11:53:36] *** Quits: jinsun (~jinsun@user/jinsun) ()
[11:54:27] *** Joins: psydroid (~psydroid@user/psydroid)
[11:57:47] *** Joins: jinsun (~jinsun@user/jinsun)
[12:04:55] *** Joins: jinsun_ (~jinsun@user/jinsun)
[12:04:55] *** Joins: mugli[m] (~muglimatr@2001:470:69fc:105::1:3bde)
[12:05:07] *** Joins: andrew[m]1 (~andrewmtx@2001:470:69fc:105::3af2)
[12:11:26] <HuntsMan> causative_: that method you mentioned I think is called line search
[12:13:29] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[12:17:21] *** Quits: C-Man (~C-Man@78.31.190.82) (Ping timeout: 256 seconds)
[13:16:16] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[13:59:30] *** Joins: Klinda (~superleag@user/klinda)
[14:05:40] *** Quits: spaceseller (~mario@31.147.205.13) (Ping timeout: 246 seconds)
[14:12:46] *** Quits: diogenese (~diogenese@diogenese.velotech.net) (Quit: Has anybody seen the bridge?)
[14:13:07] *** Joins: diogenese (~diogenese@diogenese.velotech.net)
[14:39:15] *** Joins: C-Man (~C-Man@78.31.190.82)
[15:27:24] *** Joins: bitkiller (~bitkiller@user/bitkiller)
[15:31:08] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[15:35:10] *** Quits: psydroid (~psydroid@user/psydroid) (Quit: User was banned)
[15:35:13] *** Quits: jinsun_ (~jinsun@user/jinsun) (Quit: User was banned)
[15:35:39] *** Quits: mugli[m] (~muglimatr@2001:470:69fc:105::1:3bde) (Quit: User was banned)
[15:37:19] *** Quits: andrew[m]1 (~andrewmtx@2001:470:69fc:105::3af2) (Quit: User was banned)
[15:53:28] *** Joins: tomeaton17 (~tomeaton1@dyn3133-243.wlan.ic.ac.uk)
[15:55:52] *** Joins: gareppa (~gareppa@user/gareppa)
[15:58:01] *** Quits: gareppa (~gareppa@user/gareppa) (Remote host closed the connection)
[16:19:32] *** Quits: sinaowolabi (~SinaOwola@160.152.94.96) (Read error: No route to host)
[16:21:35] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[16:31:04] *** Quits: coraxx (~coraxx@mail.8-pm.com) (Remote host closed the connection)
[16:31:21] *** Joins: coraxx (~coraxx@mail.8-pm.com)
[16:34:34] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[16:36:06] *** Quits: bitkiller (~bitkiller@user/bitkiller) (Quit: bitkiller)
[16:48:15] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 250 seconds)
[17:03:27] *** Joins: sinaowolabi_ (~SinaOwola@102.67.1.37)
[17:19:37] *** Joins: psydroid (~psydroid@user/psydroid)
[17:20:48] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[17:31:14] *** Joins: jinsun_ (~jinsun@user/jinsun)
[17:31:14] *** Joins: mugli[m] (~muglimatr@2001:470:69fc:105::1:3bde)
[17:31:26] *** Joins: andrew[m]1 (~andrewmtx@2001:470:69fc:105::3af2)
[17:34:56] *** Quits: tomeaton17 (~tomeaton1@dyn3133-243.wlan.ic.ac.uk) (Quit: Client closed)
[17:58:15] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Ping timeout: 256 seconds)
[17:59:10] *** Joins: revolve (~u0_a227@82-132-239-162.dab.02.net)
[18:03:19] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[18:07:27] *** Quits: palasso (~palasso@user/palasso) (Ping timeout: 260 seconds)
[18:15:20] *** Joins: palasso (~palasso@user/palasso)
[18:31:40] *** Quits: sinaowolabi_ (~SinaOwola@102.67.1.37) (Ping timeout: 246 seconds)
[18:39:05] <causative_> I can see how you might think line search if only one weight was optimized at a time, but the idea is that they would all be optimized simultaneously, as usual for a neural network
[18:48:51] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[18:50:27] *** Quits: revolve (~u0_a227@82-132-239-162.dab.02.net) (Ping timeout: 260 seconds)
[18:51:58] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[18:53:02] <HuntsMan> causative_: yes, line search with the gradient, not individual elements
[18:53:14] <HuntsMan> basically line search on the learning rate
[18:53:25] <HuntsMan> applied at each gradient update
[18:56:43] <HuntsMan> I saw this in a book but can't remember which one
[18:59:05] <causative_> I don't think that's it
[19:00:23] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 260 seconds)
[19:00:52] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[19:02:20] <HuntsMan> then explain further
[19:04:24] <causative_> for a single weight w (among many), the estimated partial derivative dE/dw is going to be weakly dependent on w, so that you can plot a point cloud of (w, dE/dw), and draw a regression line through it, which you can use to find the value of w that minimizes abs(dE/dw)
[19:05:14] <HuntsMan> ah, I think that is called something like coordinate descent
[19:05:33] <HuntsMan> apply some optimization step, one component of the weight matrix at a time
[19:06:33] <causative_> coordinate descent is a kind of line search
[19:06:44] <causative_> I said, "I can see how you might think line search if only one weight was optimized at a time, but the idea is that they would all be optimized simultaneously, as usual for a neural network"
[19:07:32] <HuntsMan> ah yeah true, now i understand the method you seek
[19:07:36] <HuntsMan> haven't seen it actually
[19:12:39] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[19:14:16] *** Joins: `Tim (~zenguin@user/zenguin)
[19:18:22] *** Joins: tomeaton17 (~tomeaton1@2a0c:5bc0:40:2e2f:c071:c6a0:1644:348d)
[20:39:01] *** Joins: mefistof1les (~mefistofe@user/mefistofeles)
[20:40:42] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 260 seconds)
[20:41:40] *** Quits: tomeaton17 (~tomeaton1@2a0c:5bc0:40:2e2f:c071:c6a0:1644:348d) (Quit: Client closed)
[20:43:31] *** mefistof1les is now known as mefistofeles
[21:00:47] *** Joins: blei (~blei@user/blei)
[21:14:07] *** Quits: blei (~blei@user/blei) (Quit: Client closed)
[21:34:12] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[21:39:39] *** Quits: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) (Read error: Connection reset by peer)
[21:39:56] *** Joins: tomeaton17 (~tomeaton1@92.234.2.175)
[21:41:03] *** Joins: SiegeLord (~sl@user/siegelord)
[21:42:49] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[22:21:57] *** Joins: spaceseller (~mario@31.147.205.13)
[23:00:40] *** Joins: gareppa (~gareppa@user/gareppa)
[23:08:43] *** Quits: gareppa (~gareppa@user/gareppa) (Quit: Leaving)
[23:19:59] *** Quits: spaceseller (~mario@31.147.205.13) (Quit: WeeChat 3.4.1)
[23:35:16] *** Joins: junebug (~junebug@2600:1700:3db0:2540:9ce0:4e24:4ca7:ce8)
