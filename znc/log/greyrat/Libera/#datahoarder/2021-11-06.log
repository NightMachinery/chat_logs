[00:14:17] *** Joins: CodeSpelunker (~CodeSpelu@user/codespelunker)
[00:34:18] *** Quits: ___nick___ (~quassel@cpc68286-cdif17-2-0-cust533.5-1.cable.virginm.net) (Ping timeout: 260 seconds)
[00:34:24] *** Quits: Hackerpcs (~user@user/hackerpcs) (Quit: Hackerpcs)
[00:35:49] *** Joins: Hackerpcs (~user@user/hackerpcs)
[00:40:50] *** Joins: m3ow1606 (~m3ow@5.18.151.121)
[00:42:41] *** Quits: m3ow1606 (~m3ow@5.18.151.121) (Client Quit)
[00:53:18] *** Joins: sarge (~mr_sarge@207.164.121.212)
[00:55:16] *** Joins: tf_ (~tf@user/tf)
[00:56:04] *** Joins: Jmtrix- (~qed@168.235.89.81)
[00:56:08] *** Joins: xacktm_ (xacktm@user/xacktm)
[00:56:12] *** Joins: _koolazer (~koo@user/koolazer)
[00:56:14] *** Quits: zer0bitz (~zer0bitz@dsl-hkibng31-54fae3-116.dhcp.inet.fi) (Read error: Connection reset by peer)
[00:56:38] *** Joins: Arachnophine6 (~KeterClas@vm0.max-p.me)
[00:56:38] *** Quits: bpmedley (~bpmedley@2600:1700:e2c:8410:35d3:b971:926:3cb6) (Ping timeout: 260 seconds)
[00:56:38] *** Quits: mr_sarge (~mr_sarge@207.164.121.212) (Ping timeout: 260 seconds)
[00:56:39] *** Joins: FuraiTHD (~Furai@cookiehoster.furai.pl)
[00:56:48] *** Quits: Furai (~Furai@cookiehoster.furai.pl) (Ping timeout: 246 seconds)
[00:56:54] *** Joins: Peetz0r1 (~peetz0rgl@2a01:4f9:c010:3f2a:1:1:3:3)
[00:57:06] *** Joins: NeverSummer_ (~quassel@2603-8090-1803-79bc-0225-90ff-fe64-0f15.res6.spectrum.com)
[00:57:07] *** Quits: xacktm (xacktm@user/xacktm) (Ping timeout: 268 seconds)
[00:57:13] *** Joins: Cobra_Fast_ (~coprah@wtwrp.de)
[00:57:16] *** Quits: lh (~lh@2001:470:69fc:105::2f0) (Ping timeout: 268 seconds)
[00:57:16] *** Quits: NeverSummer (~quassel@2603-8090-1803-79bc-0225-90ff-fe64-0f15.res6.spectrum.com) (Ping timeout: 268 seconds)
[00:57:18] *** Joins: Chex_ (~Chex@sleepl.northnook.ca)
[00:57:21] *** Joins: jmtd (jon@dow.land)
[00:57:45] *** Joins: NeoThermic_II (~NeoThermi@user/neothermic)
[00:57:46] *** Quits: Arachnophine (~KeterClas@vm0.max-p.me) (Ping timeout: 246 seconds)
[00:57:46] *** Arachnophine6 is now known as Arachnophine
[00:57:46] *** Joins: tofran0 (~tofran@37.189.158.134)
[00:57:47] *** Quits: Jon (jon@dow.land) (Ping timeout: 246 seconds)
[00:58:23] *** Quits: Jmtrix (~qed@user/jmtrix) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: koolazer (~koo@user/koolazer) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: john_prester[m] (~johnprest@2001:470:69fc:105::1:2934) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: jacksohn[m] (~jacksohn@2001:470:69fc:105::1:1f1) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: tf (~tf@user/tf) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: Peetz0r (~peetz0rgl@revspace/participant/peetz0r) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: Chex (~Chex@user/chex) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: Cobra_Fast (~coprah@wtwrp.de) (Ping timeout: 264 seconds)
[00:58:23] *** Quits: tofran (~tofran@37.189.158.134) (Ping timeout: 264 seconds)
[00:58:24] *** tofran0 is now known as tofran
[01:00:40] *** Joins: duds- (~duds-@gateway/tor-sasl/duds)
[01:00:56] *** Quits: NeoThermic (~NeoThermi@user/neothermic) (Ping timeout: 246 seconds)
[01:01:25] *** Quits: CodeSpelunker (~CodeSpelu@user/codespelunker) (Quit: CodeSpelunker)
[01:02:46] *** Joins: lh (~lh@2001:470:69fc:105::2f0)
[01:11:07] *** Joins: john_prester[m] (~johnprest@2001:470:69fc:105::1:2934)
[01:11:12] *** Joins: jacksohn[m] (~jacksohn@2001:470:69fc:105::1:1f1)
[01:24:10] *** Joins: kariosfox (~kariosfox@87.116.235.192)
[01:27:53] *** Quits: Johannes (~Johannes@62-220-178-82.cust.bredband2.com) (Read error: Connection reset by peer)
[01:28:36] *** Quits: Foritus (~buggery@cpc91316-watf11-2-0-cust68.15-2.cable.virginm.net) (Quit: áƒš(à² ç›Šà² áƒš) .:. <Ashley> my porn is hidden in a folder called "BBC Micro Emulator")
[01:30:42] *** Quits: Rue (~rue@2001-b011-1000-1044-7c44-1e73-e2e5-05b6.dynamic-ip6.hinet.net) (Ping timeout: 260 seconds)
[01:32:38] *** Quits: RichardG867 (~user@2804:d41:ca03:e100:a9ee:b0bb:ee8e:7c9f) (Remote host closed the connection)
[01:32:56] *** Joins: Rue (~rue@2001-b011-1000-1b55-9c7b-06f7-98df-d96c.dynamic-ip6.hinet.net)
[01:32:57] *** Joins: RichardG867 (~user@2804:d41:ca03:e100:a9ee:b0bb:ee8e:7c9f)
[01:37:18] <rmmh> does Ceph scrub in a less stupid manner than ZFS? Scrubbing doing random reads drives me nuts
[01:38:10] *** Quits: StathisA (~StathisA@80.107.87.125) (Ping timeout: 260 seconds)
[01:44:03] *** Joins: StathisA (~StathisA@178-147-109-130.haap.nym.cosmote.net)
[01:47:11] *** Quits: crabbedhaloablut (~crabbedha@user/crabbedhaloablut) (Remote host closed the connection)
[01:48:16] *** Joins: crabbedhaloablut (~crabbedha@user/crabbedhaloablut)
[01:55:59] *** Quits: crabbedhaloablut (~crabbedha@user/crabbedhaloablut) (Remote host closed the connection)
[01:56:48] *** Joins: crabbedhaloablut (~crabbedha@user/crabbedhaloablut)
[01:57:22] *** Quits: nbin (~nbin@user/nbin) (Quit: ZNC - https://znc.in)
[01:57:38] *** Joins: nbin (~nbin@user/nbin)
[01:57:50] *** Quits: riposte (~finalripo@2403-5802-74-2-20c-29ff-fef3-ce22.ip6.aussiebb.net) (Quit: Quitting)
[01:58:56] <w1kl4s> rmmh it's complicated with ceph
[01:59:10] <w1kl4s> but zfs has as sequential scrub as it gets
[01:59:17] <rmmh> ...no it doesn't
[01:59:51] <w1kl4s> > A scrub is split into two parts: metadata scanning and block scrubbing. The metadata scanning sorts blocks into large sequential ranges which can then be read much more efficiently from disk when issuing the scrub I/O. 
[01:59:59] *** Joins: riposte (~finalripo@119-17-138-70.77118a.mel.static.aussiebb.net)
[02:00:01] <rmmh> hm maybe I'm missing that
[02:00:23] <w1kl4s> it was added in 0.7 something i think? or maybe 0.8 whatever
[02:00:38] <w1kl4s> if you're running old zfs from like ubuntu 18 it might not have it
[02:00:43] <unfinity> ãƒ»ã‚œã‚œãƒ»ã€‚ã€‚ãƒ» â€‹ ã‚œã‚œ\_Ã¸<â€‹ qâ€‹uack!
[02:01:00] <w1kl4s> ,bang
[02:01:01] <unfinity> w1kl4s you shot a duck in 17.379 seconds! You have killed 1 duck in #datahoarder.
[02:01:19] <w1kl4s> anywho Ceph is way more complicated than that because it's object storage, not block storage
[02:01:26] <rmmh> 0.8.3
[02:01:38] <rmmh> typically file storage is more complex than object storage
[02:01:41] <w1kl4s> if you are mad at performance during scrubs 
[02:01:48] <w1kl4s> then trust me ceph is not for you
[02:02:13] <rmmh> S3's new log-structured backend looks good
[02:03:54] <w1kl4s> sequential scrub is from 0.8
[02:04:04] <w1kl4s> so yeah, it doesn't get better than that
[02:04:13] <rmmh> unfortunate
[02:04:20] <DataHoarder> you can configure intervals, pauses and schedules on ceph rmmh 
[02:04:25] <DataHoarder> for scrubbing
[02:04:51] <rmmh> I'll probably just do an 8x16TB RAID-Z2 instead, idk
[02:05:00] <DataHoarder> just fix your stuff w1kl4s, but yeah defalts are quite extreme for scrubbing
[02:05:08] <DataHoarder> TL;DR if it fits on a host, don't do ceph
[02:05:10] <w1kl4s> if you are looking at single node then ceph is wrong anyway
[02:05:22] <w1kl4s> it's 3 hosts minimum
[02:05:26] <DataHoarder> if it fits on many hosts, don't do ceph, unless ceph makes too much sense for you
[02:05:36] <rmmh> I was looking at this https://creativemisconfiguration.wordpress.com/2020/05/10/three-node-ceph-cluster/
[02:05:37] <DataHoarder> 1 min, 3 recommended w1kl4s :)
[02:05:53] <w1kl4s> rmmh ceph at home is last resort
[02:05:59] <DataHoarder> :(
[02:06:05] <w1kl4s> you don't want to use it if you don't have to
[02:06:07] <rmmh> asking a bunch of data hoarders for reasonable recs, lol
[02:06:23] <rmmh> right now my setup is uh, kind of Special
[02:06:30] <DataHoarder> hey I am the og ceph shill from where w1kl4s comes from :P
[02:06:36] <w1kl4s> ^
[02:07:08] <DataHoarder> if you don't want flexibility don't use ceph
[02:07:19] <w1kl4s> s/want/absolutely need/
[02:07:32] <DataHoarder> but if you want that, be ready to read so much documentation, and mailing lists 
[02:07:40] <DataHoarder> it expects you live fulltime on ceph
[02:07:47] <DataHoarder> oh right
[02:07:54] <DataHoarder> I made Ceph docs .epub :)
[02:07:58] <rmmh> I have a 4x12TB RAID-Z1 on a USB-3 4-disk docking station on top of my tower https://www.amazon.com/gp/product/B07H11KXCL
[02:08:34] <DataHoarder> https://github.com/WeebDataHoarder/ceph-epub
[02:08:41] <DataHoarder> should run this again for latest
[02:08:46] <DataHoarder> goes well on a Kindle :)
[02:09:02] <w1kl4s> ceph should be last resort option when you already have 3 proper servers with backplanes and all that, 10g networking, good chunk of free cpu time, and way too much free time on your hands
[02:09:19] <w1kl4s> don't do it unless you're bloody sure you want to
[02:09:24] <DataHoarder> ^ switched to ceph cause I was tired of handling different nodes and merging fs 
[02:09:31] <DataHoarder> and wanting to use resources across
[02:09:43] <DataHoarder> plus major flexibility
[02:09:57] <rmmh> And a 4x10TB MD-RAID5 lvm crypted XFS setup on a similar external USB setup
[02:10:16] <rmmh> the XFS array is *much* faster than the zfs one
[02:10:32] <DataHoarder> ZFS gives you a protected utility knife
[02:10:56] <DataHoarder> ceph gives you a factory of sharp blades, you tell it how long, how sharp, and how to fill your mattress with them
[02:10:59] <rmmh> and an internal 4x4TB RAID5 ext4
[02:11:14] <rmmh> so clearly I should try something dumber
[02:11:29] <DataHoarder> try btrfs on LVM on mdadm raid6
[02:11:35] <DataHoarder> fuckin Synology
[02:11:36] <rmmh> god I had btrfs for a while
[02:11:39] <rmmh> it was a complete piece of shit
[02:11:42] <DataHoarder> that thing took ages to migrate
[02:11:43] <rmmh> slowest filesystem I've ever used
[02:11:56] <DataHoarder> as I was doing in-place shrinking each step of the way
[02:12:05] <DataHoarder> of btrfs + defrag
[02:12:06] <DataHoarder> lvm
[02:12:09] <rmmh> including ZFS without SSD L2ARC for metadata on 5400RPM usb drives
[02:12:09] <DataHoarder> then mdadm
[02:12:17] <rmmh> I think I did the same migration
[02:12:48] <DataHoarder> half of it was SMR :')
[02:13:05] <rmmh> I have an external 12TB SMR drive with nilfs2
[02:13:07] <DataHoarder> 12x8TB
[02:13:07] <rmmh> that was a mistake
[02:15:45] *** Quits: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4) (Read error: Connection reset by peer)
[02:16:28] *** Joins: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4)
[02:20:42] *** Quits: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4) (Read error: Connection reset by peer)
[02:21:31] *** Joins: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4)
[02:27:41] *** Quits: Peetz0r1 (~peetz0rgl@2a01:4f9:c010:3f2a:1:1:3:3) (Quit: authenticating)
[02:28:01] *** Joins: Peetz0r1 (~peetz0rgl@2a01:4f9:c010:3f2a:1:1:3:3)
[02:29:19] *** Quits: Peetz0r1 (~peetz0rgl@2a01:4f9:c010:3f2a:1:1:3:3) (Changing host)
[02:29:19] *** Joins: Peetz0r1 (~peetz0rgl@revspace/participant/peetz0r)
[02:29:47] *** Peetz0r1 is now known as Peetz0r
[02:32:20] *** Quits: kariosfox (~kariosfox@87.116.235.192) (Remote host closed the connection)
[02:38:12] *** Quits: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4) (Read error: Connection reset by peer)
[02:39:02] *** Joins: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4)
[03:05:13] *** Quits: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4) (Read error: Connection reset by peer)
[03:05:42] *** Joins: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4)
[03:11:46] *** Quits: forseti (~forseti@2806:104e:1e:322:f008:e083:cd53:2df4) (Read error: Connection reset by peer)
[03:13:13] *** Joins: forseti (~forseti@187.131.228.6)
[03:18:33] *** Joins: dna44 (~dna@47-220-190-100.gtwncmkt04.res.dyn.suddenlink.net)
[03:23:03] *** Quits: dna44 (~dna@47-220-190-100.gtwncmkt04.res.dyn.suddenlink.net) (Ping timeout: 256 seconds)
[03:26:36] *** Joins: dna44 (~dna@47-220-190-100.gtwncmkt04.res.dyn.suddenlink.net)
[03:29:45] *** Joins: dangerousdiver (~DD@2600:8801:200:e7b:6c27:e491:21ea:b534)
[03:36:16] *** Parts: Chex_ (~Chex@sleepl.northnook.ca) ()
[03:36:39] *** Quits: dna44 (~dna@47-220-190-100.gtwncmkt04.res.dyn.suddenlink.net) (Ping timeout: 256 seconds)
[03:36:56] *** Joins: Chex (~Chex@user/chex)
[03:53:44] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (Ping timeout: 268 seconds)
[03:54:01] *** Joins: Lord_of_Life (~Lord@user/lord-of-life/x-2819915)
[04:34:47] *** Joins: NeoThermic (~NeoThermi@user/neothermic)
[04:37:17] *** Quits: NeoThermic_II (~NeoThermi@user/neothermic) (Ping timeout: 256 seconds)
[04:39:06] *** Joins: NeoThermic_II (~NeoThermi@user/neothermic)
[04:41:38] *** Quits: NeoThermic (~NeoThermi@user/neothermic) (Ping timeout: 260 seconds)
[04:58:58] <ThePendulum> apparently not too far from here there's a company called Jewagas, wonder how many iterations that name took
[05:22:26] *** Joins: NeoThermic_III (~NeoThermi@user/neothermic)
[05:25:36] *** Quits: NeoThermic_II (~NeoThermi@user/neothermic) (Ping timeout: 268 seconds)
[05:31:11] *** Quits: dangerousdiver (~DD@2600:8801:200:e7b:6c27:e491:21ea:b534) (Read error: Connection reset by peer)
[05:40:25] *** Quits: MrZeus (~MrZeus@2a02:c7f:a0aa:4400:c864:d37a:3621:8828) (Ping timeout: 268 seconds)
[06:20:43] *** Joins: MRiddickW (~quassel@068-118-123-246.res.spectrum.com)
[06:34:01] *** Quits: snaked (~snaked@user/snaked) (Ping timeout: 256 seconds)
[06:46:15] *** Quits: renrelkha (~renrelkha@user/renrelkha) (Remote host closed the connection)
[06:46:35] *** Joins: renrelkha (~renrelkha@user/renrelkha)
[06:49:29] *** Joins: snaked (~snaked@user/snaked)
[06:54:04] *** Joins: MrZeus (~MrZeus@90.206.94.111)
[06:54:50] *** Quits: DataHoarder (~DataHoard@user/datahoarder) (Ping timeout: 260 seconds)
[06:54:51] *** Joins: WeebDataHoarder (~DataHoard@user/datahoarder)
[06:56:20] *** WeebDataHoarder is now known as DataHoarder
[07:17:15] *** Quits: snaked (~snaked@user/snaked) (Remote host closed the connection)
[07:17:34] *** Joins: snaked (~snaked@user/snaked)
[07:19:41] *** Quits: snaked (~snaked@user/snaked) (Remote host closed the connection)
[07:20:04] *** Joins: snaked (~snaked@user/snaked)
[07:32:23] *** Quits: MrZeus (~MrZeus@90.206.94.111) (Ping timeout: 256 seconds)
[08:15:48] *** Quits: l4yer (~l4yer@159.48.55.8) (Ping timeout: 268 seconds)
[08:16:45] *** Quits: snaked (~snaked@user/snaked) (Read error: Connection reset by peer)
[08:17:04] *** Joins: snaked (~snaked@user/snaked)
[08:26:41] *** Quits: snaked (~snaked@user/snaked) (Remote host closed the connection)
[08:27:04] *** Joins: snaked (~snaked@user/snaked)
[08:30:01] *** Quits: riff_IRC (~riff-IRC@user/riff-irc) (Remote host closed the connection)
[08:31:16] *** Joins: riff-IRC (~riff-IRC@user/riff-irc)
[08:42:42] *** Quits: snaked (~snaked@user/snaked) (Remote host closed the connection)
[08:43:13] *** Joins: snaked (~snaked@user/snaked)
[08:48:26] *** Quits: sleepyentropy (sid280705@id-280705.lymington.irccloud.com) (Ping timeout: 245 seconds)
[08:48:51] *** Quits: SiGNAL (uid152343@user/signal) (Ping timeout: 245 seconds)
[08:48:59] *** Quits: Tommy (sid21982@user/tommy) (Ping timeout: 264 seconds)
[08:49:06] *** Quits: dez (uid92154@user/dez) (Ping timeout: 268 seconds)
[08:49:16] *** Quits: anon28 (sid281558@id-281558.uxbridge.irccloud.com) (Ping timeout: 245 seconds)
[08:49:41] *** Quits: IUSR (sid348938@id-348938.hampstead.irccloud.com) (Ping timeout: 245 seconds)
[08:49:41] *** Quits: Pent (sid313808@id-313808.lymington.irccloud.com) (Ping timeout: 245 seconds)
[08:50:04] *** Quits: Tips|Cloud (sid62231@id-62231.ilkley.irccloud.com) (Ping timeout: 260 seconds)
[08:50:39] *** Quits: jakesyl_ (sid56879@id-56879.hampstead.irccloud.com) (Ping timeout: 268 seconds)
[08:50:55] *** Quits: pierrepaul (sid504261@id-504261.uxbridge.irccloud.com) (Ping timeout: 260 seconds)
[08:51:00] *** Quits: nirojan (sid365686@user/nirojan) (Ping timeout: 260 seconds)
[08:51:00] *** Quits: ThatOneRoadie (sid336425@user/thatoneroadie) (Ping timeout: 260 seconds)
[08:51:08] *** Joins: IUSR (sid348938@id-348938.hampstead.irccloud.com)
[08:51:27] *** Joins: Tommy (sid21982@id-21982.tinside.irccloud.com)
[08:51:28] *** Joins: SiGNAL (uid152343@id-152343.lymington.irccloud.com)
[08:51:30] *** Quits: SiGNAL (uid152343@id-152343.lymington.irccloud.com) (Changing host)
[08:51:30] *** Joins: SiGNAL (uid152343@user/signal)
[08:51:31] *** Quits: Tommy (sid21982@id-21982.tinside.irccloud.com) (Changing host)
[08:51:31] *** Joins: Tommy (sid21982@user/tommy)
[08:51:40] *** Joins: Pent (sid313808@2a03:5180:f:2::4:c9d0)
[08:52:08] *** Joins: Tips|Cloud (sid62231@id-62231.ilkley.irccloud.com)
[08:52:09] *** Joins: nirojan (sid365686@user/nirojan)
[08:52:09] *** Joins: dez (uid92154@2a03:5180:f:2::1:67fa)
[08:52:10] *** Quits: dez (uid92154@2a03:5180:f:2::1:67fa) (Changing host)
[08:52:10] *** Joins: dez (uid92154@user/dez)
[08:52:19] *** Joins: anon28 (sid281558@2a03:5180:f:5::4:4bd6)
[08:52:29] *** Joins: ThatOneRoadie (sid336425@user/thatoneroadie)
[08:52:37] *** Joins: jakesyl_ (sid56879@2a03:5180:f:4::de2f)
[08:53:12] *** Quits: snaked (~snaked@user/snaked) (Read error: Connection reset by peer)
[08:54:08] *** Joins: pierrepaul (sid504261@id-504261.uxbridge.irccloud.com)
[09:02:52] *** Joins: sleepyentropy (sid280705@id-280705.lymington.irccloud.com)
[09:04:38] *** Quits: iamGavinJ (~iamGavinJ@user/iamgavinj) (Quit: iamGavinJ)
[09:35:40] *** Joins: snaked (~snaked@user/snaked)
[09:45:12] *** Quits: forseti (~forseti@187.131.228.6) (Quit: And then it exploded)
[09:59:10] *** Joins: l4yer (~l4yer@159.48.55.10)
[10:28:41] <cads> What should I do if I have a 1 TB desktop disk drive, a 600gb laptop disk drive, and 256 gb of SSD space? Right now I am tired of moving things between the SDD and HD parts of my home folder, running out of space in my root partition, and not having any backup or disaster recovery if a component fails
[10:28:46] <unfinity> ãƒ»ã‚œ â€‹ ã‚œãƒ»ã€‚ã€‚ãƒ»ã‚œã‚œ\_Ã³â€‹< quâ€‹ack!
[10:30:54] <cads> the desktop drive and ssd are 2015 units. The laptop drive is a 2010 unit that I should probably get rid of
[10:31:21] <cads> I am using less than 25% of my total disk drive space
[10:37:27] <cads> I think it's too simple to justify ceph, unless going with ceph will make it easier to add a cloud based backup and disaster recovery node
[10:39:42] *** Quits: Dragoon (~Dragoon@user/dragoon) (Read error: Connection reset by peer)
[10:40:10] <cads> DataHoarder, I noticed you mentioned ceph and zfs, and going with ceph to make node replication and merging easier than zfs
[10:40:11] <cads> if I want to set up some kind of hybrid SSD/HD workstation fs with archival capabilities, mirroring, and off premises DR node, should I work with ceph rather than ZFS out the gate?
[10:43:13] *** Joins: lkronnus (~kron@179097066103.provale.com.br)
[10:47:07] *** Quits: Sgeo_ (~Sgeo@user/sgeo) (Read error: Connection reset by peer)
[10:48:07] *** Joins: Johannes (~Johannes@62-220-178-82.cust.bredband2.com)
[11:02:15] *** Joins: Dragoon (~Dragoon@user/dragoon)
[11:38:50] *** Quits: MRiddickW (~quassel@068-118-123-246.res.spectrum.com) (Ping timeout: 260 seconds)
[11:45:32] *** Quits: nepeat (~nepeat@79.110.170.58) (Ping timeout: 260 seconds)
[12:00:18] *** Joins: nepeat (~nepeat@79.110.170.59)
[12:00:46] *** Quits: lkronnus (~kron@179097066103.provale.com.br) (Ping timeout: 260 seconds)
[12:02:54] *** Joins: lkronnus (~kron@179097066103.provale.com.br)
[12:31:26] *** Joins: ee2455 (~ee2455@user/ee2455)
[13:14:58] *** Joins: debayer24 (~debayer@2603-8000-cf00-4c0a-5c44-4716-2944-a53c.res6.spectrum.com)
[13:18:05] *** Quits: zaggynl (~zaggynl@wireguard/tunneler/zaggynl) (Quit: updates)
[13:18:25] *** Joins: debayer (~debayer@cpe-76-169-99-230.socal.res.rr.com)
[13:25:06] *** Joins: zaggynl (~zaggynl@wireguard/tunneler/zaggynl)
[13:29:56] *** Quits: debayer24 (~debayer@2603-8000-cf00-4c0a-5c44-4716-2944-a53c.res6.spectrum.com) (Quit: debayer24)
[13:30:17] *** Quits: debayer (~debayer@cpe-76-169-99-230.socal.res.rr.com) (Quit: The Lounge - https://thelounge.chat)
[13:32:22] *** Joins: debayer (~debayer@cpe-76-169-99-230.socal.res.rr.com)
[13:54:48] *** Joins: m3ow1606 (~m3ow@5.18.151.121)
[13:54:57] *** Quits: m3ow1606 (~m3ow@5.18.151.121) (Client Quit)
[14:12:39] *** Joins: NeoThermic (~NeoThermi@user/neothermic)
[14:14:56] *** Joins: zer0bitz (~zer0bitz@dsl-hkibng31-54fae3-116.dhcp.inet.fi)
[14:15:59] *** Quits: NeoThermic_III (~NeoThermi@user/neothermic) (Ping timeout: 264 seconds)
[14:33:41] <cluelessperson> I'd like to find a relatively simple rack mounted server for many disks
[14:37:39] *** Joins: factor6 (~factor@c-66-30-67-217.hsd1.ma.comcast.net)
[14:41:18] *** Quits: factor (~factor@c-66-30-67-217.hsd1.ma.comcast.net) (Ping timeout: 260 seconds)
[14:41:18] *** factor6 is now known as factor
[14:46:22] *** Quits: debayer (~debayer@cpe-76-169-99-230.socal.res.rr.com) (Remote host closed the connection)
[14:46:55] *** Joins: debayer (~debayer@cpe-76-169-99-230.socal.res.rr.com)
[14:48:23] *** Quits: Rue (~rue@2001-b011-1000-1b55-9c7b-06f7-98df-d96c.dynamic-ip6.hinet.net) (Ping timeout: 264 seconds)
[14:50:21] *** Joins: Rue (~rue@2001-b011-1000-1b55-9c7b-06f7-98df-d96c.dynamic-ip6.hinet.net)
[15:01:00] <w1kl4s> cads 23:39 <w1kl4s> ceph should be last resort option when you already have 3 proper servers with backplanes and all that, 10g networking, good chunk of free cpu time, and way too much free time on your hands
[15:01:00] <w1kl4s> 23:39 <w1kl4s> don't do it unless you're bloody sure you want to
[15:01:11] <w1kl4s> cluelessperson hp 380e g8
[15:01:42] <w1kl4s> dirt cheap, small power draw with proper cpus and 14 hdd slots in 2u
[15:06:22] *** ChanServ sets mode: +v Allie
[15:06:24] *** Allie sets mode: -o Allie
[15:06:34] <Allie> ceph is excellent and i like it very much
[15:07:10] <Allie> i had a single-box ceph "cluster" for ages, and it was real good
[15:08:25] <Allie> cads: having said that - i would just go raw freeBSD on a decent little box (380e gen8 is a good option, agreed), and then something like Borg to back your shit up to (say) b2
[15:08:49] <Allie> i run freeBSD on QNAP hardware and it suits me very well
[15:09:17] <moo> is it problematic installing normal OS on qnap? 
[15:09:30] <Allie> it's a pain in the arse initially but it's great
[15:09:44] <moo> OS installs on some builtin memory or the disks? 
[15:09:55] <moo> i remember synology was installing itself to the data disks and that sucked kinda
[15:10:05] <Allie> moo: it's got a DOM in there, which i swapped out for a bigger one
[15:10:31] <Allie> there's also an internal USB port it's capable of booting off
[15:10:40] <moo> i am waiting for qnap 464 it was just announced/released
[15:10:44] <moo> but not available yet anywhere
[15:10:50] <moo> oh neat
[15:10:51] *** Quits: debayer (~debayer@cpe-76-169-99-230.socal.res.rr.com) (Quit: The Lounge - https://thelounge.chat)
[15:11:11] <Allie> moo: that's such an exciting device formfactor tbh
[15:11:30] <moo> yeah, and 4x nvme :3
[15:11:49] <Allie> we're currently building a big new PPU at work and i'm wanting to put some kind of solid-state network storage in there
[15:12:43] <moo> my ideal setup would be like 40 tb hdd for media and 4tb for backups, network drive etc
[15:13:05] <moo> this paired with dumb nas mounted via nfs would be ideal
[15:13:36] <Allie> we basically need 10tb usable of speedy network storage for all of the graphics workstations / etc to back onto
[15:14:23] <Allie> in practice it's almost certainly going to be a g2 digital 1UPC
[15:14:25] <Allie> https://g2digital.co.uk/products/rack-pc/1u-pc/
[15:22:57] *** Joins: tech_exorcist (~tech_exor@user/tech-exorcist/x-0447479)
[15:34:55] *** Quits: Jerrk (~Jerrk@193.138.218.160) (Remote host closed the connection)
[15:44:40] <cads> w1kl4s, what's a lighter software defined storage solution that is cheap to set up on a desktop rig?
[15:45:32] <w1kl4s> uh, lightweight is probably vitastor, but i wouldn't recommend doing sds unless you really need it
[15:45:45] <w1kl4s> it's slow and complicated and not made for single node stuff
[15:46:23] <cads> software defined workstation seems to be just emerging, lol
[15:47:09] *** Joins: Stormscape (~Stormscap@2604:3d08:837f:f51e:6d5a:3589:6ca6:bf41)
[15:59:27] *** Joins: jess (~jess@libera/staff/jess)
[16:02:32] <cads> w1kl4s, vitastor is so russian and pre-release that it comes with its own bottle of vodka labeled "in case of alpha bug destroyed your precious business data like pathetic baby, drink this" and a small 3 shot revolver labeled "if evil bastard manager tries to kill you as a result, defend your pathetic ass with this"
[16:02:48] <w1kl4s> cads yes
[16:03:26] <cads> lol, and their motto is "make software defined block storege greate again"
[16:03:59] <w1kl4s> i mean, guy that makes it really knows SDS inside out 
[16:05:10] <cads> I want to audit this code base just to see how terrified I should be about using it for my little "LowFi" self hosted VM, Kubernetes, and machine learning stack.
[16:06:42] <cads> LowFi because my gpu has 2gb memory so it can only run pissy little baby models, motherboard has 16 gb memory, and total storage is 1.2 TB
[16:13:45] <Allie> "software defined storage" is a fun set of buzzwords
[16:14:49] <cads> I think it's a basic axiom of the universe that distributed object store code looks like the diahrhea of a particularly woeful deity who, having just gorged themselves on food too spicy for their digestive tract, decides to manifest a distributed FS
[16:15:13] <Allie> cads: for single-node stuff, minio backing onto zfs is good fun
[16:15:24] <Allie> assuming you want object store stuff
[16:15:25] <joepie91> lmao
[16:15:36] <joepie91> Allie: this sounds like you have opinions on minio
[16:15:45] <cads> that said I like the vitastor codebase better than the ceph code for a simple reason: there's less of it
[16:15:50] <joepie91> I am looking for opinions on minio
[16:16:00] <Allie> joepie91: i mean. it sucks. but so does ALL single-node object store stuff
[16:16:20] <joepie91> Allie: what if I want to assemble a bunch of differently-sized servers into one big object store
[16:16:29] <Allie> well then you want ceph
[16:16:31] <joepie91> and differently-shaped
[16:17:00] <joepie91> Allie: I've seen widespread claims that it deals poorly with varyingly-sized nodes
[16:17:05] <joepie91> opinions? :p
[16:17:32] <cads> what about if some and/or most of the "servers" are really smartphones running locked android distros?
[16:18:05] <cads> plus there's a coordinating server that, for instance, pushes media to a smartphone used for listening to audiobooks
[16:18:26] <cads> which is really your "single node" desktop
[16:18:36] <Allie> joepie91: you need to reweight a lot to make it work, but it does work
[16:19:27] <Allie> like. ceph is not an automated thing. it's a thing you set up manually and then it's great
[16:19:37] <joepie91> Allie: 'reweight'?
[16:19:46] <Allie> joepie91: so ceph uses "weights" to decide where to put data
[16:19:57] *** Joins: tech_exorcist_ (~tech_exor@user/tech-exorcist/x-0447479)
[16:20:13] <Allie> this is based on (but not exactly) drive size
[16:20:52] <Allie> you can manually reweight osds to ensure that your data is spread well across all your storage
[16:21:25] <joepie91> Allie: is this something you do when you add a node, or something you need to keep adjusting over time as your cluster is already in use?
[16:21:43] <Allie> joepie91: mostly the former, but it's good to do a weights audit every now and then
[16:22:01] <joepie91> is there a specific reason for the "but not exactly"?
[16:22:22] <Allie> joepie91: it's entirely arbitrary weighting. 
[16:22:34] *** Quits: tech_exorcist (~tech_exor@user/tech-exorcist/x-0447479) (Ping timeout: 260 seconds)
[16:23:35] <cads> Allie, could ZFS do weighting between drives like that?
[16:23:39] <Allie> like. it's always best to have a homogenous storage pool 
[16:24:01] <Allie> cads: ZFS *only* allows homogenous drive sizes, and will just waste any extra space
[16:25:00] <joepie91> Allie: I'm not quite following. if a system works correctly, shouldn't the weighting *exactly* match the size of the storage system?
[16:26:24] <Allie> joepie91: yes, it should. but mine never have :P
[16:27:08] <joepie91> Allie: can you sketch me a picture of how bad the disconnect is?
[16:27:13] <joepie91> or, to put it differently
[16:27:26] <joepie91> how much future-me is going to hate today-me for deploying a critical data storage system on ceph
[16:28:54] <Allie> joepie91: less than how much you'll hate yourself for picking something else
[16:28:55] <Allie> ceph is best thing around for this problem
[16:29:11] <Allie> aim for homogenous storage in the future, but don't let it stop you now
[16:29:25] <Allie> use default weights and read the docs it the performance is shit
[16:29:57] <Allie> one nice thing about ceph is that you can weight an OSD to zero and it'll migrate the data off it with zero downtime
[16:30:51] <joepie91> Allie: homogenous storage will never happen in my usecase
[16:30:59] <joepie91> so any maintenance cost I incur, I will incur forever
[16:31:00] <Allie> i have even seen this used for entire DC migrations, although the performance sucks
[16:31:17] <Allie> joepie91: well, what's the problem you're solving precisely?
[16:31:24] <Allie> do you need object or block storage
[16:31:33] *** Quits: Rue (~rue@2001-b011-1000-1b55-9c7b-06f7-98df-d96c.dynamic-ip6.hinet.net) (Quit: WeeChat 3.3)
[16:31:39] * cads is learning about petabyte sneakernets /w ceph backbones
[16:31:44] <joepie91> Allie: object storage, spread across servers in different physical locations of different sizes, cobbled-together storage cluster of spare storage space
[16:31:55] <joepie91> performance does not need to be CDN-level but needs to be acceptable
[16:32:17] <Allie> ceph doesn't cross site boundaries well
[16:34:26] <joepie91> Allie: how not-well?
[16:34:48] <joepie91> like, I'm fine with a couple roundtrips but if it takes 5 seconds to start a read request for a server 100ms away then there's a problem
[16:35:42] <Allie> "don't even bother" not well :D
[16:36:00] <Allie> latency between all your ceph stuff is important
[16:36:57] <Allie> joepie91: geographically distributed block storage is a yet unsolved problem
[16:38:11] <joepie91> Allie: note, object, not block
[16:38:24] <joepie91> I am aware that geo block storage is hopeless
[16:40:26] <Allie> joepie91: objects are just big blocks :P
[16:40:45] <Allie> it's a fairly fun thing to set up, give it a go
[16:40:58] <Allie> if the performance suits you, then great
[16:41:03] <Allie> it can only get better
[16:41:03] <joepie91> Allie: I mean, tahoe-lafs actually manages to do geo-distributed object storage quite well
[16:41:13] <joepie91> it does have performance problems but those do not seem to be fundamental
[16:41:17] <joepie91> but rather implementation-related
[16:41:34] <Allie> tahoe-lafs is slow tho ;)
[16:41:38] <Allie> but yeah, point taken
[16:41:42] <joepie91> yes but the reason why it is slow matters :p
[16:41:51] <joepie91> like for example the striping implementation it uses is inexplicably slow
[16:41:54] <Allie> ceph is funded and actively developed by people who care about solving this problem
[16:42:18] <joepie91> there are plenty of striping implementations that can function as basically port speed, but tahoe-lafs does not use one of those
[16:42:22] <joepie91> s/as/at/
[16:43:02] <joepie91> and there's some weirdness in its server selection that could be improved a lot
[16:43:26] <joepie91> also it doesn't consider geo proximity at all, currently
[16:43:45] <joepie91> there's lots of room for improvement on an implementation level without compromising the fundamental design of it
[16:43:55] <joepie91> so geo-distributed object storage very much looks possible to me on a fundamental level
[16:44:11] <joepie91> (even without these improvements, tahoe already almost operates within my requirements)
[16:46:12] *** Joins: MRiddickW (~quassel@068-118-123-246.res.spectrum.com)
[17:07:58] *** Quits: tech_exorcist_ (~tech_exor@user/tech-exorcist/x-0447479) (Quit: afk, ttyl)
[17:14:50] *** Joins: Cleverness (~clevernes@pool-108-54-152-186.nycmny.fios.verizon.net)
[17:29:56] *** Joins: ___nick___ (~quassel@cpc68286-cdif17-2-0-cust533.5-1.cable.virginm.net)
[18:09:31] *** Joins: erb_c137 (~erb_c137@user/erb-c137/x-4672984)
[18:23:08] *** Joins: bpmedley (~bpmedley@2600:1700:e2c:8410:c9db:88da:525a:13a1)
[18:25:07] *** Quits: TheCoffeMaker_ (~TheCoffeM@125-121-245-190.fibertel.com.ar) (Quit: So long and thanks for all the fish)
[18:27:03] *** Joins: TheCoffeMaker (~TheCoffeM@user/thecoffemaker)
[18:27:51] *** Joins: hel0 (~hel0@user/redrum88)
[18:32:01] *** Joins: dangerousdiver (~DD@2600:8801:200:e7b:6c27:e491:21ea:b534)
[18:35:35] *** Joins: Jerrk (~Jerrk@193.138.218.160)
[18:52:48] *** Quits: eroc1990548 (~eroc1990@075-130-106-186.res.spectrum.com) (Quit: The Lounge - https://thelounge.chat)
[19:03:01] *** Joins: eroc1990548 (~eroc1990@075-130-106-186.res.spectrum.com)
[19:03:55] *** Quits: Jerrk (~Jerrk@193.138.218.160) (Read error: Connection reset by peer)
[19:07:38] *** Joins: Jerrk (~Jerrk@193.138.218.217)
[19:24:47] *** Joins: Trieste (T@user/pilgrim)
[19:25:15] *** Quits: Trieste_ (T@user/pilgrim) (Ping timeout: 256 seconds)
[19:43:57] *** Joins: Sgeo (~Sgeo@user/sgeo)
[20:11:46] *** Quits: snaked (~snaked@user/snaked) (Ping timeout: 245 seconds)
[20:13:13] *** Joins: snaked (~snaked@user/snaked)
[20:24:40] *** Parts: ee2455 (~ee2455@user/ee2455) (Leaving)
[20:30:06] *** Quits: Lord_of_Life (~Lord@user/lord-of-life/x-2819915) (Excess Flood)
[20:31:26] *** Joins: Lord_of_Life (~Lord@user/lord-of-life/x-2819915)
[20:32:04] *** Quits: clearcasting (~clearcast@167-179-182-7.a7b3b6.bne.nbn.aussiebb.net) (Quit: ZNC 1.8.2 - https://znc.in)
[20:41:11] *** Quits: MRiddickW (~quassel@068-118-123-246.res.spectrum.com) (Ping timeout: 256 seconds)
[20:43:55] *** Joins: DrBrownBear (~DrBrownbe@70.224.22.2)
[20:44:15] *** Quits: DrBrownBear (~DrBrownbe@70.224.22.2) (Client Quit)
[20:53:30] *** Joins: tech_exorcist (~tech_exor@user/tech-exorcist/x-0447479)
[21:12:25] *** Quits: Hydragyrum (hydra@about/security/staff/Hydragyrum) (Quit: ZNC 1.8.2 - https://znc.in)
[21:21:17] *** Joins: Hydragyrum (hydra@about/security/staff/Hydragyrum)
[21:25:27] *** Joins: clearcasting (~clearcast@167-179-182-7.a7b3b6.bne.nbn.aussiebb.net)
[21:29:11] *** Quits: Hydragyrum (hydra@about/security/staff/Hydragyrum) (Quit: ZNC 1.8.2 - https://znc.in)
[21:31:53] *** Joins: Hydragyrum (hydra@about/security/staff/Hydragyrum)
[21:36:27] *** Quits: jess (~jess@libera/staff/jess) ()
[21:37:00] *** Quits: Hydragyrum (hydra@about/security/staff/Hydragyrum) (Quit: ZNC 1.8.2 - https://znc.in)
[21:41:11] *** Joins: Hydragyrum (hydra@about/security/staff/Hydragyrum)
[21:41:50] *** Quits: zaggynl (~zaggynl@wireguard/tunneler/zaggynl) (Read error: Connection reset by peer)
[21:45:47] <Snuupy> Allie: ceph v16 has single node setup flag
[21:47:48] <Snuupy> https://www.reddit.com/r/ceph/comments/mppwas/single_node_ceph_vs_zfsbtrfs/gyl81sr/?context=10000
[21:48:41] <Snuupy> lolol
[21:49:27] <Snuupy> those IOPS though :( Average IOPS: 16
[21:50:03] <Snuupy> https://i.imgur.com/aCrlurs.jpg
[21:59:37] <moo> so whats the smallest setup you could actually recommend for home use with media and backups and shit? 3? 
[21:59:40] <moo> reasonably XD
[22:12:45] *** Joins: SpiderDisco (~SpiderDis@2604:2d80:a802:f100:9940:1589:3bb1:813e)
[22:37:37] *** Quits: ___nick___ (~quassel@cpc68286-cdif17-2-0-cust533.5-1.cable.virginm.net) (Quit: https://quassel-irc.org - Chat comfortably. Anywhere.)
[22:38:13] *** Joins: nergar (~Nergar@177.225.129.22)
[22:38:26] *** Joins: llh (~llh@user/llh)
[22:39:02] *** Joins: ___nick___ (~quassel@cpc68286-cdif17-2-0-cust533.5-1.cable.virginm.net)
[22:42:30] <skye0> damn it
[22:42:43] <skye0> i lost that file where someone created a twitch notification thing
[22:42:49] <skye0> well, mispaced it
[22:49:11] *** Quits: adduc (~adduc@24.15.97.76) (Read error: Connection reset by peer)
[22:49:17] <skye0> ah nvm found it
[22:50:22] <skye0> doesn't work, may have to consider a rewrite of this. copied it over from here many months back
[22:51:02] *** Joins: Neural (~neural@pool-173-73-80-93.washdc.fios.verizon.net)
[22:52:17] *** Joins: adduc (~adduc@24.15.97.76)
[22:58:36] *** Quits: adduc (~adduc@24.15.97.76) (Read error: Connection reset by peer)
[22:58:52] *** Joins: DrBrownBear (~DrBrownbe@70.224.22.2)
[22:59:04] *** Quits: DrBrownBear (~DrBrownbe@70.224.22.2) (Remote host closed the connection)
[23:01:41] *** Joins: adduc (~adduc@24.15.97.76)
[23:01:41] *** Quits: adduc (~adduc@24.15.97.76) (Read error: Connection reset by peer)
[23:04:50] *** Joins: adduc (~adduc@24.15.97.76)
[23:37:38] *** Quits: ___nick___ (~quassel@cpc68286-cdif17-2-0-cust533.5-1.cable.virginm.net) (Quit: https://quassel-irc.org - Chat comfortably. Anywhere.)
[23:39:03] *** Joins: ___nick___ (~quassel@cpc68286-cdif17-2-0-cust533.5-1.cable.virginm.net)
[23:45:07] *** Quits: cyphase (~cyphase@user/cyphase) (Ping timeout: 268 seconds)
[23:48:19] *** Joins: cyphase (~cyphase@172-10-166-228.lightspeed.sntcca.sbcglobal.net)
[23:48:20] *** Quits: cyphase (~cyphase@172-10-166-228.lightspeed.sntcca.sbcglobal.net) (Changing host)
[23:48:20] *** Joins: cyphase (~cyphase@user/cyphase)
