[00:01:33] *** Joins: aus8 (~aus@host-87-20-41-147.retail.telecomitalia.it)
[00:01:40] <hoppity> What is the technical name for what I am asking. Nested joins?
[00:04:08] *** Quits: aus (~aus@36.90.254.84.ftth.as8758.net) (Ping timeout: 256 seconds)
[00:04:09] *** aus8 is now known as aus
[00:11:30] *** Joins: mexen (uid495612@user/mexen)
[00:18:02] *** Joins: yauhsien_ (~Yau-Hsien@61-231-21-135.dynamic-ip.hinet.net)
[00:20:33] *** Quits: notapenguin (~ryu@187.3.254.107) (Quit: WeeChat 3.4)
[00:20:56] *** Quits: yauhsien (~Yau-Hsien@61-231-19-150.dynamic-ip.hinet.net) (Ping timeout: 252 seconds)
[00:21:30] *** Joins: goepsilongo_ (~goepsilon@2603-7000-ab00-62ed-708c-1d1a-7204-62e1.res6.spectrum.com)
[00:24:01] *** Quits: goepsilongo (~goepsilon@2603-7000-ab00-62ed-4d3a-5004-9503-0c7b.res6.spectrum.com) (Ping timeout: 256 seconds)
[00:24:33] *** Joins: aus3 (~aus@8.21.9.84)
[00:26:36] *** Quits: aus (~aus@host-87-20-41-147.retail.telecomitalia.it) (Ping timeout: 240 seconds)
[00:26:36] *** aus3 is now known as aus
[00:29:48] *** Joins: notzmv (~zmv@user/notzmv)
[00:32:29] *** Quits: MarderIII (~MarderIII@2001:985:e889:1:900:7185:187d:5b4e) (Ping timeout: 252 seconds)
[00:48:41] *** Quits: mizi (~mizi@user/mizi) (Ping timeout: 245 seconds)
[00:51:21] *** Joins: rgrinberg (~textual@187.223.124.14)
[01:27:39] *** Joins: yauhsienhuangtw (~Yau-Hsien@61-231-21-135.dynamic-ip.hinet.net)
[01:31:07] *** Quits: yauhsien_ (~Yau-Hsien@61-231-21-135.dynamic-ip.hinet.net) (Ping timeout: 256 seconds)
[01:55:01] *** Joins: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net)
[01:57:50] *** Quits: infinity_fye (~infinityf@156.212.40.211) (Quit: Leaving)
[01:59:47] *** Quits: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net) (Ping timeout: 256 seconds)
[02:12:55] *** Quits: rgrinberg (~textual@187.223.124.14) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[02:14:26] *** Joins: rgrinberg (~textual@2806:101e:7:79ba:11ea:9b9c:df34:d74e)
[02:36:41] <withershins> they're not really nested, just multiple joins
[02:37:53] *** Joins: eepbooptheoryb (~mik@c-73-13-112-194.hsd1.pa.comcast.net)
[03:00:17] *** Quits: niko (~niko@libera/staff/niko) (Ping timeout: 600 seconds)
[03:03:53] *** Quits: rgrinberg (~textual@2806:101e:7:79ba:11ea:9b9c:df34:d74e) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[03:04:11] <hoppity> thank you
[03:05:19] *** eepbooptheoryb is now known as beepbooptheory
[03:08:12] *** Quits: Tongir (~tangmang@2403-580f-f5--dc79-c193-3851-84c9.ip6.aussiebb.net) (Ping timeout: 240 seconds)
[03:29:09] *** Joins: rgrinberg (~textual@2806:101e:7:70b6:358d:3a29:7d14:a6d)
[03:51:25] *** Quits: adanwan (~adanwan@gateway/tor-sasl/adanwan) (Remote host closed the connection)
[03:51:40] *** Joins: adanwan (~adanwan@gateway/tor-sasl/adanwan)
[03:58:03] *** Joins: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net)
[03:58:46] *** Quits: hoppity (~hoppity@user/hoppity) (Remote host closed the connection)
[04:19:02] *** Quits: gitgood (~gitgood@cpc104690-belf11-2-0-cust365.2-1.cable.virginm.net) (Ping timeout: 272 seconds)
[04:20:36] *** Quits: rgrinberg (~textual@2806:101e:7:70b6:358d:3a29:7d14:a6d) (Ping timeout: 240 seconds)
[05:00:34] *** Quits: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net) (Ping timeout: 260 seconds)
[05:40:32] *** Joins: c (~o@2600:1700:94c0:69a0::44)
[05:40:36] *** Quits: c (~o@2600:1700:94c0:69a0::44) (Client Quit)
[05:41:09] *** Joins: c (~o@2600:1700:94c0:69a0::44)
[05:41:35] *** c is now known as Common-Lisp
[05:42:41] <Common-Lisp> hello chat 
[05:43:08] <Common-Lisp> I'm having an issue where the OTP ssl library always fails to accept connections after the first SSL error it encounters...
[05:43:16] <Common-Lisp> Is this a known issue I can workaround somehow?
[05:55:57] *** Joins: rgrinberg (~textual@2806:101e:7:70b6:fdaa:179b:adf9:7fb4)
[06:00:51] *** Quits: KettleMan (~kttl_crn@96-19-96-19-170-253.cpe.sparklight.net) (Ping timeout: 256 seconds)
[06:04:16] <Common-Lisp> found the issue
[06:04:25] *** Joins: hoppity (~hoppity@S0106b4fbe4e5da7b.cg.shawcable.net)
[06:04:25] *** Quits: hoppity (~hoppity@S0106b4fbe4e5da7b.cg.shawcable.net) (Changing host)
[06:04:25] *** Joins: hoppity (~hoppity@user/hoppity)
[06:04:27] <Common-Lisp> as it turns out, otp ssl was fine, and my code was not!
[06:04:28] <Common-Lisp> crazy idea.
[06:07:10] *** Joins: KettleMan (~kttl_crn@96-19-96-19-170-253.cpe.sparklight.net)
[06:38:56] *** Quits: Common-Lisp (~o@2600:1700:94c0:69a0::44) (Quit: Leaving)
[06:39:26] *** Quits: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca) (Quit: WeeChat 3.4)
[07:07:05] *** Joins: niko (~niko@libera/staff/niko)
[08:25:50] *** Quits: KettleMan (~kttl_crn@96-19-96-19-170-253.cpe.sparklight.net) (Quit: leaving)
[08:29:47] *** Joins: KettleMan (~kttl_crn@96-19-96-19-170-253.cpe.sparklight.net)
[08:44:50] *** Quits: hoppity (~hoppity@user/hoppity) (Remote host closed the connection)
[09:13:57] *** Quits: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net) (Ping timeout: 240 seconds)
[09:27:27] *** Quits: srfsh (~srfsh@user/srfsh) (Remote host closed the connection)
[09:28:00] *** Joins: srfsh (~srfsh@user/srfsh)
[09:34:21] *** Quits: rgrinberg (~textual@2806:101e:7:70b6:fdaa:179b:adf9:7fb4) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[09:36:27] *** Joins: rgrinberg (~textual@2806:101e:7:70b6:fdaa:179b:adf9:7fb4)
[09:43:51] *** Quits: rgrinberg (~textual@2806:101e:7:70b6:fdaa:179b:adf9:7fb4) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[09:52:30] *** Joins: Tongir (~tangmang@2403-580f-f5--7057-4a24-28dc-6cb8.ip6.aussiebb.net)
[10:20:15] <serafeim> hello friends
[10:21:29] <whk> hi serafeim, how are you today? 
[10:21:38] <serafeim> fine and you
[10:22:05] <whk> i am doing well thanks
[10:22:18] <serafeim> cool
[10:50:46] *** Joins: Sgeo_ (~Sgeo@user/sgeo)
[10:51:21] *** Quits: Sgeo (~Sgeo@user/sgeo) (Quit: Leaving)
[10:53:56] <mhmd> Hi, is data stored in persistent_term is stored on all nodes? is it on ram or on disk?
[10:55:04] <Nicd> it's in memory
[10:56:44] <Nicd> and AFAIK it's local to the node
[10:57:13] <mhmd> is there maximum size defined for it? or it can be as large as there's available memory?
[10:57:56] <Nicd> https://www.erlang.org/doc/man/persistent_term.html#storing-huge-persistent-terms
[10:59:48] <serafeim> so peristent_terms is similar to etsd ?
[10:59:50] <serafeim> so peristent_terms is similar to ets ?
[11:00:01] <mhmd> there's no maximum size mentioned there (that can't be increased), so I'll assume that it's no limited
[11:00:04] <mhmd> no
[11:00:22] <serafeim> erlang has so many stuff that's more or less useless for most people
[11:00:40] <mhmd> it's only good for read-only data
[11:00:50] <mhmd> updating data is expensive
[11:00:51] <serafeim> mhmd: it says it's highly optiomized for reads
[11:00:53] <serafeim> yes
[11:00:59] <mhmd> but reading it, is faster that ets
[11:01:03] <serafeim> but similar to ets as in a kind of database in memory
[11:01:07] <serafeim> hmmm
[11:01:10] <serafeim> i'll rephrase
[11:01:18] <serafeim> but similar to ets as in a kind of *COMPLETELY USELESS* database in memory
[11:01:24] <mhmd> why?
[11:01:28] <serafeim> there. now we have it
[11:01:42] <serafeim> i'm waiting for the fire to start 
[11:01:47] <serafeim> come on
[11:01:50] <mhmd> lol
[11:02:10] <mhmd> I actually have a very good use for it
[11:02:13] <serafeim> well the thing is that all these erlang thingies (ets, mnesia, peristent_term) are not used by elixir
[11:02:47] <serafeim> i mean even for use cases that are fit for these thingies i've seen libraries avoiding them
[11:03:19] <mhmd> in my case data is updated only once per day, and read god knows how many times per second
[11:03:55] <serafeim> mhmd: ok *but*. don't you want to persist that data ?
[11:04:23] <mhmd> yeah, they're bonus items for those who learn about erlang
[11:04:30] <Nicd> persistent_term is not useless
[11:04:41] <mhmd> I do, it already is, but it's slow to query
[11:04:45] <mhmd> from database
[11:04:57] <Nicd> it's faster than ETS for reading and it was implemented because there was already similar things in the community (mochiglobal)
[11:05:34] <mhmd> I think the paper title was "when ETS is not fast enough"
[11:05:46] <mhmd> and ETS is pretty fast
[11:06:14] <mhmd> and has got better on newer versions, IIRC
[11:06:29] <Nicd> persistent_term is basically global variables in Erlang
[11:06:48] <serafeim> Nicd: why use this instead of a genserver ?
[11:07:02] <mhmd> but variables usually need more writes
[11:07:03] <Nicd> because a GenServer would be a bottleneck and too slow
[11:07:18] <serafeim> Nicd: can you clarify ? why would it be too slow ?
[11:07:21] <mhmd> I'm using it as a cache
[11:07:34] <Nicd> GenServer is a single process and processes each request sequentially
[11:07:41] <mhmd> no concurrent reads, I think?
[11:07:48] <Nicd> and it needs to copy the response to the receiving process's memory
[11:07:55] <serafeim> Nicd: i know but how much expensive would it be to read a variable ?
[11:08:10] <Nicd> surprisingly expensive when you get to Discord scale
[11:08:23] <serafeim> Nicd: do you know many people that are on that scale ? 
[11:08:25] <Nicd> persistent_term allows for no-copy reads
[11:08:31] <mhmd> I hope that day never comes
[11:08:34] <Nicd> I mean Erlang is _made_ for that scale
[11:08:56] <serafeim> Nicd: yes you have a point. but that also proves mine: all this stuff is useless for normal people
[11:09:07] <Nicd> if you want a language and ecosystem that is made not made for that scale, then by all means use Python
[11:09:14] <serafeim> i do
[11:09:38] <serafeim> Nicd: however don't forget that instagram is built on django
[11:10:02] <serafeim> so it all depends on the choices
[11:10:19] <mhmd> they're useful for (very) specific cases
[11:10:45] <serafeim> i agree and i also try to use them. but the erlang world is a little "different"
[11:10:56] <Nicd> but if you're saying ETS is useless then honestly I think less of your opinions
[11:11:15] <serafeim> Nicd: lol ok
[11:11:35] <serafeim> Nicd: the thing is that it's useful in very specific things. you can't use it as a database because it lacks persistance
[11:11:40] <serafeim> so it can only be used as a cache
[11:11:55] <serafeim> at least in the kind of apps i usually make (web apps)
[11:11:55] <Nicd> in-memory databases are a thing you know
[11:12:04] <Nicd> and as a cache it's indeed very useful
[11:12:22] <mhmd> I think it's not meant to be used for that, they've been made for specific use cases
[11:12:59] <serafeim> Nicd: yes i know. i was working for 2 years with a java app that was using an in memory database (actually it was a kdb). i was lucky enough to leave that sh1t
[11:14:25] <serafeim> in any casse i just wanted to start some flame war. but ok i know that these things have their usages
[11:14:41] <mhmd> I'm very curious to know how Ericsson uses erlang. does it really do everything the erlang way? or does it use more recent and commonly used solutions?
[11:17:01] <mhmd> I don't know if this data is publicly available
[11:17:17] <serafeim> i think they should use erlang since it's build for this particular use case
[11:19:19] <mhmd> yeah, they're continuing what they made in past, and why would they rewrite it with a new technology when it already works well?
[11:20:10] <Nicd> Ericsson did dump Erlang for a time
[11:21:05] <mhmd> whatsapp might be a better case to investigate, since it's built when all those new technologies existed
[11:22:51] <mhmd> when whatsapp was using a single server, was it used only for connections, or for everything?
[11:26:56] <mhmd> the hardware seems to be not enough for doing *everything*, from image/video encoding to persisting/querying data and handling connections
[11:27:41] <Nicd> did WhatsApp ever use a single server?
[11:28:17] <mhmd> I know they had 1 or 3M connections on a single server
[11:28:38] <mhmd> they're doing things very differently today
[11:28:49] <mhmd> it's for 2011, I think
[11:28:53] <serafeim> wait
[11:29:02] <serafeim> how is it possible to have 3M connections on a single server ?
[11:29:07] <Nicd> yes, but AFAIK they had many servers
[11:29:12] <Nicd> and one of those servers had 3M connections
[11:29:29] <serafeim> is this supported by tcp ? i though we can have up to 65000 ports
[11:29:39] <mhmd> maybe it was 3M per server?
[11:29:55] <Nicd> I remember them saying it was a failure of their load balancing that allowed one server to accumulate 3M connections
[11:29:59] <mhmd> https://blog.whatsapp.com/1-million-is-so-2011/?lang=en
[11:29:59] <serafeim> or are ports reused somehow ? 
[11:31:09] <Nicd> serafeim: all traffic from your server is hosted on port 443 usually
[11:31:25] <Nicd> remember when Phoenix had 2M connections on a server? https://www.phoenixframework.org/blog/the-road-to-2-million-websocket-connections
[11:31:57] <serafeim> ah
[11:32:07] <serafeim> yes you're right
[11:32:12] <serafeim> the ephemeral ports are opened on the clients
[11:34:05] <serafeim> let's suppose i've got a tcp server listening to port 1234. when i get a connection i fork a client handler. let's suppose i've got two clients. how does the server know which client handler to send the traffic to  ?
[11:34:25] <serafeim> it differentieates by the (client ip, client port) ? or uses something else ?
[11:36:12] <mhmd> I found 2014 stats: http://highscalability.com/blog/2014/3/31/how-whatsapp-grew-to-nearly-500-million-users-11000-cores-an.html
[11:36:18] <mhmd> and hardware
[12:04:04] *** Quits: Sgeo_ (~Sgeo@user/sgeo) (Read error: Connection reset by peer)
[12:10:27] <serafeim> does anybody know the answer to my tcp related question ?
[12:26:08] <croeso[m]> when you accept a connection a socket object is created
[12:26:17] <croeso[m]> It is unique to a connection
[12:26:38] <croeso[m]> And is tracked on kernel level, so kernel does the routing
[12:26:47] <serafeim> how does kernel know where to route it ?
[12:27:27] <croeso[m]> socket is identified by unique combination of client and server IP and port pairs
[12:27:41] <serafeim> nice ok that's what i thought
[12:27:44] <croeso[m]> They are obviously different for different xlients
[13:00:53] *** Joins: gitgood (~gitgood@cpc104690-belf11-2-0-cust365.2-1.cable.virginm.net)
[13:10:52] <mhmd> Is it possible to specify a function to be called if a process was restarted n times in x seconds?
[13:13:57] <mhmd> I can put a supervisor for it, and configure it to be killed if process has crashed more than n times in x seconds, but how can I run a function if supervisor has crashed?
[13:21:11] *** Joins: infinityfye (~infinityf@156.212.9.238)
[13:30:06] <serafeim> mhmd: can't this supervisor be monitored ?
[13:32:12] *** Joins: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571)
[13:40:08] *** Quits: beepbooptheory (~mik@c-73-13-112-194.hsd1.pa.comcast.net) (Ping timeout: 256 seconds)
[13:46:53] <mhmd> yep, that would work
[13:48:45] <mhmd> where does supervisor keep the record of how many times each of its children has crashed? is this data accessible?
[13:49:53] *** Quits: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571) (Ping timeout: 250 seconds)
[13:54:35] *** Joins: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571)
[14:18:58] <serafeim> i don't think so
[14:19:10] <serafeim> you should create your own supervisor holding this behavior
[14:19:20] <serafeim> holding this information
[14:20:13] *** Quits: notzmv (~zmv@user/notzmv) (Ping timeout: 240 seconds)
[14:53:30] *** Joins: notapenguin (~ryu@2804:14d:5685:96c7:8b7a:60f:54fc:4777)
[15:10:37] *** Joins: Guest10 (~Guest10@89-79-72-76.dynamic.chello.pl)
[15:10:44] *** Quits: Guest10 (~Guest10@89-79-72-76.dynamic.chello.pl) (Client Quit)
[15:21:18] *** Joins: phreaky (~mental@user/phreaky)
[15:41:35] *** Joins: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net)
[15:44:07] <serafeim> not sure how it's possible though :)
[15:59:47] *** Quits: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571) (Ping timeout: 252 seconds)
[16:02:41] *** Joins: alexandre (~alexandre@2804:14d:e643:87fc:f036:9016:6e19:3869)
[16:04:05] *** alexandre is now known as devevangelista
[16:14:04] *** Quits: gitgood (~gitgood@cpc104690-belf11-2-0-cust365.2-1.cable.virginm.net) (Ping timeout: 272 seconds)
[16:15:09] *** Joins: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571)
[16:18:30] *** Joins: Warkruid (~MarderIII@enneman.demon.nl)
[16:21:33] *** Quits: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571) (Ping timeout: 250 seconds)
[16:27:44] *** Joins: manjaroi3_ (~manjaro-i@2804:14c:72:28d2:a033:8fc7:3c70:48ac)
[16:30:21] *** Joins: moons (~quassel@pool-100-4-188-6.albyny.fios.verizon.net)
[16:30:21] *** Quits: moons (~quassel@pool-100-4-188-6.albyny.fios.verizon.net) (Client Quit)
[16:30:56] *** Joins: notzmv (~zmv@user/notzmv)
[16:33:59] *** Quits: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net) (Remote host closed the connection)
[16:35:50] *** Joins: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net)
[16:40:36] *** Quits: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net) (Ping timeout: 240 seconds)
[16:58:33] *** Joins: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571)
[17:44:23] *** Joins: mizi (~mizi@user/mizi)
[17:54:30] *** Joins: beepbooptheory (~mik-wsl@50.216.98.94)
[17:58:42] <Ankhers> mhmd: What would the usecase for this be?
[18:14:22] *** Joins: james_lavin (~jameslavi@ool-457981b2.dyn.optonline.net)
[18:40:10] *** Joins: phaleth (~user@user/phaleth)
[18:44:53] *** Joins: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net)
[18:49:15] *** Quits: yauhsien (~yauhsien@61-231-21-135.dynamic-ip.hinet.net) (Ping timeout: 256 seconds)
[18:56:18] *** Joins: Sgeo (~Sgeo@user/sgeo)
[18:56:22] *** Quits: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571) (Ping timeout: 260 seconds)
[18:56:37] *** Quits: Warkruid (~MarderIII@enneman.demon.nl) (Ping timeout: 256 seconds)
[19:16:11] *** Quits: voltone (~voltone@2a02-a453-5486-1-c7c-b04e-fe6d-534d.fixed6.kpn.net) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[19:17:28] *** Joins: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571)
[19:17:30] *** Joins: Warkruid (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571)
[19:18:03] *** Joins: hoppity (~hoppity@66.222.130.38)
[19:18:03] *** Quits: hoppity (~hoppity@66.222.130.38) (Changing host)
[19:18:03] *** Joins: hoppity (~hoppity@user/hoppity)
[19:19:06] *** Quits: phreaky (~mental@user/phreaky) (Ping timeout: 245 seconds)
[19:21:51] *** Joins: enneman__ (~MarderIII@enneman.demon.nl)
[19:24:51] *** Quits: Warkruid (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571) (Ping timeout: 250 seconds)
[19:24:51] *** Quits: MarderIII (~MarderIII@2001:985:e889:1:3546:d0af:71c1:9571) (Ping timeout: 250 seconds)
[19:25:14] *** Joins: MarderIII (~MarderIII@enneman.demon.nl)
[19:59:23] <hoppity> Hi guys, I am a bit stumped by this problem at the moment. I have some maps that include the amount of hours of usage for a given device, I am trying to loop through these maps and update an hours total variable.  Any ideas?
[20:19:45] <hoppity> I am thinking having a map be my acumulator in Enum.reduce would help me achieve what I want but I am not sure if such a thing even makes sense
[20:30:45] <Poeticode> if I'm understanding this right, I would create a list of those maps (if they're not in a list already) then: device_array |> Enum.reduce(0, fn device_map, acc -> acc + device_map["field_with_usage_hours"])
[20:30:50] <wink> guess it would help to post the structure of your maps, but it does sound like a workable solution
[20:31:40] <wink> unless you mean you have one hours total variable per map?
[20:38:03] *** Joins: perrierjouet (~perrier-j@modemcable012.251-130-66.mc.videotron.ca)
[20:47:18] <hoppity> wink: correct, I have an hours_total variable per map
[20:47:28] <hoppity> let me share my stuff, one sec
[20:48:31] <devevangelista> hi guys, I am a student of elixir and I have been programming in other languages ​​for a while, but in my studies I noticed that in elixir using a plug it is very simple to create a REST API so I would like to know if it is worth using Phenix and what is your advantage?
[20:50:22] <hoppity> wink: maybe this will illustrate better:  https://dpaste.org/zXaB
[20:54:11] <wink> first step: reduce the list of maps via fn x -> [x.id, x.equipment_hours]
[20:54:11] <wink> second step: put into your first map, somehow
[20:54:40] <wink> unless your first map is empty anyway, then a map over the second one might suffice? if you have all the 4 fields
[20:54:46] <wink> or I am misunderstanding it?
[20:56:14] <Poeticode> devevangelista You can create an api-only Phoenix project with `mix phx.new --no-html --no-assets`. The advantage would be having the controller / context / schema project structure, ability to run `mix phx.gen.auth` to easily require authentication for your API, etc.
[20:56:36] *** Joins: phreaky (~mental@89-79-72-76.dynamic.chello.pl)
[20:56:36] *** Quits: phreaky (~mental@89-79-72-76.dynamic.chello.pl) (Changing host)
[20:56:36] *** Joins: phreaky (~mental@user/phreaky)
[20:57:50] <hoppity> wink: I might be onto something here, I will report back in a few moments :)
[20:59:39] <devevangelista> @Poeticode got it, I'm trying to understand the elixir to apply some concepts that I cover in my guide on microservices, and seeing if it's worth it or not to use phenix
[21:01:14] *** Joins: rgrinberg (~textual@2806:101e:7:70b6:4d9c:b9bc:cc6f:7844)
[21:05:38] <mhmd> Ankhers: It's a network based task, and if the endpoint that this process communicates to in order to function becomes unavailable, after few retries, it should switch to another API, so it can keep at least half of its functionality
[21:05:41] <srfsh> devevangelista: I haven't done my research entirely, but I think Phoenix is the only way to do subscriptions with Absinthe.
[21:06:26] <mhmd> the second API doesn't provide the all the data that process needs, only some of it, but its much better than nothing in this case
[21:07:33] <Ankhers> mhmd: If this failure is something that is expected, you should explicitly handle it. 
[21:07:34] <mhmd> and there might be multiple of these switches, it's kinda like plan B, C, etc
[21:08:47] <mhmd> I'm not sure if I can say that it's expected, it shouldn't happen, but it has happened before, so I want to be ready for it
[21:09:35] <mhmd> Ankhers: Actually I was searching for retry mechanisms in supervisors, and found this: https://github.com/erlang/otp/pull/1287
[21:09:42] <mhmd> but haven't read it fully yet
[21:10:46] <mhmd> I haven't understood the "why" behind "https://ferd.ca/it-s-about-the-guarantees.html" yet
[21:14:51] <mhmd> if the main function of a process is or is tied to networking, then why shouldn't it crash when it can't perform its main task, and why shouldn't a supervisor handle this?
[21:16:39] <mhmd> I know that supervisors are used for isolating problems of unexpected bugs, and this is not an unexpected bug, what I haven't fully understood is that what's wrong with using supervisors for that?
[21:29:02] <jeregrine> is there a reason why you shouldn't give Oban a dedicated postgres database? 
[21:29:27] <devevangelista> @srfsh you could use absinthe_plug
[21:35:52] <Ankhers> jeregrine: It may be overkill? I can't think of any actual reason though.
[21:36:51] <sorentwo> You lose enqueuing jobs transactionally, that’s the biggest reason.
[21:37:51] <MononcQc> mhmd, the big one is a question of whether your app should or shouldn't tolerate it. If it's unexpected sure, but what happens if the other end is fully gone for 2 hours? Is this something you can guarantee won't happen and is so rare you make no provision for it?
[21:38:08] <MononcQc> eg. is the issue something you can expect to be transient and surprising, or common and always expected?
[21:38:52] <MononcQc> if you're writing a client that lives in some person's home, and has to talk to a server you have that's in the cloud, you absolutely have to expect connectivity to be dogshit garbage from time to time, and to not just crashloop for 4 hours
[21:39:06] <mhmd> MononcQc: it would be very surprising
[21:39:08] <MononcQc> so you start having to think about custom and smarter retry strategies, one that's not just a supervisor.
[21:39:32] <MononcQc> it would very much not be surprising for connectivity to be bad between a client and a server.
[21:39:42] *** Quits: rgrinberg (~textual@2806:101e:7:70b6:4d9c:b9bc:cc6f:7844) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[21:40:17] <mhmd> yes that's true, but other side going down is rare
[21:40:31] <MononcQc> it's a connection, it's both sides at once
[21:42:04] <mhmd> I'm using multiple nodes (soon on different datacenters) to make it less probable
[21:42:14] <Ankhers> mhmd: One side does not have to go "down" in order for the service to be unreachable.
[21:42:44] <MononcQc> it can be a shitty router, a bad antenna change while moving around on mobile, intermediary connections, power losses, network congestion
[21:43:22] <MononcQc> badly specced proxy, corporate network firewall, NAT gateway too short, someone dropping their device, hardware crashes on the end device, etc
[21:44:15] <MononcQc> so the question is when someone does something like reset their router, do you slam in a tight loop to try to get back ASAP by spamming the network, or do you have a sort of understanding about what it means for the device to be unreachable/online?
[21:44:30] <MononcQc> you're in a situation where you do not control the network and cannot guarantee for it to be up.
[21:44:56] <mhmd> what's the line between cases in which supervisors should be used, and cases that should be handled manually, without using OTP?
[21:45:11] <mhmd> and what makes this distinction important?
[21:45:13] <MononcQc> you can use OTP, it's just that the supervisor is not the primary retry mechanism.
[21:46:12] <MononcQc> A supervisor goes "uh, that was unexpected, let's try again right now". If you think you have a need for something fancier than that, that you may plausibly have to deal with application or device or network _states_ that are distinct, then you may need to include that failure mode as part of your software design, not just the "weird ass situation fault tolerance" of a default supervisor restart
[21:47:03] <MononcQc> like you can very much crash and restart every time a user makes a mistake and sends you a command with bad syntax. But if they tend to write long commands by hand and may be expected to make mistakes, it could be much nicer to design your interaction such that making a mistake doesn't lose all of their session's state.
[21:48:11] <MononcQc> so you sort of go from "we tolerate unexpected syntactic issues by restarting and remaining available" into "we specifically handle syntactic issues by keeping the command as it was, showing an error, and letting them fix it" -- then the fault tolerance is for all the other issues that you don'T even know exist or are just not syntax errors.
[21:48:52] <mhmd> Actually my intention was to put the logic in application, I wanted to use supervisors as a tool, an implementation detail, mostly because it was the first thing that came to my mind
[21:50:04] *** Quits: Perry (~perry@coffee-break.at) (Quit: ZNC - http://znc.in)
[21:50:50] *** Joins: Perry (~perry@coffee-break.at)
[21:51:09] <mhmd> I understand that no all problems can/should be solved with restarts, in your example, for instance
[21:52:29] <MononcQc> yeah. So generally the most productive designs I had used that whole guarantees structure: what you know must exist for the app to work is encoded in the supervisors.
[21:52:38] <mhmd> and I want to implement a mechanism for handling network issues, showing errors, switching to a different API, and finally giving up if they didn't work
[21:53:02] <MononcQc> if there are situations that may not be tolerated, I make the process handling that aware of them (eg. network issues meaning you can't have a connection yielding a process telling you {error, offline} or whatever and forcing the handling onto the caller) 
[21:53:42] <MononcQc> right. So then you need to be able to ideally get a return value from your network layer that tells you "the network isn't working" and upon learning that, your thing that's trying to talk to the other end can decide what to do: crash, store to disk, buffer in RAM, try a different network, etc.
[21:54:06] <MononcQc> but the network handler has this idea in its design of "the network might be down" and forwards the responsibility of coping with it to its caller.
[21:54:07] <jeregrine> @sorentwo if all oban jobs are inserted into the same repo do we lose transactions? 
[21:54:34] <MononcQc> IF the network was guaranteed to work, then you could decide "if I, the network layer handler, can't do this, crash and restart" and then the caller has no big option there: they call you and they make you crash
[21:54:56] <MononcQc> but that's fine because the network handler was designed with the idea that the network _has_ to be available and it's not the caller's fault or responsibility to deal with that
[21:55:11] <Ankhers> jeregrine: I think it was more along the lines of, "insert user and then queue an oban job based on that user in the same transaction"
[21:55:20] <jeregrine> ohhh
[21:55:34] <Ankhers> If the user and oban jobs are in different databases, that no longer becomes a transaction. 
[21:56:56] <mhmd> MononcQc: so it's more about who's responsibility is to handle this problem, if it's not in my control, I can't fix it, so I won't try
[21:57:20] <mhmd> If I understood you correctly
[21:58:52] <Ankhers> Though for really simple situations you can fake it using Multi.run. But for complex transactions, that does not work.
[22:00:17] <mhmd> MononcQc: In my case there's only one side, it's an algorithmic trading platform, and it will have very few users who will be working with me, so I can't let the client decide to retry or not, because there are no clients in that sense
[22:00:52] <MononcQc> yeah. So some of them is just how hard of an assumption is baked in there
[22:02:31] <mhmd> so, in this case, do you think using supervisors not mainly for that, but only as a tool in one part of the system as a tool, can be a good choice?
[22:03:06] <mhmd> it would be more like an implementation detail, and not the overall system design
[22:06:32] <MononcQc> it's that a lot of people want to use their supervisor as a retry mechanism and it's far more about providing a structure of what are the things that should solidly be inplace for later-coming processes
[22:07:34] <MononcQc> they are as useful as a mechanism to properly setup and teardown a system as they are to retry things
[22:12:28] <mhmd> I see, I haven't thought of it much from this aspect, I learned something new :)
[22:13:23] *** Quits: yauhsienhuangtw (~Yau-Hsien@61-231-21-135.dynamic-ip.hinet.net) (Quit: Leaving)
[22:14:17] *** Joins: yauhsien (~Yau-Hsien@61-231-21-135.dynamic-ip.hinet.net)
[22:18:51] *** Quits: ghenry (~ghenry@ghenry.plus.com) (Ping timeout: 250 seconds)
[22:22:44] *** Quits: phaleth (~user@user/phaleth) (Quit: WeeChat 2.8)
[22:31:32] *** Joins: ghenry (~ghenry@ghenry.plus.com)
[22:41:21] *** Joins: livoreno (~livoreno@user/notzmv)
[22:42:09] *** Joins: rgrinberg (~textual@200.68.171.127)
[22:45:31] *** Quits: infinityfye (~infinityf@156.212.9.238) (Quit: Leaving)
[23:00:29] *** Quits: enneman__ (~MarderIII@enneman.demon.nl) (Quit: Leaving)
[23:00:47] *** Quits: MarderIII (~MarderIII@enneman.demon.nl) (Quit: Leaving)
[23:02:44] *** Joins: gitgood (~gitgood@cpc104690-belf11-2-0-cust365.2-1.cable.virginm.net)
[23:03:47] <jeregrine> @Ankhers thanks. 
[23:14:48] *** Quits: rgrinberg (~textual@200.68.171.127) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[23:18:14] *** Joins: wootehfoot (~wootehfoo@user/wootehfoot)
[23:37:29] <hoppity> wink: thanks for your help earlier, I solved my problem by using Enum.reduce and a map as my accumulator
[23:37:46] <wink> sweet
[23:38:12] <wink> I had to read up on all the Enum and Map functions for work a few hours ago :P
[23:38:23] <wink> not sure if I created some abomination, but... weekend
