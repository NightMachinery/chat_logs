[00:00:41] *** Joins: Malvolio (~Malvolio@user/malvolio)
[00:11:24] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[00:24:09] *** Joins: Codaraxis__ (~Codaraxis@user/codaraxis)
[00:28:06] *** Quits: Codaraxis_ (~Codaraxis@user/codaraxis) (Ping timeout: 250 seconds)
[00:33:47] *** Quits: markcerv (~markcerv3@52-119-125-204.PUBLIC.monkeybrains.net) (Read error: Connection reset by peer)
[00:38:46] *** Joins: SiegeLord (~sl@user/siegelord)
[00:41:34] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[00:45:25] *** Joins: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0)
[00:45:31] *** Joins: spaceseller (~spacesell@31.147.205.13)
[00:48:19] *** Quits: spaceseller (~spacesell@31.147.205.13) (Client Quit)
[01:00:25] *** Quits: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0) (Quit: Esteri Libera)
[01:00:39] *** Joins: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0)
[01:07:45] *** Joins: neilthereildeil (~neilthere@151.200.23.246)
[01:10:34] <hodapp_> boo, if you hung around more than 15 minutes maybe one of us could have answered
[01:33:44] *** Quits: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0) (Quit: Esteri Libera)
[01:33:55] *** Joins: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0)
[01:47:40] *** Quits: neilthereildeil (~neilthere@151.200.23.246) (Quit: Ping timeout (120 seconds))
[01:50:29] *** Joins: neilthereildeil (~neilthere@151.200.23.246)
[01:56:31] *** Parts: neilthereildeil (~neilthere@151.200.23.246) ()
[02:04:31] *** Quits: kir0ul2 (~kir0ul@89.234.141.147) (Remote host closed the connection)
[02:04:32] *** Quits: kir0ul (~kir0ul@89.234.141.147) (Remote host closed the connection)
[02:05:19] *** Joins: kir0ul (~kir0ul@89.234.141.147)
[02:05:20] *** Joins: kir0ul2 (~kir0ul@89.234.141.147)
[02:27:33] *** Joins: theseb (~theseb@47-220-214-54.cnrocmta03.res.dyn.suddenlink.net)
[02:27:37] <theseb> anyone have a simple explanation how NoSQL data can be queried with SQL?  I think at least for JSON you can create metadata.  Reason this came up is AWS Glue generates metadata for this i think?
[02:30:11] *** Quits: maxyz (~maxy@80.254.172.91) (Ping timeout: 256 seconds)
[02:32:02] <_ohm> simple explanation? move nosql data to sql
[02:32:21] <dostoyevsky2> theseb: Dump it into a table with key and value columns...
[02:34:37] *** Quits: etolier_ (~somewhere@122-199-44-127.ip4.superloop.com) (Quit: No Ping reply in 180 seconds.)
[02:34:37] <theseb> dostoyevsky2: so you first scout the entire data source for ALL keys and make them each a separate column?
[02:34:44] <theseb> i like it!
[02:35:51] *** Joins: etolier (~somewhere@122-199-44-127.ip4.superloop.com)
[02:35:54] <hodapp_> it's sort of hard to take explicitly non-relational, schema-less data and just expect to use SQL on it
[02:36:50] <theseb> hodapp_: well at least for JSON dostoyevsky2 nailed it with an easy method!
[03:01:18] *** Quits: etolier (~somewhere@122-199-44-127.ip4.superloop.com) (Quit: No Ping reply in 180 seconds.)
[03:02:32] *** Joins: etolier (~somewhere@122-199-44-127.ip4.superloop.com)
[03:06:54] *** Quits: theseb (~theseb@47-220-214-54.cnrocmta03.res.dyn.suddenlink.net) (Quit: Leaving)
[03:20:16] <dostoyevsky2> Which kind of reminds me that I was trying to reverse engineer AWS S3 the other day... which is a distributed k/v database... but it has the ability to be iterated like a directory... so I was wondering about a good data structure that would allow to have a distributed k/v store that would be easy to traverse like a filesystem... so then it dawned me: each PUT/DELETE into the k,v
[03:20:22] <dostoyevsky2> store gets also inserted into a SQL database dedicated for that bucket and use it for the list-objects... So much for it being a k/v store ;-)
[03:21:51] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[03:53:14] *** Quits: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0) (Quit: Esteri Libera)
[03:53:31] *** Joins: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0)
[03:53:38] *** Quits: Esteri (~Esteri@2a01:e34:ec10:4bf0:1819:9698:2493:c0c0) (Client Quit)
[03:54:31] *** Quits: OverCoder (~OverCoder@user/overcoder) (Quit: Quitting? That's odd.)
[04:33:55] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[05:01:37] *** Joins: taeaad_ (~taeaad@user/taeaad)
[05:03:45] *** Quits: SiegeLord (~sl@user/siegelord) (Ping timeout: 256 seconds)
[05:05:28] *** Joins: SiegeLord (~sl@user/siegelord)
[05:09:07] *** Quits: taeaad (~taeaad@user/taeaad) (Ping timeout: 240 seconds)
[05:59:47] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) ()
[06:09:49] <_ohm> dostoyevsky2: minio
[07:31:20] *** Joins: sinaowolabi_ (~SinaOwola@160.152.43.20)
[08:16:01] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[08:17:40] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[08:33:55] *** Joins: spaceseller (~spacesell@31.147.205.13)
[08:35:15] *** Quits: spaceseller (~spacesell@31.147.205.13) (Remote host closed the connection)
[09:39:54] *** Quits: Malvolio (~Malvolio@user/malvolio) (Killed (erbium.libera.chat (Nickname regained by services)))
[09:40:16] *** Joins: Malvolio (~Malvolio@user/malvolio)
[10:25:02] *** Joins: `Tim (~zenguin@user/zenguin)
[10:50:46] *** Joins: maxyz (~maxy@80.254.172.91)
[10:53:03] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[10:55:52] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[10:58:29] *** Quits: akurilin_ (uid322841@id-322841.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[11:19:49] *** Joins: manti7 (~manti7@176.10.104.94)
[13:07:37] *** Quits: Jong (~Jong@2620:10d:c090:400::5:c9a0) (Ping timeout: 240 seconds)
[13:13:40] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[14:12:59] *** Joins: Esteri (Esteri@gateway/vpn/protonvpn/esteri)
[14:36:41] *** Joins: jlrnick (~josephler@2a01cb040a1594007c4273d50a89f6ac.ipv6.abo.wanadoo.fr)
[15:17:32] *** Quits: sinaowolabi_ (~SinaOwola@160.152.43.20) (Ping timeout: 240 seconds)
[15:29:58] *** Joins: sinaowolabi_ (~SinaOwola@169.159.124.243)
[16:03:28] *** Joins: clime (~clime@37-48-61-114.nat.epc.tmcz.cz)
[16:05:38] <clime> hey, is it possible to feed neural network with "too much data"? It seems that if I narrow data feeding to my network, it starts to actually learn something, whereas if i use superset of that data, it stops learning. Would using more complex architecture help to "ignore the noise"?
[16:09:58] *** Joins: ethrl (~ethereal@user/doforlove33)
[16:15:12] <dostoyevsky2> clime: I don't think there is a conclusive answer to that.  The saturation point seems to depend on the architecture.  Also with e.g. dropout architectures are not only about learning but also about forgetting: https://news.ycombinator.com/item?id=29940435
[16:15:13] <SigmoidFroid> ⇒  New theory proposes ‘forgetting’ is a form of learning | Hacker News
[16:20:20] <clime> dostoyevsky2: ok, thanks, i use dropout. I think it is something specific to my use-case (time-series prediction) that causes it. I can now see, that with more data i am achieving high training accuracy but low test accuracy, with less data i am achieving less training acc but more test acc. So my network is overfitting with more data it seems...
[16:21:15] <clime> it's rather more features that more data that i am using - i wasn't precise
[16:22:03] <clime> *than more
[16:31:32] *** Quits: sinaowolabi_ (~SinaOwola@169.159.124.243) (Ping timeout: 240 seconds)
[16:32:18] *** Joins: mnl (~mnl@user/mnl)
[16:32:35] <dostoyevsky2> clime: Time series are really challenging, as you first need to understand how much data you actually have for training.  If your time series represents any kind of "market" it means that the underlying model is always adapting and you will never have that amount of data required for proper training
[16:37:07] <clime> yes, it's market data, and yeah, i think you are right that the model should be adapting all the time to make correct predictions...and then i might struggle to collect "enough data" if situation changes too fast. That's a good point.
[16:45:32] *** Joins: sinaowolabi_ (~SinaOwola@160.152.158.187)
[16:48:20] *** Quits: mnl (~mnl@user/mnl) (Quit: cya~)
[16:51:24] *** Joins: mnl (~mnl@user/mnl)
[18:06:32] *** Quits: sinaowolabi_ (~SinaOwola@160.152.158.187) (Ping timeout: 240 seconds)
[18:09:27] <lericson> it is absolutely possible to have too much data and too little signal
[18:10:33] <lericson> clime: do you use the same test set in your comparison?
[18:11:31] <clime> lericson: same test set to what, sorry i don't understand...
[18:12:02] <clime> lericson: oh, i get it, yes, i used the same test set in both cases
[18:12:02] <lericson> you had two scenarios, one with all data and one with a subset of your data
[18:12:08] <lericson> i see
[18:12:47] <clime> lericson: ye, but i was also always passing the same amount of samples in training set in both cases but different number of features - that made a difference
[18:14:24] <clime> according to what i have read higher number of sampless in training set shouldn't be a problem - i.e. the more the better usually should hold there.
[18:15:25] <clime> but if i enlarged feature set for each input sample, then my network started to overfit
[18:19:53] *** Joins: sinaowolabi_ (~SinaOwola@160.152.179.19)
[18:20:08] <lericson> i think that makes sense, don't you?
[18:21:56] <clime> yes
[18:22:14] <clime> i do
[18:22:54] <lericson> if you have a very simple regression task of a linear function like y = kx + m where you have samples of x,y, then you start feeding your network extraneous inputs that don't actually mean anything, you could easily see that it's harder to train
[18:24:10] *** Joins: georgios (~georgios@user/georgios)
[18:24:30] *** Quits: sinaowolabi_ (~SinaOwola@160.152.179.19) (Ping timeout: 250 seconds)
[18:24:46] <clime> ye, initially, i didn't notice that my network is overfitting, i just saw test results are worse and it seemed strange to me, so i came here to ask, should have been more perceptive :)
[18:28:48] <clime> anyway, i got some useful side-info here, so thanks
[18:39:21] *** Joins: marcello42 (~mp@2001:1a81:139b:b900:b5bb:1d66:db88:b0f)
[18:42:30] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[18:49:54] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[18:55:07] <lericson> i'm just sittin' over here playing with random forests
[18:57:42] <lericson> softmax temperature -- higher means tending to more uniform?
[18:58:27] <lericson> i reckon it's the opposite
[19:00:37] <lericson> indeed
[19:01:00] <lericson> guess it's easy to remember if λ = 0 then it becomes exp(0)/(N exp(0)) which is just 1/N
[19:01:36] <hodapp_> how'd softmax come up in the context of random forests?
[19:02:03] <lericson> i'm sampling from it
[19:02:46] <lericson> and the softmax is just letting me control the sampling process's tendency to uh be creative
[19:03:55] <hodapp_> adjusting temperature of softmax - or something softening/sharpening a probability distribution - is a pretty normal thing to do
[19:04:24] <hodapp_> it's an easy way when you get into Bayesian methods to kind of say "pay more attention to this evidence" or "pay less attention"
[19:07:19] <lericson> mostly i'm just curious how classical methods fare in sequence prediction
[19:07:35] <lericson> i had fairly impressive next step prediction % but sampling from such a model is.... less impressive
[19:10:30] <hodapp_> classical methods like what, ARIMA?
[19:10:36] <lericson> random forests
[19:10:45] <lericson> and knn
[19:13:36] <lericson> i even tested gradient boosting but that turned out less well, this LightGB thing?
[19:14:48] <lericson> ah LightGBM
[19:18:40] *** Quits: marcello42 (~mp@2001:1a81:139b:b900:b5bb:1d66:db88:b0f) (Ping timeout: 250 seconds)
[19:21:44] <hodapp_> hmm, those aren't classically used for sequences though
[19:25:11] <lericson> i know
[19:25:15] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[19:25:33] <lericson> what did people even do for language modelling back in the day
[19:25:39] <lericson> markov models?
[19:30:22] <lericson> i guess the answer is nothing
[19:31:13] <hodapp_> HMMs were quite common. NLP goes back a ways
[19:32:05] <hodapp_> sequence prediction is not quite language modelling either; that is just how deep-learning had recent success with the problem
[19:32:45] <lericson> well, that's what i'm interested in at least - the sequence prediction side of it
[19:33:29] <hodapp_> but - for instance - you can take a look at Tesseract's 'classical' engine and see how some language modelling was done prior to deep learning
[19:34:17] <hodapp_> its current engine is a hybrid of the classical one + an LSTM but in both cases it adds a strong probabilistic prior to guide the OCR given a statistical model of things like letters and words in a language
[19:34:54] <hodapp_> which is part of why Tesseract starts to fall over badly as soon as you want it to handle disorganized letters that don't really fit any language's lexicon (e.g. serial numbers)
[19:38:54] *** Joins: sinaowolabi_ (~SinaOwola@160.152.179.19)
[19:43:57] *** Quits: sinaowolabi_ (~SinaOwola@160.152.179.19) (Ping timeout: 256 seconds)
[19:56:23] *** Joins: sinaowolabi_ (~SinaOwola@169.159.109.48)
[20:04:05] <clime> hodapp_: is Tesseract about understanding the language given to it in machine form already or about recognizing written sentences (because you mentioned OCR, "optical character recognition")? it seems like two different things...
[20:04:39] <hodapp_> clime: the accuracy OCR is aided by a language model providing probabilistic priors
[20:05:03] <hodapp_> it is not exactrly NLP but it is aided by language models that also have application in NLP
[20:06:23] <clime> right, ok, i wasn't sure if we are talking about extracting meaning or recognizing written sentences
[20:07:04] <hodapp_> https://en.wikipedia.org/wiki/Natural_language_processing#Common_NLP_tasks however, OCR is considered more broadly a part of NLP still
[20:07:05] <SigmoidFroid> ⇒  Natural language processing - Wikipedia
[20:09:39] <clime> cool, thx
[20:23:40] *** Quits: jlrnick (~josephler@2a01cb040a1594007c4273d50a89f6ac.ipv6.abo.wanadoo.fr) (Ping timeout: 250 seconds)
[20:24:20] *** Joins: jlrnick (~josephler@2a01cb040a1594007c4273d50a89f6ac.ipv6.abo.wanadoo.fr)
[20:28:37] *** Quits: jlrnick (~josephler@2a01cb040a1594007c4273d50a89f6ac.ipv6.abo.wanadoo.fr) (Ping timeout: 240 seconds)
[20:50:35] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[20:52:31] *** Quits: sinaowolabi_ (~SinaOwola@169.159.109.48) (Ping timeout: 256 seconds)
[20:55:27] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[21:02:14] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[21:39:56] *** Quits: mnl (~mnl@user/mnl) (Ping timeout: 250 seconds)
[22:43:02] *** Quits: ethrl (~ethereal@user/doforlove33) (Ping timeout: 240 seconds)
[23:15:07] *** Joins: SiegeLord (~sl@user/siegelord)
[23:29:59] *** Joins: marcello42 (~mp@2001:1a81:139b:b900:b5bb:1d66:db88:b0f)
