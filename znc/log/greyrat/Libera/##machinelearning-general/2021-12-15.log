[00:13:19] <mefistofeles> ugh, 300GB of data to copy to my workstation xD
[00:31:59] *** Quits: Andrynet1 (~Andrynet@186.46.205.112) (Ping timeout: 252 seconds)
[00:34:08] *** Joins: Andrynet1 (~Andrynet@186.46.205.112)
[00:59:50] *** Joins: akevinhuang2 (~thekevinh@user/thekevinhuang)
[01:02:03] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 252 seconds)
[01:24:48] *** Joins: mnl (~mnl@user/mnl)
[01:47:04] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[01:47:36] *** Joins: jerome-- (~jerome@88.173.24.196)
[01:48:53] <Klinda> usually I should use the smote only to the training set?
[01:48:58] *** Quits: jerome- (~jerome@88.173.24.196) (Ping timeout: 260 seconds)
[01:50:19] *** Quits: `Tim (~zenguin@user/zenguin) (Ping timeout: 256 seconds)
[01:53:22] *** Quits: jerome-- (~jerome@88.173.24.196) (Quit: -)
[01:53:39] *** Joins: jerome- (~jerome@88.173.24.196)
[01:56:38] <Hunts> Klinda: yes
[01:56:57] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:06:16] <Klinda> thanks
[02:08:07] <Klinda> now I go goodnight
[02:08:31] <Klinda> buonanotte Hunts
[02:08:59] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[02:17:21] *** Joins: `Tim (~zenguin@user/zenguin)
[02:45:24] *** Quits: mnl (~mnl@user/mnl) (Quit: gn8)
[02:47:05] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[02:50:59] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[03:06:32] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[03:15:19] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[03:29:41] *** Joins: sinaowolabi (~SinaOwola@41.58.250.79)
[04:02:00] <acresearch> mefistofeles: Hunts sorry guys went out, just returned... line 18 and then 41 and 42 (not sure if there is an extra step in keras to ensure the data get augmented rather than used as is)    Hunts: nothing is wrong pre se (getting overfitting and trying to figure our whether it is my implmentation of VGG16 or my dataset), you know the saying "garbage in garbage out" i am working on
[04:02:03] <acresearch> this alone and just wanted an extra pair of eyes to catch anything i might have missed to implemented incorrectly, that is it
[04:42:20] <Hunts> acresearch: no, you are not overfitting, you are not training the model for enough epochs
[04:42:54] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[05:08:59] *** Quits: Scarecr0w (scarecr0w@user/scarecr0w) (Quit: IRC Bouncer crashed ...)
[05:16:52] *** Quits: sinaowolabi (~SinaOwola@41.58.250.79) (Ping timeout: 265 seconds)
[05:31:06] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 260 seconds)
[05:35:01] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[05:53:31] *** Joins: Scarecr0w (scarecr0w@user/scarecr0w)
[06:04:29] *** akevinhuang2 is now known as akevinhuang
[06:30:40] *** Joins: Jong (~Jong@2620:10d:c090:400::5:3f74)
[06:31:30] <Jong> Hello.
[06:32:21] <Jong> Is the idea of self attention basically the idea that a neural network can use some of its internally generated feature maps as weights that its other internally generated feature maps can forward pass through?
[06:42:21] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[06:47:32] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[06:49:58] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 260 seconds)
[06:56:02] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) ()
[07:00:02] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 240 seconds)
[07:11:43] <hodapp> Jong: it's perhaps more that the weighting of tokens (or, the feature maps corresponding to those tokens) can now vary based on the input
[07:13:03] <hodapp> the feature maps themselves aren't the weights
[07:14:03] *** Quits: Scarecr0w (scarecr0w@user/scarecr0w) (Remote host closed the connection)
[07:14:03] <hodapp> the weights come from something like scaled dot-product attention - which will involve the feature maps being put first through some linear/FC layer
[07:32:55] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[07:54:34] *** Joins: Scarecr0w (scarecr0w@user/scarecr0w)
[07:56:05] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[08:40:23] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Quit: Reconnecting)
[08:40:37] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[09:18:39] *** Joins: sinaowolabi (~SinaOwola@41.58.51.98)
[09:22:19] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[09:23:39] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[09:38:29] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[09:42:36] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[09:43:27] <Jong> if weights implies trainable parameters, then feature maps aren't directly weights.  But if weights means an array that a feature map forward passes through, then technically isn't one feature map being treated as the weights array and another as the activation values?
[09:44:37] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[09:45:54] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[09:50:03] *** Quits: sinaowolabi (~SinaOwola@41.58.51.98) (Ping timeout: 250 seconds)
[09:58:57] *** Quits: Colt (~Colt@user/colt) (Quit: Leaving)
[10:02:02] *** Joins: sinaowolabi (~SinaOwola@41.58.226.176)
[10:07:46] *** Joins: jlrnick (~josephler@2a01cb040a159400d9d2762a1f23b84a.ipv6.abo.wanadoo.fr)
[10:21:07] *** Joins: SiegeLord (~sl@user/siegelord)
[10:33:02] *** Quits: jlrnick (~josephler@2a01cb040a159400d9d2762a1f23b84a.ipv6.abo.wanadoo.fr) (Remote host closed the connection)
[10:34:32] *** Joins: jlrnick (~josephler@2a01cb040a159400d9d2762a1f23b84a.ipv6.abo.wanadoo.fr)
[10:50:27] *** Quits: jlrnick (~josephler@2a01cb040a159400d9d2762a1f23b84a.ipv6.abo.wanadoo.fr) (Read error: Connection reset by peer)
[10:53:33] *** Joins: jlrnick (~josephler@2a01cb040a159400d9d2762a1f23b84a.ipv6.abo.wanadoo.fr)
[11:14:48] *** Joins: manti7 (~manti7@176.10.104.94)
[11:19:24] <acresearch> Hunts: ah i see, ok i will try longer training, thanks :-)
[11:25:34] *** Joins: mnl (~mnl@user/mnl)
[11:50:21] *** Joins: palasso (~palasso@user/palasso)
[11:57:10] *** Joins: Klinda (~superleag@user/klinda)
[12:02:11] <Klinda> morning
[12:02:16] <Klinda> buongiorno
[12:03:02] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Ping timeout: 240 seconds)
[12:57:37] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[13:07:22] *** Quits: mnl (~mnl@user/mnl) (Quit: cya~)
[13:31:18] *** Quits: trace987 (~trace@ip5b429834.dynamic.kabel-deutschland.de) (Ping timeout: 260 seconds)
[13:34:27] *** Quits: Jong (~Jong@2620:10d:c090:400::5:3f74) (Ping timeout: 252 seconds)
[14:06:19] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df047-236.dhcp.inet.fi)
[14:09:39] <Klinda> https://cs229.stanford.edu/summer2019/cs229-notes1.pdf in page 5, there is the gradient descent, my teacher just put - learning rate * ( y - h(x) )
[14:09:40] <SigmoidFroid> ⇒  CS229 Lecture Notes Andrew Ng slightly updated by TM on June 28, 2019 Supervised learning Let’s start by talking about a few examples of supervised learning problems. Suppose we have a dataset giving the living areas and prices of 47 houses from Portland, Oregon: Living area (feet2 ) Price (1000$s) 2104 400 1600 330 (232K)
[14:10:55] <Klinda> soory learning rate * h(x) - y
[14:11:15] <Klinda> in the stanford they do the inverse
[14:11:18] <Klinda> is it the same?
[14:11:47] <Klinda> my teacher does - learning rate * ( h(x) - y)
[14:12:07] <Klinda> in stanford + learning rate * (y - h(x))
[14:12:47] <Klinda> what's the difference?
[14:13:34] *** Joins: trace987 (~trace@ip5b429834.dynamic.kabel-deutschland.de)
[14:20:14] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[14:25:25] *** Quits: paairs (~rabidus@user/paairs) (Ping timeout: 240 seconds)
[14:27:20] *** Joins: paairs (~rabidus@user/paairs)
[14:28:59] <Klinda> it's the same xD
[14:36:56] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[14:44:11] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[15:03:28] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[15:06:51] *** Joins: Gurkenglas (~Gurkengla@dslb-002-203-144-204.002.203.pools.vodafone-ip.de)
[15:09:48] <Gurkenglas> There's a bunch of work on mutual information estimation. Suppose I train a network to discriminate between a sample from (X,Y) and a pair of samples from X and Y. Then its average logit answer to a sample of the first sort should be precisely (up to competence of the discriminator) be the mutual information between X and Y. Does this technique have a name?
[15:11:44] <hodapp> why would you expect that its logits would precisely be mutual information when never being trained explicitly on this?
[15:14:08] <Gurkenglas> Suppose that X and Y are the first 10 and the last 10 bits of a uniform-random vector of 19 bits. Then if the shared bit from the given X,Y pair matches, that's one bit of evidence in favor of the input being from (X,Y), and the network should be expected to output 66%.
[15:35:55] <Gurkenglas> hodapp, makes sense?
[15:43:24] *** Joins: Jong (~Jong@2620:10d:c090:400::5:3f74)
[16:24:37] *** Quits: acresearch (~acresearc@user/acresearch) (Quit: WeeChat 2.8)
[17:01:55] *** Joins: georgios (~georgios@user/georgios)
[17:05:28] *** Joins: jailop (jailop@gateway/vpn/protonvpn/jailop)
[17:23:37] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[17:34:57] *** Quits: jailop (jailop@gateway/vpn/protonvpn/jailop) (Ping timeout: 256 seconds)
[17:35:08] <hodapp> that makes sense as the definition of mutual information; it doesn't make sense why the network "should be expected" to output that given just logit
[17:36:34] *** Joins: jailop (jailop@gateway/vpn/protonvpn/jailop)
[17:36:58] <hodapp> also not totally clear what the relation is between the actual mutual information I(X;Y), which is 1 bit, and the 66%, given that mutual information isn't really defined over samples, only over two random variables
[17:37:14] *** Quits: jlrnick (~josephler@2a01cb040a159400d9d2762a1f23b84a.ipv6.abo.wanadoo.fr) (Ping timeout: 260 seconds)
[17:40:17] <dostoyevsky2> Wouldn't it make more sense to just train for three categories: X, Y, mutual -- and then you'd get a percentage for each
[17:41:18] <dostoyevsky2> Not sure if there is a dedicated name for it, as discovering those commonalities in the data according to the model is part of every training
[17:47:23] *** Quits: jailop (jailop@gateway/vpn/protonvpn/jailop) (Ping timeout: 250 seconds)
[17:49:13] *** Joins: jailop (~jailop@ool-944bb915.dyn.optonline.net)
[17:51:48] <hodapp> Jong: sorry, I might have misinterpreted 'feature map' just as some feature space / embedded space
[17:54:29] <Klinda> so in the end Stochastic Gradient Descent SGD only use 1 random vector in the training set on each update, or a small subset of training set? because I understand that batch gradient descent -> full training set, stochastic gradient descent --> only one random example (so not fixed) on each update, mini-batch ---> a small fixed subset of the training set
[17:56:15] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df047-236.dhcp.inet.fi) (Read error: Connection reset by peer)
[17:59:29] <hodapp> Jong: Each layer of each attention head produces an NxN attention matrix (which you could call a feature map, I guess), which is treated as weights that determine how much of each of the N "value" tokens make it through to the next layer
[18:00:25] <hodapp> the attention matrix doesn't refer to any other attention matrix of feature map, but it does (sort of) refer to some feature space
[18:09:16] <Hunts> Klinda: what random vector?
[18:26:41] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df047-236.dhcp.inet.fi)
[18:32:30] <Klinda> Hunts: I mean it choose randomy an example of the training set
[18:32:48] <Klinda> on each update
[18:33:17] <Klinda> is mini-batch the subset picked randomly too at each update
[18:33:18] <Klinda> ?
[18:33:54] <Klinda> if the training set is http://en.wikipedia.org/wiki/Special:Search?go=Go&search=1,2],[3,4],[7,8
[18:33:55] <SigmoidFroid> ⇒  Search - Wikipedia
[18:33:58] <Klinda> ups
[18:34:11] <Hunts> Klinda: that is not really how it works, you use one example, the order might be random, but in the end all examples are used
[18:34:16] <Hunts> its a sequential process
[18:34:30] <Klinda> no true
[18:34:53] <Klinda> I mean so what's the point to use batch with the full entrie training set?
[18:35:49] *** Quits: jailop (~jailop@ool-944bb915.dyn.optonline.net) (Ping timeout: 265 seconds)
[18:35:54] <Klinda> I understand that batch gradient descent --> in the formula you will use the full training set
[18:36:08] <Hunts> only in gradient descent you use the full training set in a single batch
[18:36:16] <Hunts> but these are silly definitions
[18:36:24] <Hunts> people usually say gradient descent to mean mini-batch gradient descent
[18:36:52] <Klinda> mini-batch uses a subset of training set, right ?
[18:37:29] <Klinda> so if I have 1,2,3,4,5 subsets
[18:38:02] <Klinda> the algorithm does the update only using the subset 1 ?
[18:38:10] <Klinda> (for example) ?
[18:38:45] <Hunts> no, man, did you read up what is a batch?
[18:38:55] <Hunts> it is a subset, yes, but it is sequential
[18:39:06] <Klinda> what do you mean?
[18:39:07] <Hunts> you use all the subsets, but one at a time to compute gradients
[18:39:30] <Klinda> so the subsets are randomly picked?
[18:39:35] <Klinda> at each update?
[18:39:45] <Klinda> until it converges?
[18:40:06] <Hunts> no
[18:40:11] <Hunts> sequentially
[18:40:47] <Klinda> as I understand you pick one subset and you use the formula
[18:41:26] <Hunts> what formula?
[18:41:44] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[18:42:22] <Klinda> do you want vectorize formula or the other?
[18:42:52] <Hunts> I want the name of the formula
[18:43:07] <Klinda> Stochastic Gradient Descent Xd
[18:43:31] <Klinda> but in general the gradinet descent one
[18:43:47] <Hunts> too many things can have that name
[18:43:55] <Hunts> I used the description of gradient update
[18:44:03] <Klinda> you have the formula Oj = Oj - alfa (h(x)-y) xj
[18:44:13] <Hunts> because variations of gradient descent are just how the gradient is computed
[18:44:35] <Klinda> this is formula is specifc for every theta
[18:44:49] <Klinda> j
[18:46:41] <Klinda> the in this formula you repeat this process for every theta... than if you repeat this process with the full training set --> batch gradient descent, if you repeat the process using one random example at each update --> sthocastic gradient descent
[18:47:09] <Klinda> theta*
[18:48:05] <Hunts> its not one random example for each update
[18:48:27] <Hunts> you take one example at a time, until you take all of them
[18:49:08] <Klinda> so why using batch is more expensive?
[18:49:48] <Klinda> with sthocastic you can converge before?
[18:49:55] <Klinda> stochastic*
[18:50:05] <Hunts> by batch you mean what exactly?
[18:50:15] <Klinda> full training set
[18:50:50] <Klinda> I mean if you take full training set I understand the computation is really slow for each step
[18:51:01] <Hunts> yeah because it requires a lot more computation and memory
[18:51:33] <Klinda> but in the end you do the same calculus if you pick only one example at time.... until full training set
[18:51:59] <Hunts> no
[18:52:08] <Gurkenglas> hodapp, "If the bits match, say 66%. If they don't, say 0%." is the _optimal_ discriminator in my example. On the logit scale, 66% is "1 bit". Our network should be expected to converge to this.
[18:52:23] <Klinda> in some steps can you take the same sample you used before Hunts?
[18:52:27] <Klinda> I don't understand then
[18:52:31] <Hunts> Klinda: not in the same epoch
[18:52:38] <hodapp> Gurkenglas: but why should it be expected to converge to this?
[18:52:54] <Klinda> Hunts: so you randomly pick it, as I said
[18:53:16] <Hunts> Klinda: I already told you that is wrong
[18:53:21] <Hunts> it is not randomly picked
[18:53:39] <Klinda> an epoch is just the value of the cost/loss function?
[18:53:54] <Hunts> no!
[18:54:06] <Hunts> I think you should go back to the book and keep reading
[18:54:18] <Hunts> an epoch is the whole pass over the training set
[18:54:25] <Hunts> its not even a value
[18:54:32] <Klinda> ah ok
[18:54:41] <Klinda> yes
[18:54:53] <Gurkenglas> hodapp, the discriminator would, as usual as far as I know?, be trained with cross-entropy loss, that is, minimizing the entropy of its output with respect to the true distribution. It is very easy for basically any initial model to be trained into the if-else statement i described.
[18:55:34] <Klinda> so tell me why is different compute the full training set on each update vs compute one sample on each update
[18:56:11] <Klinda> in the end you do the same calculus
[18:56:21] <Hunts> its not the same computation
[18:56:27] <Hunts> because you do not get the same gradient
[18:56:32] <Hunts> and each gradient update is different
[18:56:41] <Hunts> (weights are changing after each gradient update)
[18:56:50] <Gurkenglas> hodapp, maybe I misunderstood the question. We should expect a handwritten-digit classifier to converge to something that does no worse than random, right?
[18:57:43] <Gurkenglas> A discriminator is just a classifier with two classes.
[18:58:19] <Gurkenglas> (where you usually have 50% of each class in the training data afaik)
[19:01:38] <Gurkenglas> For the sake of argument we can have the discriminator in our example be a giant lookup table. For every pair of 10-bit vectors where the bit matches, the corresponding table entry will converge to 66%, right?
[19:04:27] <Klinda> yes Hunts but intutivly I don't get it the cause is more "faster", if in the end you scan the entrie training set... is possible that it can converge before it, right?
[19:07:49] <Hunts> Klinda: because evaluating gradient in the whole training set is not always possible to do, you would go out of memory
[19:08:48] <Klinda> after I watched the stanford  lesson, I completly understand another thing ahah
[19:09:52] <Klinda> but in this is way is possible to converge also without doing all the steps to scan the entrie training set?
[19:10:05] <Klinda> if you do one by one?
[19:10:20] <Hunts> what?
[19:10:32] <Klinda> if you pick only one example at each update
[19:10:42] <Klinda> is possible to converge before you exahusted
[19:10:45] <Klinda> all the examples?
[19:11:07] <Hunts> maybe, but that would be random
[19:11:12] <Hunts> what does convergence mean to you?
[19:11:17] *** Quits: sinaowolabi (~SinaOwola@41.58.226.176) (Ping timeout: 256 seconds)
[19:13:18] <Klinda> in stanford they just: pick a smaller constant and check the value of the gradient, or check the difference between O^(t) - O^(t-1), or the J(theta^(t)) - J(theta^(t-1))..... my teacher I think just plot the epocs and see if there the cost function doesn't change a lot?
[19:13:52] <Klinda> I mean the smaller constant  < that condtions
[19:14:46] <Klinda> math speaking convergence is when you reach the global minium
[19:26:19] <Klinda> so Hunts you use at each update a different sample in the stochastic, and in mini-batch you use a different subset of the training set... both of them until an epoch (hopefully not, better if before xD)
[19:33:21] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[20:04:40] *** Joins: mnl (~mnl@user/mnl)
[20:12:18] <Klinda> in page 7 https://cs229.stanford.edu/notes2021spring/notes2021spring/lecture2_draft.pdf
[20:12:20] <SigmoidFroid> ⇒  superusest.IE RNNcf definitions Linear Regression gate stockade gnament descent 1 NormalEquations Superuseble arning Giver i TrainingSET Preachy Cx cy Lemay x tX yheu h X 4 De find good h X Y hypothesis Image containseat TEXT IS HATESreecu thisJOB of trainingalgorithm House Dara Price WE OSE ch ON new DATA x call this (4M)
[20:12:41] <Klinda> seems that is useful because you can avoid picking duplicates
[20:17:33] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[20:18:23] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Max SendQ exceeded)
[20:18:58] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[20:37:19] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df047-236.dhcp.inet.fi) (Read error: Connection reset by peer)
[21:16:16] *** Quits: trace987 (~trace@ip5b429834.dynamic.kabel-deutschland.de) (Remote host closed the connection)
[21:24:15] <dostoyevsky2> pdf2text produces `TEXT IS HATE' for above pdf...  *scratches head*
[21:28:29] *** Joins: Colt (~Colt@user/colt)
[21:29:00] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[21:29:21] *** Joins: Klinda (~superleag@user/klinda)
[21:32:19] *** Quits: jinsun (~quassel@user/jinsun) (Read error: Connection reset by peer)
[21:35:52] *** Joins: jinsun (~quassel@user/jinsun)
[21:36:45] *** Joins: jlrnick (~josephler@2a01cb040a159400d9d2762a1f23b84a.ipv6.abo.wanadoo.fr)
[21:36:59] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 252 seconds)
[21:39:40] *** Joins: SiegeLord (~sl@user/siegelord)
[21:50:29] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[22:17:47] <Klinda> are you talking to my pdf dostoyevsky?
[22:18:54] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[22:41:53] *** Joins: tomeaton17 (~tomeaton1@92.234.2.175)
[22:43:32] *** Quits: Starfoxxes (~Starfoxxe@2a02:8070:5390:d00:12bf:48ff:feb8:38c8) (Ping timeout: 252 seconds)
[22:54:33] *** Quits: Andrynet1 (~Andrynet@186.46.205.112) (Ping timeout: 256 seconds)
[22:56:23] *** Joins: Andrynet1 (~Andrynet@190.152.206.29)
[22:59:28] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: chill out &eat the heat with top-shelf winter hacks YAKFRM212112122121)
[23:06:56] *** Joins: Starfoxxes (~Starfoxxe@2a02:8070:5390:d00:12bf:48ff:feb8:38c8)
[23:22:06] *** Quits: tomeaton17 (~tomeaton1@92.234.2.175) (Quit: Client closed)
[23:26:17] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[23:31:10] *** Joins: trace987 (~trace@ip5b429834.dynamic.kabel-deutschland.de)
[23:44:45] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[23:45:05] *** Joins: Klinda (~superleag@user/klinda)
