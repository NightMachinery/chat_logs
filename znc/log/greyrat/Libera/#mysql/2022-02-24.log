[00:04:09] <skyfall> How are sensitive data in a MySQL database be shared to developers? (assuming the database admin doesn't give full access to developers)? Will the db admin create "SQL Views" and export those non-sensitive view to the developers? Where exactly are those views stored and given to the developers? I've never worked for a company, so I don't have a
[00:04:09] <skyfall> background in this, sorry.
[00:05:25] <lopid> GRANTs, initially
[00:05:45] *** Quits: lara99 (~lara99@93-36-163-139.ip61.fastwebnet.it) (Quit: Client closed)
[00:08:18] <skyfall> I see. Yeah, GRANTs makes sense, but how are the views distributed? Are SQL Views created at the time of database creation? Like `CREATE TABLE user_table (user_id ..., name ..., password ..., sensitive_data_1 ..., sensitive_data_2 ...); CREATE VIEW export_to_developers AS SELECT user_id, name, password FROM user_table;` ? At which point are these
[00:08:18] <skyfall> views created by db admin?
[00:11:58] <lopid> they can be. a database has a good change to exist already
[00:12:25] <lopid> it's prudent to make a record of such configuration, but not necessary
[00:21:11] *** Joins: funhouse (~funhouse@user/funhouse)
[00:28:50] *** Quits: lamneth (~IceChat95@2607:fa49:2107:c900:f843:7d47:1ad2:a4d2) (Ping timeout: 260 seconds)
[00:33:47] *** Joins: gamara (~gamara@cpe9050cade20e3-cm9050cade20e0.cpe.net.cable.rogers.com)
[00:35:03] *** Quits: funhouse (~funhouse@user/funhouse) (Quit: Client closed)
[00:39:18] <dlam> `
[00:39:34] *** Quits: magga (magga@ti0036a400-3577.bb.online.no) (Remote host closed the connection)
[00:39:50] *** Joins: magga (magga@ti0036a400-3577.bb.online.no)
[00:40:50] <dlam> noob full-text search guy here:  can i test MATCH(AGAINST(... manually at the shell?  i wanna test patterns against specific strings!
[00:45:37] <lopid> why not?
[00:45:56] *** Joins: lamneth (~IceChat95@modemcable025.99-56-74.mc.videotron.ca)
[00:47:50] <zero> how can i escape ' on user passwords when i do "identified by PASSWITH'HERE" ?
[00:48:09] <lopid> try ''
[00:48:41] <lopid> SELECT 'foo''bar'
[00:49:16] <zero> error in sql syntax
[00:49:42] <lopid> you are quoting the entire password?
[00:50:22] <zero> yes
[00:51:18] <zero> ... identified by 'pass''word';
[00:51:53] <thumbs> what is the full error?
[00:53:42] <zero> ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'identified by 'pass''word'' at line 1
[00:54:07] <lopid> what's before that?
[00:55:10] <zero> mysql > grant all on *.* to 'user'@'localhost' identified by 'pass''word';
[00:56:16] <lopid> what mysql version?
[00:57:20] <zero> 8.0.28-0ubuntu0.20.04.3 for Linux on x86_64 ((Ubuntu))
[00:57:56] <lopid> that was a copypaste, no typo?
[00:57:57] <zero> oh
[00:58:09] <zero> the password also contains an underscore
[00:58:23] <zero> maybe i have to escape it also?
[00:58:42] <lopid> no
[00:59:00] <lopid> but that answers my copypaste question
[00:59:28] <zero> yeah i'm not copy pasting
[00:59:52] <lopid> are typoing?
[01:00:02] <zero> yes. different machine
[01:00:07] <thumbs> no
[01:00:21] <thumbs> you have to create the user separately with 8.0
[01:00:30] <thumbs> you can no longer create a user with the GRANT command.
[01:00:45] <thumbs> please refer to the manual for such changes.
[01:00:49] <zero> the user exists already
[01:01:01] <lopid> then you don't need "identified by…"
[01:01:34] <zero> ah right
[01:01:53] <zero> thanks
[01:16:35] *** Joins: funhouse (~funhouse@user/funhouse)
[01:21:35] *** Joins: onion (~zzz@user/zero)
[01:21:58] <onion> how can i export a whole database to csv?
[01:25:59] <lopid> mysqldump's --fields… options is one way
[01:29:24] *** Quits: brentaarnold (~brentaarn@24.112.56.42) (Ping timeout: 240 seconds)
[01:32:54] *** Quits: magga (magga@ti0036a400-3577.bb.online.no) (Remote host closed the connection)
[01:35:47] *** Quits: gamara (~gamara@cpe9050cade20e3-cm9050cade20e0.cpe.net.cable.rogers.com) (Quit: Client closed)
[01:52:57] *** Quits: shokohsc (~shokohsc@lfbn-idf2-1-1472-129.w92-169.abo.wanadoo.fr) (Quit: Ping timeout (120 seconds))
[01:54:16] *** Joins: shokohsc (~shokohsc@lfbn-idf2-1-1472-129.w92-169.abo.wanadoo.fr)
[01:57:41] *** Quits: nyuszika7h (nyuszika7h@lykos/dev/nyuszika7h) (Read error: Connection reset by peer)
[01:58:05] *** Quits: shokohsc (~shokohsc@lfbn-idf2-1-1472-129.w92-169.abo.wanadoo.fr) (Quit: Ping timeout (120 seconds))
[01:59:25] *** Joins: shokohsc (~shokohsc@lfbn-idf2-1-1472-129.w92-169.abo.wanadoo.fr)
[02:12:34] *** Joins: brentaarnold (~brentaarn@24.112.92.157)
[02:32:25] *** Quits: lamneth (~IceChat95@modemcable025.99-56-74.mc.videotron.ca) (Ping timeout: 256 seconds)
[02:35:02] *** Joins: doug16k (~doug16k@172-97-224-241.cpe.distributel.net)
[02:37:22] <doug16k> what does "MySQL Connection not available" mean (in python connector)
[02:38:09] <doug16k> available?
[02:39:55] <doug16k> the phrase "not available" never occurs in any of the three python connector pdfs
[02:40:44] *** Joins: lamneth (~IceChat95@modemcable025.99-56-74.mc.videotron.ca)
[02:40:49] <doug16k> I guess MySQL already has a boyfriend
[02:42:31] <doug16k> does it mean "oh weird this file descriptor / socket doesn't work"?
[02:42:44] <doug16k> like a fork pulled the rug out?
[02:46:36] <doug16k> https://github.com/mysql/mysql-connector-python/blob/90eaeca65a6bbfc1fd9218aad5303957798215c3/lib/mysql/connector/connection.py#L571
[02:47:49] <doug16k> that catches attribute errors that occur in make_command too. sloppy
[02:48:35] *** Joins: ferdna (~ferdna@user/ferdna)
[02:49:15] <doug16k> it's not from anything, destroys stack trace
[03:01:02] *** Quits: lamneth (~IceChat95@modemcable025.99-56-74.mc.videotron.ca) (Ping timeout: 256 seconds)
[03:05:31] <thumbs> doug16k: check the MySQL error log.
[03:08:38] <doug16k> aww. it missed being 2022-02-02 22:22:22 by 8 minutes :P
[03:10:32] <doug16k> nothing. last log row is 2022-02-11
[03:11:39] <doug16k> is this the log you mean? https://gist.github.com/doug65536/1b661d74d33649e1ef9953d93ce17d58
[03:14:22] <thumbs> yes, and that isn't showing any relevant errors
[03:14:48] <thumbs> doug16k: can you connect with the mysql client on a machine in the same range as the python connector?
[03:15:42] <doug16k> this usually works. it is mostly fine. it is intermittently failing connect with that message
[03:16:11] <thumbs> that is already more useful information
[03:16:18] *** Quits: funhouse (~funhouse@user/funhouse) (Quit: Client closed)
[03:16:21] <doug16k> what would it look like if the server was overwhelmed with connections? connection not available?
[03:17:11] *** Quits: Guest6792 (~vit@chello085216193138.chello.sk) (Ping timeout: 256 seconds)
[03:19:44] <doug16k> it's RDS db.t3.2xlarge. how many connections should that handle? a lot?
[03:19:59] <doug16k> 16GB or so?
[03:20:29] <doug16k> it has a laughable load
[03:21:05] <doug16k> if it is wide open to the internet, would attackers that haven't authenticated yet take up my connections?
[03:22:22] <thumbs> they could
[03:22:28] <thumbs> solution: don't do that
[03:22:33] <doug16k> we have intermittent too many connections, I argue that we should tighten it up, the problem goes away, and nothing gets changed
[03:23:17] <doug16k> I guess that's for #makemanagementlisten
[03:34:48] <doug16k> RDS figures on 12MB of memory per connection? our server has 32GB, freeable is 26GB, so 26000/12 connections?
[03:35:08] <doug16k> ~2000?
[03:35:34] <doug16k> does that sound about right?
[03:37:25] <doug16k> connection count hasn't exceeded 190 in the last month
[03:39:04] <doug16k> I had them upgrade the server to tons of ram to see if it really was a too-many-connections
[03:39:22] <doug16k> apparently not
[03:43:38] <doug16k> I made my own pool that doesn't spin them all up up front, and I even check that the PID didn't change every time someone gets from the pool, in case it is a goofy python multiprocessing child process (because spawn is illegal in lambda)
[03:43:55] <doug16k> lazily creates them
[03:45:05] *** Quits: onion (~zzz@user/zero) (Quit: onion)
[03:45:11] <doug16k> fork child process*
[04:00:51] <KnowledgeShark> Isotopp: I completed my first pull request (You should see it) & the final .SQL file can be found here: https://archive.org/details/harvard.-cap.-meta-data.-jurisdiction.-us-02.21.2022 | You're the Man! And again, Thank you for being my Mentor! ^_^
[04:05:57] <dlam> mysql full-text search:  is tehre a way to see everything is indexed?    I'm doing a search that *should* work but it isnt!  im super puzzled
[04:22:00] <doug16k> dlam, you can look for table scans in performance schema
[04:22:33] <doug16k> look at top query by execution time, it should take you straight to the worst ones
[04:23:26] <doug16k> also look for unused indexes, you might have made indexes it can't actually use
[04:23:53] <doug16k> (because they don't help)
[04:38:12] <dlam> ooo kk (sounds hard, i dunno how query)
[04:38:27] <doug16k> do you use mysqlworkbench?
[04:40:10] <doug16k> in management under performance, there is a thing to set it up. performance reports are there too
[04:43:55] <doug16k> dlam, if you know exactly which query already, then you could just run that SELECT, but put EXPLAIN before SELECT
[04:44:28] <dlam> ooo (i dont!)  me only using mysql shell  on ubuntu/linux!
[04:45:16] <doug16k> it will produce a result that explains the execution plan
[04:46:43] <doug16k> it might tell you which part is hard
[04:50:34] <doug16k> maybe the text search part is fine and a join or subquery is awful
[04:53:15] <doug16k> if it had to look at every record anyway to satisfy another condition, the it might not even bother with the fulltext stuff
[05:24:14] *** Quits: BLZbubba (~mark@185.56.20.124) (Ping timeout: 260 seconds)
[05:24:22] *** Joins: BLZbubba (~mark@185.56.20.124)
[05:30:29] *** Joins: domas (~midom@2620:10d:c090:400::5:a5e8)
[05:30:29] *** Quits: domas (~midom@2620:10d:c090:400::5:a5e8) (Changing host)
[05:30:29] *** Joins: domas (~midom@facebook/engineering/domas)
[05:30:38] <domas> hi
[05:30:45] <domas> how does one limit amount of memory mysql will use?
[05:35:15] <thumbs> domas: being stern works
[05:35:51] <thumbs> seriously, buffers suck
[05:47:26] *** Quits: domas (~midom@facebook/engineering/domas) (Ping timeout: 245 seconds)
[05:48:44] <skyfall> Hey, is this table implementation https://bpa.st/PT5A correct way to implement this sample problem/ER model? https://media.geeksforgeeks.org/wp-content/uploads/20190801135655/333.jpeg I'm not sure if I've implemented the "Payment number" partial key correctly. Is this the correct way to implement the partial key? Like, it's just the FOREIGN KEY of
[05:48:44] <skyfall> the strong entity right?
[05:49:06] *** Joins: Avago_Broadqual0 (~Avago_Bro@74-194-130-160.bcstcmtk03.res.dyn.suddenlink.net)
[05:50:37] *** Quits: Avago_Broadqual (~Avago_Bro@74-194-130-160.bcstcmtk03.res.dyn.suddenlink.net) (Ping timeout: 240 seconds)
[05:50:37] *** Avago_Broadqual0 is now known as Avago_Broadqual
[06:02:55] *** Joins: domas (~midom@2620:10d:c090:400::5:a5e8)
[06:14:50] *** Quits: brentaarnold (~brentaarn@24.112.92.157) (Ping timeout: 256 seconds)
[06:16:55] *** Joins: lamneth (~IceChat95@modemcable025.99-56-74.mc.videotron.ca)
[06:25:42] *** Joins: brentaarnold (~brentaarn@24.112.92.157)
[06:32:39] *** Quits: domas (~midom@2620:10d:c090:400::5:a5e8) (Changing host)
[06:32:39] *** Joins: domas (~midom@facebook/engineering/domas)
[06:32:48] <domas> thumbs: not even thinking about buffers now
[06:33:24] <domas> just... memory footprint of what is passed between optimizer and execution is problematic enough
[06:33:35] <domas> i.e. how in list is represented
[06:34:39] <domas> i.e. WHERE a IN (1,2,3) AND b IN (1,2,3) AND c IN (1,2,3) <--- wheeee
[06:35:03] <thumbs> domas: oh I know what you mean
[06:35:54] <thumbs> skyfall: that schema looks like you are missing key parts
[06:36:12] *** Quits: brentaarnold (~brentaarn@24.112.92.157) (Ping timeout: 240 seconds)
[06:36:42] <thumbs> domas: how many items are you using to test that issue?
[06:42:13] *** Quits: lamneth (~IceChat95@modemcable025.99-56-74.mc.videotron.ca) (Quit: Do fish get thirsty?)
[06:44:44] *** Quits: mickey8 (~user@user/mickey) (Quit: Ping timeout (120 seconds))
[06:44:58] *** Joins: mickey8 (~user@user/mickey)
[06:49:03] <domas> 10 * 100 * 100
[06:49:21] <thumbs> use joins!
[06:49:40] <domas> haha, I tried, optimizer won't use IN list on a joined table
[06:50:22] <domas> it was funny, in one prod case I did AND c+0 IN (1,2,3) essentially
[06:50:33] <domas> to avoid index matching
[06:50:38] <thumbs> no, use my udf that converts 1,2,3 to a table, wrap it in a CTE, and join that CTE
[06:51:03] <domas> magic
[06:51:17] <domas> where's the udf
[06:51:30] <thumbs> huh, I had it somewhere on GH I think
[06:51:35] <thumbs> let's see if I can find it
[06:52:50] <domas> in the end, limiting index dives was useful too
[06:53:00] <domas> doing 100 range scans is cheaper than doing 10000 index dives
[06:53:25] <domas> but generally, if I have a workload where 1000 queries do 10*100*100
[06:53:41] <domas> server memory footprint goes up a lot!
[06:53:59] <domas> there're other similar cases
[06:54:04] <domas> like one I documented 10 years ago!
[06:57:48] *** Quits: Vacuity (~Vacuity@user/vovo) (Ping timeout: 240 seconds)
[06:58:13] <domas> oh cool
[06:58:18] <domas> MyRocks on AWS RDS!
[06:59:03] *** Joins: brentaarnold (~brentaarn@24.112.92.157)
[06:59:37] *** Joins: Vacuity (~Vacuity@user/vovo)
[07:07:35] *** Joins: Emet-Selch (~haise01@user/haise01)
[07:10:37] *** Quits: Azem (~haise01@user/haise01) (Ping timeout: 240 seconds)
[07:10:39] <thumbs> hold on, stupid work
[07:19:50] *** Joins: bob (~bob@cpe-70-120-206-105.hot.res.rr.com)
[07:20:28] *** Quits: bob (~bob@cpe-70-120-206-105.hot.res.rr.com) (Client Quit)
[07:20:42] *** Joins: boberton (~boberton@cpe-70-120-206-105.hot.res.rr.com)
[07:32:38] *** Quits: boberton (~boberton@cpe-70-120-206-105.hot.res.rr.com) (Ping timeout: 272 seconds)
[07:46:28] *** Quits: ferdna (~ferdna@user/ferdna) (Quit: Leaving)
[07:59:15] *** Joins: toastloop (~toastloop@user/toastloop)
[08:09:50] *** Quits: yuesbeez (uid458354@id-458354.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[08:12:05] *** Joins: funnybunny (~funnybunn@user/funnybunny)
[08:13:09] <funnybunny> Can anyone help me figure out why I'm getting "using filesort" from explain? I am ordering by multiple columns, but I have a combined index that includes every single column
[08:20:44] <doug16k> funnybunny, if it has to use a different index to filter, the index of all the columns don't help
[08:20:52] <doug16k> doesn't*
[08:22:01] <funnybunny> It's using the index to filter rows but not for sorting I guess
[08:33:11] <funnybunny> doug16k: Any idea why?
[08:36:53] <doug16k> depends on your where clause
[08:37:27] <doug16k> adding more to an index doesn't mean it is more likely to be used
[08:37:40] <funnybunny> The where clause also only includes columns that are in the index. I have two that check datetines and one that checks a char
[08:38:01] <funnybunny> The index is getting used for filtering the rows but not for sorting
[08:38:09] <funnybunny> I don't get it since it already has the index
[08:38:38] <doug16k> ranges of datetime?
[08:39:05] <funnybunny> No, just >= <somedate>
[08:39:43] <doug16k> so tons of ranges then
[08:39:59] <funnybunny> What do you mean?
[08:40:09] <doug16k> it's two datetime >= ?
[08:40:15] <funnybunny> Yeah
[08:40:19] <funnybunny> Separate columns
[08:40:26] <funnybunny> Like a >= somedate and b >= somedate
[08:40:30] <doug16k> and third index column is that char?
[08:40:33] <funnybunny> Yeah
[08:41:04] <funnybunny> where status = 'something' and a >= somedate and b >= somedate
[08:41:06] <doug16k> ok then the index sucks for the char because it is a bunch of ranges
[08:41:33] <funnybunny> Can you explain what you mean by a bunch of ranges?
[08:41:46] <funnybunny> I thought you meant a >= somedate and a < someotherdate
[08:42:36] <doug16k> if it is sorted status,a,b then if it finds all the a >= then there would be lots a that are >=, and each of those is another range of maybe >= b
[08:43:44] <funnybunny> OK
[08:44:02] <funnybunny> I still don't understand why it uses filesort
[08:44:09] <funnybunny> Why not still use the index?
[08:47:28] <doug16k> let's say it's sorted a,b   and you have these records (10,10), (10,20), (10,30), (11, 10), (11,20), (12, 10), (12, 20), (13, 10), (13, 20)..   ok now find all where a > 10 and b > 10. see the gaps?
[08:48:12] <doug16k> but yeah, it will encounter them in the right order if it really used the index
[08:49:01] <doug16k> every time a changes it has to search for the start of b >=
[08:51:23] <funnybunny> I read this https://dev.mysql.com/doc/refman/8.0/en/order-by-optimization.html
[08:51:38] <funnybunny> Assuming that there is an index on (key_part1, key_part2), the following queries may use the index to resolve the ORDER BY part. Whether the optimizer actually does so depends on whether reading the index is more efficient than a table scan if columns not in the index must also be read.
[08:52:53] <skyfall> thumbs: Sorry, for delayed reply. Key work? Not sure what you mean though.
[08:53:07] <skyfall> You mean Primary Keys?
[08:53:57] <funnybunny> doug16k: I don't understand your exmaple. They are all a > 10 and b > 10
[08:54:11] <doug16k> no, the ,10 aren't greater
[08:54:11] <funnybunny> Oh, nvm, not >=
[08:54:39] <doug16k> ya sorry I was confusing >= and > a bit
[08:54:55] <doug16k> you see what I mean about multiple ranges now though right?
[08:55:36] <doug16k> it is way harder to use an index like that then when you just find the start and end and walk everything between
[08:56:12] <funnybunny> So the index doesn't do any good
[08:56:26] <doug16k> it might not do as much good as it first seemed
[08:56:33] <funnybunny> What is a case where it could be used efficiently?
[08:56:36] <funnybunny> Like without ranges
[08:56:41] <doug16k> it could help if there are a massive number of records
[08:59:16] <doug16k> is the result big? I wouldn't worry about filesort if it isn't a lot
[08:59:33] <funnybunny> About 10k rows
[09:00:36] <doug16k> fits in CPU then. that's tiny
[09:02:07] <doug16k> does it take long?
[09:02:37] <funnybunny> No, but just trying to optimize for many queries at once
[09:02:51] <funnybunny> Want to allow arbitrary sorting on columns
[09:03:05] <funnybunny> Or at least the ones in the index
[09:03:55] <doug16k> you should not just go hunting for random speedups, ever. you should look at performance data and work on the worst things only
[09:04:54] <funnybunny> The thing is that currently it is not possible to sort on so many columns so there will be no data unless I simulate it
[09:07:17] *** Joins: Guest6792 (~vit@chello085216193138.chello.sk)
[09:07:31] <funnybunny> Thinking about your example, I don't see how the index can be useful for sorting in any case except for the first column. Seems like there will always be gaps
[09:10:58] <doug16k> if you had some = conditions for the first few columns of the index you used, then did a range on the next column of the index, then it could find the start and end and everything between is in order
[09:11:04] *** Quits: mmlj4 (~mmlj4@ip174-69-109-162.no.no.cox.net) (Ping timeout: 256 seconds)
[09:11:51] <doug16k> if you only read columns that are included in the index, then it can even skip reading the actual rows and only read the index
[09:12:24] <funnybunny> But as soon as I do a range on the first column, I can't do a range on anything else?
[09:12:28] <funnybunny> Basically
[09:12:33] <doug16k> it would be lots of ranges
[09:12:48] <doug16k> it might not decide to even approach it like that
[09:13:12] <doug16k> it might decide it might as well do something else for some other reason
[09:14:03] <doug16k> https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html
[09:15:01] <doug16k> https://dev.mysql.com/doc/refman/8.0/en/multiple-column-indexes.html
[09:15:17] <funnybunny> If I have a > somedate and b > somedate and somedate is always the same, is there a way to create a good index?
[09:15:30] <funnybunny> Like it is the same for a and b
[09:15:38] <funnybunny> in the query
[09:15:53] <doug16k> you could create a generated column and index that
[09:16:11] <doug16k> use the min or max depending on which way
[09:16:26] <funnybunny> What do you mean?
[09:17:00] <funnybunny> Oh
[09:17:03] <funnybunny> I see
[09:17:10] <funnybunny> A column which is the min of a and b
[09:17:19] <doug16k> say you needed to query a massive number of records for a > x or b > x, then I can generate max(a,b) and use that to filter out most of them, then actually check a > x and b > x against those
[09:17:44] <doug16k> how much that helps depends on the values
[09:17:59] <funnybunny> I don't need a > x or b > x. I need just a > x and b > x
[09:18:33] <doug16k> it would work for and too
[09:18:42] <doug16k> it's just a hint. it maybe included
[09:18:44] <funnybunny> I think you would want to use min
[09:18:58] <doug16k> yeah I didn't think it through :)
[09:19:57] <funnybunny> Do you know how I would write a query that adds a column and sets it to the min of a and b?
[09:20:11] <funnybunny> Do I have to add the column separately?
[09:20:15] <doug16k> you wouldn't. generated columns are added
[09:20:23] <doug16k> alter table
[09:20:34] <doug16k> when you insert or update it might fixup the generated column and update the index
[09:20:47] <doug16k> but wait though
[09:20:47] <funnybunny> Oh, I can have it automatically insert the min?
[09:20:52] <doug16k> yeah
[09:20:57] <funnybunny> I thought I had to do that in my program
[09:22:54] *** Joins: mmlj4 (~mmlj4@ip174-69-109-162.no.no.cox.net)
[09:23:06] <funnybunny> doug16k: Why did you say but wait?
[09:23:40] <doug16k> whether that helps depends on how many things it excludes vs just dropping the b condition and brute force scanning
[09:24:33] <doug16k> it might even decide not to use it
[09:25:59] <doug16k> I was just throwing out a way that you can optimize multiple ranges into one range then scan
[09:26:33] <doug16k> maybe awesome or crap depending on the values
[09:27:26] <doug16k> all this maybe is why you react to measured problems when optimizing, not pre-optimize
[09:28:59] <doug16k> you can try it and see that it's amazing, or doesn't help
[09:29:10] <funnybunny> I am trying it
[09:31:52] <funnybunny> Why does it say 0 rows affected when dropping an index?
[09:32:30] <funnybunny> Same with adding
[09:33:07] <doug16k> I always guessed that it is because there are no result rows, it tells you the affected record count, which is nonsensical when altering
[09:34:06] <doug16k> 0 rows were affected, it's not wrong
[09:34:32] <funnybunny> Holy crap
[09:34:35] <funnybunny> It filtered half
[09:36:21] <funnybunny> Huh
[09:36:24] <funnybunny> But it didn't use my key
[09:37:24] <doug16k> you added the min(a,b) > x to the where?
[09:37:40] <funnybunny> Yes
[09:37:51] <funnybunny> I'm checking to make sure I didn't mess something up
[09:39:15] *** Quits: jlc (~jlc@cpe-107-15-173-212.nc.res.rr.com) (Ping timeout: 250 seconds)
[09:40:34] *** Joins: jlc (~jlc@cpe-107-15-173-212.nc.res.rr.com)
[09:46:15] <funnybunny> It says it's a possible key but it's "Using where"
[09:46:23] <funnybunny> doug16k: ^
[09:48:00] <funnybunny> It uses it if I don't add an order by
[09:50:17] *** Quits: brentaarnold (~brentaarn@24.112.92.157) (Ping timeout: 240 seconds)
[09:58:07] <funnybunny> Is the number under "rows" in explain before or after "filtered"?
[10:01:05] *** Joins: brentaarnold (~brentaarn@24.112.92.157)
[10:07:12] *** Quits: rvalue (~rvalue@user/rvalue) (Read error: Connection reset by peer)
[10:07:25] *** Joins: rvalue (~rvalue@user/rvalue)
[10:15:37] <funnybunny> I don't understand why I get filtered 33.33 with no index and filtered 50.00 with an index even though explain in both cases says no index is used
[10:25:49] *** Joins: magga (magga@ti0036a400-3577.bb.online.no)
[10:27:20] *** Quits: toastloop (~toastloop@user/toastloop) (Remote host closed the connection)
[10:27:45] *** Joins: toastloop (~toastloop@user/toastloop)
[10:32:42] *** Joins: Naktibalda (~Naktibald@88.135.22.17)
[10:39:28] *** Quits: funnybunny (~funnybunn@user/funnybunny) (Ping timeout: 272 seconds)
[10:51:39] *** Joins: ejjfunky (~ejjfunky@125.164.22.188)
[10:53:01] *** Joins: funnybunny (~funnybunn@user/funnybunny)
[10:53:09] *** Quits: ejjfunky (~ejjfunky@125.164.22.188) (Remote host closed the connection)
[10:53:34] *** Joins: ejjfunky (~ejjfunky@125.164.21.7)
[11:09:03] *** Joins: csm3105 (~csm3105@143.red-83-48-84.staticip.rima-tde.net)
[11:12:25] *** Quits: funnybunny (~funnybunn@user/funnybunny) (Ping timeout: 272 seconds)
[12:05:40] *** Quits: toastloop (~toastloop@user/toastloop) (Quit: WeeChat 3.5-dev)
[12:08:48] *** Quits: brentaarnold (~brentaarn@24.112.92.157) (Read error: Connection reset by peer)
[13:19:54] *** Joins: sultand (~sultand@office.rackhosting.com)
[13:20:24] *** Quits: csm3105 (~csm3105@143.red-83-48-84.staticip.rima-tde.net) (Ping timeout: 256 seconds)
[13:28:01] *** Joins: toastloop (~toastloop@user/toastloop)
[13:39:43] *** Joins: csm3105 (~csm3105@143.red-83-48-84.staticip.rima-tde.net)
[13:44:15] *** Quits: domas (~midom@facebook/engineering/domas) (Ping timeout: 256 seconds)
[14:13:25] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[14:21:33] *** Quits: acidsys (~LSD@2a03:4000:55:d20::3) (Excess Flood)
[14:22:12] *** Joins: acidsys (~LSD@2a03:4000:55:d20::3)
[14:58:29] *** Quits: acidsys (~LSD@2a03:4000:55:d20::3) (Excess Flood)
[14:59:09] *** Joins: acidsys (~LSD@2a03:4000:55:d20::3)
[15:20:49] *** Quits: sultand (~sultand@office.rackhosting.com) (Ping timeout: 256 seconds)
[15:44:12] *** Joins: maroloccio (~marolocci@pousada3ja.mma.com.br)
[15:57:35] *** Quits: shibboleth (~shibbolet@user/shibboleth) (Ping timeout: 240 seconds)
[16:12:36] *** Quits: jkavalik (~jkavalik@ip-78-102-141-139.net.upcbroadband.cz) (Ping timeout: 240 seconds)
[17:06:21] *** Quits: uhu (~uhu@static.101.168.216.95.clients.your-server.de) (Excess Flood)
[17:06:45] *** Joins: uhu (~uhu@static.101.168.216.95.clients.your-server.de)
[17:16:15] <CyberCr33p> Hello. I need some help --> https://dpaste.com/GR5NHL45B.txt  . What indexes can I create to speed it up ?
[17:16:45] <CyberCr33p> Without "order by" it's fast
[17:17:19] <Xgc> CyberCr33p: SHOW CREATE TABLE ...\G   for each table.
[17:18:10] <Xgc> CyberCr33p: Why did you use a RIGHT JOIN if you are ordering by columns in the first table?
[17:18:28] <CyberCr33p> Xgc It's a joomla extension called K2
[17:18:29] <Xgc> CyberCr33p: and grouping by t1.id
[17:19:02] <Xgc> CyberCr33p: That wasn't my point.  The first table in that RIGHT JOIN will produce nulls.  Do you know if nulls are expected?
[17:19:55] <Xgc> Oh. That's just wrong logic.  It's an inner join.
[17:20:20] <Xgc> Whatever produced that logic doesn't understand what RIGHT JOIN does.
[17:20:37] <CyberCr33p> https://dpaste.com/C4L5FWFEZ.txt
[17:21:12] <CyberCr33p> https://getk2.org this is the joomla extension
[17:29:22] *** Quits: toastloop (~toastloop@user/toastloop) (Quit: WeeChat 3.5-dev)
[17:34:24] *** Joins: lamneth (~IceChat95@modemcable025.99-56-74.mc.videotron.ca)
[17:37:23] <CyberCr33p> Xgc any idea if I can use an index to make it faster?
[17:38:43] <Xgc> CyberCr33p: What kind of performance are you seeing now?
[17:39:39] <CyberCr33p> Xgc now it takes 2.7 sec - without order by it takes 0.0035 sec
[17:42:21] <Xgc> CyberCr33p: Just for kicks, try reducing the select list to: SELECT i.id, i.catid FROM and see what the timing is.
[17:42:22] <CyberCr33p> Xgc also if I change ORDER BY i.created with ORDER BY i.id it's fast
[17:43:19] *** Joins: amoe (~amoe@visarend.solasistim.net)
[17:43:21] <CyberCr33p> Xgc I prefer not to touch the query in php files. As I don't know where else is used. That's why I try to see if an index will help
[17:43:27] <Xgc> CyberCr33p: It's possible you can reduce the logic to find the keys of the rows, and then grab the rest of the columns.
[17:43:40] <Xgc> CyberCr33p: The test is to find out if a possible change could help.
[17:44:07] <Xgc> CyberCr33p: The final query would be identical, logically.
[17:44:20] <CyberCr33p> ok
[17:45:11] <Xgc> I' hoping you're just testing via the command line client.
[17:45:28] <CyberCr33p> I use phpmyadmin
[17:45:31] <amoe> I have a many to many relationship between products and properties.  Properties have an "is_active" boolean flag.  I want to query all products where all associated properties have is_active = TRUE.  Not sure how to do this
[17:46:03] <Xgc> amoe: Add that logic to the ON clause.
[17:46:04] <CyberCr33p> SELECT i.id, i.catid  FROM ..... takes 0.14 sec
[17:46:44] <Xgc> CyberCr33p: Now you can wrap that (with the LIMIT) in a derived table and select the full set of columns.  The ids are everything you need to know.
[17:47:17] <Xgc> CyberCr33p: If I read your query correctly, you only wanted the first 4 results.
[17:47:37] <CyberCr33p> Xgc I will try because the query string is split in many parts in the php file, it's automatically generated by some options
[17:47:41] <CyberCr33p> yes
[17:49:11] <Xgc> SELECT full column list FROM (SELECT i.id, i.catid FROM ... ORDER BY .. LIMIT 0, 4) AS xxx JOIN items ON xxx.id = items.id JOIN cats ON xxx.catid = cats.id; to get just those 4 rows.
[17:49:28] <Xgc> That should be quick too.
[17:49:52] <CyberCr33p> ok thank you for the help
[17:52:48] <Xgc> amoe: Oh.  You mean "all" as in all the properties are is_active = 1 and none are is_active = 0.
[17:52:56] *** Quits: bytestream (~bytestrea@user/bytestream) (Remote host closed the connection)
[17:53:02] <Xgc> amoe: Several ways to do that.
[17:53:14] <amoe> Xgc: Yeah, that's right
[17:53:18] <Naktibalda> amoe: you could join each property separately,   LEFT JOIN properties AS p5 ON products.id = p5.product_id AND p5.name ='prop_name' AND is_active = true
[17:53:52] <amoe> Naktibalda: not practical in this case, unfortunately
[17:54:35] <Naktibalda> or you could do SELECT FROM products JOIN properties ON products.id = properties.product_id HAVING SUM(properties.name='prop_name' AND is_active=true) > 0 AND  SUM(properties.name='prop_name2' AND is_active=true) > 0
[17:54:49] <Naktibalda> add as many SUM expressions as you need properties
[17:55:01] <Naktibalda> I missed GROUP BY products.id
[17:55:15] <amoe> In this case I have about ~1000 properties
[17:55:21] <Naktibalda> and you need them all?
[17:56:28] <Naktibalda> ah, you want to get products having 0 properties with is_active=FALSE, right?
[17:56:39] <amoe> any product can be linked to any number of properties, the product should only be returned if all properties it's linked to has 'is_active' as true.
[17:56:47] <amoe> yep, exactly
[17:56:49] *** Quits: maroloccio (~marolocci@pousada3ja.mma.com.br) (Quit: WeeChat 3.0)
[17:57:21] <Naktibalda> SELECT products.* FROM products LEFT JOIN properties ON products.id = properties.product_id AND properties.is_active = FALSE WHERE properties.product_id IS NULL
[17:57:23] <Xgc> amoe: SELECT pp.prod_id FROM prod_props AS pp JOIN props AS p ON p.prod_id = pp.prod_id GROUP BY pp.prod_id HAVING SUM(p.is_active = 0) = 0
[17:58:09] <Xgc> amoe: I'm assuming 0 and 1 are the only values of is_active.
[17:59:50] <Xgc> amoe: That's just to get the ids.  That can be expanded to obtain the rest of the products detail, as needed.
[18:04:23] <amoe> Xgc: that works!  thank you :)
[18:17:55] <CyberCr33p> Xgc thank you for the help
[18:20:13] <CyberCr33p> Xgc I finally change from K2 settings the articles ordering from "Most recent first" (uses i.created) to "Default" (uses i.id DESC) which is the same as the latest articles are at the top of the page
[18:34:54] *** Joins: shibboleth (~shibbolet@user/shibboleth)
[19:00:47] *** Quits: ejjfunky (~ejjfunky@125.164.21.7) (Read error: Connection reset by peer)
[19:31:36] *** Joins: enlinq (~enlinq@2603-8080-f60c-6d68-61fc-45c1-5219-d1e0.res6.spectrum.com)
[19:36:17] *** Joins: tex (~super@user/dix)
[19:37:37] *** Joins: DrowningElysium (uid190788@user/drowningelysium)
[19:40:36] *** Quits: csm3105 (~csm3105@143.red-83-48-84.staticip.rima-tde.net) (Ping timeout: 240 seconds)
[19:55:19] *** Parts: zero (~z@user/zero) ()
[19:56:59] *** Quits: enlinq (~enlinq@2603-8080-f60c-6d68-61fc-45c1-5219-d1e0.res6.spectrum.com) (Quit: Leaving)
[19:57:16] *** Joins: enlinq (~enlinq@2603-8080-f60c-6d68-61fc-45c1-5219-d1e0.res6.spectrum.com)
[20:19:48] *** Joins: enlinqz (~enlinqz@2603-8080-f60c-6d68-61fc-45c1-5219-d1e0.res6.spectrum.com)
[20:19:49] *** Quits: enlinq (~enlinq@2603-8080-f60c-6d68-61fc-45c1-5219-d1e0.res6.spectrum.com) (Quit: Leaving)
[20:28:48] <enlinqz> It's not possible to add foreign keys in strict mode when a table is not strict-mode compatible: https://stackoverflow.com/questions/71036129/mysql-cannot-create-fk-because-incorrect-timestamp-values/71037118
[20:28:52] <enlinqz> Is it possible, without adding foreign keys, to check whether a table is strict-mode compatible?
[20:31:25] <Naktibalda> strict mode is a connection property, not table property
[20:40:07] *** Quits: gutts (~gutts@li131-142.members.linode.com) (Quit: leaving)
[20:43:57] *** Quits: ueberall (ueberall_l@user/ueberall) (Quit: ZNC)
[20:45:56] *** Joins: ueberall (ueberall_l@user/ueberall)
[20:57:03] *** Joins: domas (~midom@163.114.132.4)
[21:04:27] *** Joins: csm3105 (~csm3105@2a02:2e02:3e4:d00:4b21:8c8f:fc36:ee33)
[21:09:04] *** Quits: csm3105 (~csm3105@2a02:2e02:3e4:d00:4b21:8c8f:fc36:ee33) (Client Quit)
[21:19:26] *** Quits: domas (~midom@163.114.132.4) (Read error: Connection reset by peer)
[21:19:35] *** Joins: domas (~midom@2620:10d:c090:400::5:5296)
[21:35:23] *** Quits: Guest6792 (~vit@chello085216193138.chello.sk) (Ping timeout: 256 seconds)
[21:48:55] *** Joins: Guest6792 (~vit@chello085216193138.chello.sk)
[22:03:55] *** Quits: Naktibalda (~Naktibald@88.135.22.17) (Quit: Some folks are wise, and some otherwise.)
[23:22:07] <enlinqz> Some tables are not compatible with strict mode for various reasons, however. Right? For instance a TIMESTAMP column must not have a "zero date" ('0000-00-00 00:00:00').
[23:22:59] <enlinqz> I want to know if a table passes all the strict checks. Something like `CHECK AGAINST STRICT RULES table_name;` or `CHECK AGAINST SQL_MODE table_name;`
[23:23:36] <enlinqz> The only way I currently know how to do that is to try to add a foreign key to the table.
[23:31:06] *** Joins: yuesbeez (uid458354@id-458354.tinside.irccloud.com)
[23:44:33] *** Joins: nyuszika7h (nyuszika7h@lykos/dev/nyuszika7h)
