[00:06:02] *** Quits: marcello42 (~mp@2001:1a81:125e:800:c345:e197:48e:2f4b) (Ping timeout: 240 seconds)
[00:11:32] <Jong> Anyone know where I can find a very small sample of the imagenet dataset?
[00:11:56] <Jong> Anywhere from 1 to 10 examples? In that range?
[00:12:05] <Jong> honestly, a single sample would work
[00:22:38] <Jong> nevermind I found one
[00:25:17] <Hunts> in google images
[00:27:07] *** Quits: Crowley99 (~Crowley99@208.59.191.14) (Quit: Connection closed)
[00:51:33] *** Joins: mnl (~mnl@user/mnl)
[01:12:24] *** Joins: SiegeLord (~sl@user/siegelord)
[01:25:55] *** Quits: sinaowolabi_ (~SinaOwola@160.152.24.117) (Ping timeout: 256 seconds)
[01:27:40] *** Joins: sinaowolabi_ (~SinaOwola@160.152.24.117)
[01:43:39] *** Quits: mnl (~mnl@user/mnl) (Quit: cya~)
[01:58:03] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[03:03:17] *** Quits: palasso (~palasso@user/palasso) (Ping timeout: 268 seconds)
[03:03:30] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: "freedom on land is delusion")
[03:06:15] *** Joins: Malvolio (~Malvolio@user/malvolio)
[04:17:02] <Jong> anyone know how to compute the number of flops for a block consisting of conv2d + batchnorm ?
[04:59:50] *** Quits: sinaowolabi_ (~SinaOwola@160.152.24.117) (Ping timeout: 268 seconds)
[05:13:58] *** Joins: sinaowolabi_ (~SinaOwola@169.159.110.74)
[05:29:26] *** Quits: sinaowolabi_ (~SinaOwola@169.159.110.74) (Ping timeout: 268 seconds)
[05:30:50] *** Joins: sinaowolabi_ (~SinaOwola@169.159.110.74)
[05:47:17] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 240 seconds)
[05:49:16] *** Joins: shoky (uuuggg@141.226.193.67)
[06:06:06] <Jong> I just realized something interesting.   "Dimension" is equivocal in NLP.   In one meaning, the dimension of an embedding vector is simply how long the vector is.  In the other meaning, the dimension refers to the dimensions of the tensor. An embedding vector would be 1 dimensional. The shape of the tensor would be a tuple consisting of one number. (if that is a tuple?)
[06:07:17] <Jong> It's the not so obvious equivocations that are the most dangerous!
[06:08:41] *** Quits: sinaowolabi_ (~SinaOwola@169.159.110.74) (Ping timeout: 256 seconds)
[06:11:19] <Jong> I suppose channels in an image can be considered the # of dimensions of a pixel. Agree or disagree?
[06:13:24] <mefistofeles> Jong: it depends on the conventions of the field, in my opinion, the dimensions of an image is 2 (2D), but not sure if that's the convention in terms of ML
[06:16:25] <hodapp> "dimensionality" and "rank" are also often confused
[06:16:27] <Jong> it's a grey area.  In some contexts the word "dimension" requires that each dimension is orthogonal to each other. Free parameters where the movement of one does not impact the other. The way that temperature and sound have nothing to do with each other.
[06:16:56] <Jong> but I'm no expert
[06:18:29] <mefistofeles> hodapp: yeah, for tensors it gets frequently confused
[06:19:11] <mefistofeles> Jong: actually temperature and sound have a lot to do with each other, but I understand what you are trying to say :)_
[06:19:25] <hodapp> but yeah, things like Pytorch use 1D, 2D, 3D, etc. to refer to a tensor of rank 1, 2, 3, etc.
[06:20:15] <hodapp> whereas in other settings, if they call a dataset N-dimensional all they might mean is that it consists of M points in N-dimensional space, i.e. is MxN or NxM
[06:21:40] <hodapp> pytorch largely follows numpy convention there, but in Xarray (for instance) a dimension is a completely orthogonal degree of freedom
[06:22:00] *** Joins: sinaowolabi_ (~SinaOwola@169.159.122.81)
[06:22:38] <Jong> An N-dimensional dataset means that each sample is a tuple of N numbers in it?
[06:23:33] <hodapp> it can mean that
[06:23:50] <hodapp> with each sample as an N-tuple or N-vector or however you want to look at it
[06:35:42] <Jong> I notice that ML/AI teachers almost never say "by x, you might think I mean such-and-such, but that's not what i mean. I actually mean ..."  Yet there are so many instances where people misinterpret a word.   Dimensions of data is one. Thee may think tensor shape, not data vector size.  The other is the meaning of making a "model". Some people think model airplanes, not neural net topology
[06:41:23] <hodapp> their usage of 'model' fairly well follows what is used in statistics and statistical learning
[07:03:41] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[07:17:22] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[07:20:37] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 240 seconds)
[07:44:22] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:68c6:1034:8cf9:5a2d)
[07:55:40] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[07:59:29] <Jong> hodapp  it would follow statistics if only trained models were called models and untrained models had another word altogether. A freshly initialized net is technically a "not-modelling-anything" net.
[08:05:48] *** Quits: sinaowolabi_ (~SinaOwola@169.159.122.81) (Ping timeout: 250 seconds)
[08:06:54] *** Joins: sinaowolabi_ (~SinaOwola@169.159.122.81)
[08:45:11] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[08:47:27] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[08:56:59] *** Quits: sinaowolabi_ (~SinaOwola@169.159.122.81) (Ping timeout: 256 seconds)
[08:57:39] *** Quits: kir0ul2 (~kir0ul@89.234.141.147) (Quit: The Lounge - https://thelounge.chat)
[08:57:39] *** Quits: kir0ul (~kir0ul@89.234.141.147) (Quit: The Lounge - https://thelounge.chat)
[08:58:06] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:68c6:1034:8cf9:5a2d) (Quit: I have gone to sleep. ZZZzzz…)
[08:58:14] *** Joins: kir0ul (~kir0ul@89.234.141.147)
[08:58:14] *** Joins: kir0ul2 (~kir0ul@89.234.141.147)
[09:04:22] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[09:10:06] *** Joins: sinaowolabi_ (~SinaOwola@41.58.235.31)
[10:15:15] *** Joins: manti7 (~manti7@176.10.104.94)
[11:25:30] *** Joins: palasso (~palasso@user/palasso)
[11:28:39] *** Quits: [_] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[11:53:08] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi)
[12:17:43] *** Joins: jlrnick (~josephler@2a01cb040a1594007dfc79c9afe919a1.ipv6.abo.wanadoo.fr)
[13:04:17] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 240 seconds)
[13:23:49] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[13:30:59] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[13:40:13] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi) (Read error: Connection reset by peer)
[14:44:59] <hodapp> Jong: you're conflating models, nets, and weights
[14:45:40] <hodapp> a statistical model can have parameters that are as-yet unspecified, and can have an iterative process which refines those parameters
[14:46:12] <hodapp> said iterative process can involve random initialization
[14:47:21] <hodapp> it doesn't become a "model" only at some arbitrary step in the iteration
[14:48:42] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi)
[14:55:37] <hodapp> the model is the more abstract thing underneath that relates parameters and data
[14:55:51] <hodapp> the *weights* are not the model
[14:56:20] <hodapp> the weights after the result of training are still not a model - they have meaning only in conjunction with some model
[15:05:00] <hodapp> but particularly - there is no precedent whatsoever in statistics that the term "model" by definition says that it *empirically* must model something
[15:07:27] <hodapp> e.g. a linear model, Y = b0 + b1*X1 + b2*X2 + ..., is still a linear model even if its parameters (b0, b1, ...) are only hypothetical or are useless with respect to some set of samples
[15:10:57] *** Quits: sinaowolabi_ (~SinaOwola@41.58.235.31) (Ping timeout: 240 seconds)
[15:23:59] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[15:25:42] *** Joins: sinaowolabi_ (~SinaOwola@41.190.30.65)
[15:45:46] *** Quits: sinaowolabi_ (~SinaOwola@41.190.30.65) (Read error: Connection reset by peer)
[16:02:28] *** Joins: sinaowolabi_ (~SinaOwola@160.152.24.180)
[16:12:31] *** Joins: `Tim (~zenguin@user/zenguin)
[16:17:16] <Jong> hodapp  wow very well explained
[16:18:47] <Jong> hodapp  I think I was coming from a physics perspective where I remember in undergrad professors said some equation is a model of some phenomina
[16:20:01] <Jong> thanks hodapp  for taking the time to explain what you explained!
[16:20:34] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi) (Read error: Connection reset by peer)
[16:20:57] <Jong> machine learning is so cool
[16:21:07] <Jong> I get excited just thinking about it
[16:30:44] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi)
[17:10:13] *** Joins: MrBech (~MrBech@152.115.67.90)
[17:26:57] <hodapp> hah, welcome
[17:30:37] *** Quits: Jong (~Jong@2620:10d:c090:400::5:4f54) (Ping timeout: 240 seconds)
[17:34:32] <MrBech> Hello
[17:35:18] *** Parts: MrBech (~MrBech@152.115.67.90) (Textual IRC Client: www.textualapp.com)
[17:35:26] *** Joins: MrBech (~MrBech@152.115.67.90)
[17:35:45] <MrBech> Hello again
[17:36:04] <MrBech> It annoys me that a company that learns data patterns, say they use AI. Is this justified? Or should I change my way of thought?
[17:40:52] <hodapp> it's kind of justified and it's kind of pure hype
[17:42:05] <hodapp> a lot of them *are* using techniques that qualify as AI... but a lot of them don't call it "AI" because it's true, they call it that because it sounds futuristic and way more advanced than it actually is, and they know it
[17:44:39] <MrBech> Do you think we will ever be able to create Strong AI?
[17:53:10] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi) (Read error: Connection reset by peer)
[17:57:36] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi)
[18:25:55] *** Joins: gist__ (~jack@94.134.88.166)
[18:26:11] *** Quits: MrBech (~MrBech@152.115.67.90) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[18:42:34] *** Joins: Jong (~Jong@2620:10d:c090:400::5:78f7)
[19:35:24] *** Joins: georgios (~georgios@user/georgios)
[19:51:32] *** Quits: jlrnick (~josephler@2a01cb040a1594007dfc79c9afe919a1.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[20:11:16] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[20:14:39] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi) (Quit: Leaving)
[20:30:17] *** Joins: MrBech (~MrBech@152.115.67.90)
[20:36:08] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[21:27:03] <[itchyjunk]> https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/
[21:27:03] <SigmoidFroid> ⇒  Researchers Build AI That Builds AI | Quanta Magazine
[21:32:35] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[21:48:47] <mefistofeles> [itchyjunk]: oh wow, that sounds like a good read, thanks for sharing
[22:00:54] *** Joins: SiegeLord (~sl@user/siegelord)
[22:00:57] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[22:17:29] *** Quits: xs (~xs@user/xs/x-9591474) (Ping timeout: 256 seconds)
[22:17:41] *** Joins: xs (~xs@user/xs/x-9591474)
[22:42:52] *** Quits: MrBech (~MrBech@152.115.67.90) (Quit: Textual IRC Client: www.textualapp.com)
[22:44:23] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: brb)
[22:46:15] *** Joins: Malvolio (~Malvolio@user/malvolio)
[22:57:56] <[itchyjunk]> np
[22:58:20] <[itchyjunk]> thought representing ANN's as graphs then training ANN's on those graphs was a very neat idea
[23:01:33] <mefistofeles> yeah
[23:01:59] <mefistofeles> [itchyjunk]: some in physics are doing things like that to do/understand physics itself, the most popular might be Stephen Wolfram
[23:22:23] *** Quits: brand0 (~brandon@user/brand0) (Quit: WeeChat 3.2)
[23:41:49] <[itchyjunk]> yes wolframs computational model of physics uses hypergraphs as the underlying structure
[23:42:11] <[itchyjunk]> apparently he's shown his model to be equivalent to type theorotic model of computation
[23:43:26] <hodapp> meh, I tend to take a lot of his work with a grain of salt because he has a very strong tendency to vastly overstate the importance of the work he takes credit for
[23:45:27] <[itchyjunk]> Sure but if he shows equivalence to other foundational theory, he has at least shown his model is consistanse
[23:45:38] <[itchyjunk]> it might not be useful at all but he admits to that himself
[23:47:05] <[itchyjunk]> I know people generally frown upon reinventing the wheel but i think it's a cool thing to reinvent the wheel in novel ways
[23:49:48] <mefistofeles> yeah, he's also not the only one doing similar work
[23:50:12] <mefistofeles> Lee Smollin is working on the foundations of why that kind of approach would make sense, fundamentally
[23:50:48] <mefistofeles> not just as a tool to discover patterns or emergent behavior in physics, but as a fundamental structure of the universe itself, but I'm far from understanding the details on this, of course
[23:53:57] <dostoyevsky2> [itchyjunk]: isn't reinventing the wheel in novel ways part of what pure Mathematics is all about?
[23:56:15] <hodapp> reinventing the wheel in novel ways isn't what I take issue with
[23:58:23] <[itchyjunk]> dostoyevsky2, well it's part of what pure math is about
[23:58:28] <[itchyjunk]> that's why i think it's cool
[23:58:43] <[itchyjunk]> but outside of math, it's generally frowned upon, in my experience
[23:59:18] <[itchyjunk]> well, wolframs computational model of universe is verymuch reinventing the wheel for now
[23:59:32] <dostoyevsky2> yeah, but I feel with neural networks similarly... a good model can transform the view on many things in novel ways
[23:59:42] <[itchyjunk]> since it doesn't have any obvious promise of novelty and at best it might be another equivalent type theoritic model
