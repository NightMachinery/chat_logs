[00:11:10] *** Joins: c10l (~c10l@89.34.167.207)
[00:12:31] <artok> brain lube, that solution that has molecules of two carbon, six hydrogen, and one oxygen atoms.
[00:17:32] *** Quits: Gustavo6046 (~Gustavo60@user/gustavo6046) (Quit: ZNC 1.8.2 - https://znc.in)
[00:17:50] *** Joins: Gustavo6046 (~Gustavo60@user/gustavo6046)
[00:26:14] <Lutin> artok try beer instead, alcohol :D
[00:27:49] *** Quits: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks) (Ping timeout: 244 seconds)
[00:30:34] *** Quits: mattchis (~mattchis@c-73-243-45-46.hsd1.co.comcast.net) (Quit: The Lounge - https://thelounge.chat)
[00:41:26] *** Quits: withered_wolf (~withered_@111.sub-174-192-82.myvzw.com) (Remote host closed the connection)
[00:42:49] <artok> isn't that same thing?
[00:43:55] <jochum> Beer with lemon - I like it a lot
[00:45:09] *** Quits: Haxxa (~Haxxa@122.199.45.186) (Quit: Haxxa flies away.)
[00:47:44] *** Quits: yamchah2 (~yamchah2@user/yamchah2) (Ping timeout: 258 seconds)
[00:47:44] *** Joins: Haxxa (~Haxxa@89nnjg0xckz9ggn6r5xm.ip6.superloop.com)
[00:48:51] *** Quits: enkodr (~Srain@210.203.60.94.rev.vodafone.pt) (Ping timeout: 252 seconds)
[00:51:19] *** Joins: yamchah2 (~yamchah2@user/yamchah2)
[01:04:28] *** Joins: mattchis (~mattchis@c-73-243-45-46.hsd1.co.comcast.net)
[01:06:55] <clime> docker seems to override /etc in an imported image where it is a symlink. Why?
[01:07:28] <clime> i mean `docker import image.tgz` <- overrides /etc
[01:13:59] <artok> clime: import is doing image from that tar
[01:14:11] <artok> clime: what you're trying to do ?
[01:15:30] <clime> artok: yes, it is doing image from tar, i am trying to import that tarball image unchanged
[01:15:49] <artok> it should be the _IMAGE_ you have
[01:15:59] <clime> artok: what do you mean?
[01:16:10] <clime> it isn't, /etc is getting messed up
[01:16:19] <clime> but idk why
[01:16:24] <artok> what are your steps?
[01:17:01] <artok> if you start _container_ from image, do things on that, you need to create _image_ from container and then export it to tar, that you can import
[01:18:08] <artok> so I'm calling you to tell what are trying to do and how are you doing it
[01:18:09] <clime> docker import image.tgz; docker tag sha... image; docker run -dit --privileged --network-host --name=instance image; docker exec -it instance /bin/bash
[01:18:20] <clime> pretty much
[01:18:31] <artok> well that image is what it is
[01:18:33] *** Quits: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963) (Ping timeout: 268 seconds)
[01:18:40] <clime> $ ls -la and seeing /etc got messed up
[01:18:44] <artok> it will start new "chroot"
[01:19:07] <artok> it is what is inside that image
[01:19:20] <artok> your image.tgz was created with... what?
[01:19:39] <clime> tar and gzip
[01:19:44] <artok> no
[01:19:46] <clime> sorry i don't think that matters
[01:19:48] <artok> it can't be
[01:19:56] <clime> i'll try a few things
[01:20:16] <artok> it is created with docker export or nothing
[01:20:47] <artok> if you want to create image otherways, you use docker build and have Dockerfile
[01:20:49] <clime> why? .tgz file is always created by tar and gzip
[01:21:04] <artok> sure, but is it container image?
[01:21:12] <clime> it will be
[01:21:37] <artok> no... it needs to be container image before you import it
[01:22:08] <clime> it's just archived filesystem, shouldn't be a big deal
[01:22:16] <artok> if you have whole root system on tgz, you do Dockerfile that will extract that tgz, and create image from that
[01:22:20] <clime> what's extra thing docker export does?
[01:22:42] <akik> here's how you create an image with tar https://docs.docker.com/develop/develop-images/baseimages/
[01:23:08] <artok> it has layer information, so if you have plain system on tgz, you do Dockerfile, do ADD blaablaa.tgz on it and docker build will handle it to image
[01:23:26] <programmerq> clime▸ elaborate on "messed up etc" and "override /etc" -- be more specific. did you create the tarball with preserving symlinks?
[01:23:36] <clime> programmerq: yes
[01:23:40] <programmerq> yes to which?
[01:23:45] <programmerq> elaborate.
[01:23:55] * artok turns to leonard coehen record again
[01:23:58] <programmerq> show commands
[01:24:04] <programmerq> use a pastebin/gist service
[01:24:04] <akik> artok: which tune?
[01:24:11] <clime> all other symlinks are preserved under / (like /opt -> /usr/opt or /home -> /var/home) except /etc
[01:24:21] <programmerq> share a minimal reproducible example
[01:25:33] <clime>  /etc points to /usr/etc in the archive but in the running container it is a directory with some minimal content (like os-release, resolv.conf, mtab, ... few other files)
[01:27:20] <clime> hard to provide a minimum working example...i can probably try to export some image, make symlink out of /etc and then reimport and see but ...that will take some time and i cannot really share it
[01:30:01] *** Quits: BenjiProd (~BenjiProd@user/benjiprod) (Remote host closed the connection)
[01:30:12] *** Quits: gordonjc1 (~gordonjcp@valmet.gjcp.net) (Quit: The Lounge - https://thelounge.chat)
[01:32:29] <clime> artok: well what you said about ADD blaablaa.tgz makes sense but i don't get why it just docker import shouldn't work - i will try to examine what i get after `docker save` ... if there is some extra info
[01:33:06] <clime> also just /etc/ getting messed up and everything else being okay after import of the plain archived filesystem is pretty strange
[01:33:14] *** Joins: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963)
[01:33:22] <akik> clime: i don't think linux has ever had /etc in /usr/etc
[01:33:51] <artok> docker save does some sha information there, so unless you've done that tar file from docker save, you don't have correct extra files on it
[01:33:56] <gordonjcp> akik: not recently
[01:34:03] <clime> artok: ok thanks
[01:34:12] <akik> gordonjcp: i mean never
[01:34:14] <artok> hence, base image thingie, it is to have Dockerfile with single ADD instruction
[01:34:17] <gordonjcp> akik: no, I have seen that
[01:34:24] <clime> akik: well, it's a good point
[01:34:32] <artok> docker build forever !
[01:34:33] <gordonjcp> akik: it made old SunOS stuff happy doing it that way, over NFS
[01:34:43] <gordonjcp> akik: you can safely map that value to "never"
[01:35:08] <akik> gordonjcp: what option?
[01:35:10] <clime> ostree images save /etc content under /etc
[01:35:50] <clime> i am usually creating chroot from those image and creating the symlink to make them work normally (i.e. all the /etc stuff at the usual place)
[01:36:10] <clime> here i tried to create a docker image from that chroot but something is off along the way
[01:36:17] <clime> anyway, i can avoid this
[01:36:34] <clime> but if artok says this is not the right approach then i guess will do it through ADD
[01:36:43] <gordonjcp> akik: I mean, it was sufficiently unusual that it's safe to say "never" for /usr/etc - but it was a thing back in the olden days
[01:37:04] <artok> akik already sent the link how to do base image
[01:37:06] <akik> gordonjcp: i was there in 1995
[01:37:24] <clime> i meant "ostree image save /etc under /usr/etc" btw
[01:38:08] <artok> your goal is still not known to me, but still assume you have chroot that you want to have as container
[01:38:11] <artok> =)
[01:38:45] <akik> i'd try first with the debian image and docker.com documentation :)
[01:39:02] <clime> artok: yes, pretty much
[01:39:04] <akik> debian debootstrap that is
[01:39:23] <gordonjcp> akik: :-)  Slackware?
[01:39:33] <akik> gordonjcp: debian 1.2 or something
[01:39:45] <akik> too much time has passed 
[01:40:14] <gordonjcp> akik: somewhere I still have the first 1.44MB disk images for a Linux 0.95 Boot and Root set
[01:40:16] <clime> well, ok, docker save really generates something which has a special structure
[01:40:29] <gordonjcp> akik: I no longer have the disks, or if I do they are unlikely to be readable
[01:40:42] <akik> gordonjcp: i have that red 6-cd package still :)
[01:40:46] <gordonjcp> akik: !
[01:40:51] <artok> akik: I lost that one!
[01:41:04] <gordonjcp> akik: the first Redhat I used was RH6.0, in about 1999
[01:41:06] <clime> it doesn't make much sense though that it "almost works" with what i did :(
[01:41:17] <gordonjcp> akik: I found the CD in the CD tray of a PC that someone had thrown out!
[01:41:18] <akik> gordonjcp: https://archive.org/details/ldr_0496_6cd
[01:41:31] <gordonjcp> :-D
[01:41:35] <artok> might be that I left that on my friend's flat on dorm
[01:41:41] <gordonjcp> akik: oh, I turned up a Fedora 7 CD tidying up, from 2007
[01:42:01] <gordonjcp> wait, fedora 7?  Maybe I'm wrong about 7
[01:42:20] <gordonjcp> could be 7
[01:43:26] <clime> `docker export` doesn't create anything special though
[01:43:39] <clime> i don't see any extra info
[01:43:48] <clime> just normal fs tree after unpacking
[01:43:58] <akik> clime: i use docker save to save the built images
[01:44:14] <akik> it'll be a tar file with a few files
[01:44:21] <clime> ye
[01:44:34] <clime> is the result supposed to be importable with `docker import`?
[01:44:49] <clime> because `docker export` produces something else than `docker save`
[01:44:54] <akik> clime: with docker load
[01:45:00] <clime> ah ok, cool
[01:45:02] <clime> thanks
[01:45:26] <Geo> wez: sorry, went to bed. the docker-compose mounts it with a relative path, and I then do a ./filename
[01:45:39] <Geo> so foo:/container/path/to/foo
[01:46:48] <artok> clime: but to be dockerish on the work, do docker build for your base image =)
[01:46:59] <artok> even it is just that one ADD command
[01:47:19] <clime> artok: i would like to avoid Dockerfile at this point, is it possible?
[01:47:36] <artok> clime: it is, but not good
[01:48:05] <artok> I'd go with docker build cache thingie and let it do the sha matching and so on.
[01:48:09] <clime> artok: i guess perhaps just because i need to get familiar with this chroot->image transform
[01:48:30] <artok> and it is just that link akik gave
[01:49:14] <artok> < akik> here's how you create an image with tar https://docs.docker.com/develop/develop-images/baseimages/
[01:49:23] <clime> ye, i can see FROM scratch ....
[01:49:29] <clime> ye, i have it open
[01:50:10] <artok> when you do that from dockerfile, you have base CMD setting and everything
[01:50:57] <clime> right...might come handy
[01:55:39] <gordonjcp> is there a way (an easy way) to do the opposite of volumes, and mount a volume that's in one container on a different container?
[01:57:33] <jochum> gordonjcp: reference it multiple times?
[01:57:37] <clime> artok: ok, well, let me try that approach if it maintains the /etc symlink (because i've just reproduced that behavior again with export -> change /etc to a symlink -> tar -czf /tmp/img.tgz . -> docker import /tmp/img.tgz -> docker run -it sha:... /bin/bash)
[01:58:27] <clime> i still think that what i am doing is not wrong because i didn't find any "extra info" in unpacked image exported by `docker export`
[01:58:56] <clime> maybe i overlooked something but it seems to be just archived plain fs tree, nothing else
[02:01:06] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 268 seconds)
[02:01:26] <gordonjcp> jochum: reference it how?
[02:01:51] <gordonjcp> jochum: I can mount a volume on the host to multiple containers, but that's the wrong way round
[02:02:01] <jochum> gordonjcp: never did it, i just have the feeling in docker-compose you can use it multiple times, I'm sorry its just a feeling.
[02:02:17] <gordonjcp> jochum: use what multiple times?
[02:02:24] <jochum> gordonjcp: i show you
[02:03:01] <clime> artok: so the ADD method through Dockerfile has exactly the same problem
[02:03:12] <clime> i will show a screenshot mmt
[02:03:48] <jochum> gordonjcp: https://bpa.st/AGNQ
[02:04:19] <jochum> gordonjcp: this one of my yaml's, you see i have the volume defined on the bottom
[02:04:25] <jochum> end
[02:04:48] <jochum> gordonjcp: now I'm gonna create a second pgadmin container with the same volume, I believe it works
[02:06:36] <jochum> gordonjcp: second pgadmin works
[02:06:46] *** Quits: rpthms (~rpthms@user/rpthms) (Remote host closed the connection)
[02:07:00] <clime> well, i guess without screenshot because it would be anti-climactic now 
[02:07:23] *** Joins: rpthms (~rpthms@user/rpthms)
[02:07:31] <jochum> gordonjcp: data is available
[02:08:00] <artok> clime: then your tar has problems
[02:08:34] <clime> not really
[02:08:43] *** Quits: pycurious (~Adium@user/pycurious) (Quit: Leaving.)
[02:08:43] <clime> let me still show it https://clime.cz/strange-docker-etc-stuff.png
[02:09:27] <clime> btw.: Docker version 20.10.7, build f0df350
[02:11:23] *** Quits: NeuroWinter (~NeuroWint@107.172.219.233) (Quit: ZNC 1.8.2 - https://znc.in)
[02:12:06] *** Joins: NeuroWinter (~NeuroWint@107.172.219.233)
[02:13:53] <artok> now then
[02:14:50] *** Joins: pycurious (~Adium@user/pycurious)
[02:14:53] <artok> if you do docker run image ls -l
[02:15:04] <artok> what does the symlink tell you=
[02:15:50] <artok> (ie docker run -ti hello /bin/bash, and then ls -l
[02:15:55] <clime> here is more complete info: https://clime.cz/strange-docker-stuff-2.png
[02:16:23] <clime> artok: it's no longer a symlink after docker run, it's just normal dir with a few files...
[02:16:57] <artok> you don't have usr directory?
[02:17:17] <artok> on that tgz
[02:17:23] <clime> bash-4.2# ls /etc
[02:17:24] <clime> hostname  hosts  mtab  resolv.conf
[02:18:02] <clime> i have /usr dir there, the screenshot doesn't show the whole tarball content because it didn't fit the screen but the fact that /etc is a symlink should be captured there at the last line
[02:18:56] <artok> well I don't see it according those commands so why would it be on container...
[02:19:05] *** Quits: rgl (~rgl@bl12-47-147.dsl.telepac.pt) (Remote host closed the connection)
[02:19:37] <clime> artok: i don't what you mean, i am displaying the tarball content at the end, and i needed to scroll up to capture the beginning
[02:19:51] <artok> you just copied tgz to content, didn't move to that folder and hit tar -x on it.. doesn't make sense to me
[02:19:52] <clime> clearly the screen is cut because you don't see a prompt
[02:20:06] *** Joins: vxrx (~vxrx@proxy01.autarkic.org)
[02:20:16] *** Quits: vxrx (~vxrx@proxy01.autarkic.org) (Client Quit)
[02:20:25] <clime> the second screenshot show more complete picture
[02:20:45] <ash_worksi> artok: oh really? that's why I've never had problems before; because I've always been mounting directories
[02:20:48] <ash_worksi> artok: thanks
[02:20:51] <artok> I'm watching that
[02:21:32] <clime> artok: ah, ye, the move to content was unnecessary there
[02:21:51] <clime> i forgot to switch there so it unpacked into current dir among Dockerfile etc. that was a mistake
[02:21:53] <artok> clime: you did tar x and it didn't create usr folder for your host 
[02:22:16] <artok> last ls -la doesn't have usr folder
[02:22:44] <clime> i mentioned several times that the screen is cut
[02:22:46] <artok> so I still stand with "your tar isn't complete base file"
[02:22:53] <clime> the file listing continues
[02:23:09] <clime> well, that's because you want to believe that
[02:23:47] <artok> no, I have that until you prove me wrong, and with all given information, you haven't done that =)
[02:24:11] <artok> I hate guessing game, and you should stop that by giving as much information that you have
[02:24:15] <clime> ok, but i have spent way too much time already...
[02:24:33] <clime> i'll try once more with /etc pointing somwhere else than under /usr
[02:24:34] <artok> first of all, how you did your chroot, then what was the command for tar, and so on
[02:25:40] <clime> oh, you want to know way too much here, that i already mentioned before....e
[02:26:33] <artok> doublechecking
[02:26:35] <clime> it won't fit a screen :(
[02:26:57] <artok> use normal text, not pictures
[02:27:18] <clime> well, then you can say that i fabricated it
[02:27:23] <artok> you can use any paste service for text and that has less bytes spent on internet than pictures
[02:29:53] <artok> inside chroot you do tar chzf fs.tgz . and you add that to image, should work
[02:30:17] <clime> it messes up /etc sadly
[02:30:41] <artok> docker just does tar x for the file
[02:31:15] <artok> ADD foo.tgz /  is same as tar xf foo.tgz -C /
[02:33:03] <Lutin> artok how uch containers were you going to run on one node ?
[02:33:35] <artok> clime: remember that I don't say that you're wrong, I just say the behavior is weird =)
[02:33:59] <Lutin> artok but you always want to be right so someone needs to be wrong :P
[02:34:01] <clime> artok: well, i say the same thing
[02:34:02] <artok> and we're really trying to bisect what is happening
[02:34:06] <clime> it is super weird
[02:34:14] <Lutin> clime we are weird, welcome
[02:34:30] <Lutin> clime artok even thinks he is a DJ! :P
[02:34:30] <artok> Lutin: what ever is the power of the node
[02:34:38] <clime> but probly docker run cannot cope with /etc being a symlink or something else in the machinery, who knows...
[02:34:48] <Lutin> artok yeah I know... but 200 containers ? I doubt it
[02:34:54] <artok> Lutin: thinking that from -93 is really well distorted mind =)
[02:34:54] <clime> heh, good for him, must be a fun life
[02:35:13] <Lutin> artok it's that cold there ?
[02:35:20] <artok> no
[02:35:31] <artok> thanks to global warming
[02:35:38] <Lutin> where did -93 came from then ?
[02:35:51] <Lutin> artok stop doing IT... you help stopping global warming :D
[02:35:57] <artok> 1993 was the first time when I touched dj set
[02:36:14] <Lutin> artok so I assume the same for women then :D
[02:36:21] <Lutin> as they like DJ's
[02:36:28] <artok> well it was around then
[02:36:33] <artok> but that is just urban legend
[02:36:36] *** Quits: c10l (~c10l@89.34.167.207) (Ping timeout: 276 seconds)
[02:36:39] <Lutin> Whaha!
[02:37:07] <Lutin> artok no but really. abotu 200 containers on a host, nah you woul do ?
[02:37:09] <Lutin> would
[02:37:16] <artok> they don't like dj's... as they don't wait until the night is over, so they take any other guy on the dancefloor
[02:37:16] <Lutin> node
[02:37:35] <Lutin> artok then... be the man on that dancefloor! delegate!
[02:37:53] <Lutin> artok or get a dog... they always wait for you :D
[02:38:29] <artok> Lutin: k8s has recommendations: 110 pods per node, no more than 5000 nodes, no more than 150000 total pods, no more than 300000 containers
[02:38:53] <Lutin> artok true indeed
[02:38:55] <Lutin> thanks
[02:39:04] <artok> so I keep that 100 as limit for what ever node there would be
[02:39:18] <Lutin> yeah I might even lower that down
[02:39:23] <Lutin> because of subnetting
[02:39:30] <artok> but I won't have that kind of node that would do the work of that
[02:40:11] <artok> ofcoz I'd do metrics of cpu usage and such if doing large cluster
[02:40:15] <Lutin> artok me neither, we are doing to deliver preconfigured containers for customers but... I need nice subnetting there
[02:40:37] <Lutin> so each customer get's his own VPS... etc
[02:41:05] <artok> well that is quite assumed thing nowadays
[02:42:40] <artok> clime: put that tar xf img.tgz output now into some paste service and we'll thing what to do next
[02:43:17] <clime> artok: i can put it my server
[02:43:35] <Lutin> artok not really
[02:43:51] <Lutin> artok you can but some companies don't
[02:43:54] *** Quits: ezekyel (~ezekyel@user/ezekyel) (Ping timeout: 240 seconds)
[02:44:21] <clime> i decided to one more test so let me do it, i guess i can upload the stuff in the end
[02:44:22] <artok> Lutin: sure, there are webhotels that do just adding line to named virtualhost 
[02:44:44] <Lutin> artok yap
[02:44:51] <Lutin> same happens for containers
[02:45:04] <artok> clime: take your time
[02:45:19] * Lutin takes artok his time
[02:45:37] <artok> Lutin: as system operator and business manager I wouldn't have it otherways
[02:45:56] <Lutin> artok time ?
[02:46:05] <artok> because it's easier to turn off VM if the bill hasn't been paid than try to manage their part of containers =)
[02:48:15] <artok> management of the cluster is everything, and when telling that "well it's easy to just turn off their VPS" to management, CEO and marketing people, it will usually be applauded =)
[02:51:17] *** Joins: ezekyel (~ezekyel@user/ezekyel)
[02:53:08] <Lutin> yap 
[02:53:34] <Lutin> but we have it all API'ed so we don't care
[02:53:48] *** Joins: c10l (~c10l@89.34.167.207)
[02:54:33] <Lutin> artok also IP/bind wise it's easier with a VPS
[02:54:36] *** Joins: pvdp (~Pieter@static-n58-105-183-94.rdl4.qld.optusnet.com.au)
[02:55:30] *** Joins: Ryu945 (~Ryu945@181.214.227.72)
[02:58:37] <clime> so here is the third screenshot with even more complete info: https://clime.cz/strange-docker-etc-stuff-3.png
[02:59:07] <clime> i had some embarrassing problems with privileges along the way but i already spent way too much time on this
[03:00:58] <clime> (also note that i have: cd() { builtin cd "$@"; ls; } in my .bashrc - that's why the last two lines of the first screen happened)
[03:01:02] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[03:02:17] <clime> the second screen on the right is continuation of the first one
[03:03:09] <clime> this time i moved /etc under /media and symlinked there
[03:03:22] *** Quits: DoofusCanadensis (~DoofusCan@2604:3d09:47c:f970:f8db:ee37:a601:5191) (Quit: So as you can see from this flowchSQUIRREL!!)
[03:04:30] <clime> and also this is another machine just to try another one. Here: Docker version 20.10.6, build 370c289, the result is the same
[03:04:40] <clime> so i guess, i'll just avoid /etc being a symlink
[03:08:08] <artok> what I could tell is that you have different uid and permissions on symlinks and their targets, that might slightly mess up things
[03:09:22] *** Quits: junktext (~junktext@77.247.181.218) (Ping timeout: 240 seconds)
[03:09:53] <clime> artok: ye, there will always be some thing that i could have done potentially wrong
[03:10:08] <clime> for me the case is clear already
[03:12:14] *** Quits: c10l (~c10l@89.34.167.207) (Quit: Ping timeout (120 seconds))
[03:12:34] *** Joins: c10l (~c10l@89.34.167.207)
[03:13:42] <artok> but to be clear, if you're not trying to create some new package manager thingie, use something that already has base image. and if you don't need any dependencies (you create statically linked program) you can then just put that single executable into container like FROM scratch COPY --from=builder /bins/service /service CMD /service
[03:14:04] <clime> artok: that's beside the point however
[03:14:50] <clime> anyway, i spent way too much time, i reported this strange issue on github with the screenshot, that's it for me, i'll just avoid /etc being symlinked
[03:14:55] <clime> thanks for discussion
[03:14:57] *** Quits: clime (~clime@ip-94-199-195-39.acvyskov.cz) (Quit: WeeChat 3.0.1)
[03:15:47] <artok> well then.. time spent well
[03:17:29] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[03:24:49] *** Joins: junktext (~junktext@77.247.181.209)
[03:31:03] <tabakhase> artok afaik that 110 is somewhat k8s specific and doesnt matter on "a local docker daemon"
[03:31:26] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 272 seconds)
[03:32:51] <tabakhase> afair hard max are more about open files and max1024 interfaces per bridge-device (what can be workarounded as well...) - but dont quote me on that...
[03:32:55] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[03:36:07] <artok> tabakhase: sure, but since Lutin asked about my opinion in a way, I take that k8s cluster as rule of thumb, as if my stack would need more resources on customer demand, it would be k8s stack anyways
[03:36:59] <Lutin> artok yap!
[03:37:09] <artok> and as I said, with metrics of CPU/MEM usage so the correct insatnce to run things would be selected
[03:37:31] <artok> just to get into ballpark
[03:37:37] <Lutin> tabakhase a rule of toes is nice to have there
[03:37:54] <Lutin> artok I prefer toes...
[03:40:14] <tabakhase> but it aint a ballpark - thats like... idk, not taking more than 1 suitcase on a trip cause "it generally costs more with planes" - while youre actually on the road not just with a car, but a whole mobileHome ;D
[03:41:09] <tabakhase> you aint "wrong", but also very much no :P
[03:41:29] <artok> it was on ballpark with aws cost doing some basic things, but as said metrics will tell the reality
[03:41:52] <artok> monitoring is good to have
[03:43:38] <artok> have enough servers that you don't have single point of failure, then start adding them into each zone as many as needed so you'll have overhead to handle your peak traffic
[03:44:40] *** Quits: junktext (~junktext@77.247.181.209) (Ping timeout: 272 seconds)
[03:44:52] <artok> better would be that you'd know when you'll have peak and have autoscaler to start more until they tell you that they're idle and they would shoot their own head
[03:45:25] <wez> Geo: It needs to be an absoulte path otherwise Docker will create a volume named foo.
[03:46:00] <tabakhase> (unless its docker-compose notably)
[03:48:05] <Geo> wez: I can see the file structure there, though
[03:48:13] <Geo> and yes, it is docker-compose
[03:48:15] <Geo> thanks tabakhase 
[03:48:34] <wez> Ah?
[03:48:36] <wez> win 13
[03:50:07] <tabakhase> compose basically "cheats" and translates a "./" quickly to a $PWD/ before sending it on to the real docker-daemon :P
[03:50:40] <tabakhase> and then docker works by a somewhat stupid "does the name start with a '/' yes/no" ;D
[03:50:45] <wez> tabakhase: I didn't know that (then again, I don't use docker-compose).
[03:50:59] <Geo> the files there; I can cat a text file fine
[03:51:05] <Geo> it just doesn't like the binary
[03:52:18] <tabakhase> if i remember your case correct from the backlog - what EXACT error msg you get? (best copypasta inc above&below...)
[03:52:20] <wez> Geo: is the binary linked to a library that doesn't exist in your docker container?  
[03:52:49] <tabakhase> there are a few things that can kick out quite crypty errors... like linmked libs, or running on the wrong architecture and such
[03:53:16] <Geo> wez: probably
[03:53:17] <wez> Geo: If the binary is a dynamic executable that is.
[03:53:24] <Geo> yeah, pretty sure
[03:53:27] <wez> Geo: Use the ldd command to see if it is
[03:53:54] *** Quits: svm_invictvs (~svm_invic@user/svm-invictvs/x-6696469) (Ping timeout: 240 seconds)
[03:54:05] <tabakhase> also you can strace/ptrace containers fairly well, that should def. spit out some more
[03:54:12] <Geo> I'll have to wait an hour or two to get back to that to try
[03:54:15] <Geo> but thank you for the tip
[03:54:21] <Geo> that sounds like a very possible culprit
[03:54:38] <wez> Geo: You can try in your host system first to see if it is a dynamically linked executable
[03:55:36] <wez> also if the binary was compiled under MacOs and you are trying to run it under linux, that sort of stuff won't work either
[03:57:12] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 276 seconds)
[04:00:41] *** Joins: aeon_flux (~textual@d-207-244-191-202.fl.cpe.atlanticbb.net)
[04:02:45] *** Joins: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net)
[04:03:35] *** Quits: c10l (~c10l@89.34.167.207) (Quit: The Lounge - https://thelounge.chat)
[04:04:39] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[04:04:40] *** Joins: c10l (~c10l@89.34.167.207)
[04:04:44] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[04:06:42] *** Quits: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net) (Ping timeout: 240 seconds)
[04:08:12] *** Quits: gschanuel6 (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[04:08:59] *** Joins: gschanuel6 (~gschanuel@user/gschanuel)
[04:10:19] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[04:14:18] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Quit: Atum_)
[04:24:38] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 272 seconds)
[04:36:48] *** Quits: kenoba (~kenoba@user/kenoba) (Quit: The Lounge - https://thelounge.chat)
[04:46:23] *** Joins: Gustavo6046_ (~Gustavo60@user/gustavo6046)
[04:47:26] *** Quits: Gustavo6046 (~Gustavo60@user/gustavo6046) (Ping timeout: 272 seconds)
[04:47:41] *** Joins: kenoba (~kenoba@user/kenoba)
[04:48:13] *** Gustavo6046_ is now known as Gustavo6046
[05:02:45] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[05:03:23] <glik22> i'm trying to use buildx on my mac but don't see linux/arm64/v8 as a platform when running docker buildx inspect
[05:21:19] <BtbN> I doubt docker supports M1 macs yet
[05:29:00] <dman777> using node app and mongodb in a docker image. Is it possible to use the container ipaddress in the image? Right now I have (in node)  const mongoDbUri = 'mongodb://localhost/formbuilder'; but from reading there is no localhost in docker
[05:30:28] *** Quits: vlm (~vlm@user/vlm) (Quit: "")
[05:33:43] <tabakhase> dman777 thats likely not what you want anyhow
[05:34:02] <tabakhase> your db will be another container - and when you stick them into a network together they can resolve each other by "name" using dns
[05:34:18] <tabakhase> so youll endup with some mongoDbUri = 'mongodb://mydb/formbuilder' or such in the end
[05:34:20] *** Joins: vlm (~vlm@user/vlm)
[05:35:22] <tabakhase> (and docker "has a localhost" - just like, each container his own, so noone can get to it :P)
[05:37:53] *** Quits: Ryu945 (~Ryu945@181.214.227.72) (Quit: Leaving)
[05:40:23] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[05:45:27] *** Quits: shokohsc (~shokohsc@161.88.195.77.rev.sfr.net) (Read error: Connection reset by peer)
[05:46:28] *** Joins: shokohsc (~shokohsc@161.88.195.77.rev.sfr.net)
[05:46:42] <dman777> tabakhase: I thought it would be good if the node app (which is really small) used the same localhost as mongodb 
[05:46:54] <dman777> but I am not a expert at docker archetecture. 
[05:47:51] <tabakhase> dman777 docker containers shall do just one thing - there is basically "0 overhead" so starting it in an extra container has like no downsides ((its not like a vm where you have init and all that running, dockers have JUST that one process))
[05:48:29] <tabakhase> so in fact youll hinder yourself by trying to build a "2 in 1 container" - when you could just reuse node&mongo straight from the hub 
[05:48:35] *** Joins: physikoi (~physikoi@user/physikoi)
[05:49:51] <tabakhase> (exceptions to that rule obvs exist ;-) but thats out of scope here) --  if you haven tocuhed docker-compose yet, have a look, this will make "multiple containers" become a thing that seems much more reasonable then "having to execute four commands that are each  2434 characters long in the right order" :D
[05:52:58] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Read error: Connection reset by peer)
[05:56:29] <dman777> tabakhase: oh...ok. sounds good. when you say straight from the hub, you mean I do not even have to write a Docker file...correct?
[05:58:00] <tabakhase> dman777 kinda yea - you prob will still have a 3-linr for your node app (to install dependencies and copy your code) - but mongo straight from the hub should be just fine
[05:58:28] <tabakhase> (unless you "just use docker as runtime" - then just bind-mounting the code from your disk into it will be fine too :P
[05:58:53] <tabakhase> as in, may not even need a node dockerfile)
[06:01:50] <dman777> tabakhase: which is better to do by convention for a node app... make a Docker file and build a image or just run command: sh -c 'cd /usr/src/frontend-widgets && npm run dev' in the docker-compose.yml?
[06:02:36] *** Joins: jazzy (~jaziz@2600:380:c145:1176:ccc4:f85a:7d87:128)
[06:03:57] <tabakhase> both is reasonable - for dev the seond is certainly nicer ;-)
[06:04:23] <physikoi> hey yall. when i log into a docker container, i have an issue with terminal interpretation of arrow key presses. Basically, instead of moving the cursor, escape sequences are output to buffer. EG: https://www.icloud.com/iclouddrive/07ui2sdlJ_vB1_0hdN1TD_cxA#Screen_Shot_2021-08-02_at_9.32.16_PM
[06:04:24] <dman777> tabakhase: ok, thank you
[06:04:35] <physikoi> how might i fix this? ty
[06:05:20] <tabakhase> but even for prod, both concepts can be reasonable... dockerish "yea you want to build a app-image, so only this needs to be published" - but that doesnt mean that "using docker just as runtime with bindmounts" is unseen in prod..
[06:07:27] <tabakhase> physikoi what command did you run? - (and wtf icloud screenshots, doesnt allow viewing, only download)
[06:08:38] *** Joins: keypushe- (keypusher@user/keypusher)
[06:09:08] *** Quits: keypusher (keypusher@user/keypusher) (Ping timeout: 272 seconds)
[06:10:08] <physikoi> sorry. brb. i'll try again with dropbox.
[06:10:44] <tabakhase> <3 imgur ;-) --- but its fine for that one, downloaded it already
[06:11:54] *** keypushe- is now known as keypusher
[06:15:45] <physikoi> tabakhase: ok, thanks for that. here's the command: `└─▪ docker exec -t newhire-fullstack-test-master_web_1 /bin/bash -c "export COLUMNS=`tput cols`; export LINES=`tput lines`; exec bash"`
[06:15:59] <physikoi> oops, ignore that big of prompt that snuck in
[06:16:10] <physikoi> bit*
[06:19:49] <physikoi> i should clarify, that the simpler command provided by Docker Desktop suffers the same result: `docker exec -it 123123123123123 /bin/sh`
[06:20:19] <tabakhase> so it aint even listening to you, start by throwing a "-i" with in there if its supposed to be interactive :P
[06:22:58] <tabakhase> https://i.imgur.com/jv0Drsd.png left side takes "enter" and also allows to recall bashhistory - right is ded
[06:23:52] <physikoi> trying the -i thing
[06:24:11] <physikoi> tabakhase: you fixed it lol
[06:24:14] <physikoi> thank you :)
[06:27:24] *** Joins: Gustavo6046_ (~Gustavo60@user/gustavo6046)
[06:27:54] *** Quits: Gustavo6046 (~Gustavo60@user/gustavo6046) (Ping timeout: 240 seconds)
[06:29:44] *** Gustavo6046_ is now known as Gustavo6046
[06:39:23] *** Quits: physikoi (~physikoi@user/physikoi) (Quit: leaving)
[06:52:09] *** Joins: svm_invictvs (~svm_invic@user/svm-invictvs/x-6696469)
[07:15:14] *** Quits: c10l (~c10l@89.34.167.207) (Quit: The Lounge - https://thelounge.chat)
[07:15:36] *** Joins: c10l (~c10l@89.34.167.207)
[07:16:19] *** Quits: indigaz (~indigaz@c-73-168-117-231.hsd1.in.comcast.net) (Quit: The Lounge - https://thelounge.chat)
[07:20:40] *** Joins: indigaz (~indigaz@c-73-168-117-231.hsd1.in.comcast.net)
[07:34:10] *** Quits: jazzy (~jaziz@2600:380:c145:1176:ccc4:f85a:7d87:128) (Ping timeout: 240 seconds)
[07:43:26] *** Joins: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks)
[08:03:05] *** Joins: earthbreaker9000 (~earthbrea@li778-229.members.linode.com)
[08:03:19] *** Joins: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net)
[08:07:58] *** Quits: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net) (Ping timeout: 256 seconds)
[08:10:56] *** Quits: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963) (Ping timeout: 252 seconds)
[08:11:16] *** Joins: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963)
[08:11:45] *** Joins: oxum (~oxum@122.172.39.197)
[08:12:53] *** Quits: earthbreaker9000 (~earthbrea@li778-229.members.linode.com) (Quit: I Break D Ert)
[08:13:07] *** Joins: earthbreaker9000 (~earthbrea@li778-229.members.linode.com)
[08:16:22] *** Quits: oxum (~oxum@122.172.39.197) (Ping timeout: 245 seconds)
[08:22:46] *** Quits: artok (~azo@mobile-access-b04845-49.dhcp.inet.fi) (Ping timeout: 272 seconds)
[08:24:48] *** Quits: earthbreaker9000 (~earthbrea@li778-229.members.linode.com) (Quit: I Break D Ert)
[08:41:33] *** Quits: c10l (~c10l@89.34.167.207) (Read error: Connection reset by peer)
[08:43:47] *** Joins: c10l (~c10l@89.34.167.207)
[08:45:44] *** Joins: artok (~azo@mobile-access-b04845-49.dhcp.inet.fi)
[08:48:14] *** Joins: saunders (~nicholas@172.103.152.166)
[08:48:40] *** Quits: fjmorazan (~quassel@user/fjmorazan) (Quit: fjmorazan)
[08:49:17] *** Joins: fjmorazan (~quassel@user/fjmorazan)
[08:56:03] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Ping timeout: 265 seconds)
[08:57:14] *** Joins: oxum (~oxum@122.172.39.197)
[08:57:55] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[09:02:12] *** Quits: oxum (~oxum@122.172.39.197) (Ping timeout: 252 seconds)
[09:17:44] *** Joins: dmalteseknight (~dmaltesek@user/dmalteseknight)
[09:21:25] *** Quits: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net) (Read error: Connection reset by peer)
[09:21:36] *** Quits: nebiros (nebiros@user/nebiros) (Ping timeout: 272 seconds)
[09:26:10] *** Joins: nebiros (~nebiros@user/nebiros)
[09:30:39] *** Quits: c10l (~c10l@89.34.167.207) (Quit: The Lounge - https://thelounge.chat)
[09:31:00] *** Joins: c10l (~c10l@89.34.167.207)
[09:36:28] *** Quits: Shaun (~soneil@user/shaun) (*.net *.split)
[09:36:28] *** Quits: useful_idiot (~useful_id@154.3.42.62) (*.net *.split)
[09:36:28] *** Quits: chasmo77 (~chas77@71.63.241.168) (*.net *.split)
[09:36:28] *** Quits: matdev (~matdev@2a01:6e60:10:793:666:feed:dead:beef) (*.net *.split)
[09:36:28] *** Quits: churnd (~churnd@user/churnd) (*.net *.split)
[09:36:28] *** Quits: flommi (~flommi@puck942.startdedicated.de) (*.net *.split)
[09:36:28] *** Quits: daniel-k7 (~daniel-k@192.241.135.211) (*.net *.split)
[09:36:28] *** Quits: francis (francis@user/francis) (*.net *.split)
[09:36:29] *** Quits: Nothing4You (N4Y@nothing4you.w.tf-w.tf) (*.net *.split)
[09:36:29] *** Quits: TastyCorn (~TastyCorn@ec2-52-10-164-55.us-west-2.compute.amazonaws.com) (*.net *.split)
[09:36:29] *** Quits: effprime (~effprime@user/effprime) (*.net *.split)
[09:36:29] *** Quits: bencevans (~bencevans@static.153.97.4.46.clients.your-server.de) (*.net *.split)
[09:36:29] *** Quits: g1eb (~g1eb@user/g1eb) (*.net *.split)
[09:36:29] *** Quits: itok (sid418430@2001:67c:2f08:3::6:627e) (*.net *.split)
[09:36:29] *** Quits: Neoon (~Neoon@2607:5300:60:23a5::1111) (*.net *.split)
[09:36:29] *** Quits: lhtseng_ (sid15322@id-15322.brockwell.irccloud.com) (*.net *.split)
[09:36:29] *** Quits: coax_ (sid481900@id-481900.brockwell.irccloud.com) (*.net *.split)
[09:36:29] *** Quits: cmagina (sid286142@user/cmagina) (*.net *.split)
[09:36:29] *** Quits: shrysr (~shrysr@user/shrysr) (*.net *.split)
[09:36:29] *** Quits: fxrs (~fxrs@192.227.212.130) (*.net *.split)
[09:36:30] *** Quits: waldo323__ (~waldo323@d149-67-45-83.clv.wideopenwest.com) (*.net *.split)
[09:36:30] *** Quits: mva (znc@gentoo/developer/mva) (*.net *.split)
[09:36:30] *** Quits: infernix (nix@2a02:22a0:bbb1:d500::2) (*.net *.split)
[09:36:30] *** Quits: statusbot (~statusbot@ec2-34-198-122-184.compute-1.amazonaws.com) (*.net *.split)
[09:36:30] *** Quits: dthpulse (~quassel@podciarou.sk) (*.net *.split)
[09:36:35] *** Joins: Shaun_ (~soneil@user/shaun)
[09:36:39] *** Joins: g1eb_ (~g1eb@user/g1eb)
[09:36:42] *** Joins: m4td3v (~matdev@2a01:6e60:10:793:666:feed:dead:beef)
[09:36:50] *** Joins: daniel-k (~daniel-k@192.241.135.211)
[09:37:08] *** Joins: dthpulse (~quassel@podciarou.sk)
[09:37:17] *** Joins: useful_idiot (~useful_id@154.3.42.62)
[09:37:19] *** Joins: bencevans (~bencevans@static.153.97.4.46.clients.your-server.de)
[09:37:21] *** Joins: Neoon (~Neoon@2607:5300:60:23a5::1111)
[09:37:22] *** Joins: flommi_ (~flommi@puck942.startdedicated.de)
[09:37:22] *** Joins: mva_ (znc@gentoo/developer/mva)
[09:37:40] *** Joins: itok (sid418430@id-418430.charlton.irccloud.com)
[09:38:06] *** Joins: statusbot (~statusbot@ec2-34-198-122-184.compute-1.amazonaws.com)
[09:38:09] *** Joins: waldo323 (~waldo323@d149-67-45-83.clv.wideopenwest.com)
[09:38:29] *** Joins: fxrs (~fxrs@192.227.212.130)
[09:38:46] *** Joins: Nothing4You (N4Y@nothing4you.w.tf-w.tf)
[09:39:30] *** Joins: effprime (~effprime@user/effprime)
[09:40:26] *** Joins: churnd (~churnd@user/churnd)
[09:41:44] *** Joins: TastyCorn (~TastyCorn@ec2-52-10-164-55.us-west-2.compute.amazonaws.com)
[09:44:00] *** Quits: polymorphic (~polymorph@066-169-158-111.res.spectrum.com) (Ping timeout: 252 seconds)
[09:44:01] *** Quits: andycooper (uid246432@id-246432.brockwell.irccloud.com) (Quit: Connection closed for inactivity)
[09:45:54] *** Quits: Zelec (~Zelec@135-23-82-85.cpe.pppoe.ca) (Ping timeout: 240 seconds)
[09:46:37] *** Joins: Zelec2 (~Zelec@135-23-82-85.cpe.pppoe.ca)
[09:46:59] *** Joins: polymorphic (~polymorph@066-169-158-111.res.spectrum.com)
[09:58:04] *** Joins: infernix (~nix@2a02:22a0:bbb1:d500::2)
[09:59:41] *** Quits: Sasazuka (~Sasazuka@user/sasazuka) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[10:03:42] *** Joins: jazzy (~jaziz@2600:380:465b:aa32:a5e0:69d:ea69:d1e4)
[10:05:01] *** Quits: rpthms (~rpthms@user/rpthms) (Remote host closed the connection)
[10:06:17] *** Quits: setesat40 (~setesat@94.4.156.220) (Quit: The Lounge - https://thelounge.chat)
[10:06:42] *** Joins: setesat40 (~setesat@94.4.156.220)
[10:07:57] *** Joins: rpthms (~rpthms@user/rpthms)
[10:12:57] *** Joins: lemonzest (~lemonzest@user/lemonzest)
[10:30:22] *** Quits: Zelec2 (~Zelec@135-23-82-85.cpe.pppoe.ca) (Quit: The Lounge - https://thelounge.chat)
[10:30:46] *** Joins: Zelec2 (~Zelec@135-23-82-85.cpe.pppoe.ca)
[11:14:35] *** Quits: saunders (~nicholas@172.103.152.166) (Remote host closed the connection)
[11:17:42] *** Quits: pycurious (~Adium@user/pycurious) (Quit: Leaving.)
[11:17:54] *** Joins: pycurious (~Adium@user/pycurious)
[11:18:41] *** Joins: incognito (~relativit@user/incognito)
[11:22:53] *** Quits: Flash (~Flash@user/flash) (Ping timeout: 252 seconds)
[11:23:30] *** Joins: Flash (~Flash@user/flash)
[11:28:58] *** Joins: shrysr (~shrysr@user/shrysr)
[11:46:04] *** Joins: vidbina_ (~vid@dynamic-046-114-037-233.46.114.pool.telefonica.de)
[11:50:50] *** Joins: devslash (~devslash@46.232.211.210)
[11:53:33] *** Quits: devslash (~devslash@46.232.211.210) (Client Quit)
[11:54:15] *** Joins: devslash (~devslash@46.232.211.210)
[12:02:33] *** Quits: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks) (Remote host closed the connection)
[12:02:59] *** Joins: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks)
[12:04:11] *** Joins: oxum (~oxum@122.172.39.197)
[12:06:40] *** Quits: oxum (~oxum@122.172.39.197) (Remote host closed the connection)
[12:21:22] *** Joins: Lutin (~Lutin@user/lutin)
[12:25:10] *** Quits: jarthur (~jarthur@2603-8080-1540-002d-e09d-5ac8-afbf-2237.res6.spectrum.com) (Quit: jarthur)
[12:38:02] *** Quits: vidbina_ (~vid@dynamic-046-114-037-233.46.114.pool.telefonica.de) (Ping timeout: 245 seconds)
[12:45:55] *** Joins: francis (francis@user/francis)
[12:46:52] *** Quits: Bardon (~Bardon@user/Bardon) (Ping timeout: 258 seconds)
[13:02:34] *** Joins: brickfat (~brickfat@user/brickfat)
[13:07:41] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[13:09:00] *** Joins: Lutin (~Lutin@user/lutin)
[13:11:39] *** Quits: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 276 seconds)
[13:14:57] <Lutin> morning!
[13:15:21] *** Quits: andypandy (~andypandy@h-178-174-148-234.A163.priv.bahnhof.se) (Quit: Bye)
[13:17:47] *** Joins: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[13:20:09] *** Joins: varaindemian (~varaindem@86.124.78.162)
[13:20:58] *** Joins: andypandy (~andypandy@h-178-174-148-234.A163.priv.bahnhof.se)
[13:22:28] *** Quits: zakame (~zakame@user/zakame) (Ping timeout: 256 seconds)
[13:23:26] *** Joins: zakame (~zakame@user/zakame)
[13:34:38] *** Quits: c10l (~c10l@89.34.167.207) (Read error: Connection reset by peer)
[13:36:53] *** Joins: c10l (~c10l@89.34.167.207)
[13:46:01] <Activ8> Hello Jim
[13:46:19] <Activ8> Anything broke today Lutin? lol
[14:00:32] *** Joins: rsx (~dummy@ppp-188-174-148-117.dynamic.mnet-online.de)
[14:10:59] *** Joins: TomyWork (~TomyLobo@p200300e80f133c007585c42c4fccc2ad.dip0.t-ipconnect.de)
[14:18:12] <Lutin> Activ8 not yet but the day is still young :)
[14:19:53] <Activ8> ha ha
[14:35:15] *** Quits: mei (~mei@user/mei) (Quit: mei)
[15:09:13] *** Joins: Atum_ (IRC@user/atum/x-2392232)
[15:16:25] <ash_worksi> I have an old project I am trying to iron out; I want to share a git-tracked volume across multiple projects but...... (1) I don't really want to bind-mount a directory outside the scope of a single project unless I have to (2) I can't use a named container because if something changes in the repo it should be reflected and (3) I don't really want to duplicate the volume across each project
[15:16:54] <Lutin> ash_worksi :D
[15:17:49] <Lutin> ash_worksi why not a named container ?
[15:19:17] <ash_worksi> Lutin: because of (2) but I just read something interesting about 'mountpoint'
[15:19:37] <Lutin> ash_worksi explain
[15:19:42] <Lutin> I don't get your point
[15:19:51] <Lutin> I thought you might could use a NFS server
[15:20:50] <ash_worksi> Lutin: I was thinking, "I can't use a named volume because there's no way for me to map that to the host machine where I can pull changes" but it looks like that may not be true
[15:21:15] <Lutin> indeed I wonder why that woudn't work as well
[15:22:22] <ash_worksi> re: `driver-opts:` "Those options are driver-dependent - consult the driver’s documentation for more information."
[15:22:44] <ash_worksi> I am looking at https://docs.docker.com/storage/storagedriver/overlayfs-driver/ but I don't see any additional specifications
[15:23:41] <ash_worksi> where do I find the options for "driver_opts"?
[15:31:49] *** Quits: Atum_ (IRC@user/atum/x-2392232) (Quit: Atum_)
[15:33:40] <ash_worksi> Lutin: actually "mountpoint" appears to be part of a plugin someone wrote for docker
[15:34:07] <ash_worksi> it might be worth my while, but I would still like guidence on just how to do this normally
[15:34:31] <Lutin> ash_worksi what does that has todo with it ? I don't get your point
[15:36:26] <ash_worksi> Lutin: sorry, I would answer but I am not sure what "that" and "it" are: What does <?> have to do with <?> ?
[15:41:37] <ash_worksi> Lutin: the main thing is, how do I *mount* a named volume?
[15:43:21] <geirha> -v volume-name:/mount/point
[15:43:32] <ash_worksi> geirha: not what i mean
[15:43:51] <ash_worksi> geirha: I want to keep the volume up-to-date with git outside the container
[15:43:59] <ash_worksi> well, outside and in
[15:44:10] <ash_worksi> the container should not need git, in other worfds
[15:44:12] <ash_worksi> words*
[15:44:13] <geirha> use another container instead
[15:44:58] <ash_worksi> geirha: like a container that mounts the named volume and has a local mount to the same location?
[15:45:28] <geirha> a container that mounts the named volume, and does whatever you were going to do from the host
[15:45:51] <ash_worksi> according to one stack-overflow article, it claims I can do `volumes: <name_of_volume>: driver_opts: device: /path/on/host` but I don't think so... device: is for like `sda` and whatnot, no?
[15:47:25] *** Quits: tkob (tkob@user/thekingofbandit) (Ping timeout: 258 seconds)
[15:48:06] <Lutin> yeah just mount a named volume multiple times
[15:50:08] *** Joins: Atum_ (IRC@user/atum/x-2392232)
[15:50:13] *** Joins: tkob (tkob@user/thekingofbandit)
[15:51:36] *** Joins: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net)
[15:52:00] *** Quits: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net) (Read error: Connection reset by peer)
[15:52:06] *** Parts: Geo (vanosg@user/geo) (WeeChat 3.1)
[15:52:15] *** Joins: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net)
[15:54:03] <ash_worksi> wow, the `device` thing worked
[15:54:25] <ash_worksi> wonder if that's not ideal for some reason
[15:56:54] *** Quits: tkob (tkob@user/thekingofbandit) (Ping timeout: 252 seconds)
[15:57:29] *** Joins: tkob (~tkob@user/thekingofbandit)
[16:06:47] <Lyn> are there any (good) reasons *not* to ignore node_modules via .dockerignore?
[16:08:27] *** Quits: jazzy (~jaziz@2600:380:465b:aa32:a5e0:69d:ea69:d1e4) (Ping timeout: 245 seconds)
[16:11:03] <Lyn> in easier language: should I always .dockerignore node_modules
[16:11:34] <Lutin> artok ping
[16:15:31] *** Joins: dpreacher (~dpreacher@user/dpreacher)
[16:17:06] <ash_worksi> Lyn: afaiac, yes
[16:19:24] <ash_worksi> Lyn: just be careful where you put the `npm install` line; if it comes before anything that frequently changes, it'll have to download them all over again
[16:19:30] <aminvakil> Lyn: i would ignore them and if i should download them too much over time and this bothers me, i would mount a volume inside container and put node_modules there, this would work if volume is not there, if it's empty or modules have already been downloaded
[16:20:31] <ash_worksi> aminvakil: that's not really relavant to .dockerignore though.
[16:20:55] <Lyn> ash_worksi: do you mean before or after? I think the 'npm i' layer should come before the layers that change frequently
[16:20:57] <aminvakil> ah, i meant i would definitely ignore it and if bothers me i would ...
[16:21:17] <ash_worksi> Lyn: sorry, yes, I mean after
[16:21:50] <artok> Lutin: pong
[16:22:37] <ash_worksi> aminvakil: I get what you're saying, and i support that, but what I mean is whether you choose to mount the volume or not, building will still be the same
[16:22:43] <Lutin> artok if you make clusters. so you start over with naming nodes for each custer like -01 -02, etc ? or do you keep counting ?
[16:23:26] <Lyn> aminvakil: Is this something that should be done in production as well? To mount a (basically shared) volume for node_modules for containers to use
[16:23:36] <aminvakil> ash_worksi: right
[16:23:42] <ash_worksi> Lyn: no
[16:23:50] <ash_worksi> Lyn: just bake in your deps
[16:24:35] <Lyn> ah, thanks
[16:24:37] <artok> lyn: if you have node app, have app on image and start container from that, don't try to update application without loading new image
[16:25:35] * ash_worksi feels like artok normally helpful advice was just more confusing this time
[16:25:38] <ash_worksi> ¯\_(ツ)_/¯
[16:25:48] <ash_worksi> hopefully Lyn understood that
[16:26:00] <ash_worksi> artok's*
[16:26:22] <artok> well it goes: docker image should have all application data installed through Dockerfile instructions (so npm install in the end), not injected in runtime =)
[16:26:38] *** Quits: dpreacher (~dpreacher@user/dpreacher) (Ping timeout: 256 seconds)
[16:26:52] <aminvakil> Lyn: putting node_modules on host is something that MAYBE can be done for developing application if downloading and building it over and over becomes annoying, not for production
[16:27:08] <ash_worksi> artok: "in the end"? I would tend to think you'd want it near the beginning
[16:27:30] <Lutin> ash_worksi artok might have a hangover, still ;)
[16:27:43] <artok> ash_worksi: end, as first you have to install other dependencies for the app
[16:27:57] <Lyn> haha, I think that answer is for production? the supposedly confusing answer, that is. I think dev env does well with nodemon and a bind mount
[16:28:05] *** Joins: scaleww (~scaleww@77-41-20-31.ftth.glasoperator.nl)
[16:28:13] <ash_worksi> artok: I figured that's probably what you meant
[16:28:53] *** Quits: Atum_ (IRC@user/atum/x-2392232) (Ping timeout: 258 seconds)
[16:29:28] <ash_worksi> Lyn: generally, you probably want to start your dev env with whatever image you're intending to deploy and just mount files you change
[16:29:59] <ash_worksi> Lyn: although you can certainly change things up with multistage builds
[16:30:24] <Lyn> alright, now I'll try to figure out if there's a way to get away with just 1 docker-compose.yml file
[16:30:43] <ash_worksi> Lyn: I have always found it useful to have at least 2
[16:31:08] <Lyn> ah, well I guess that's the way to go then
[16:31:10] <ash_worksi> Lyn: 1 to orchestrate misc services, and the other to work deploys exactly how I want them
[16:31:41] <ash_worksi> Lyn: you *can* get away with 1, if you want to type out your `docker run` command for deploymen
[16:31:44] <ash_worksi> t
[16:33:11] <Lyn> would the 1 file be for dev purposes then? trying to wrap my head around this
[16:33:27] <ash_worksi> Lyn: that's what I do
[16:34:54] <Lyn> hmm maybe there's a way to specify the .yml file in the 'docker-compose up' command
[16:35:05] <ash_worksi> yes
[16:35:18] <ash_worksi> docker-compose -f ... up
[16:35:35] *** Joins: rewrit3 (~rewrit3@user/rewrit3)
[16:35:35] <Lyn> that sounds like a good solution
[16:35:42] <ash_worksi> basically I have paths like /some/project/{app/,db/,cache/,etc...,docker-compose.yml} and db, cache, etc are all just fake services for me to get app to work
[16:35:42] *** Quits: nrg (~NRG@user/nrg) (Quit: ZNC - https://znc.in)
[16:36:12] <ash_worksi> anything that should actually get deployed as a service finds it's way in /some
[16:36:34] <ash_worksi> so /some/{real_cache,real_db,real_etc...}
[16:36:50] *** Joins: nrg (~NRG@user/nrg)
[16:37:41] <Lyn> two copies of the same service?
[16:38:39] <ash_worksi> Lyn: yes, because /some/real_db might have configuration like hot-standby, or "official" migration scripts 
[16:38:55] <ash_worksi> /some/project/db will just contain a dump file usually
[16:39:59] <ash_worksi> /some/project/proxy will have some bare-minimum nginx config; /some/real_proxy will have more elaborate config that checks for ssl certs and what not
[16:40:37] <Lyn> I see, maybe two .env files could work as well, but maybe not so well
[16:41:00] <ash_worksi> also, /some/real_proxy will probably handle mutliple vhosts, /some/project/proxy will just be whatever I can get away with
[16:41:50] <ash_worksi> Lyn: not sure where you're going with the .env files.
[16:41:53] <ash_worksi> ¯\_(ツ)_/¯
[16:44:58] *** Quits: dmalteseknight (~dmaltesek@user/dmalteseknight) (Ping timeout: 240 seconds)
[16:45:21] <Lyn> different configs for dev and prod, but it'd pretty much require having two versions of the same code in 1 file
[16:56:07] <ash_worksi> Lyn: well, I don't rely on compose for my environment; if anything I just set the project name; if I can't use docker secrets, I export variables in prod; and I just set dev env in the dev compose file
[16:56:25] <ash_worksi> erm I don't rely on the compose *.env* for my environment
[17:08:19] <Lyn> can you elaborate about export variables as substitutes for secrets, is it like passing env vars to containers via shell commands via docker run
[17:14:17] *** Joins: thiras (~thiras@user/thiras)
[17:18:30] *** Joins: cmagina (sid286142@user/cmagina)
[17:20:59] <artok> on entrypoint, you check for file that has your secret. in development, you mount directory, on production swarm/k8s you can then use docker/k8s secrets
[17:28:13] *** Joins: andycooper (uid246432@id-246432.brockwell.irccloud.com)
[17:50:56] *** Joins: Bardon (~Bardon@user/Bardon)
[18:09:20] *** Joins: junktext (~junktext@77.247.181.217)
[18:09:44] *** Joins: vidbina_ (~vid@dynamic-078-054-116-112.78.54.pool.telefonica.de)
[18:13:00] *** Quits: pvdp (~Pieter@static-n58-105-183-94.rdl4.qld.optusnet.com.au) (Remote host closed the connection)
[18:13:20] *** Quits: yamchah2 (~yamchah2@user/yamchah2) (Ping timeout: 265 seconds)
[18:14:15] *** Joins: yamchah2 (~yamchah2@user/yamchah2)
[18:15:21] *** Quits: varaindemian (~varaindem@86.124.78.162) (Quit: Client closed)
[18:22:18] <raub> Is it possible to run a command on the docker server from within a docker-compose file before starting a service?
[18:23:28] <BtbN> You'd write that into the Dockerfile of your image.
[18:27:56] <raub> BtbN: that sounds like an interesting idea. Now to learn how to do that at the dockerfile level!
[18:27:59] *** Joins: johnny_sitar (~artur@078088015209.bialystok.vectranet.pl)
[18:29:34] <BtbN> You... just write it in there? After all your other commands.
[18:30:25] <ash_worksi> raub: perhaps you could tell us what command you want to run?
[18:31:27] <ash_worksi> Lyn: if you have some sensitive data like a db password and I don't have the luxury of docker secrets, I would set it on the server like `read -r -s DBPASS` and run my container with it `docker run -e DBPASS ...` -- this just keeps the sensitive data completely in memory. For a litany of env vars, I would probably just use --env-file
[18:39:16] *** Quits: johnny_sitar (~artur@078088015209.bialystok.vectranet.pl) (Quit: Leaving.)
[18:40:26] <artok> my approach makes your images k8s/swarm ready, just to point out again
[18:44:47] *** Quits: TomyWork (~TomyLobo@p200300e80f133c007585c42c4fccc2ad.dip0.t-ipconnect.de) (Remote host closed the connection)
[18:51:21] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Ping timeout: 250 seconds)
[18:52:15] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[18:53:57] <Lyn> now I'm trying to figure out a good way to set up live debugging -- first I 'npm install' in image, then I "COPY . ." while .dockerignoring everything except the source files, but now if I bind mount the same directory, then the .dockerignored files will probably creep into the container anyway
[19:03:49] <raub> BtbN: What I have found so far was https://stackoverflow.com/questions/42735468/execute-command-on-host-during-docker-build, which suggests to have a build.sh script which does things at the host level (such as populate some variables) and then call docker build passing those newly created variables.
[19:05:35] <BtbN> I thought you want them in the container?
[19:05:45] <BtbN> Just literally add another RUN to your Dockerfile
[19:06:17] <BtbN> Running commands on the host from a Dockerfile is probably not possible and also a bad idea
[19:07:08] <raub> I am doing now what the link suggested, but that does not seem to work well for a docker-compose file
[19:11:33] *** Quits: Bardon (~Bardon@user/Bardon) (Ping timeout: 268 seconds)
[19:13:26] <programmerq> docker-compose is really just a fancy way to submit a build job to the docker build backend. the compose build functionality would be equivalent to the 'docker build' step in that stackoverflow answer. The 'build.sh' wrapper on the host would still be its own 'build.sh' that would ensure any keys are set or repos cloned and whatnot.
[19:14:46] *** Quits: rsx (~dummy@ppp-188-174-148-117.dynamic.mnet-online.de) (Quit: rsx)
[19:15:27] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 252 seconds)
[19:16:47] <akik> programmerq: isn't docker build running everything in containers?
[19:18:17] <artok> yes, each Dockerfile instruction creates new intermediate container and diff is saved as layer
[19:18:41] <akik> which option does that url give you to run something on the host?
[19:20:40] <raub> akik: see programmerq's last reply. And mine to BtbN. Bottom line it does not fit my needs. I will come up with another way
[19:20:53] <programmerq> the 'docker build' cli tars up the build context and sends it up to dockerd. docker then untars that, looks for the Dockerfile, and then starts running through the instructions. RUN is indeed implemented with a container that runs the command and then commits the changes to the filesystem as a new image layer.
[19:20:56] <akik> that just looks like they run build.sh manually before docker build
[19:21:28] *** Joins: Bardon (~Bardon@user/Bardon)
[19:21:29] <programmerq> I'm a fan of using a Makefile for cases where a 'docker build' needs any sort of input-- including dynamically determined build args.
[19:21:39] <artok> build.sh is usually something that creates dockerfile by checking architecture and so on
[19:22:06] <programmerq> I really only use 'docker-compose build' for the simplest build usecases. It's quite a valid pattern to have a Makefile that can 'docker build && docker push' and then 'docker-compose up -d' and use the image that was just built.
[19:22:31] <raub> Yes. BtbN thought I wanted to run something inside the container so he recommended just using Dockerfile. H eknows now that is not the case.
[19:22:44] <raub> programmerq: the makefile idea is making me think
[19:22:47] <artok> + additionally Makefile makes compose.yml from template and makes correct image version to be run
[19:23:40] <artok> many of here have seen me write: "don't build with compose, keep buildtime and runtime separate"
[19:25:16] <raub> artok: in my defense, that is how they do it at work, so I was trying to be consistent. With that said, trying to fit in is usually how I get hurt
[19:25:39] *** Joins: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net)
[19:26:38] <raub> So splitting the process up as you all are suggesting sounds saner for me
[19:27:44] <artok> have good Makefile/whatever build script and they'll start using that also
[19:36:47] *** Quits: yamchah2 (~yamchah2@user/yamchah2) (Ping timeout: 245 seconds)
[19:37:26] *** Joins: yamchah2 (~yamchah2@user/yamchah2)
[19:39:05] *** Joins: Null_A (~null_a@2601:645:8700:2290:7da4:a1bb:6c20:daba)
[19:59:59] <Lyn> so apparently I can use .dockerignore to determine which files are not taken into the production container, AND I can use anonymous volumes to determine which (sub)directories are not taken into the development container
[20:03:15] <artok> simply put: .dockerignore handles what files aren't sent in tar package to daemon on docker build
[20:03:56] <artok> that way you control what files are available on COPY instruction
[20:04:50] <Lyn> oh right, good distinction - I'm trying to write documentation for myself since the bind mount copies everything regardless of COPY or .dockerignore
[20:05:17] <artok> ..because is bind mount
[20:05:55] <Lyn> ye, and I rather keep node_modules untouched on the container
[20:10:15] <ada_> morning
[20:12:14] <artok> mornin!
[20:14:17] *** Quits: vidbina_ (~vid@dynamic-078-054-116-112.78.54.pool.telefonica.de) (Ping timeout: 258 seconds)
[20:15:19] *** Joins: fdan (~fdan@192.146.154.3)
[20:15:37] *** Quits: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com) (Quit: WeeChat 3.0.1)
[20:15:57] <Lyn> hello
[20:20:48] *** Quits: Bardon (~Bardon@user/Bardon) (Ping timeout: 258 seconds)
[20:33:11] *** Joins: waldo323_ (~waldo323@d149-67-45-83.clv.wideopenwest.com)
[20:34:54] <Lyn> oh boy it's so satisfying to see the setup work so far
[20:36:15] *** Quits: waldo323 (~waldo323@d149-67-45-83.clv.wideopenwest.com) (Ping timeout: 276 seconds)
[20:41:36] *** Joins: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com)
[21:07:10] *** Quits: fdan (~fdan@192.146.154.3) (Ping timeout: 246 seconds)
[21:08:27] <Lyn> where do people usually store their databases? VPS/Cloud provider's "volumes"?
[21:08:36] *** Joins: onizu (uid373383@id-373383.highgate.irccloud.com)
[21:15:39] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[21:16:53] *** Joins: Bardon (~Bardon@user/Bardon)
[21:19:48] <artok> depending on size of deployment. there is time when using service provider's db service is cheaper, when you count how many hours you need to spend on your db management
[21:21:22] <programmerq> deploying a containerized database isn't that different than deploying a non-containerized one. you still need to make sure the database process has a mounted directory backed by a block device with adequate IO for your usecase and requirements. docker will pretty much present a directory to a container when you use a volume/bind mount.
[21:22:57] <programmerq> kubernetes has several abstractions and volume interfaces for several storage vendors, including cloud providers. So you can do stuff like say "hey, I need a persistent volume and it should be backed by this storage class that I defined, which says to mount an EBS volume with xyz number of iops per gig and n gigs in size" and the kube stuff will handle making the filesystem and attaching it to the
[21:22:58] <programmerq> container. it's pretty nice.
[21:23:17] <programmerq> docker does have some support for driver interfaces for volumes, but it's all pretty stale compared to the storage interface on the kube side.
[21:24:06] <artok> rex-ray being one plugin
[21:27:28] *** Quits: junktext (~junktext@77.247.181.217) (Ping timeout: 272 seconds)
[21:37:39] *** Joins: Sasazuka (~Sasazuka@user/sasazuka)
[21:41:24] *** Quits: realies (~realies@user/realies) (Ping timeout: 272 seconds)
[21:45:52] *** Joins: Crassus (~Crassus@user/crassus)
[22:02:18] <tabakhase> Lyn back to node_modules (and others of that sort) - there is bit of a split sometimes, you want to be sure the container has data not messed with by the user, so "one would never reuse that for a build, but always use the package-tooling to get fresh&verified data" - but for dev you may still also "want those files on your disk so your IDE can read em for autocomplete/intelisense..." - now 
[22:02:19] <tabakhase> add ontop that node_modules is a little extra special if you also have windows users, symlinks will get horrendously goofed to the point where "they literally break the packing on docker build ." - so one may even end up with quirky hybrid setups (shaddowing it into a volume, and `npm i`-ing a) in your build, b) in your upstart/manually for dev and c) on your windows host for the ide
[22:02:26] <tabakhase> also wall of text
[22:11:02] *** Joins: ztx (~ztx@user/ztx)
[22:33:02] *** Quits: dreamer (~dreamer@user/dreamer) (Remote host closed the connection)
[22:38:23] *** Joins: physikoi (~physikoi@user/physikoi)
[22:39:02] <physikoi> Hello #docker. Is it possible to recover some files from an image that I accidentally nuked without losing my own work?
[22:39:23] <ada_> physikoi: I think you mean a container
[22:39:37] <physikoi> ada_: probably :)
[22:39:40] <ada_> physikoi: explain the series of events that transpired
[22:40:31] *** Joins: Atum_ (IRC@user/atum/x-2392232)
[22:42:16] <physikoi> ada_: So, I was trying to figure out why a setup script was taking forever for a given volume(image? forgive me if i'm mis-using terms). Turns out script had a `cp -R node_modules ...someplace`. Sooo, I decided that I would delete the destination folder. Yet, someone, I would up deleting files from the origin folder. 
[22:42:45] <ada_> can you show me teh command you ran to start your container
[22:43:27] <physikoi> I'm using docker-compose. Would that help? Or, do you want the Dockerfile?
[22:43:33] *** Shaun_ is now known as Shaun
[22:43:34] <ada_> show me the compose file in that case
[22:44:15] <ada_> cat docker-compose | nc termbin.com 9999   or you can use your favorite paste tool
[22:44:28] <ada_> docker-compose.yml*
[22:45:24] <physikoi> ok, here we are: https://pastebin.com/raw/MScAFJY9
[22:45:59] <ada_> which service is the relevant one?
[22:46:29] <physikoi> that would be the 'frontend' service
[22:47:20] <ada_> ok, so you docker-compose up, and then your containers start, and then you notice that a setup script is taking a long time... how did you do the 'delete' operation?  did you use docker exec to get into your container and then delete something?
[22:48:04] <physikoi> that's right, i did `docker exec` to get into it and then did the screw up
[22:48:13] <ada_> what was the path you deleted?
[22:51:10] *** Joins: jarthur (~jarthur@2603-8080-1540-002d-697d-76f9-c1bf-e9e5.res6.spectrum.com)
[22:52:05] <physikoi> looking at the local filesystem, i'm seeing that, at least, my dockerfile and entrypoint.sh, are missing. I could try entering the container again, though I would have to alter the command as those files are missing. I *thought* I did `rm -rf *` from within /frontend_node_modules, but I may have done it in /frontend
[22:52:28] <ada_> if you did it in /frontend, then your files are probably gone gone
[22:53:11] <ada_> there might be some utility you can try to use to recover them from memory or something;  I don't really know about data recovery 
[22:53:48] <ada_> if you deleted files that existed in the docker image, what I'd probably do is spin up another copy of the same container and copy the files you need OUT and back into your actual container
[22:54:18] <ada_> but if you deleted files that existed on your host, like the ones under the bind mount at ./frontend, then that's your host filesystem and there's not much you can do except look into data forensics / data recovery tools for linux 
[22:54:34] <physikoi> Well, thankfully, it doesn't look like anything i created was destroyed
[22:55:17] <ada_> are you on linux, mac, windows?
[22:55:18] <physikoi> So, would the best course be to copy my files and redownload the project?
[22:55:21] <physikoi> mac
[22:56:05] <ada_> more complicated, since docker for mac uses an embedded VM to run docker, and we use a file sharing mechanism to get your files from your Mac filesystem into the VM and then into the container
[22:56:32] <physikoi> oh no
[22:57:25] <physikoi> so, it wouldn't work by simply redownloading and then copying the files I created into the freshly cloned repo?
[22:57:49] <ada_> well, I'm still not 100% sure what you did, so I'm not sure I can say
[22:58:11] <ada_> right now, what's missing from your container?
[22:59:24] <physikoi> sorry, i could be clearer. I was given a base docker project from which to create a widget. It appears that all my widget files were unharmed by my carelessness; instead, just some of the files that came with the base project were nuked. i hope that clears it up a bit
[22:59:34] <ada_> it doesn't
[22:59:46] <ada_> I don't know your project layout, or your container filesystem, so I can't really say from that description what you did
[22:59:46] <physikoi> lol sorry
[22:59:55] <ada_> if you could show me literally the commands you ran verbatim, then that would help
[23:01:11] <ada_> if you know the path inside the contianer that you deleted, then you could cross reference to the volumes that are specified in your compose file.  if you deleted files under a bindmount, then those "files" live on your host filesystem.  you may have deleted them from your host filesystem.  if those "files" were part of the docker image, then another container spawned from the same image will have
[23:01:13] <ada_> them.  if your files were created by some process in your container and stored at the named volume location, then running another container to create a new set of files would work
[23:01:33] <physikoi> ill try. So, this* is the project I was given from which to base my work: * https://www.dropbox.com/s/xhksxmbig383u7k/newhire-fullstack-test-master.zip?dl=0
[23:02:31] <physikoi> I downloaded. cd'd into directory, and ran `docker-compose up`. After that, the Docker desktop app could be used to view logs and what not, since i've got that installed too
[23:03:25] <physikoi> I noticed that the /frontend service command appeared to hang, so i `docker exec`d into that container and proceeded to screw up the contents in the VM(?).
[23:04:32] <ada_> well, if you go into that project directory on your host, are the files still there?
[23:04:36] <physikoi> Yeah, some of those files were definitely deleted from host filesystem, but,thankfully, it appears that my own work was luckly unscathed
[23:04:57] <ada_> ok, so, in that case I would probably clone a new copy of your project and just copy your changes over to it
[23:05:05] <ada_> there are git commands you could run but
[23:05:15] <ada_> https://xkcd.com/1597/
[23:05:20] *** Quits: Atum_ (IRC@user/atum/x-2392232) (Quit: Atum_)
[23:05:50] <physikoi> it's so little work, that simply copying the files into the new download is just fine. OK, thank you, ada_ !!
[23:05:59] <ada_> sure thing
[23:06:34] <physikoi> haha, i used to read xkcd all the time back in grad school. good stuff
[23:06:43] <ada_> xkcd, always relevant
[23:18:02] *** Joins: roxlu (~roxlu@2a01:7c8:aac0:286:8c3a:b01c:3830:245b)
[23:18:06] <roxlu> hi
[23:18:40] <Lyn> why does it take a long time for a container to show up in Docker Desktop when I do docker-compose -f XX up, and when I do docker-compose down, it doesn't respect the dependency order and doesn't stop them all
[23:19:51] <akik> is docker compose now included in docker-ce ?
[23:20:25] <roxlu> this is probably a very stupid question ... I've created an image (built it from a Docker file), then I executed an interactive bash shell using something like `docker run -it roxlu/mydock bash`. When I run `docker images -a` I see about ~10 items, with different image ids and all taking ~2GB. Does that mean I've created 20GB of data?
[23:22:00] <gordonjcp> roxlu: yes and no
[23:22:24] <roxlu> haha thanks, though I'm curious to what you said yes :)
[23:23:28] <gordonjcp> roxlu: so some of those will just be the same image repeated over and over
[23:24:12] <roxlu> ok, is there a way to check that?
[23:24:40] *** Quits: Crassus (~Crassus@user/crassus) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[23:25:01] <gordonjcp> -a shows everything including intermediate images
[23:25:09] <gordonjcp> they might not actually be real images though
[23:26:15] <roxlu> ah! so when I do `docker images`, which only shows 3 items that's actually the real "images" that have been created?
[23:26:21] <gordonjcp> yes
[23:26:45] <gordonjcp> if you look around in /var/lib/docker you'll see where the overlayfses are stored, there won't be 20GB worth most likely
[23:27:03] *** Joins: junktext (~junktext@77.247.181.218)
[23:27:24] <roxlu> ok thanks! So when I run `docker run -it roxlu/mydock bash` it will add an extra entry to `docker images -a`. Why does it "remember" those runs/instances?
[23:27:25] <gordonjcp> roxlu: someone who knows far more about it than me will be along in a sec to explain why this is total bullshit
[23:27:38] <gordonjcp> roxlu: but basically you're creating a new overlay when you do that
[23:27:52] <gordonjcp> the overlay is small because it only contains the changes you made to the image
[23:28:07] <roxlu> Ok thanks for explaining
[23:28:07] <gordonjcp> the *image* is 2GB because the base image is 2GB plus whatever you added on
[23:28:12] <gordonjcp> but it's only 2GB when you make it real
[23:28:19] <gordonjcp> that's the clever part
[23:28:47] *** Quits: pycurious (~Adium@user/pycurious) (Quit: Leaving.)
[23:29:16] <artok> akik: yes, lrwxr-xr-x  1 root  admin  62 Jun 10 21:23 /usr/local/bin/docker-compose -> /Applications/Docker.app/Contents/Resources/bin/docker-compose
[23:29:34] <artok> that is true on mac (and win?)
[23:29:40] <akik> artok: that looks like macos, not docker-ce
[23:30:03] <Lyn> oh wow, so I actually have to specify the file too when I do docker-compose stop/down
[23:31:14] <artok> still the docker daemon itself is -ce engine
[23:31:35] <akik> i only use docker-ce on linux
[23:32:05] <artok> sure, and your distro package has compose on different package as before
[23:32:40] <akik> it's just weird how they package things
[23:32:43] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[23:33:01] <akik> "you'll get these features, but not on linux"
[23:33:02] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[23:33:13] <artok> <insert any distro here> has it's own policy
[23:33:45] <gordonjcp> what's the crack with seeing "docker compose <thing>" in documentation recently?
[23:33:48] <artok> and since linux engine is used on real life deployments, why bother installing cli tools
[23:34:03] <gordonjcp> is docker integrating docker-compose into itself?
[23:34:33] <artok> you need cli tools on the machine that will deploy to the engine, so packaging that way is totally valid
[23:34:59] <akik> artok: no i meant docker
[23:35:11] <akik> artok: how docker packages these things
[23:35:15] <artok> I too.
[23:35:49] <akik> gordonjcp: it's there for the docker desktop for windows and macos
[23:35:52] <artok> be it .deb or whatever package, it is understood that there is no need for all cli on the host that is running the daemon
[23:37:04] <gordonjcp> akik: okay, but is that the way it's going on Linux too?
[23:37:14] <akik> gordonjcp: that's what i'm asking
[23:37:40] <artok> gordonjcp: I doubt, for the fact that I've been trying to say =)
[23:38:16] <akik> some other things that are missing are the included kubernetes setup and development environments
[23:38:25] <artok> you have remote daemon, you deploy to it from your workstation, api is there
[23:40:45] <artok> as mac is great dev env, bundling all things on that is just good service
[23:42:05] <artok> sure, adding kind of docker-ce-dev metapackage to install all same stuff would also be great
[23:44:26] <gordonjcp> artok: I'm not quite sure why
[23:45:28] <artok> to keep the actual docker host lean?
[23:45:45] *** Quits: lemonzest (~lemonzest@user/lemonzest) (Quit: Quitting)
[23:46:21] <artok> why spend extra MB installing stuff that would not be needed on the host? why would you install doc packages to host that is remote connected?
[23:46:25] *** Joins: realies (~realies@user/realies)
[23:46:38] <artok> everything counts in large amounts
[23:46:59] <akik> artok: so that people who use linux as their workstations wouldn't feel left out
[23:46:59] *** Quits: Null_A (~null_a@2601:645:8700:2290:7da4:a1bb:6c20:daba) (Remote host closed the connection)
[23:47:30] <artok> akik: that why I took that metapackage into play
[23:47:34] *** Joins: Null_A (~null_a@2601:645:8700:2290:7da4:a1bb:6c20:daba)
[23:47:37] <artok> that's
[23:47:37] *** Quits: physikoi (~physikoi@user/physikoi) (Ping timeout: 245 seconds)
[23:48:39] *** Quits: Null_A (~null_a@2601:645:8700:2290:7da4:a1bb:6c20:daba) (Remote host closed the connection)
[23:48:53] *** Joins: Null_A (~null_a@2601:645:8700:2290:7da4:a1bb:6c20:daba)
[23:50:14] <artok> if you check descriptions of the .deb packages, they are quite straight forward
[23:51:41] <artok> actually now checking... cli package should have compose in it, but might be that they want to have python dependency out of it
[23:52:20] <artok> no wait.. it isn't anymore python one?
[23:53:29] <artok> +1 for adding it to docker-ce-cli package
[23:53:40] <akik> artok: it's not in docker-ce-cli-20.10.7-3.el7.x86_64.rpm
[23:58:16] *** Joins: BenjiProd (~BenjiProd@user/benjiprod)
