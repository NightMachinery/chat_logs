[00:00:19] <mfiano> copec: You might also be interested in Inform
[00:01:28] <mfiano> https://en.wikipedia.org/wiki/Inform#Inform_7_programming_language
[00:02:04] <mfiano> As a game developer, I would have too much fun with that and wouldn't want to program games :)
[00:06:37] <White_Flame> copec: yeah, I've done some prolog, and a ton of prolog-in-lisp
[00:08:50] <shka> i've implemented prolog-ish in lisp
[00:08:56] <shka> unfolding interpreter
[00:09:07] <shka> pl: the best
[00:14:45] <mfiano> Inform7 uses natural language to program complex logical stories
[00:14:58] <shka> this sounds interesting
[00:15:35] <mfiano> There's also AAL, which is sort of the same idea, but Lispified, but I don't know where the source is: https://groups.google.com/g/rec.games.programmer/c/YUPb72OrJj0/m/iSbnMin0UloJ?hl=en
[00:15:36] -ixelp- Adventure Authoring Language (long)
[00:15:50] <shka> i kinda want to make interpreter for that 
[00:15:52] <shka> is that bad?
[00:15:59] <mfiano> Nope!
[00:30:57] *** Joins: Qwnavery (~Qwnavery@user/qwnavery)
[00:31:12] *** Quits: Qwnavery (~Qwnavery@user/qwnavery) (Client Quit)
[00:37:26] *** Quits: shka (~herr@109.231.62.239) (Ping timeout: 252 seconds)
[00:37:50] <White_Flame> I've done compilers for prologish-in-lisp
[00:38:02] <White_Flame> it gets pretty intense
[00:40:03] <gilberth> How so? I once hacked a Prolog compiler and that was pretty straight forward.
[00:40:56] <gilberth> So I wonder what sophistication I might have missed.
[00:42:13] <White_Flame> when you want better performance than the WAM
[00:42:49] <White_Flame> and implement scoping & better declarative programmer constructs than the very imperative standard prolog stuff
[00:43:10] <White_Flame> standard prolog has a lot of downsides & weaknesses that should be advanced through
[00:51:21] <White_Flame> it's basically designed to be easier to implement, and puts a lot of hidden & overt complexity on the programmer
[00:51:50] <White_Flame> especially when it comes to optimization
[00:53:09] <gilberth> I see. Indeed my implementation was simplistic.
[00:54:09] <mfiano> That and modules and portability are a lot of work
[01:01:32] <White_Flame> right, we did a few different take on namespaces, libraries, etc
[01:29:23] <copec> How do you all organize research papers that you download?
[01:33:20] <moon-child> I don't
[01:33:27] <moon-child> they all go in a directory called 'tmp'
[01:33:57] <moon-child> together with videos I have yet to watch, specifications, e-books, ...
[01:34:48] *** Joins: hayley (~theemacss@user/hayley)
[01:38:05] *** Joins: dave0 (~davezero@069.d.003.ncl.iprimus.net.au)
[01:38:28] <dave0> maw
[01:40:20] <hayley> shka: My compiler goes RE -> DFA -> annotated DFA -> Lisp -> machine code
[01:41:54] <hayley> Going from 50 Mchars/s to 700 Mchars/s is pretty big imo
[01:43:45] <hayley> Someone has offered to port the cl-ppcre regex parser, but I said I want to follow POSIX semantics, so we'll see how it turns out.
[01:56:41] <gilberth> Good morning, hayley! We need to get a better compiler, I manually translated "sun" to C and get 20ns.
[01:58:02] <gilberth> We perhaps could have Perl semantics, if we wished to have that brain damage.
[01:58:51] * hayley blames SIMD searching somehow
[01:59:29] <gilberth> For what?
[01:59:49] <hayley> For your C compiler using SIMD to search.
[02:00:29] <gilberth> My compiler doesn't do that.
[02:07:11] <hayley> Huh, GCC and Clang still use a loop?!
[02:07:36] <moon-child> -ftree-vectorize maybe
[02:07:57] <hayley> Still nothing.
[02:08:13] <gilberth> Yes. But GCC somehow recognizes that we could check bounds once for the minimum of three chars to match.
[02:08:23] <hayley> https://godbolt.org/z/4GzbGr3Gn ‚Üê My translation somehow.
[02:08:24] -ixelp- Compiler Explorer
[02:08:40] <hayley> Well, my compiler does that too now.
[02:09:00] <gilberth> <http://clim.rocks/gilbert/sun.c> <http://clim.rocks/gilbert/sun.s>
[02:10:05] <gilberth> hayley: What is your code? It's not a DFA.
[02:10:38] <hayley> Indeed it isn't, just how I thought I'd write a substring search.
[02:11:04] <gilberth> Yes.
[02:12:51] <hayley> The only change I made was that I got lazy and put the START and END registers in the vector rather than arguments to the continuation function, but I doubt it's that bad.
[02:13:10] <hayley> Time to use SB-SPROF on assembly code?
[02:14:01] <gilberth> But look at the assembler code I got. What the hell happened to the bound check?
[02:15:19] * hayley scratches head
[02:16:32] * hayley puts https://www.youtube.com/watch?v=dVZwu5524f0 on the jukebox and scratches head some more
[02:16:35] <gilberth> It's still there, I am blind. It's the cmpq %rax, %rdx.
[02:17:27] <gilberth> Nope. /me scratches head, too.
[02:17:53] <gilberth> I misread.
[02:18:28] <hayley> Still, now that I have a minimum length per state, scymtym gave me the idea to check multiple characters at once using (a XOR b) OR (c XOR d) = 0 to tell if a = b and c = d. Look ma, no branches.
[02:19:53] <gilberth> Still something I am not concerned with.
[02:20:15] <hayley> You very nearly do the same thing in SIMD, except that you get a real = instruction.
[02:20:50] <gilberth> Sure, but again nothing I would need for a scanner.
[02:21:07] <moon-child> gilberth: cmpq %rdx, %rsi
[02:21:39] <hayley> You could skip over a whole token with only one branch?
[02:22:26] <gilberth> hayley: Tokens aren't that long.
[02:22:38] <gilberth> moon-child: Thanks!
[02:22:45] <hayley> But someone who designed SSE for Intel was cheap on intel....er...anyway there's only MOVMSK for bytes and double floats.
[02:22:49] <hayley> gilberth: Still, more than one character?
[02:24:14] <gilberth> hayley: Seldom. You usually don't tell keywords from identifiers in the scanner. What however would be needed is to look for end of a comment or string literal.
[02:25:16] <hayley> Hm, okay.
[02:27:15] <gilberth> It usually is faster to just stick your reserved words into the symbol table and go with a smaller scanner. Also better because of the cache.
[02:28:20] <hayley> https://www.youtube.com/watch?v=9oWSJpIHhYk
[02:28:20] -ixelp- Radio Prague - YouTube
[02:28:24] <gilberth> Then one of my profs always said you should better do lexical analysis in one go and then parse and not interleave it. Makes sense.
[02:28:45] <moon-child> do them concurrently!
[02:29:22] <gilberth> In the 90s? But otherwise: Sure.
[02:29:41] <hayley> https://old.reddit.com/r/LispMemes/comments/pu40gm/when_the_symbolics_lawyers_ask_where_i_got_my/
[02:29:41] -ixelp- When the Symbolics lawyers ask where I got my copy of Genera : LispMemes
[02:29:46] <moon-child> :)
[02:31:22] <moon-child> there is a copy floating around of genera source code
[02:32:09] <hayley> https://thepiratebay10.org/torrent/3769989/Symbolics_Open_Genera_2.0_for_Alpha_-_complete_package_with_Lisp including a comment from Mr Schmidt himself.
[02:32:09] -ixelp- Symbolics Open Genera 2.0 for Alpha - complete package with Lisp (download torrent) - TPB
[02:32:19] <gilberth> I'd rather would like to have the sources of the emulator.
[02:32:38] <moon-child> https://github.com/lassik/vlm ?
[02:32:39] -ixelp- GitHub - lassik/vlm: Symbolics Virtual Lisp Machine (C version)
[02:32:58] <gilberth> Has someone fixed the CLX bug yet?
[02:35:05] * gilberth reads <https://github.com/lassik/vlm/blob/master/documentation/64-bit-porting-guide.txt> and mumbles <stdint.h>
[02:35:33] *** Quits: robin (~robin@user/terpri) (Read error: Connection reset by peer)
[02:35:52] *** Joins: robin (~robin@user/terpri)
[02:35:54] <moon-child> is from 1993
[02:36:02] <moon-child> 92
[02:36:06] <moon-child> stdint appeared in 1999!
[02:37:06] <moon-child> (or, at least, was standardised)
[02:38:18] <gilberth> Still, typedef already was there :-p
[02:38:45] <gilberth> moon-child: Anyhow, looks like the right thing. Thanks.
[02:39:18] <moon-child> even #define! :)
[02:40:09] <gilberth> I always wondered, if "FILE" predates typedef.
[02:43:36] *** Joins: dec0d3r (~dec0d3r@2001:8003:4810:9600:7275:1afb:1707:8eaa)
[02:45:27] <gilberth> Speaking of which, I have some trouble with the CADR emulator with regard to the keyboard. I once sped up graphics output quite a bit though.
[02:46:11] <hayley> Well, my performance regression is not due to using a vector for all the result indexes.
[02:46:17] *** Quits: dec0d3r (~dec0d3r@2001:8003:4810:9600:7275:1afb:1707:8eaa) (Remote host closed the connection)
[02:56:54] <gilberth> I wonder how expensive it would be to build a real CADR from the schematics. Perhaps with more modern RAM, of course.
[02:57:19] *** Quits: random-nick (~random-ni@87.116.178.231) (Ping timeout: 265 seconds)
[02:59:08] <gilberth> And how large it would be.
[02:59:24] <pjb> gilberth: not very expensive, indeed if you want to simulate it (but IIRC, this has already been done). Another way would be to use an ASIC or a big EEPROM; that would be cheap and easy enough (mostly software).
[03:00:05] <pjb> gilberth: then of course, you could want to build it from integrated circuits, or from discrete component, then it would be more work, more expensive.
[03:00:06] * hayley tries to test Python re again and now it refuses to match on the email example.
[03:00:21] <gilberth> pjb: The CADR already runs on an FPGA. I was contemplating using 74xx chips as the real thing.
[03:00:49] <hayley> Oh, I want .search not .match
[03:01:25] <pjb> gilberth: a 6502 is only a few thousands transistors.  I'd guess a CADR wouldn't be more than 50000 transistors. about 5000 74xx I'd say.  But of course you wouldn't just use basic gates. 
[03:02:01] <gilberth> Instead of 'F you could use 'FCT which is CMOS, so the power consumption shouldn't be too bad.
[03:02:19] <pjb> gilberth: there are probably software that can map schematics to a minimum number of standard chips.
[03:02:47] <gilberth> pjb: Well, I am contemplating here to use the original schematics which uses 74xx, but ECL for the graphics board. And I would of course use denser RAM chips.
[03:03:26] <pjb> That probably would be slower than a FPGA or ASIC‚Ä¶
[03:03:56] <gilberth> And slower than a current CPU emulating it. I am not after speed here.
[03:04:01] <hayley> The downside: (the-cost-of-nothing:bench (compile-regular-expression "¬´(a|b|c)+¬ª@¬´(a|b|c)+¬ª")) ‚áí 36.71 milliseconds
[03:04:22] <pjb> And the FPGA is probably slower than a simulator on a contemporaneous processor. Only an ASIC could reach similar performance as a simulator, probably.
[03:04:58] <gilberth> pjb: I won't be that sure that an FPGA could outpace a CPU.
[03:05:43] <gilberth> But an ASIC is out of reach.
[03:06:39] <pjb> Then of course, a CADR like this https://monster6502.com
[03:06:41] -ixelp- MOnSter 6502
[03:06:43] <pjb> would bev ery nice.
[03:07:08] <gilberth> pjb: Are you insane?
[03:07:31] *** Quits: Mandus (~aasmundo@0.51-175-33.customer.lyse.net) (Ping timeout: 252 seconds)
[03:07:37] <gilberth> Why less dense than the 74xx original design?
[03:08:00] <pjb> estimated the cost at between $2k - $4k for this monster 6502.  Probably 5x-10x more for a similar CADR (and a surface between 1m¬≤ and 2m¬≤).
[03:08:13] <pjb> gilberth:  for the nice leds and seeing it working.
[03:08:24] <gilberth> Heh.
[03:08:44] *** Joins: Mandus (~aasmundo@0.51-175-33.customer.lyse.net)
[03:08:53] <pjb> Nah, if you what to play with a CDR, use an simulator on a Raspberry Pi.
[03:08:58] <pjb> That said, good night!
[03:09:29] <gilberth> You find bills or material for the CADR on its tapes. I would love to be able to read the CAD files though. That are some "SUDS" files and was not successful to figure out anything about that file format.
[03:09:35] <hayley> Right, seems that compilation time is nearly nearly entirely SBCL's problem.
[03:16:46] *** Joins: lisp123 (~lisp123@5.30.23.247)
[03:18:35] <gilberth> Well, in principle it shouldn't be hard to write a dedicated compiler for our DFAs.
[03:19:34] <gilberth> We could do that in C even and get famous.
[03:21:15] <hayley> https://www.youtube.com/watch?v=eW7IPtWG26I
[03:21:15] -ixelp- This Is Helena - YouTube
[03:23:08] <hayley> Time to install R**t again to compare their regex engine. Any bets on the slowdown?
[03:24:20] <gilberth> Compile time or run time?
[03:24:39] <hayley> Both?
[03:24:53] <gilberth> hayley: Find good examples which trigger backtracking.
[03:25:17] <hayley> gilberth: Apparently they run in linear time and only have an NFA for submatches.
[03:25:21] *** Quits: lisp123 (~lisp123@5.30.23.247) (Ping timeout: 265 seconds)
[03:25:23] <hayley> https://github.com/rust-lang/regex/blob/master/PERFORMANCE.md
[03:25:23] -ixelp- regex/PERFORMANCE.md at master ¬∑ rust-lang/regex ¬∑ GitHub
[03:26:36] <gilberth> Yes, we are the only ones who use a DFA for submatch addressing, everbody else doesn't know how to do that.
[03:27:10] <gilberth> Doing it with a DFA for w/o submatch addressing is part of every text book.
[03:27:17] <hayley> Time to see how long this damn compiler takes.
[03:28:15] <hayley> 41.72s for a debug build. Though, granted, it also had to fetch the package list (which apparently is huge)
[03:29:31] <hayley> 311ms for 1e6 iterations ‚Üí 311ns/iteration?
[03:29:33] <gilberth> Oops, I believed I found a safe spot to hide, yet somebody came along and shot me. :(
[03:29:42] <hayley> What?
[03:30:01] <gilberth> I am playing S.T.A.L.K.E.R.
[03:30:08] <hayley> I see.
[03:30:21] <gilberth> 311ns for sun?
[03:30:32] <hayley> 311ns for the email with submatches.
[03:30:43] <hayley> Compare to 92ns with my compiler.
[03:30:50] * gilberth looks at hayley's benchmarks.
[03:31:46] <gilberth> hayley: See. This is why I didn't bet.
[03:32:02] * hayley reads on how to use a micro-benchmark harness.
[03:33:12] <hayley> How do I give it an object (the compiled regex) from outside the benchmark?
[03:33:22] <hayley> Er, never mind.
[03:35:03] <hayley> "help: provide a type for the constant: `ITERATIONS: i32`" lol
[03:35:18] <gilberth> email address with submatches is 821ns with libc.
[03:35:48] <hayley> Hell, even trying to build the benchmark library takes forever and pulls in too many dependencies.
[03:36:55] <hayley> match                   time:   [293.50 ns 294.31 ns 295.15 ns]
[03:38:20] <hayley> Also amusing that this benchmark tool has a "warm-up" phase. I thought there wasn't a JIT or GC so no warm-up?
[03:38:36] <gilberth> Cache?
[03:38:48] <hayley> Perhaps.
[03:39:31] <gilberth> It's tricky to benchmark these days with CPUs not running on a constant clock.
[03:40:00] * hayley is kidding
[03:40:14] <gilberth> You're kidding?
[03:40:36] <hayley> Yes.
[03:40:50] <gilberth> Me?
[03:41:17] <hayley> Maybe?
[03:41:25] <gilberth> Why?
[03:41:34] <hayley> gilberth: https://gist.github.com/no-defun-allowed/2c6e29db15539cbc2c2fd10b121244ff
[03:41:34] -ixelp- regex engines ¬∑ GitHub
[03:42:17] <hayley> New headline: "Lisp 3.5√ó as fast as Rust"
[03:42:50] *** Joins: CrashTestDummy (~CrashTest@ool-ad02813b.dyn.optonline.net)
[03:43:25] <gilberth> Find an example with backtracking and Lisp could be as faster as you wish than Rust.
[03:43:59] <hayley> Not sure how to do that, as it only backtracks with something to do with submatches.
[03:44:47] <hayley> "`find` can use the DFA matcher for this, but must run it forwards once to find the end of the match and then run it backwards to find the start of the match"
[03:44:50] <hayley> Wait what the fuck why
[03:45:13] <hayley> n.b. it also does the BMH prefix trick
[03:45:32] *** Quits: CrashTestDummy3 (~CrashTest@ool-ad02813b.dyn.optonline.net) (Ping timeout: 252 seconds)
[03:45:37] <gilberth> Yes, this forward/backward trick doesn't work.
[03:46:21] <gilberth> IIRC it was proposed as one solution to the submatch addressing problem, but can't do nesting.
[03:47:18] <hayley> Go figure.
[03:47:30] <gilberth> It works for things like (a*)(a), where you first match forward for a+ and then backwards to find where )( is.
[03:48:05] *** Joins: kakuhen (~kakuhen@user/kakuhen)
[03:48:16] <gilberth> IIRC it was proprosed as a solution for trailing context in lex(1).
[03:51:14] <hayley> I admit I still want to crack 1GB/s/core, but I probably need BMH or SIMD for that.
[03:51:16] <gilberth> I should have kept notes, as I read every available paper about that subject once.
[03:51:58] <hayley> But why would they run the DFA backwards to find the start? Don't they track it already?
[03:52:30] <gilberth> The start?
[03:52:35] <hayley> Yep.
[03:52:56] <gilberth> Of the whole match?
[03:53:10] <hayley> Precisely.
[03:53:34] <gilberth> Now you're kidding me, milady.
[03:57:36] <gilberth> Does Rust claim to implement POSIX semantics?
[04:07:20] <hayley> "Its syntax is similar to Perl-style regular expressions, but lacks a few features like look around and backreferences."
[04:07:26] <hayley> Crabs only know syntax and not semantics.
[04:09:05] <gilberth> hayley: "(aba|ab|a)(aba|ab|a)(aba|ab|a)" "abbabbaaababab"
[04:09:16] <hayley> Interesting!
[04:09:42] <gilberth> libc: 952ns, clex2: 61ns
[04:09:54] <hayley> What about the (a|aa)* even/odd thing?
[04:10:07] <gilberth> Good idea!
[04:10:59] <hayley> 39.67ns here.
[04:11:12] <gilberth> Rust sure is fast! Wow.
[04:11:20] <hayley> No, that's one-more-re-nightmare.
[04:11:35] <gilberth> I know, I was kidding you, hayley.
[04:12:22] <hayley> I wonder if the speedup is due to bounds checking removal. Or it could be machine variance, but we found them to be within ¬±10% IIRC
[04:13:11] <gilberth> 10% of what?
[04:13:21] <gilberth> Now, bench Rust for me.
[04:13:34] <hayley> Doing it now...
[04:14:05] <hayley> 438.36ns
[04:14:29] <gilberth> So Lisp is 11x faster!
[04:14:42] <gilberth> Almost 12.
[04:14:54] <hayley> (format nil "Lisp ~$√ó as fast as Rust" (/ 438.36 39.67)) ‚áí "Lisp 11.05√ó as fast as Rust"
[04:15:34] <gilberth> I did the calculation in my head.
[04:16:03] <hayley> BTW what should it match? I get start=6 end=11 1=(6, 7) 2=(7, 8) 3=(8, 11)
[04:16:32] <gilberth> That's correct.
[04:16:39] <hayley> YAY!
[04:18:08] <gilberth> It's time for a beer, do we have some?
[04:18:17] <hayley> Don't think so. I could make tea though.
[04:19:23] <gilberth> Thanks, I'll go downstairs and fetch some of my fake beer. And besides? Why the hell did we run out of beer at Lisp Caf√©?
[04:20:43] <dave0> there was an ordering snafu
[04:21:15] <gilberth> dave0: Where do you come from all of a sudden?
[04:21:53] <hayley> So, I just have to reconstruct the "high level interface" with ALL-MATCHES, ALL-MATCHES-AS-STRINGS, etc, then get a POSIX parser, and then world domination?
[04:22:33] <gilberth> You could still borrow my parser unless you suffer from NIH.
[04:23:32] <hayley> My only real constraint is that the parser shouldn't be that big, as the rest of the RE engine is (relatively) tiny. Hence the use of esrap to generate a parser.
[04:23:33] <gilberth> For full POSIX we still need lookahead and lookbehind.
[04:24:31] <gilberth> Fair enough. My parser does basic regular expressions, extended regular expressions, and flex.
[04:28:44] <hayley> Eh, I'd do ^ and $ and call it a day.
[04:29:12] <hayley> ^ is amenable to prefix searching: start at the first character, then after every newline.
[04:35:20] <gilberth> hayley: Tiny enough? <http://clim.rocks/gilbert/parse-re.lisp>
[04:36:24] <hayley> Bit hard to read but I think I can understand it. Maybe.
[04:36:39] * hayley doesn't handwrite parsers for anything other than Lisp. Sorry.
[04:37:20] <hayley> Huh, Python re also reports 420ns for the submatching stress test.
[04:38:15] <hayley> gilberth: I put all the results in https://gist.github.com/no-defun-allowed/2c6e29db15539cbc2c2fd10b121244ff
[04:38:16] -ixelp- regex engines ¬∑ GitHub
[04:40:35] <gilberth> lol
[04:40:55] <gilberth> I love it how you named my example!
[04:42:10] <hayley> wtf, SBCL needs 430ms to compile my state machine for your example.
[04:42:31] <hayley> Am I using SBCL or LLVM these days?
[04:43:06] <gilberth> Heh. There goes your performance advantage out the window!
[04:43:34] <hayley> :[
[04:45:06] * hayley uploaded an image: (86KiB) < https://libera.ems.host/_matrix/media/r0/download/matrix.org/mqVmtzaBvJQTFKRJLEQHMJVh/Screenshot_2021-09-24_11-21-29.png >
[04:45:18] <gilberth> Here it's 70ms for SBCL and 46ms total compilation time.
[04:45:33] <gilberth> * 46ms for CCL
[04:45:37] <hayley> From experience, usually SBCL tries too hard to do type inference.
[04:46:26] <hayley> Can't just say "trust me dude, everything's a fixnum". Or maybe you can.
[04:46:41] <gilberth> idk. My time includes DFA construction and all the scanner generator boiler plate.
[04:47:27] <gilberth> hayley: Well, my template type-declares about everything.
[04:47:56] <hayley> OTOH one does not simply put a time limit on Kildall's algorithm, because you only get correct constraints after running it to completion.
[04:48:30] <gilberth> What is Kildall's algorithm?
[04:48:31] <hayley> gilberth: So does mine (short of some temporaries) but IIRC SBCL likes to find subtypes of FIXNUM.
[04:48:46] <gilberth> ok
[04:48:49] <dave0> my friend is here... logging off for a while
[04:48:53] *** Quits: dave0 (~davezero@069.d.003.ncl.iprimus.net.au) (Quit: dave's not here)
[04:49:57] <hayley> Kildall's algorithm is a "pattern" for doing flow analysis. You have a work list, update things, log things that you changed, then update the things that depend on the logged changes...
[04:51:05] <gilberth> I see. Yes, you can't just stop anywhere.
[04:52:30] <hayley> https://calhoun.nps.edu/bitstream/handle/10945/42162/Kildall_A_unified_approach_1973.pdf
[04:53:02] <gilberth> You already told me what it is.
[04:53:33] <hayley> Yeah, well, it's a pattern because it can be expressed using any old semi-lattice.
[04:54:19] *** Joins: Qwnavery (~Qwnavery@user/qwnavery)
[04:56:52] <gilberth> Yes.
[04:57:22] <gilberth> I just was not aware that it has a name.
[04:58:14] <gilberth> Or I forgot.
[04:58:18] <hayley> It doesn't seem to be called that usually. The first search result for "Kildall's algorithm" is even Bike's blog post on using it for escape analysis in the Cleavir compiler.
[04:59:28] <hayley> So it's probably a SICL-ism. When I did register allocation, I had to figure out what the "pool" was in POOL-JOIN or something like that.
[05:02:59] <gilberth> Still, what is the point in finding subranges of fixnums?
[05:03:32] <hayley> Helps to tell if e.g. the result of addition will still be a FIXNUM, as CL does arithmetic correctly.
[05:03:56] <hayley> But I had this same problem with a Brainfuck compiler, except that compilation for one Mandelbrot program took about four times as long as running it.
[05:05:28] <gilberth> Compilation is a curious thing. In theory you could write an interpreter for some language and when passing the code to interpret as a constant a compiler might actually compile that code.
[05:06:31] <hayley> https://morepypy.blogspot.com/2011/04/tutorial-part-2-adding-jit.html Tracing Brainfuck JIT?
[05:06:32] -ixelp- PyPy Status Blog: Tutorial Part 2: Adding a JIT
[05:06:38] <gilberth> I am eager to have a very mild version of that working for argument list parsing, given an argument list with constant keywords e.g.
[05:07:36] <hayley> Maybe I will write a tracing JIT for the bytecode VM we use in class from next week in a similar manner.
[05:11:21] <gilberth> That sounds like a hack.
[05:12:12] <hayley> Hey, if it's supposed to resemble a real computer, I'm gonna get closer to running it on a real computer.
[05:19:29] <hayley> gilberth: At least it's not the Nimble type inferencer: https://plover.com/~mjd/misc/hbaker-archive/TInference.html#RTFToC21
[05:19:30] -ixelp- The Nimble Type Inferencer for Common Lisp-84
[05:19:51] <hayley> "The Nimble type inferencer required about 30 seconds to type the TAK function given above on a 4 Megabyte Macintosh Plus with a 16MHz 68020 accelerator running Coral Common Lisp."
[05:26:38] <hayley> I can halve the compile time by using (SPEED 0) rather than (SPEED 3) but it's still 250ms or so.
[05:28:33] <gilberth> That's a lot.
[05:29:02] <hayley> Back to the profiler!
[05:29:05] <gilberth> Still using your 16MHz machine?
[05:29:18] <hayley> No, the 3.6GHz one sadly.
[05:29:45] <gilberth> Upgrade! 16 is faster than 3.6.
[05:30:16] <gilberth> Or depress the turbo button. :)
[05:30:28] <hayley> So, this "Nimble CL" project was to make a CL system designed for deployment, with more static analysis and whole program optimization. Baker started it in 1989, and folded in 1997. Craig Chambers finished his PhD thesis in 1991.
[05:31:11] <gilberth> And today? It's gone?
[05:31:54] *** Joins: CrashTestDummy2 (~CrashTest@ool-ad02813b.dyn.optonline.net)
[05:32:01] <hayley> I don't think anything on it was published publicly. The only paper I found describing the project was only accessible from a paywall and I only got to read the first page.
[05:33:10] * hayley suspects she will have to track down the best compiler settings for each implementation.
[05:34:52] *** Quits: CrashTestDummy (~CrashTest@ool-ad02813b.dyn.optonline.net) (Ping timeout: 252 seconds)
[05:36:14] <hayley> Hell, even Clozure takes 140ms to compile.
[05:36:41] <gilberth> What the heck are you doing?
[05:37:19] <gilberth> Your DFA exploded or what?
[05:37:30] <hayley> The only change since the old compiler that I can think of is that the winning and losing states are basically inlined, which provides a nice speedup at runtime, but that's it?
[05:37:37] <hayley> Guess I should check still.
[05:39:11] <hayley> Apparently I have a lot of unused lexical variables, which is a bad sign. Not sure how that happens, as I only generate lexical variables when the code generator uses them.
[05:39:26] <gilberth> My dyslexia DFA has 20 states including the pseudo ones.
[05:40:39] <hayley> Is that in a grep[] machine or not?
[05:41:14] <hayley> Here I have 14 states with no grep[] and 35 with grep[].
[05:41:29] <gilberth> Let me double check.
[05:41:59] <gilberth> No. Mine is a scanning one.
[05:43:12] <hayley> Well, it seems I should implement better peephole optimization for adapting PSETQs to duplicate states.
[05:43:33] <gilberth> The greping one has 33 total states.
[05:44:24] <gilberth> hayley: Data flow analysis?
[05:44:35] <hayley> There's some 144 registers, which is a few too many.
[05:44:43] <gilberth> lol
[05:44:44] *** Joins: dave0 (~davezero@069.d.003.ncl.iprimus.net.au)
[05:44:48] <gilberth> sorry.
[05:44:52] <hayley> gilberth: I mean, the adaptation code basically generates A_1 <- POSITION; A_2 <- A_1
[05:45:00] <dave0> maw
[05:45:01] <hayley> Killing A_1 could be done locally.
[05:45:05] <hayley> Hey dave0
[05:45:11] <dave0> hi hayley 
[05:45:23] <dave0> time for lunch afk
[05:45:30] <gilberth> Hayley still believes in linear something.
[05:45:53] <hayley> Hey, it works for everything but loops.
[05:46:26] *** Quits: waleee (~waleee@2001:9b0:216:8200:d457:9189:7843:1dbd) (Ping timeout: 260 seconds)
[05:46:28] <gilberth> Which loop? There is non in dyslexia.
[05:46:54] <hayley> There are reused states, but you're right they're not loops.
[05:47:31] <hayley> Though I think I will start with optimizing a much simpler DFA.
[05:48:59] <gilberth> Have fun, I'll dive into S.T.A.L.K.E.R. again waiting to be shot yet another time.
[05:49:58] <hayley> BTW "linear tagged DFA" is a shit name because search engines think you're asking if DFAs run in linear time.
[05:50:21] <gilberth> Google is just getting worse each day.
[05:50:48] <hayley> What's the other one, affine types? We do "discard" variables but that is implicit.
[05:50:56] <gilberth> Try searching for "reverse regular expression" --- no useful hits anymore.
[05:51:23] <hayley> A linear tagged regex is linear because tags are explicitly duplicated, and a particular tag can only be assigned once in a linear tagged regex.
[05:52:37] <gilberth> Ah that's a term?
[05:52:57] <hayley> Linear tagged regular expressions are my invention.
[05:53:38] <gilberth> Good, I thought so.
[05:54:31] <gilberth> I fear, milady, I really ought to prepare to publish somehow. Dammit.
[05:54:52] <hayley> But it was only invented because I could be sure that the derivatives weren't broken if they were linear.
[05:57:07] <gilberth> Hmm?
[05:57:52] <hayley> The tag variables of a linear RE can't self-clobber by definition.
[05:58:04] * hayley now needs to start saying affine RE. Dammit.
[05:58:49] * hayley reads a paper called "Linear types can change the word!" about side effects. Cheeky bastard.
[05:59:13] * gilberth shrugs.
[05:59:49] <hayley> s/word/world/
[06:00:09] <gilberth> The world! Side effects?
[06:00:16] <hayley> Yes.
[06:00:22] <moon-child> ugh
[06:01:34] <gilberth> Terrific! I am on my way to –ü—Ä–∏ÃÅ–ø—è—Ç—å and macOS has nothing better to do than to inform me about a up^Wdowngrade. I HATE THAT.
[06:02:22] <gilberth> And no option "Shut up, bastard!"
[06:03:54] <hayley> moon-child: Do you think calling these things "affine linear regular expressions" is too much of an abuse of naming?
[06:04:04] <hayley> Er, "affine tagged regular expressions"
[06:04:13] * hayley now has muscle memory
[06:04:45] <gilberth> Heh.
[06:06:18] <gilberth> F*** you, Apple. "Autmatic updates^Wdowngrades turned on"? Are you kidding? I just turned that off.
[06:07:22] <gilberth> Apple, please just keep the machine as it is, thanks.
[06:13:10] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Quit: Aurora_v_kosmose)
[06:15:46] *** Quits: hendursaga (~weechat@user/hendursaga) (Quit: hendursaga)
[06:16:16] *** Joins: hendursaga (~weechat@user/hendursaga)
[07:05:02] <White_Flame> apple is for people who want to defer their experience to The Man
[07:05:28] <White_Flame> every power user on apple grits their teeth and bears the typical apple behavior
[07:07:27] <gilberth> What's a power user anyway?
[07:08:32] *** Quits: semz (~none@user/semz) (Ping timeout: 246 seconds)
[07:08:48] <contrapunctus> gilberth: "up^Wdowngrade" I like the German term "Verschlimmbesserung" for that. üòÅ
[07:09:07] <contrapunctus> Apparently in English it's a "disimprovement"
[07:09:26] <gilberth> contrapunctus: Indeed, a lovely word.
[07:12:24] <dave0> "unprovement"
[07:12:45] <dave0> not as good :-)
[07:13:03] <gilberth> I believe "disimprovement" is a better translation.
[07:15:30] <gilberth> I recently up^Wdowngraded my phone. What happened? (1) The battery doesn't last as long as before and (2) I need to unlock the phone to charge it. (3) Stock quotes on the start screen (or what ever that is called) are not up to date anymore. Sure a disimprovement. Rhymes with disappointment.
[07:18:08] * hayley does some peephole optimization and SBCL now takes 211ms.
[07:19:16] <hayley> Seriously, I only killed 20 of the 144 variables that way?
[07:21:37] *** Joins: semz (~none@user/semz)
[07:22:39] <Qwnavery> hayley: areyou hacking sbcl?
[07:22:48] *** Joins: lisp123 (~lisp123@5.30.23.247)
[07:22:53] <hayley> Qwnavery: A code generator that feeds out a pathetic Lisp form to SBCL.
[07:22:54] * gilberth mumbles sth about data flow.
[07:22:59] <hayley> gilberth: How's your damned data flow analysis work then?
[07:23:12] <gilberth> By the text book?
[07:23:58] <Qwnavery> what do you mean by "feeds out?" and what sort of code are you generating (for what purpose?)?
[07:24:10] <hayley> Fine, I will pull out the Dragon book just for you. But what am I supposed to do from a DFA?
[07:24:33] <gilberth> Milady, that's a program too, isn't it?
[07:24:35] <hayley> I generate a CL form which represents a function that runs a DFA, SBCL compiles it to machine code.
[07:24:42] <hayley> Yes.
[07:24:59] <gilberth> hayley: Or use the secret source.
[07:25:11] <hayley> No thanks, I don't need you suing me.
[07:25:38] <gilberth> lol
[07:25:59] <gilberth> hayley: You're just afraid because I am married to a lawyer.
[07:28:44] <Qwnavery> do you have any material that you're working off to model the DFA?
[07:28:47] <hayley> So what do I do, I find equivalence groups between variables and use those to eliminate both of them?
[07:28:57] <hayley> gilberth's paper draft which he refuses to publish.
[07:29:07] <Qwnavery> ;-;
[07:29:45] <moon-child> https://hastebin.com/ojapoluhic.cs  logicmoo model of behaviour
[07:29:45] -ixelp- hastebin
[07:31:26] *** Quits: lisp123 (~lisp123@5.30.23.247) (Ping timeout: 260 seconds)
[07:32:06] <gilberth> hayley: Yes, kind of. My comment says I do "copy propagation and constant folding".
[07:32:34] <hayley> "We also identify variables, which could share one storage location"
[07:33:01] <gilberth> Who says that?
[07:33:39] <hayley> You.
[07:34:16] <gilberth> Oh! Then I do that too, yes.
[07:34:45] <Qwnavery> welp this is completely beyond me, even with a wiki page up I have no idea where to start lol
[07:35:12] <gilberth> Read the book. :)
[07:37:10] <Qwnavery> "A Unified Approach to Global Program Optimization"?
[07:37:33] * hayley reads the generated code and spots some variables which are never used.
[07:38:17] <hayley> Hell, SBCL even warns about it.
[07:38:20] <hayley> https://www.youtube.com/watch?v=Fqy-fCf6Ymg
[07:38:21] -ixelp- Bela Lugosi's Dead (Official Version) - YouTube
[07:39:06] <Qwnavery> Oh. I see what you're trying to do now. Damn.
[07:39:14] <gilberth> hayley: I believe that followes from copy propagation.
[07:40:09] <hayley> Yes, but somehow these variables are still deemed live. So perhaps I do want a global liveness check.
[07:40:18] <gilberth> As two registers assigned the same P will be merged.
[07:42:04] <hayley> Right.
[07:42:14] <gilberth> Qwnavery: "Damn"? Hayley enjoys her night mare. I believe.
[07:42:28] <hayley> https://www.youtube.com/watch?v=ZVKirxPRxkY
[07:42:28] -ixelp- One More Red Nightmare - YouTube
[07:42:45] <gilberth> And when in doubt she may blame me.
[07:44:46] <Qwnavery> gilberth: I believe this may have a very elegant solution...
[07:45:03] <gilberth> What exactly?
[07:46:09] <Qwnavery> I don't know quite yet. I don't know enough about SBCL's backend to be able to formulate one.
[07:46:30] <Qwnavery> I don't think it a nightmare though. Getting it working would be very rewarding.
[07:46:45] * hayley tries to write a LIVE-VARIABLES pass and finds Kildall again.
[07:47:31] <gilberth> Qwnavery: It is already working. Hayley just needs her time to trust me.
[07:48:02] <Qwnavery> :D
[07:49:10] <hayley> So two registers are equivalent iff they are assigned P at the same time?
[07:49:55] <gilberth> Two registers are equivalent if they always hold the same value.
[07:50:41] <hayley> Same thing, except that there are also the assignments due to reusing states.
[07:50:58] <Qwnavery> so you have R1 and R2 if R2 = {1, 2, 3} and R1 = R1 = {3, 1, 2} R1 and R2 are still equal?
[07:51:01] <gilberth> Yes, this is why I formulated it that way.
[07:51:50] <gilberth> Qwnavery: I can't read your notation.
[07:53:11] <gilberth> Basically copy propagation for each basic block says that R1 is a copy of R2 and hence both have the same value. When R1 and R2 are having the same value in each basic block, one of them is redundant.
[07:54:26] <Qwnavery> so if the intersection of the 2 sets is zero then they have redunant values? 
[07:54:51] <gilberth> Which sets?
[07:54:53] <Qwnavery> each register or "pool" being a set of assigned values
[07:55:21] <gilberth> What I said was not entirely correct as this is not reflexive.
[07:56:06] <gilberth> Consider: R2 = R1 ... (use of R2) ... Then we could instead say (use of R1) and R2 becomes dead.
[07:57:54] <Qwnavery> ok, erm is there any difference here between a pool and a register? or is a pool like a block of registers
[07:58:49] <gilberth> R2=R1 turns dead, when we substitute R1 for R2 later. However for that to work we need to "prove" that R1 is not altered in between. This is what I meant earlier. The algorithm works by keeping this sets of equivalent registers.
[07:59:23] <hayley> How do you compute the sets of equivalent registers?
[08:00:43] <hayley> With no assignments, all registers are equivalent I suppose, and then assignments induce partitions.
[08:01:07] <gilberth> hayley: When I come across R2=R1, I know that R2 is eqv. to R1 further on. This is a forward data flow analysis. And when information contradicting the believe (R1 eqv R2) meets, that believe is killed.
[08:01:31] <hayley> Right, okay.
[08:01:51] <gilberth> You rinse and repeat until a fix point.
[08:03:10] <gilberth> hayley: How much does your uni charge again? ;)
[08:03:53] <hayley> I don't think this course covers compiler design.
[08:05:43] <gilberth> Qwnavery: To answer your question. This is a matter of interpretation really. In essence SSA uses this to identify what we call here "registers" with values and hence also what is called "pool" tonight.
[08:06:19] <gilberth> hayley: Hey, you are a professional compiler writer, forgot?
[08:06:30] <hayley> gilberth: Not really.
[08:06:49] <gilberth> You charge for it and hence it's professional. :-p
[08:07:59] <Qwnavery> :< I don't know enough mathematics for this...
[08:08:35] <gilberth> Mathematics?
[08:09:29] <Qwnavery> There's weird notation that I haven't seen before.
[08:09:46] <gilberth> On Wikipedia?
[08:10:26] <Qwnavery> https://web.archive.org/web/20170629213307/http://static.aminer.org/pdf/PDF/000/546/451/a_unified_approach_to_global_program_optimization.pdf
[08:11:02] <Qwnavery> This is what I've been reading through
[08:11:17] <gilberth> Ah that paper hayley showed us tonight?
[08:11:45] <Qwnavery> Huh, idk I found it at the bottom of the wikipedia article :P
[08:13:16] <gilberth> Heh, hayley showed it to me, because I didn't knew it went by the name Kildall.
[08:13:40] *** Quits: Oddity (Oddity@user/oddity) (Remote host closed the connection)
[08:15:44] <Qwnavery> on a bit of a segway, you wouldn't happen to know any way to do division quickly, like in your head
[08:15:55] <gilberth> Qwnavery: By weird you mean the big ‚à™ and ‚à©?
[08:16:25] <Qwnavery> no, I get union and intersection (XOR, XAND)
[08:16:45] <gilberth> I my head? I am bad at doing calculations in my head.
[08:17:31] <moon-child> Qwnavery: get rid of all but one or two digits from the lesser number (along with however many it is necessary to retain of the larger)
[08:17:38] <Qwnavery> so am I, but I got my senior finals in 4 weeks and I never memorized my division tables...
[08:17:47] <Qwnavery> (or my multiplication)
[08:17:56] <moon-child> oh, precise, not approximate?
[08:18:18] <Qwnavery> yeahh
[08:18:57] <moon-child> why do you need to do such a thing in your head?
[08:19:09] <Qwnavery> I can do calculus, sling code and install gentoo but I can't do basic mathematics in my head.
[08:19:17] <Qwnavery> Tech Free exam.
[08:20:13] <Qwnavery> It's high school maths.
[08:20:31] <Qwnavery> ._.
[08:21:00] <gilberth> Still, you are required to do calculations in your head?
[08:21:01] <moon-child> I have never properly learned how to do long division by hand
[08:21:08] <moon-child> or, rather, I have learned several times and then forgotten
[08:21:22] <gilberth> Long division is a hoax.
[08:21:54] <gilberth> I mean unlike multiplication and addition, you don't do long division digit by digit.
[08:22:15] <Qwnavery> gilberth: yup. and quickly. It's a 2hr exam, then there's another 2hr exam for Tech active. Those 2 exams account for 50% of my grade for the entire year ;-;
[08:22:40] <gilberth> Oops. What the heck.
[08:23:20] <Qwnavery> I have failed every exam and assignment, so my entire mark falls down to these exams. :D
[08:24:02] <gilberth> I mean: When doing long division of say 7654321 by 123 you need to divide 765 by 123 and not just 7 by 1. Hence a hoax.
[08:24:48] <gilberth> Qwnavery: How did you managed to do that?
[08:25:28] <Qwnavery> gilberth: dumb. and slow. and procrastination...
[08:26:44] <gilberth> Oh dear. The last part I am a pro at.
[08:28:03] <gilberth> Qwnavery: So you now hope for a miracle?
[08:29:12] <Qwnavery> no. I kick procrastinations testicles. Alas I'm not sure how to get better at basic arithmetic.
[08:29:35] <gilberth> Practice.
[08:30:33] <Qwnavery> I wrote a program a while ago (in python ew) that randomly generates mathematical equations but it's just... I dunno. Another form of procrastination. I don't know if I even have the code for it anymore.
[08:31:16] <gilberth> Are we talking about calculus or calculations?
[08:31:27] * hayley is still confused about how to turn inequalities into equivalence sets.
[08:31:29] <Qwnavery> Calculus is easy, the calculations are hard.
[08:31:45] <Qwnavery> Like, wth is 7/12?
[08:32:03] <hayley> 7/12 is the canonical form of 7/12.
[08:32:11] <Qwnavery> 0.5 + 1/12 but then what's 1/12 in decimal?
[08:32:28] <gilberth> What kind of calculations are we talking about. Do they really test you on kindergarden questions like "Do long division on 123456789 over 77?"
[08:32:42] <White_Flame> 1/12 doesn't have a finite representation in decimal
[08:32:48] <moon-child> Qwnavery: 1/12 is 1/3 of 1/4
[08:32:50] <Qwnavery> No, all that shit's expected knowledge.
[08:33:12] <moon-child> that is 1/3 of 0.25, or 1/3 of (0.24 + 0.01), or 0.08 +0.003333, or 0.0833333333
[08:34:00] <hayley> So, you make sets by transitive closure of A = B, then break then when you observe A ‚â† B. What's there to iterate?
[08:34:02] <Qwnavery> It's not just divion. Like say I need to factorize a quadraic or do literally anything in relation to surds. My brain will cave in. Or repeated muliplication (i.e. powers) even if it's shit I should know.
[08:34:10] <gilberth> Which is finite once you allow for an overbar. :-p
[08:34:40] <White_Flame> instructions unclear, now have a 3 with an umlaut
[08:34:54] <Qwnavery> like say I get a cubic that's x^4 and then I'm told to derive it and find it's instantanious rate of change at 7 seconds,
[08:35:07] <moon-child> gilberth: yea, but idk how to type those in irc
[08:35:11] <moon-child> oh, here we go
[08:35:16] <moon-child>     _
[08:35:17] <gilberth> White_Flame: Yep, just remove the gap. :)
[08:35:18] <moon-child> 0.083
[08:35:22] <moon-child> dammit gilberth!
[08:35:25] <gilberth> sorry
[08:35:34] <Qwnavery> so 4x^3 let x = 7, expands out to 4*7*7*7
[08:35:37] <hayley> No umlaut combining character?
[08:36:14] <Qwnavery> moon-child: get a space-cadet loool
[08:36:39] <White_Flame> ‚Üã80.0
[08:36:50] <Qwnavery> but then I have to somehow calculate in my brain really quickly what 4*7*7*7
[08:37:14] <hayley> 0.08ÃÖ3
[08:37:28] <gilberth> I see the bar at the "8".
[08:37:45] <moon-child> Qwnavery: 7*7 is 49.  That is 50 less 1.  7*50 is (7/2)*100, that is 350, then less a 7 it is 343.  Times 4 is 1200 + 160 + 12
[08:39:29] <Qwnavery> yeah, but in an exam (like all the previous ones I've done) I might have to spend like 3 minutes on just that
[08:39:59] <Qwnavery> because O
[08:40:01] <Qwnavery> I'm slow
[08:40:06] <hayley> So, we're saying A ‚â† B iff A is set and B isn't, and both A and B are live?
[08:40:44] * gilberth would do 4*7*7*7 = 28*(50-1) = 1400 - 28 = 1372.
[08:41:38] <gilberth> hayley: Yes, we do.
[08:41:52] <moon-child> gilberth: that works.  Having done it that way it is obviously easier, but my initial triage-assessment is that multiplication by 4 is easy, by 7 is hard; and by large numbers is hard; so I should avoid doing things that are doubly-hard
[08:42:32] <Qwnavery> It helps if you have multiplication tables down
[08:42:40] <Qwnavery> I never memorized them ;-;
[08:43:24] <gilberth> moon-child: Well, I just saw 49 as an opportunity for a low hanging fruit.
[08:44:04] <gilberth> Qwnavery: Well, that could be fixed fast. Just remember the damn "small" multiplication table.
[08:44:21] * moon-child wonders how much easier such arithmetic gets in base 6, or 12, that have handier factors
[08:44:52] <gilberth> That won't really matter much.
[08:47:19] <gilberth> Qwnavery: Seriously. Learn that damn table. For single digit multiplication I don't even have to think.
[08:47:48] <Qwnavery> How I would calculate that is 4*7*7*7 = (2*7 + 2*7) * (7 * 7) = (14 + 14) * (2*7 + 2*7 + 2*7 + 7) = 28 * (28 + 21) = 28 * 28 + 28 * 21 = 20*20 + 2*20*8 + 8*8 + 20*20 + 2*20*1 + 8*8 or something like that
[08:47:48] <hayley> When I was 10 the teacher showed how people are better at 7 √ó 8 than 8 √ó 7.
[08:47:53] <gilberth> moon-child: I mean 10 = 2 * 5 and 6 = 2 * 3, how is 3 more handy than 5?
[08:48:02] <Qwnavery> basically all I do is addition. ;-;
[08:48:45] <moon-child> gilberth: 3 is smaller, and so divides more numbers evenly than does 5
[08:49:17] <gilberth> hayley: When I was at primary school I only learned half the table. The other children needed quite some time to realize that multiplication is commutative.
[08:49:51] <gilberth> moon-child: 5 divides as many numbers evenly as 3 does. :-p
[08:49:55] <Qwnavery> hayley: how are people better at 7*8 than 8*7???
[08:49:59] <hayley> Hm, I remember that from primary school. We had to make rectangles from square tiles, and the teachers insisted the first number would be one axis, and the second another.
[08:50:06] <moon-child> gilberth: weeeeeell...
[08:50:15] <moon-child> :)
[08:50:32] <hayley> However, I picked the other association and had an argument about it.
[08:50:49] <gilberth> hayley: I could imagine.
[08:51:17] <Qwnavery> gilberth: ok. any suggestions how I memorize my mulitplication tables???
[08:51:31] <Qwnavery> just rote ?
[08:51:54] <gilberth> Qwnavery: I could only speculate. Perhaps because 8 + 8 + 8 + 8 + 8 + 8 + 8 is shorter than 7 + 7 + 7 + 7 + 7 + 7 + 7 + 7?
[08:52:45] <Qwnavery> maybe it's because adding even numbers is a bit easier as 2 even numbers will always sum to be even?
[08:53:06] <gilberth> Qwnavery: rote? Perhaps. Do I really need to teach you how to memorize things?
[08:53:47] <Qwnavery> ._. no. smol brain here. sorry for being needy lol ok I'm just gonna write the tables 100 times.
[08:54:21] <Alfr> Qwnavery, or learn squares only, then assuming a<=b: For a+b even, let c = (a+b)/2 and d = (b-a)/2, then: c^2-d^2 = (c+d) (c-d) = b * a. If a+b is odd, compute a*(b+1) and subtract a afterward.
[08:55:22] <moon-child> Alfr: hmm, cute!
[08:55:47] <Qwnavery> Alfr: holy crap, so use perfect squares and a little bit of additive magic to produce multiplication... that's holy crap.
[08:55:49] <gilberth> Alfr: I fear Qwnavery won't have the time for that during exam.
[08:56:33] <Qwnavery> gilberth: perhaps perhaps not. if I can do all of it in my head fast enough I might be able to.
[08:57:44] <Qwnavery> What if i end up with a+b/2 = something like 7/12?
[08:57:58] <Qwnavery> wait no
[08:57:59] <Qwnavery> nvm
[08:58:28] <Alfr> gilberth, hm ... maybe.
[08:58:29] <gilberth> So I would suggest to start with Alfr's method, if you like it. But exams in my experience are easy, given you had all the time, which you don't. This is why you may run out of time without memorizing the table.
[09:00:04] <Alfr> Qwnavery, you might be faster by using an additional sheet of paper to do scratch computations on.
[09:02:17] * gilberth used to use tons of scratch paper in any exam he ever took.
[09:03:16] <Alfr> gilberth, you just stuck your examiner down with it?
[09:03:22] <gilberth> And because we weren't allowed to bring our own for obvious reasons, this was at times annoying to whomever overlooked the exam.
[09:03:32] <Alfr> *struck
[09:03:42] <gilberth> Alfr: Why should I?
[09:04:28] <Alfr> gilberth, I don't know, a ton of paper is a lot. And you can kill someone by simply dropping a ton of paper on them.
[09:04:37] <Qwnavery> if (a<=b) { if ((a+b) mod 2 == 0) { c=(a+b)/2; d=(b-a)/2; c^2-d^2 == (c+d)(c-d) == b*a} else { a*(b+1)-1 }
[09:05:10] <gilberth> Alfr: If properly folded I guess less than a ton would do.
[09:05:44] <Alfr> Qwnavery, that a<=b isn't really a requirement, for a>b, simply use commutativity to exchange the factors.
[09:05:47] <Qwnavery> if you've seen mythbusters you'd know that you can only fold a piece of paper a certain number of times :)
[09:06:32] <gilberth> My paper is infinite thin, so I could. :-p
[09:07:28] <Qwnavery> ah a fellow definite integral from negative infinity to infinity = zero enjoyer
[09:07:57] <gilberth> For every epsilon less than zero ...
[09:08:27] <hayley> Invalid index for array, should be a non-negative number below 0
[09:08:51] <gilberth> Which still is a correct statement.
[09:08:57] <Qwnavery> what ???
[09:09:04] <hayley> (aref #() 1)
[09:09:16] <Qwnavery>  oh. ._.
[09:09:33] <hayley> ECL: In function AREF, the index into the object #() takes a value 1 out of the range (INTEGER 0 -1).
[09:10:54] * hayley thinks about how to write this damn equivalence finder.
[09:11:13] * gilberth goes lalalalaa
[09:11:42] <hayley> With affine tagged DFAs it would be the case that exactly one state assigns to a value? Else someone duplicates?
[09:12:07] <gilberth> You still don't trust me, do you?
[09:12:20] <hayley> I do, but I need to implement it.
[09:12:49] <gilberth> Then implement copy propagation, the rest follows.
[09:13:04] <hayley> THAT'S WHAT I'M TRYING TO DO
[09:13:22] <gilberth> THEN DON'T TRY. DO!
[09:13:31] <contrapunctus> üò∂
[09:14:07] <gilberth> hayley: Come on, it's not hard, even I managed to do that.
[09:14:27] <hayley> Yeah, well, I never hacked regex before, and I never hacked compilers before SICL.
[09:14:56] <gilberth> What makes you believe I ever hacked REs before?
[09:15:06] <hayley> Your paper says 2016 on it?
[09:15:20] <gilberth> And before that?
[09:15:36] <hayley> That's like five times as long as I've hacked them.
[09:16:02] <gilberth> Yes, but I was finished in '16.
[09:16:15] <gilberth> So, I meant the other before.
[09:19:57] *** Joins: CrashTestDummy3 (~CrashTest@ool-ad02813b.dyn.optonline.net)
[09:20:37] <hayley> So what, I make partitions when observing some variables being assigned simultaneously, then split them when variables can be observed to have different values due to other assignments?
[09:23:05] <gilberth> Might work.
[09:23:07] *** Quits: CrashTestDummy2 (~CrashTest@ool-ad02813b.dyn.optonline.net) (Ping timeout: 252 seconds)
[09:25:38] <gilberth> Though I don't carry partitions.
[09:26:25] <gilberth> I am just dumb here and do it again and again until I am trapped at a fix point.
[09:29:00] <hayley> What's "it" then? Trying to replace a variable with another?
[09:30:18] <gilberth> My GEN function just records single assignments and when done with one pass, I use that to substitute the copied register for the register used. Then I repeat. This way I catch the transitive closure. Dumb.
[09:31:12] <gilberth> One pass = the whole forward analysis.
[09:35:31] <gilberth> There might or might not be a reason why I do that in this stupid iterative fashion. I don't recall.
[09:38:08] *** Joins: CrashTestDummy (~CrashTest@ool-ad02813b.dyn.optonline.net)
[09:40:43] *** Quits: CrashTestDummy3 (~CrashTest@ool-ad02813b.dyn.optonline.net) (Ping timeout: 252 seconds)
[09:43:08] *** Joins: shka (~herr@109.231.62.239)
[10:21:01] <hayley> Apparently this pass works for a while, but then it gets stuck replacing (END 861) for (END 608) and I'm not exactly sure how it does that.
[10:28:56] *** Joins: kakuhen_ (~kakuhen@user/kakuhen)
[10:32:25] *** Quits: kakuhen (~kakuhen@user/kakuhen) (Ping timeout: 252 seconds)
[10:51:41] * hayley failed to write a working REMOVE-ALIASES
[10:52:03] <hayley> Currently it's even slower than letting SBCL handle aliasing.
[10:53:51] <gilberth> I was reading files from the CADR tapes meanwhile. Mails are funny at times.
[10:54:28] <gilberth> Anyhow, a price calculation mentions 750 logic ICs for the CPU.
[10:54:57] <hayley> It is clearly wrong, as it removes all but one replica of each variable. 
[10:55:22] <gilberth> All but one? Exactly one?
[10:55:33] <hayley> Yes.
[10:56:12] <gilberth> A pity that I don't have my pick and place machines anymore. Guess, hayley has to volunteer.
[10:59:55] <hayley> Down to 26 variables for gilberth's dyslexia RE scanner. But it's still broken somehow.
[11:02:03] <gilberth> Most ICs are memory. Like 150 chips for 12k control memory. And 528 chips for 256k main memory.
[11:02:24] *** Joins: dec0d3r (~dec0d3r@2001:8003:4810:9600:7275:1afb:1707:8eaa)
[11:04:13] <gilberth> So when somebody really wishes, I'd say it's feasible to build a CADR. A faster one even.
[11:04:56] <gilberth> hayley: You figure it out. I need rest, I am overdue.
[11:05:04] *** Quits: kakuhen_ (~kakuhen@user/kakuhen) (Quit: Leaving...)
[11:07:32] <hayley> Well, even this broken alias detection does not make SBCL compile any faster.
[11:07:47] <gilberth> Curious.
[11:09:48] <gilberth> IIRC my RISC design already has like ~300 ICs. Would be fun to figure out which machine would turn out faster. ;)
[11:10:25] <hayley> But consider the following: at 200ms/specialization and 4 specializations, we only add 800ms to the compile time. Compared to rustc...
[11:10:35] <gilberth> As I read that the CADR didn't really have special hardware for tagging.
[11:11:23] <gilberth> hayley: I am not following you.
[11:11:55] <hayley> If we compile at compile-time, it's still closer to instantaneous than trying to use the Rust regex engine.
[11:12:10] <gilberth> And why four? Do you really care for US-ASCII?
[11:12:31] <hayley> idk, I pulled that number out of nowhere.
[11:12:53] <hayley> I'd still prefer if, say, the compiler was an order of magnitude faster.
[11:12:59] <gilberth> I was assuming simple/non-simple x base-char/chararcter.
[11:13:41] <gilberth> Glorious idea of SBCL to have two string types. I wonder: What for?
[11:14:13] <hayley> Space savings. Fitting more characters into a SIMD register :P
[11:14:30] <gilberth> Octets, you mean octets.
[11:15:24] <gilberth> And since when does CMUCL^WSBCL care about space?
[11:16:00] <hayley> I recall debug information being compressed somehow.
[11:16:21] <gilberth> Nah, milady, for really fast greping you want to work on UTF-8 octet vectors anyway.
[11:17:02] <gilberth> hayley: Still, SBCL has the largest footprint.
[11:17:18] <pjb> Which thanks to the properties of utf-8 is perfectly feasible.
[11:18:08] <hayley> It seems to me that SBCL did some copy propagation and used fewer registers. But there are a lot of redundant stores, lol.
[11:18:14] <hayley> ; BE54:       4531E4           XOR R12D, R12D ; BE57:       4531E4           XOR R12D, R12D
[11:18:16] <hayley> Good job!
[11:18:40] <gilberth> Hilarious.
[11:18:42] <moon-child> lol
[11:19:16] <moon-child> and yeah utf8 is definitely ideal
[11:19:18] <hayley> 46 lines of mostly redundant XORs and MOV [...], 0
[11:19:40] <moon-child> and utf8 lets you use the simd mask store instruction!
[11:20:00] <hayley> I wonder if gilberth still wants SIMD UTF-8 codecs.
[11:20:12] <gilberth> hayley: See. I told you, SBCL does not care about space. ;-)
[11:20:44] <gilberth> hayley: Sure, decoding and encoding UTF-8 is too slow for my taste.
[11:21:18] <hayley> gilberth: Go make a fortune on that, because Rust programmers only worked out SIMD validation.
[11:21:25] <moon-child> utf8 is 99% ascii.  Quick branch, you can check 32 chars at a time on any recent hw
[11:21:55] <gilberth> Yep. And that approach already beats iconv.
[11:22:10] *** Quits: dec0d3r (~dec0d3r@2001:8003:4810:9600:7275:1afb:1707:8eaa) (Remote host closed the connection)
[11:22:15] <moon-child> iconv?  Does anyone take that seriously?
[11:22:34] <gilberth> This is what I tested against.
[11:22:50] <hayley> moon-child: You say that, then it turns out someone does.
[11:22:56] <hayley> Happens every time.
[11:23:43] <moon-child> hayley: I like to pretend otherwise.  It is much more pleasant than facing up to reality!
[11:24:10] <gilberth> I mean, it's depressing, I could scan at like 7\tau per char and just decoding the UTF-8 takes the same time.
[11:24:14] <hayley> "Who would ever fall for a joke like that?" - Alan Kay on MS-DOS and C++ before revealing the punchline
[11:24:53] <moon-child> gilberth: well, depending on what you are doing, you do not _have_ decode the utf8
[11:25:30] <gilberth> moon-child: For a general purpose tool, I do want to keep the character abstraction of CL.
[11:25:54] <moon-child> and, I bet you cannot parse or do semantic analysis that quickly!
[11:26:38] <gilberth> Don't underestimate the lexical analysis. Think C header files.
[11:26:41] <hayley> Yeah, but Doug Lea said 12 times faster than C means you haven't started optimizing, and I'm only 11 times faster than Rust.
[11:27:30] <gilberth> hayley: Add more dyslexia or some Kleene stars, and you get as faster as you wish.
[11:27:43] <hayley> Noted.
[11:27:49] <moon-child> c gets a feature to import arbitrary files as data at compile time.  But what I do not understand is--why can we not do anything we like at compile time??
[11:28:22] <gilberth> ?
[11:28:55] <moon-child> https://thephd.dev/preprocessor-embed-std-embed-the-last-spring
[11:28:56] -ixelp- Preprocessor Embed and Language Embed - The Last Sprint | The Pasture
[11:32:53] <gilberth> Bah.
[11:33:30] <gilberth> That's awful.
[11:35:19] *** Joins: hendursa1 (~weechat@user/hendursaga)
[11:38:51] *** Quits: hendursaga (~weechat@user/hendursaga) (Ping timeout: 276 seconds)
[11:39:04] <gilberth> I reads like: Ok, we can embed binary files, but oops, that are just octets?! Hack around it and have the compiler evaluate our parser at compile time.
[11:39:17] <gilberth> * It
[11:40:02] <gilberth> Another thing we just don't need with CL.
[11:41:55] <gilberth> And it's doomed to fail for text, anyway.
[11:49:33] <gilberth> Done: (defmacro embed (filename &key (element-type '(unsigned-byte 8))) (with-open-file (input filename :element-type element-type) (let ((res (make-array (file-length input) :element-type element-type))) (read-sequence res input) res)))
[11:49:45] <gilberth> Next, please.
[11:50:13] <moon-child> :D
[11:50:16] *** Joins: X-Scale` (~ARM@31.22.200.105)
[11:50:26] *** Quits: X-Scale (~ARM@50.77.166.178.rev.vodafone.pt) (Ping timeout: 265 seconds)
[11:51:34] *** X-Scale` is now known as X-Scale
[11:52:13] <gilberth> I was cheating. Does not need to work for text files because of FILE-LENGTH.
[11:52:20] <hayley> I wonder how well V's 1 bajillion LOC/second compilation speed is going. Could use a faster compiler.
[11:53:12] <gilberth> Seriously, CL could need a proper compiler.
[11:54:24] <Gnuxie> what's a proper compiler?
[11:54:40] <hayley> HotSpot C2
[11:54:49] <gilberth> A compiler that does not emit redundant MOVs.
[11:55:04] <hayley> Well, I saw C2 do a redundant MOV actually.
[11:55:16] <gilberth> I mean this is hilarious. This just shouldn't happen.
[11:55:49] <Gnuxie> ono
[11:56:08] <hayley> https://shipilev.net/jvm/anatomy-quarks/1-lock-coarsening-for-loops/ C-f LOL HotSpot
[11:56:08] -ixelp- JVM Anatomy Quark #1: Lock Coarsening and Loops
[11:56:12] <gilberth> The first XOR hayley showed is dead, how could that be emitted?
[11:56:23] <hayley> https://www.youtube.com/watch?v=Fqy-fCf6Ymg XOR's Dead
[11:56:23] -ixelp- Bela Lugosi's Dead (Official Version) - YouTube
[11:56:57] <moon-child> hayley: I heard once a story: somebody joined the hotspot team and noticed the compiler emitted a bunch of redundant movs.  He said, 'hey, why don't we fix this?'.  Response: we don't have to; ilp papers it over
[11:57:15] * hayley uploaded an image: (38KiB) < https://libera.ems.host/_matrix/media/r0/download/matrix.org/gQEXSZUqFdzhEqoOmaVNpgbs/cliffclick.jpg >
[11:58:11] <hayley> Well, I can't talk, gilberth and I use memory-level parallelism to paper over read barriers.
[11:59:13] *** Joins: treflip (~user@95.79.32.99)
[12:03:52] <hayley> But SBCL is simultaneously not producing as pleasing output and taking too long to do it.
[12:04:41] <hayley> One of the things the sea of nodes does objectively well is that it hash conses to make better use of cache.
[12:11:03] <hayley> Still, I wonder why SBCL takes its time. I don't see what there is to optimize really.
[12:14:36] <hayley> If you evaluate (setf sb-c::*compile-progress* t) then compile it prints out the passes.
[12:14:57] <hayley> There's a "Copy" pass, and I apparently hit a reoptimize limit.
[12:18:16] <hayley> With (setf sb-c::*reoptimize-limit* 1) I can get to 180ms from 250ms or so, with no loss of performance. Time to find more internal compiler settings.
[12:28:14] <hayley> How funny that I'm flipping all the switches to make SBCL generate slower code, but they don't change anything.
[12:43:44] * hayley starts to think that gilberth's funny regex shouldn't compile to 12.2kB of machine code anyway.
[12:47:57] <dave0> 12k isnt even a blip
[12:48:08] <dave0> hello world compiles to me than that!
[12:48:12] <dave0> more
[12:48:52] *** Joins: lisp123 (~lisp123@5.30.23.247)
[12:50:07] <hayley> (make-dfa-from-expression (make-search-machine (parse-regular-expression "¬´aba|ab|a¬ª¬´aba|ab|a¬ª¬´aba|ab|a¬ª"))) ‚áí #<HASH-TABLE :TEST EQL :COUNT 35 {100CECEC43}>
[12:50:12] <hayley> Only 35 states there.
[12:52:46] *** Joins: notzmv (~zmv@user/notzmv)
[12:55:58] *** Quits: lisp123 (~lisp123@5.30.23.247) (Ping timeout: 252 seconds)
[13:17:03] *** Quits: moon-child (~moon-chil@cardinal.elronnd.net) (Quit: !)
[13:21:37] *** Joins: moon-child (~moon-chil@cardinal.elronnd.net)
[13:45:54] *** Joins: random-nick (~random-ni@87.116.178.231)
[14:13:15] <hayley> mfiano: BTW here are some benchmarks to brag about to yourself: https://gist.github.com/no-defun-allowed/2c6e29db15539cbc2c2fd10b121244ff
[14:13:15] -ixelp- regex engines ¬∑ GitHub
[14:13:41] <hayley> gilberth: In #sbcl scymtym told me to use (simple-array character 1) rather than simple-string, as the latter includes (simple-array base-char 1) and thus costs a branch I didn't spot.
[14:17:43] *** Joins: selwyn (~selwyn@user/selwyn)
[14:17:59] *** Quits: selwyn (~selwyn@user/selwyn) (Remote host closed the connection)
[14:18:24] *** Joins: selwyn (~selwyn@user/selwyn)
[14:22:45] <hayley> gilberth: So now I achieve 3.7\tau per character or so.
[14:23:19] <mfiano> What does this regex engine bring over existing solutions?
[14:23:30] <mfiano> Please don't say performance :)
[14:26:03] <hayley> Well, only the compiler takes exponential time. The run time is entirely linear.
[14:27:16] <hayley> As well as practically kicking butt, it theoretically kicks butt too.
[14:28:35] <hayley> I'm also working on a protocol for extending the compiler, I guess?
[14:29:03] *** Quits: selwyn (~selwyn@user/selwyn) (Read error: Connection reset by peer)
[14:32:29] <hayley> .oO(Also, zero cost submatching?)
[14:34:15] <mfiano> https://www.youtube.com/watch?v=rHIkrotSwcc
[14:34:16] -ixelp- CppCon 2019: Chandler Carruth ‚ÄúThere Are No Zero-cost Abstractions‚Äù - YouTube
[14:35:54] <hayley> Submatching with gilberth's scheme is literally just a few more MOVs. It's not really zero-cost, but it's low cost enough to misuse the term surely.
[14:45:37] <Gnuxie> https://www.theguardian.com/food/2021/sep/24/boris-johnson-asked-for-emergency-food-deal-says-jair-bolsonaro-brazilian-president
[14:45:37] -ixelp- Boris Johnson asked for ‚Äòemergency‚Äô food deal, says Bolsonaro | Supply chain crisis | The Guardian
[14:45:39] <Gnuxie> bruhhh
[14:49:10] *** Joins: selwyn (~selwyn@user/selwyn)
[14:49:38] <shka> oh yeah, let's import food from across the ocean
[14:49:53] <shka> and Amazonian Jungle is very important
[14:49:56] *** Quits: selwyn (~selwyn@user/selwyn) (Remote host closed the connection)
[14:50:02] <shka> but beef is must be
[14:50:20] *** Joins: selwyn (~selwyn@user/selwyn)
[14:51:16] <hayley> https://www.youtube.com/watch?v=BiB356hH0L0
[14:51:17] -ixelp- David Bowie - Fashion (Live from Phoenix Festival 97) [Official Audio] - YouTube
[15:19:33] <shka> https://www.youtube.com/watch?v=dp-KXWfisHA
[15:19:34] -ixelp- Turbo Lover (Remastered) - YouTube
[15:20:04] <hayley> https://www.youtube.com/watch?v=ay4XCf-tdEU
[15:20:05] -ixelp- Levin Brothers "Elephant Talk" Live at the Bearsville Theater 2021 - YouTube
[15:24:12] <selwyn> lol https://nitter.net/pic/media%2FE_wsTDRVQAUpPCp.jpg%3Fname%3Dorig
[15:24:20] <selwyn> in which reddit understands how languages work
[15:26:44] *** Quits: treflip (~user@95.79.32.99) (Quit: ERC (IRC client for Emacs 27.2))
[15:34:15] <selwyn> turkeys from brazil wtf
[15:34:20] <selwyn> is that what you guys were talking about?
[15:44:42] <shka> selwyn: a lot of beef consumed in the UK is imported from brazil
[15:44:53] <shka> and was even before the world wars
[15:45:30] <selwyn> yes
[15:45:44] <selwyn> south america was said to be an informal colony
[15:45:49] <selwyn> but i don¬¥t know the details
[15:46:27] <shka> uk imported a lot of food for a long time
[15:46:28] <selwyn> bolsonaro claims the uk asked him for an emergency food deal to supply turkeys for christmas
[15:46:40] <shka> that's why that woonton pie was a thing even :P
[15:46:44] <selwyn> iirc it was basically self sufficient in 1945
[15:46:57] <shka> nah
[15:47:25] <shka> you know what was the ration of fresh eggs during the world war II?
[15:47:31] <selwyn> powdered egg lol
[15:47:37] <shka> fresh eggs 
[15:47:38] <selwyn> we have to learn about that shit in school lol
[15:47:40] <selwyn> oh
[15:47:45] <shka> powdered was imported from the us
[15:47:59] <shka> 1 fresh egg per person per week
[15:48:15] <selwyn> grim
[15:48:23] <shka> that's about the self sufficiency in the 1945
[15:48:29] <shka> onions were not rationed 
[15:48:38] <shka> because there was no onions on the market anyway 
[15:48:51] <pl> btw, rationing in UK ended in late 1950s if not later
[15:49:01] <shka> most was imported from France
[15:49:02] <pl> and not because it became self-sufficient
[15:49:16] <selwyn> 1953 for food iirc? petrol rationing went on for longer
[15:49:24] <shka> sounds about right
[15:49:50] <shka> UK could be self sufficient, but it was not in over 100 years 
[15:50:09] <pl> it would also require considerable change in economy to be self-sufficient
[15:50:40] <selwyn> idly wondering whether it is better to be a food importer or food exporter
[15:50:42] <pl> there's also a huge difference between "sufficient on calorie mate rations, and sufficient in good food"
[15:50:45] <shka> i think that XIX century period of peace made brits a little bit too comfy :P
[15:50:57] <selwyn> consider how unfun it is to be ukraine, which everyone wants for its productive land historically
[15:51:00] <pl> shka: what XIX century period of peace?
[15:51:11] <pl> that's when brits fought a fuckton of wars
[15:51:25] <shka> local, small scale wars
[15:51:29] <shka> lasting few months
[15:51:34] <pl> few years at a time
[15:52:01] <shka> excluding Napoleonic wars?
[15:52:05] <selwyn> well
[15:53:01] <selwyn> it¬¥s fairly uncontroversial to say that britain felt very secure at that time
[15:53:12] <selwyn> otoh there was a lot of anti russia hysteria (lol)
[15:54:08] <shka> pax britanica was a thing
[15:54:17] <shka> until it exploded into world war
[15:56:05] <pl> shka: excluding 
[15:56:17] <selwyn> have come to believe that if it wasn¬¥t for ww1, then britain would have fought a brutal colonial war in ireland that could have destroyed the empire
[15:56:50] <selwyn> as it is, the rising happened in 1916 while the army was busy and ireland got its independence
[15:56:58] <pl> It was end of 1800s that had a time of speeding up industrial warfare capability while not much in terms of using it 
[15:57:25] <pl> And start of 1900s,of course 
[15:57:35] <pl> Plus you have a lot of "problematic" movements 
[15:58:12] <pl> Like proles asserting they have rights in reality not just in theory 
[16:00:34] <selwyn> in british historiography, the 19th century is often defined as 1815-1914 precisely to exclude the napoleonic wars
[16:00:58] <selwyn> ¬¥the imperial century¬¥
[16:18:47] <pl> There's 's bunch of smaller conflicts that took over a year but which happened on the far end of the empire 
[16:19:04] <pl> Also, pretty sure opium wars were latter half 
[16:19:45] <pl> USA invasion of British Empire is conveniently cut off by that 1815 date but it's mostly Canadians that talk about it 
[16:23:14] *** Joins: kevingal (~quassel@2a02:8084:4140:f300:c447:6440:aae4:7a3e)
[16:25:47] <selwyn> pl: i mean, the tendency is to not count wars that are far away :)
[16:26:21] <selwyn> sort of like how us presence abroad is not that controversial or much of a concern in domestic american politics
[16:27:20] <selwyn> i would guess that the afghan wars were the most talked about in britain at the time, and the most traumatic
[16:31:29] <hayley> Rate my dumb Idea: we use tiered compilation for compiling REs at runtime. We start with the classic chain of closures, then if someone tries to run the chain on enough bytes, we concurrently build a single optimized function.
[16:32:45] <selwyn> kiered compilation
[16:34:53] <shka> hayley: just in time optimization, i like the sound of it
[16:35:14] *** Quits: Qwnavery (~Qwnavery@user/qwnavery) (Quit: WeeChat 3.3)
[16:37:14] <Gnuxie> selwyn: apparently there are people queuing up for petrol all over 
[16:37:22] <Gnuxie> they didn't do a very good job of stopping the panic 
[16:38:31] <selwyn> return to 1970s
[16:38:46] <selwyn> i heard they are going to let foreign lorry drivers in
[16:39:59] <hayley> shka: Precisely, but it's the execution of it that matters.
[16:40:41] <hayley> Best is still to know the RE at compile time, but we offer a function which takes any ole string...
[16:40:48] <shka> pl: no long last wars between major powers
[16:40:55] <shka> gotta go AFK
[16:41:21] <hayley> https://v8.dev/blog/regexp-tier-up Bastards beat me to it.
[16:41:33] <Gnuxie> lel
[16:41:33] <shka> hayley: i think that execution of that should work in such a way that it allows to skip the cold start
[16:42:30] <shka> and gathering statistics/optimization phase should have no bearing on the optimized function execution
[16:42:43] <pl> Opium wars were essentially breaking of a previously thought to be major power 
[16:43:27] <pl> Crimean War is probably most remembered as it was first "far" war with fast news 
[16:43:47] <selwyn> good point
[16:43:55] <hayley> shka: I don't think there's many statistics to get for regex execution. And I can't do much to avoid the cold start.
[16:43:57] <shka> so in other words at some point use the (optimize statistics-gathering-regex) to obtain optimized regex
[16:44:21] <shka> it can be latter on stored in the lisp image or something
[16:44:36] <shka> anyway, that's just my ideas, i really need to go now
[16:44:40] <shka> see you all later!
[16:44:47] <pl> IIRC that's how Victoria Cross happened? 
[16:44:52] <pl> shka: cya
[16:44:54] <hayley> Sure, see ya.
[16:46:22] <selwyn> bye
[16:53:08] <hayley> But it's not going from regex -> optimized regex. We go regex -> DFA which is nearly free, then DFA -> CL -> machine code which takes a while.
[17:04:28] * hayley uploaded an image: (100KiB) < https://libera.ems.host/_matrix/media/r0/download/matrix.org/lKGFzzPwjjEOHXTAmjkXzdVa/uniform-reference-semantics.jpeg >
[17:42:34] *** Joins: waleee (~waleee@2001:9b0:216:8200:d457:9189:7843:1dbd)
[17:50:29] <dave0> nite all
[17:51:23] *** Quits: dave0 (~davezero@069.d.003.ncl.iprimus.net.au) (Quit: dave's not here)
[18:01:49] *** Joins: raeda_ (~raeda@72.49.219.33)
[18:01:49] *** Quits: raeda (~raeda@72.49.219.33) (Read error: Connection reset by peer)
[18:11:50] *** Quits: hendursa1 (~weechat@user/hendursaga) (Quit: hendursa1)
[18:12:15] *** Joins: hendursaga (~weechat@user/hendursaga)
[18:34:35] *** Quits: JSharp (sid4580@id-4580.lymington.irccloud.com) (Ping timeout: 260 seconds)
[18:35:25] *** Joins: JSharp (sid4580@id-4580.lymington.irccloud.com)
[18:35:26] <easye> /me wonders what kind of CL could run in 320k <https://docs.platformio.org/en/latest//integration/ide/emacs.html>
[18:35:26] <easye>  
[18:35:27] -ixelp- Emacs ‚Äî PlatformIO latest documentation
[18:37:33] * easye wonders what sort of ANSI CL runs in 320Kb?  
[18:38:02] <pl> ANSI might be a stretch 
[18:38:14] <pl> But L ran in 1MB of which it had only half 
[18:38:40] <easye> A Lisp Machine had how much RAM?
[18:39:20] *** Quits: shka (~herr@109.231.62.239) (Ping timeout: 265 seconds)
[18:39:44] <easye> "more than 1 MB RAM" <https://en.wikipedia.org/wiki/Lisp_machine>
[18:40:00] *** Quits: kevingal (~quassel@2a02:8084:4140:f300:c447:6440:aae4:7a3e) (Remote host closed the connection)
[18:40:05] <pl> Some externally-hosted might work 
[18:40:42] <easye> pl: right.   I want something stripped down (Lua!) that executes more as a client.
[18:40:48] <pl> A lot of MACLISP work was done with I think 512kW 
[18:41:05] <easye> I think the watch boards only wake up every minute to poll for tasks.
[18:41:33] <easye> Or a guess I could make Rust binaries.
[18:42:00] <easye> I think I would rather have the discipline and bondage of Rust over Lua.
[18:42:04] <pl> easye: note that there's at least one SNES game programmed using CL, just using "DSL" that generated 6502 code 
[18:42:33] <pl> easye: at 320kB of ram, I'd suggest Ada rather than rust 
[18:42:39] <easye> pl:  yeah, I guess that's the way I should think of it: DSL for various graphs of s-exprs.
[18:42:53] * easye never touched Ada
[18:43:05] <easye> I think I read a manual around thirty years ago. 
[18:43:17] <easye> And perhaps I looked at the GCC Ada frontend.
[18:43:43] <pl> It even has CL-like SATISFIES in type system XD
[18:44:12] <easye> Hoo.  Prof. Jim Newton has a candidate for comparison.
[18:54:29] *** Joins: lisp123 (~lisp123@5.30.23.247)
[18:56:04] <selwyn> which snes game?
[18:59:05] *** Quits: lisp123 (~lisp123@5.30.23.247) (Ping timeout: 252 seconds)
[19:00:16] <gilberth> CLISP used to run on the Atari ST. The CADR was pretty much optimized for memory use. IMHO opinion more so than for speed.
[19:07:57] <gilberth> Which makes sense. The price.list file on the CADR tapes quote $9,000 for 256k (word) memory and $12,000 for the CPU of which $4,500 are again for the 12k control memory.
[19:08:24] <gilberth> So the major cost of the machine was RAM.
[19:17:44] <gilberth> This is what makes me believe that a CADR rebuild would be feasible, if one wishes.
[19:19:49] <gilberth> You could cheat on the I/O and use one of the modern single board computers as a front end processor for graphics, disk, keyboard, mouse, and network.
[19:20:26] <semz> isn't this more or less what the Symbolics Ivory did
[19:20:50] <gilberth> I believe so, yes.
[19:22:05] <gilberth> At least the FEP was used to load the microcode and world.
[19:24:19] <gilberth> Anyhow, Brad Parker mentions that the original netlists didn't work for him.
[19:25:11] <gilberth> And good luck on routing all those 750 ICs. Or you go wire-wrap like the original.
[19:50:36] <pl> Ivory was pretty straightforward, it was one of the earlier (I think 3600) that had microcoded disk io
[19:52:15] <gilberth> Hmm, so just like the CADR?
[19:52:24] *** Quits: chiselfuse (~chiselfus@user/chiselfuse) (Remote host closed the connection)
[19:52:49] <gilberth> Modulo word-width.
[19:52:54] *** Joins: chiselfuse (~chiselfus@user/chiselfuse)
[20:03:25] <pl> I don't remember if CADR had IO driver in microcode? 
[20:04:01] <pl> 3600 I think did, its microcode was multi-tasked with I think 15 priority levels 
[20:06:04] <pl> There was I think a block storage intrinsic that on 36xx compiled into single instruction that triggered microcode disk driver, and on Ivory put a request to the pluggable i/o system 
[20:08:42] <pl> The reason was that ivory machines had considerably more varied disks 
[20:09:41] *** Quits: selwyn (~selwyn@user/selwyn) (Read error: Connection reset by peer)
[20:10:21] *** Joins: retropikzel (~retropikz@2001:999:20e:11a2:d95a:1793:c4fd:e091)
[20:34:52] <pjb> gilberth:  of course it's possible to rebuild it. Using modern tools and materials, it can even be very cheap. It might be more expensive if you want to build it identically.
[20:40:10] <gilberth> pjb: Well, I would allow cheating for the RAM and I/O. That is: no programmable logic.
[20:41:18] <gilberth> And "very cheap" is a relative term. Affordable? Definitely.
[20:42:12] <gilberth> One issue though would be the ALU as the '181 isn't made any more and there is no good substitute.
[20:42:57] <gilberth> A 4-bit adder still is made, but it has no carry-lookahead support.
[21:30:33] <pjb> gilberth: I guess you already are running a simulator? https://github.com/sbp/cadr
[21:30:33] -ixelp- GitHub - sbp/cadr: CADR Lisp Machine Simulator
[21:42:11] <copec> I here they have the Internet on computers now https://youtu.be/YozC8yFrZKI
[21:42:12] -ixelp- Internet: On computers now - YouTube
[21:58:09] *** Joins: Aurora_v_kosmose (~LispyLigh@user/lispylights)
[22:12:16] <gilberth> pjb: You guess correct. ;)
[22:12:23] <lotuseater> copec: https://youtu.be/Hj7LwZqTflc?t=5
[22:12:24] -ixelp- Futurama Moments that Aged a Bit Too Well - YouTube
[22:16:37] <copec> hah
[22:54:02] *** Joins: selwyn (~selwyn@user/selwyn)
[23:16:38] <gilberth> The CADR simulator reports a cycle time of 35ns, I believe the original machine was at like 200ns. So much for that.
[23:20:16] <gilberth> The window system is really primitive. You print something to another partly obscured window and that PRINT would block until you "expose" that other window.
[23:24:54] *** Quits: retropikzel (~retropikz@2001:999:20e:11a2:d95a:1793:c4fd:e091) (Quit: Leaving)
[23:28:36] <pjb> gilberth: interesting.
[23:29:12] <gilberth> The former or the latter?
[23:30:11] <pjb> blocking until expose events.
[23:30:48] <pjb> You know, for some applications, we have to work extra to ensure this kind of features in GUIs‚Ä¶ 
[23:31:12] <pjb> Alerts and other important information mustn't go unnoticed‚Ä¶
[23:31:44] <gilberth> Well, from reading the documentation windows refer to a (2d) displaced array into the frame buffer or a bit save array.
[23:32:21] <gilberth> Still my PRINT blocked. Somehow.
[23:32:58] <pjb> Use a tiling window manager!
[23:33:34] <gilberth> This is completely different from today where we have some kind of display list. And it explains of some of really silly parts of CLIM. And why I quit McCLIM.
[23:34:04] <gilberth> pjb: "Split Screen" on the CADR does exactly that. Let me try, if that then still blocks.
[23:36:41] <gilberth> It doesn't. That is when the other window is fully "exposed" the PRINT happens right away.
[23:50:11] <gilberth> pjb: <http://free-clim.org/cadr-2.mov> See how first the output to LISTENER-1 is blocked and then after I raised^Wexposed that window, the output appears and the output of the return value to LISTENER-2 is blocked.
[23:59:58] <gilberth> Tiling: http://free-clim.org/cadr-tiling.mov
