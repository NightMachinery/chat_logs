[00:02:36] <Skarsnik> Hello lizmat, should I fill a bug repport for my INIT/BEGIN confusing behavior in a module or is that hidden in the spec somewhere? xD
[00:05:00] *** Joins: pony (sid524992@smol/hors)
[00:05:34] <lizmat> Skarsnik: yes, please
[00:05:51] <lizmat> although it should probably be a problem solving issue
[00:14:32] <Skarsnik> Language Issue?
[00:22:05] *** Quits: abraxxa-home (~alex@2a01:100:2000:2500::22) (Remote host closed the connection)
[00:47:11] <Skarsnik> lizmat, I realise also we maybe need more phaser for module? since all the compile time phaser in a module only happen at precomp.
[00:56:22] <gfldex> Skarsnik: Do you want to run code at EXPORT-time?
[00:58:13] <Skarsnik> Not really, I wanted to forbide a sub to be run at BEGIN time
[00:59:17] <Skarsnik> so I added a INIT phaser in it to set something so you know it's run time or not in the exported sub, but INIT get run on use/export for the module
[01:10:53] <SmokeMachine> Hi there! I'm trying to make it easier to share connection, schema, models, etc inside a project, to do that, I've made something that when using like this: https://github.com/FCO/Red/pull/524/files#diff-edcf07bb952f59d3f8d892e2a8c7a3d12a7f2650938cc3768d0eff64a8d27303R7 it would export all Red, your connection, your models and your schema. But to accomplish that, a use should write something like this: 
[01:10:53] <SmokeMachine> https://github.com/FCO/Red/pull/524/files#diff-d6f89617d084c512e03cca6a43ade8998b7f073ba0be011f82df7671733aedfeR3 I'd like to have that settled without the user having to write the EXPORT sub. Is that possible (without using macro?) is there some way of doing that?
[01:20:12] <SmokeMachine> using that, `red-config` does almost everything, but the user needs to place it inside a EXPORT... is there a way to make the EXPORT not necessary? I've tried exporting a fund called EXPORT, but it's exported before the function call... so I can't customise the export...(only if I put that inside a BEGIN, I suppose... but if I have to put that inside anything, the EXPORT would be better)
[01:22:06] *** Quits: linkable6 (lin-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[01:22:06] *** Quits: evalable6 (eva-able@2001:41d0:2:5eb5::) (Read error: Connection reset by peer)
[01:22:31] <SmokeMachine> or would it be better to leave the EXPORT there to make it visible it's being exported?
[01:24:13] *** Joins: evalable6 (eva-able@2001:41d0:2:5eb5::)
[01:24:37] *** Joins: linkable6 (lin-able@2001:41d0:2:5eb5::)
[01:37:19] *** Joins: dogbert17 (~dogbert@c83-253-59-194.bredband.tele2.se)
[01:39:03] *** Quits: dogbert11 (~dogbert@c83-253-59-194.bredband.tele2.se) (Ping timeout: 256 seconds)
[02:16:35] *** Quits: xkr47 (xkr47@91-150-17-137.customer.karistelefon.fi) (Ping timeout: 264 seconds)
[02:17:30] *** Joins: xkr47 (xkr47@91-150-17-137.customer.karistelefon.fi)
[02:21:40] <tonyo> bloatware
[02:29:15] *** Quits: Skarsnik (~Skarsnik@91-170-31-218.subs.proxad.net) (Ping timeout: 250 seconds)
[02:57:01] *** Quits: djerius (~quassel@pool-108-20-43-199.bstnma.fios.verizon.net) (Quit: No Ping reply in 180 seconds.)
[02:57:16] *** Joins: djerius (~quassel@pool-108-20-43-199.bstnma.fios.verizon.net)
[03:32:43] *** Quits: reportable6 (rep-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[04:10:07] *** Joins: frost (~frost@user/frost)
[04:33:38] *** Joins: reportable6 (rep-able@2001:41d0:2:5eb5::)
[04:49:16] *** Quits: MasterDuke (~MasterDuk@94.0.188.131) (Ping timeout: 245 seconds)
[05:49:16] *** Quits: linkable6 (lin-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[05:49:16] *** Quits: evalable6 (eva-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[05:50:40] *** Joins: evalable6 (eva-able@2001:41d0:2:5eb5::)
[06:08:45] *** Quits: xinming__ (~xinming@115.219.38.153) (Ping timeout: 244 seconds)
[06:09:50] *** Joins: xinming__ (~xinming@115.219.38.153)
[07:09:51] *** Quits: notable6 (not-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: bloatable6 (blo-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: nativecallable6 (nat-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: quotable6 (quo-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: squashable6 (squ-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: evalable6 (eva-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: tellable6 (tel-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: sourceable6 (sou-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: coverable6 (cov-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: statisfiable6 (sta-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: greppable6 (gre-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: unicodable6 (uni-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: benchable6 (ben-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: shareable6 (sha-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: bisectable6 (bis-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: reportable6 (rep-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: committable6 (com-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:09:51] *** Quits: releasable6 (rel-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[07:10:13] *** Joins: squashable6 (squ-able@2001:41d0:2:5eb5::)
[07:10:15] *** Joins: notable6 (not-able@2001:41d0:2:5eb5::)
[07:10:22] *** Joins: reportable6 (rep-able@2001:41d0:2:5eb5::)
[07:11:12] *** Joins: sourceable6 (sou-able@2001:41d0:2:5eb5::)
[07:11:21] *** Joins: shareable6 (sha-able@2001:41d0:2:5eb5::)
[07:11:22] *** Joins: committable6 (com-able@2001:41d0:2:5eb5::)
[07:11:26] *** Joins: bloatable6 (blo-able@2001:41d0:2:5eb5::)
[07:11:33] *** Joins: unicodable6 (uni-able@2001:41d0:2:5eb5::)
[07:11:50] *** Joins: nativecallable6 (nat-able@2001:41d0:2:5eb5::)
[07:12:00] *** Joins: greppable6 (gre-able@2001:41d0:2:5eb5::)
[07:12:38] *** Joins: releasable6 (rel-able@2001:41d0:2:5eb5::)
[07:12:48] *** Joins: evalable6 (eva-able@2001:41d0:2:5eb5::)
[07:50:42] *** Joins: linkable6 (lin-able@2001:41d0:2:5eb5::)
[08:11:32] *** Joins: benchable6 (ben-able@2001:41d0:2:5eb5::)
[08:12:49] *** Joins: coverable6 (cov-able@2001:41d0:2:5eb5::)
[08:12:58] *** Joins: statisfiable6 (sta-able@2001:41d0:2:5eb5::)
[08:14:24] *** Quits: frost (~frost@user/frost) (*.net *.split)
[08:14:24] *** Quits: dogbert17 (~dogbert@c83-253-59-194.bredband.tele2.se) (*.net *.split)
[08:14:24] *** Quits: Scotteh (~Scotteh@user/scotteh) (*.net *.split)
[08:14:24] *** Quits: chronon (~chronon@user/chronon) (*.net *.split)
[08:14:24] *** Quits: goblin (~jaa@yatima.uukgoblin.net) (*.net *.split)
[08:14:24] *** Quits: gfldex (~dex@ip5f5ab74b.dynamic.kabel-deutschland.de) (*.net *.split)
[08:14:24] *** Quits: a3r0 (~aero@61.100.184.210) (*.net *.split)
[08:15:12] *** Joins: frost (~frost@user/frost)
[08:15:12] *** Joins: dogbert17 (~dogbert@c83-253-59-194.bredband.tele2.se)
[08:15:12] *** Joins: Scotteh (~Scotteh@user/scotteh)
[08:15:12] *** Joins: chronon (~chronon@user/chronon)
[08:15:12] *** Joins: goblin (~jaa@yatima.uukgoblin.net)
[08:15:12] *** Joins: gfldex (~dex@ip5f5ab74b.dynamic.kabel-deutschland.de)
[08:15:12] *** Joins: a3r0 (~aero@61.100.184.210)
[09:04:12] *** Quits: tejr (~tejr@user/tejr) (Remote host closed the connection)
[09:04:44] *** Joins: tejr (~tejr@user/tejr)
[09:07:11] *** Quits: casaca (~casaca@user/casaca) (Ping timeout: 245 seconds)
[09:12:30] *** Joins: tellable6 (tel-able@2001:41d0:2:5eb5::)
[09:12:36] *** Joins: quotable6 (quo-able@2001:41d0:2:5eb5::)
[09:24:23] *** Quits: japhb (~geoff@mugs/japhb) (Ping timeout: 264 seconds)
[09:32:32] *** Quits: reportable6 (rep-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[09:35:15] *** Joins: reportable6 (rep-able@2001:41d0:2:5eb5::)
[09:47:54] *** Quits: tejr (~tejr@user/tejr) (Ping timeout: 276 seconds)
[09:49:18] *** Joins: tejr (~tejr@user/tejr)
[09:53:30] *** Quits: frost (~frost@user/frost) (Ping timeout: 244 seconds)
[09:54:07] *** Joins: Skarsnik (~Skarsnik@91-170-31-218.subs.proxad.net)
[10:19:51] *** Joins: japhb (~geoff@mugs/japhb)
[10:47:21] *** Quits: seednode (~seednode@user/seednode) (Quit: Nihil supernum.)
[10:48:11] *** Joins: seednode (~seednode@user/seednode)
[11:03:13] *** Joins: abraxxa (~ahartmai@smtp.hartmaier.priv.at)
[11:05:07] *** Quits: abraxxa (~ahartmai@smtp.hartmaier.priv.at) (Remote host closed the connection)
[11:06:27] *** Joins: abraxxa (~ahartmai@smtp.hartmaier.priv.at)
[11:10:59] *** Quits: abraxxa (~ahartmai@smtp.hartmaier.priv.at) (Ping timeout: 250 seconds)
[11:11:44] *** Joins: abraxxa (~ahartmai@tsa-tc-flod-1.t-systems.at)
[11:52:22] *** Quits: Sgeo (~Sgeo@user/sgeo) (Read error: Connection reset by peer)
[12:27:50] *** Joins: MasterDuke (~MasterDuk@94.0.188.131)
[12:40:14] *** Joins: dakkar (~dakkar@home.thenautilus.net)
[13:20:52] *** Joins: hkdtam (~hkdtam@13.66.204.139)
[13:26:18] *** Joins: casaca (~casaca@user/casaca)
[13:59:55] <Skarsnik> I crashed a vm trying to build rakudo in it, that's new x)
[14:11:06] <SmokeMachine> is there a way of exporting an EXPORT sub that will export something defined by the unit that imported that EXPORT (without using BEGIN)? 
[14:12:54] <lizmat> If you find a way, let me know  :-)
[14:15:51] <SmokeMachine> I was trying something like: `my %exports; sub red-config($schema) { %exports = $schema.models }; sub EXPORT(--> Map()) { '&EXPORT' => sub { %export.Map } }` and on the importing one: `use Red::Config; use Red; red-config schema Bla, Ble`but it's exporting before populating %exports (and that makes sense...)
[14:17:47] <SmokeMachine> I've also tried: `macro export-red-config(|c) { quasi { sub EXPORT(--> Map()) { red-config({{{ |c }}}) } } }` with no luck (but I'm sure I'm doing the quasi wrong...)
[14:26:18] *** Quits: linkable6 (lin-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[14:26:19] *** Quits: evalable6 (eva-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[14:28:09] *** Joins: linkable6 (lin-able@2001:41d0:2:5eb5::)
[14:48:32] *** Quits: eseyman (~manu@lfbn-idf1-1-491-163.w86-242.abo.wanadoo.fr) (Ping timeout: 240 seconds)
[15:11:35] *** Joins: bisectable6 (bis-able@2001:41d0:2:5eb5::)
[15:32:55] *** Quits: reportable6 (rep-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[15:46:09] *** Quits: Skarsnik (~Skarsnik@91-170-31-218.subs.proxad.net) (Ping timeout: 250 seconds)
[15:48:15] *** Joins: A26F64 (~A26F64@cpe-74-69-167-143.stny.res.rr.com)
[15:49:27] *** Quits: A26F64 (~A26F64@cpe-74-69-167-143.stny.res.rr.com) (Client Quit)
[16:43:01] *** Quits: thundergnat (~steve@2601:80:8600:2543:2129:3fcf:f399:9779) (Ping timeout: 245 seconds)
[16:57:30] *** Joins: ufobat (~martin@dynamic-077-009-058-203.77.9.pool.telefonica.de)
[16:57:55] *** Joins: Sgeo (~Sgeo@user/sgeo)
[17:05:11] *** Joins: A26F64 (~A26F64@cpe-74-69-167-143.stny.res.rr.com)
[17:18:13] *** Quits: Geth (~LizBot@ipv4-wenzperl.connected.by.freedominter.net) (Remote host closed the connection)
[17:18:20] *** Joins: Geth (~LizBot@ipv4-wenzperl.connected.by.freedominter.net)
[17:20:14] *** Joins: thundergnat (~steve@2601:80:8600:2543:2129:3fcf:f399:9779)
[17:43:40] *** Quits: jmcgnh (~jmcgnh@wikipedia/jmcgnh) (Remote host closed the connection)
[17:52:19] *** Joins: jmcgnh (~jmcgnh@wikipedia/jmcgnh)
[18:26:35] *** Quits: [Coke] (~coke@172.58.239.67) (Ping timeout: 256 seconds)
[18:28:32] *** Joins: evalable6 (eva-able@2001:41d0:2:5eb5::)
[18:29:39] *** Joins: [Coke] (~coke@172.58.239.67)
[18:33:42] *** Joins: reportable6 (rep-able@2001:41d0:2:5eb5::)
[18:46:05] *** Quits: squashable6 (squ-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[18:54:03] *** Joins: ramiroencinas (~ramiroenc@127.red-88-16-14.dynamicip.rima-tde.net)
[19:06:53] <ramiroencinas> Hi everyone. I have discovered an annoying size limitation in the .write method of IO::Socket::Async class. This .write method doesn't write more than 65535 chars. However, the same method .write from the IO::Socket::Async::SSL class doesn't have this size limitation allowing to write more than 65535 chars.
[19:08:36] <ramiroencinas> I have tested it in Debian and Alpine Linux with the same results.
[19:10:07] <ramiroencinas> I don't know if this size limitation is a bug or not.
[19:21:43] <MasterDuke> it looks like IO::Socket::Async::SSL uses openssl, but IO::Socket::Async uses libuv
[19:23:52] <MasterDuke> the docs don't mention a size limit for IO::Socket::Async.write, but it returns the number of bytes written, i guess you're supposed to handle retrying with the rest if not all is written
[19:27:14] <ramiroencinas> Thanks MasterDuke, I will try to handling the rest of the chars.
[19:30:18] <MasterDuke> fell free to create a rakudo issue if you think it should do something differently (maybe the docs should at least mention the limit for rakudo)
[19:35:07] *** Joins: Sgeo_ (~Sgeo@user/sgeo)
[19:35:11] *** Quits: Sgeo (~Sgeo@user/sgeo) (Read error: Connection reset by peer)
[19:46:12] <ramiroencinas> Thanks MasterDuke, I have created a Rakudo issue.
[19:46:51] *** Quits: ramiroencinas (~ramiroenc@127.red-88-16-14.dynamicip.rima-tde.net) (Quit: Connection closed)
[19:46:58] <MasterDuke> thanks
[19:55:30] *** Joins: whatnext (~whatnext@static.120.96.63.178.clients.your-server.de)
[19:56:29] *** Joins: [Coke]_ (~coke@172.58.239.67)
[19:58:08] *** Quits: [Coke] (~coke@172.58.239.67) (Ping timeout: 246 seconds)
[19:58:17] *** [Coke]_ is now known as [Coke]
[19:59:19] <whatnext> hello all :)  question about deleting using Red. I am basically doing the following `Table.^delete` and getting `DB::Pg::Error::FatalError.new(message => "syntax error at end of input" ...` I can't seem to get any visibility on what SQL statement it is trying to execute. Just thought I'd check if anyone had come across this before raising it as an
[19:59:19] <whatnext> issue? :)
[20:00:33] <SmokeMachine> whatnext: could tou try `my $*RED-DEBUG = True` before the delete to show us the SQL, please?
[20:01:22] <SmokeMachine> whatnext: Table.^delete? were you meaning `Table.^all.delete`?
[20:02:57] <whatnext> I was following this: https://fco.github.io/Red/tutorials/start.html which says `Person.^delete` at the bottom of the page?
[20:03:49] <SmokeMachine> whatnext: yes, sorry... I've forgotten about that
[20:04:40] <SmokeMachine> whatnext: but yes... I can reproduce it here...
[20:05:57] <whatnext> should I be using `Table.^all.delete` then?
[20:07:19] <SmokeMachine> whatnext: no, both should work...
[20:07:50] <SmokeMachine> whatnext: that's an issue... it seems it's starting a transaction and never finishing it... :(
[20:08:48] <SmokeMachine> whatnext: no, sorry, forget about it! it's not working inside a transaction... but it's failing for some reason...
[20:09:11] <whatnext> I guess this must have broken recently?
[20:09:23] <SmokeMachine> whatnext: I'll try to fix that after work. Would you mind to create a issue for that, please?
[20:09:32] <SmokeMachine> whatnext: yes, that's new...
[20:09:34] <whatnext> Ok will do
[20:12:33] <andinus> is there a way to locate memory leaks? say to narrow it down to a block?
[20:15:41] <MasterDuke> well, a --profile will give you a count of allocated objects and where they were allocated, which may help
[20:16:12] <whatnext> SmokeMachine: raised issue here https://github.com/FCO/Red/issues/525
[20:16:27] <SmokeMachine> whatnext: thanks!
[20:16:58] <MasterDuke> if you add --profile-kind=heap you'll get a heap snapshot, which can be more useful (but a bit trickier to understand)
[20:18:14] <MasterDuke> or you can try running under something like heaptrack (though you might want to use --full-cleanup with rakudo with that)
[20:19:55] <MasterDuke> that will give you moarvm-level information, but you might be able to figure out where that corresponds to in your script
[20:23:50] <andinus> i see, i'll try with heap kind 
[20:24:56] <MasterDuke> i think you'll need p6-app-moarvm-heapanalyzer to open them. maybe comma does to?
[20:32:06] <SmokeMachine> whatnext: It's going to be an easy fix... sorry for that...
[20:32:35] <andinus> instrumented profile says, inlining eliminated the need to create .. followed by a very large number
[20:32:54] <SmokeMachine> whatnext: are you liking Red and it's documentation? (there are a lot missing on the documentation...)
[20:33:00] <andinus> js says NaN
[20:33:55] <andinus> # MoarVM oops: MVM_str_hash_entry_size called with a stale hashtable pointer
[20:34:32] <andinus> ^ i get these crashes but i'm not able to reliably reproduce it, should i file an issue for it?
[20:35:33] <whatnext> SmokeMachine: it's certainly the best ORM for raku right now, and there's a lot I do like about it
[20:36:49] <MasterDuke> that usually means you're writing to a hash from multiple threads, which isn't allowed
[20:36:50] <whatnext> documentation is a bit sparse it's true, but to be expected given the early stage of development
[20:37:23] <MasterDuke> you're going to want to look in the allocations tab
[20:38:50] <andinus> my @p; for @lines -> $iter {  if @p.elems == $batch {await @p;  @p = [];}}
[20:39:16] * lizmat clickbaits https://rakudoweekly.blog/2021/11/08/2021-45-two-commas/
[20:39:18] <andinus> ... push @p, start {} ==> later in the loop it does this
[20:39:24] <whatnext> one thing that I would have preferred is a more DBIx::Class like interface, rather than the SQLAlchemy style. However, currently I use some fairly simple wrapper code which ends up making it pretty similar
[20:40:26] <andinus> is this usage problematic?
[20:40:28] <whatnext> I guess my main issue is namespacing - because with the SQLAlchemy style you end up `use`ing a module for every table
[20:40:34] <MasterDuke> what are you doing in the start block? or is it really empty?
[20:40:53] <perryprog> lizmat what's that picture?
[20:41:33] <lizmat> it's from a number of chocolates presented by the late Jeff Goff at the 2015 YAPC::NA in Salt Lake City
[20:41:33] <SmokeMachine> whatnext: I'm "fixing" that with this: https://github.com/FCO/Red/pull/524
[20:41:34] <andinus> MasterDuke: its this script https://github.com/andinus/fornax/blob/main/lib/Fornax/CLI.rakumod
[20:41:42] <perryprog> oh wow, nice
[20:41:56] <andinus> it's using Cairo togenerate lots of pngs 
[20:42:14] <andinus> i suspect it's Cairo that's causing the memory leak
[20:43:45] <SmokeMachine> whatnext: with that, you can create a module like this: https://github.com/FCO/Red/pull/524/files#diff-d6f89617d084c512e03cca6a43ade8998b7f073ba0be011f82df7671733aedfeR1-R9 and where you use it, you'll import all your models, the schema and the connection...
[20:47:48] <MasterDuke> andinus: any particular reason you're doing manual batching and starts instead of using `race for @lines.skip.race.kv <...>`?
[20:49:08] <MasterDuke> oh, you're also using run. i think that's known to (at least appear to) leak
[20:49:13] <andinus> yes, race didn't do anything last i checked, i'll check it again (2 mins)
[20:49:19] <andinus> run is used at the end
[20:49:27] <andinus> https://andinus.unfla.me/writings/2021/fornax-generating-4.8-million-frames.html
[20:49:36] <MasterDuke> ah, right
[20:49:46] <andinus>  here i summarize the leak, it has screenshots as the script progressed
[20:49:58] <andinus> you can see the memory usage rising as the iterations pass
[20:50:22] <whatnext> SmokeMachine: can I ask when you will merge into master? :)
[20:50:33] <andinus> also seems like --full-cleanup always terminates with `SIGSEGV (Address boundary error)` on OpenBSD
[20:51:46] <andinus> i dont have comma, i'll get the heapanalyzer
[20:52:41] <MasterDuke> turns out comma doesn't analyze heap snapshots yet, but they're in the process of adding that
[20:54:00] <MasterDuke> andinus: i can just try some of the .fornax files in the repo to test with?
[20:54:27] <SmokeMachine> whatnext: I'm still trying to find out a way to the user not need to manually write the EXPORT sub... (I don't think there Is a way... but when I'm sure I'll probably merge that)
[20:57:35] <andinus> MasterDuke: yes, 50 is around 1200 iterations, i run, raku -Ilib bin/fornax --skip-video resources/solutions/DFS-50.fornax
[20:58:07] <andinus> just tested with .race, doesn't help, with manual batching, i see noticeable speedup
[20:58:17] *** Quits: evalable6 (eva-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[20:58:17] *** Quits: linkable6 (lin-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[20:59:33] <SmokeMachine> whatnext: https://github.com/FCO/Red/commit/5dcfbc134b3b2b2a461e436da9ed295cf818df74
[21:01:09] *** Joins: linkable6 (lin-able@2001:41d0:2:5eb5::)
[21:02:42] <whatnext> @SmokeMachine: ok I see - that does look like a simple fix :)
[21:02:47] <whatnext> thanks
[21:03:31] <whatnext> I guess currently this is only an issue if no filter terms are specified - would that be correct?
[21:03:57] <SmokeMachine> whatnext: yes, exactly
[21:04:31] <SmokeMachine> whatnext: and that wasn't being tested... :(
[21:05:49] <SmokeMachine> whatnext: if you test it and that's working, would you mind to close the issue, please?
[21:07:02] *** Quits: dakkar (~dakkar@home.thenautilus.net) (Ping timeout: 240 seconds)
[21:07:55] <lizmat> ok
[21:08:49] <whatnext> SmokeMachine: ok will do :)
[21:09:20] <SmokeMachine> whatnext: thanks!
[21:11:05] <SmokeMachine> whatnext: and if you'd like to help even more, I'm needing all possible help to close all this issues (https://github.com/FCO/Red/projects/3) to finally launch our first stable version
[21:16:39] <MasterDuke> andinus: turns out race doesn't support multi-arg blocks. but if you switch to .pairs instead of .kv and then pull out the $idx and $iter manually it works. a little bit simpler overall
[21:17:07] <MasterDuke> still does have the occasional MVM_oops
[21:17:34] <andinus> ah i see, i'll try with .pairs
[21:18:14] <andinus> what could be causing the MVM_oops after this?, nothing is shared (written) between threads
[21:18:29] <MasterDuke> so locally i have `race for @lines.skip.pairs.race(:degree($batch)) {` and in the block `my ($idx, $iter) = .key, .value;`, and i removed @p entirely
[21:18:48] <andinus> i see, makes sense
[21:18:49] <whatnext> SmokeMachine: well I won't rule out pitching in on that completely - I am pretty stretched right now though, so it might be some time before I get to it
[21:19:57] <whatnext> possibly I could pick up some of the documentation if it's still not done when I have more free time
[21:19:59] *** Quits: abraxxa (~ahartmai@tsa-tc-flod-1.t-systems.at) (Remote host closed the connection)
[21:20:59] <SmokeMachine> whatnext: that would help a lot! Thanks!
[21:21:09] <MasterDuke> the thread that oopsed was in Cairo's write_png
[21:22:08] <MasterDuke> timo: ping
[21:24:10] <andinus> is .race helping when you run locally? here it's not making much of a difference, manual batching seems to do better
[21:24:52] <MasterDuke> it was roughly the same, maybe 1s faster with .race
[21:24:52] <andinus> ah i see
[21:25:25] <andinus> yes, and with manual batching the speed is ~4x, as expected 
[21:25:55] <MasterDuke> e.g., 22s vs 21s with default batch size
[21:26:20] <andinus> ah wait, that might be it, default batch is 64 iirc
[21:27:24] <MasterDuke> i mean .race (with a batch of 4) was the same speed as manual batching (with a batch of 4)
[21:27:28] <timo> sorry, i'm just heading out the door
[21:29:35] <MasterDuke> --batch=8 is almost twice as fast
[21:29:44] <andinus> MasterDuke: ah, not able to reproduce that behaviour, (openbsd), manual batching (4) is faster than .race(:4batch)
[21:30:09] <MasterDuke> ah, maybe you're confusing race's batch and degree
[21:30:50] <andinus> ah got it, :degree($batch), i should rename the var
[21:32:51] <andinus> indeed, .race seems faster, testing on DFS-10 (30 iterations) seems like manual batching beats it, maybe it performs well with more iterations
[21:32:58] <andinus> If there's an I/O operation inside the loop, there might be some contention so please avoid it.
[21:33:08] <andinus> ^ .race docs do say this
[21:33:25] *** Quits: reportable6 (rep-able@2001:41d0:2:5eb5::) (Remote host closed the connection)
[21:34:08] <ugexe> a lot of it is going to depend on your cpu
[21:34:18] <MasterDuke> what it you do .race(:1batch, :degree($batch))
[21:35:13] <MasterDuke> and the specifics of the workload, yeah
[21:35:36] <gfldex> andinus: you could try to use a Channel to steam to-be-written data into a separat thread that does the IO.
[21:36:11] <MasterDuke> i'm not saying you should definitely switch to .race, it just looked like the manual batching was pretty straightforward and a good candidate for converting
[21:36:27] <andinus> :1batch doesnt change much. 
[21:36:56] <andinus> without .race/batch: 24s, with .race (:degree($batch)): 12s, with manual batching: 6s
[21:37:56] <andinus> :1batch :degree($batch) should reproduce manual batching behaviour though
[21:38:08] <andinus> gfldex: i see, i'll read about Channels
[21:39:40] <MasterDuke> on my system .race and manual is pretty much the same, even with :1batch or not and different values of :degrees
[21:39:48] <gfldex> You can `use Telemetry;` to get an idea how well the worker threads are satured. see: https://gfldex.wordpress.com/2017/11/05/racing-rakudo/
[21:40:25] <gfldex> How long does take one iteration on a single core?
[21:40:34] <lizmat> -Msnapper also nowadays  :-)
[21:41:19] <MasterDuke> i'm actually seeing the same scaling factor for both ways, but manual consistently being a couple seconds slower
[21:46:55] <andinus> i just tried it on a ubuntu machine, manual: 2.88, .race: 3.40, single core: 4.78
[21:47:24] <andinus> maybe the iterations are too low (40) for .race to benefit
[21:47:57] <andinus> MasterDuke: do you have .pairs.race or .race.pairs?
[21:48:14] <MasterDuke> pairs.race
[21:48:47] <andinus> ah, both same times
[21:48:59] <andinus> race.pairs would be better right?
[21:49:32] <andinus> gfldex: thanks for the hint!
[21:49:43] *** Parts: A26F64 (~A26F64@cpe-74-69-167-143.stny.res.rr.com) ()
[21:50:02] <andinus> i'll document these and update the writing later tomorrow
[21:50:17] <MasterDuke> i think it's really the same, adding the .race is only needed/used to set the degree and batch. it's the race prefix that's doing the work
[21:51:59] <andinus> ah, i was thinking .pairs is applied first so it would be single core and then .race multi threads the loop
[21:52:41] <MasterDuke> andinus: btw, you can return a value from a given block. so something like `my IterStatus $status = do given $iter.substr(0, 1) { when '|' { Completed } ... };` should work
[21:53:22] <andinus> ah,i do remember trying something similar
[21:53:28] <andinus> i was missing "do"
[22:01:09] *** Joins: evalable6 (eva-able@2001:41d0:2:5eb5::)
[22:13:26] <codesections> How do I tell zef that I want it to run tests that live somewhere other than ./t ?  zef test takes a path, but pointing it to a different directory (or test file) doesn't seem to do the trick
[22:14:54] <ugexe> there is no way
[22:15:17] <codesections> Oh, interesting.  Thanks
[22:17:51] <ugexe> a test has no way of communicating where the distribution its testing lives
[22:18:16] <ugexe> and with `zef test .` the `.` is referring to a distribution
[22:18:54] <ugexe> if it was like `zef test a/b/c/d` it would be impossible to tell if that is supposed to be a distribution or a test directory
[22:20:02] <codesections> yeah, that makes sense.  I wasn't thinking that it would let me put the tests _outside_ of the distribution.  But I was thinking I could specify a different directory so long as a META6.json was somewhere in its ancestors 
[22:20:35] *** Quits: ufobat (~martin@dynamic-077-009-058-203.77.9.pool.telefonica.de) (Ping timeout: 246 seconds)
[22:21:19] <ugexe> there is no "go up a level till you find a META6.json", which would mean you might not be able t have e.g. test files called META6.json
[22:21:22] *** Quits: bdju (~bard@user/bdju) (Ping timeout: 260 seconds)
[22:21:50] <ugexe> plus you can run tests against installed versions of a distribution instead of e.g. -Ilib
[22:22:44] <ugexe> there is probably a sane way to expand that `zef test $dist` to account for specific test files but it might be a bit verbose like having --test-file=... --test-file=...
[22:22:57] <codesections> Makes sense.  I'm not complaining â€“ I just figured I was missing something obvious :) 
[22:23:21] <codesections> Could it be a field in the META6?
[22:25:04] <ugexe> Probably, although it is also weird that it would be referencing paths that are nowhere else in the META6.json
[22:26:13] <codesections> wouldn't is just be a path starting from the same relative root as the paths in "provides" ?
[22:27:47] <ugexe> yeah but it doesnt refer to anything after the distribution is installed
[22:28:09] *** Joins: bdju (~bard@user/bdju)
[22:28:44] <ugexe> that doesn't discount the idea, but to me it is weird that an installed META6.json would refer to paths/files that are no longer part of it as a whole
[22:28:56] <ugexe> we could of course installed tests >:)
[22:30:01] <ugexe> which would let people do stuff like $dist = CURI.candidates("MyFoo"); CURI2.install($dist); run-tests-on-curi2($dist)
[22:30:25] <MasterDuke> ugexe: i just did a `zef search Cairo` and get
[22:30:29] <MasterDuke> 0 |Zef::Repository::LocalCache      |Cairo:ver<0.2.4>
[22:30:37] <MasterDuke> 8 |Zef::Repository::Ecosystems<p6c> |Cairo:ver<0.2.7> 
[22:30:59] <MasterDuke> but `zef upgrade Cairo` says `All requested distributions are already at their latest versions`
[22:31:34] <ugexe> what version is installed?
[22:32:07] <MasterDuke> 0.2.4 (which oddly enough was from a `zef install Cairo` just earlier today)
[22:33:36] <gfldex> codesections: I got a script that likes to test things. If you run it with `raku-test-all test ./some-dir/ it does what you need. https://raw.githubusercontent.com/gfldex/bin/master/raku-test-all
[22:33:43] <codesections> Hmm, I think that's getting deeper into zef's implementation than where I can have an informed opinion.  I'd kind of thought that zef *did* install the tests somewhere in the sources/ folder of hashed paths
[22:33:57] *** Joins: reportable6 (rep-able@2001:41d0:2:5eb5::)
[22:36:40] <MasterDuke> ugexe: oh, that was happening when i was using zef from its checkout. if i use the zef in <prefix>/share/perl6/site/bin/ an upgrade works fine
[22:37:24] <ugexe> if those are both the same version they should work the same
[22:38:01] <ugexe> might be worth doing --version to be sure the zef in checkout isnt loading some old zef from the e.g. home repo or some such
[22:38:11] <MasterDuke> ah, checked out one is 0.13.0, installed one is 0.13.1
[22:38:52] <MasterDuke> i need to remember to not use the checked out one
[22:39:07] <ugexe> fwiw you should update zef again
[22:39:17] <ugexe> to at least 0.13.3
[22:40:22] <MasterDuke> just upgraded to 0.13.4, thanks
[22:41:33] *** Joins: melezhik (~melezhik@c-73-32-143-85.hsd1.tx.comcast.net)
[22:42:15] <ugexe> codesections: it probably could similar to what we do for bin/ files, although really we need a spec for that too instead of just having zef grep for all files in that directory (some of which might not even be raku and thus shouldnt have the bin shim applied to them by rakudo)
[22:45:03] <ugexe> at a minimum users would need to list files in bin/ they want installed, and also a way to identify that e.g. bin/zef is a raku script and not like a bash script
[22:45:24] <ugexe> most of this seems like it could apply to tests as well
[22:46:04] <ugexe> although tests have the benefit of file extensions
[22:46:12] <codesections> those all sounds like good ideas (and things that it'd be good to get spec'd before the ecosystem gets too huge)
[22:46:18] *** Quits: whatnext (~whatnext@static.120.96.63.178.clients.your-server.de) (Quit: Connection closed)
[22:46:48] <ugexe> yeah the npm thing the other day got me thinking of how we can allow users to disallow installing bin scripts
[22:48:21] <codesections> Yeah.  Though if we do have a supply chain vulnerability issue at some point, Raku's flexibility will really work against us.
[22:50:17] <ugexe> hopefully having a thoroughly strict ecosystem and disabling non strict ecosystems can mitigate a lot of issues 
[22:51:37] <ugexe> in the future non-strict ecosystems would ideally be limited to darkpans
[22:52:00] <codesections> yeah.  And having a culture that doesn't encourage piles of untrusted transitive dependencies
[22:52:49] <codesections> I was just looking at a report the other day that tracked average dependencies per project by language, and there's a huge swing
[22:53:22] <codesections> from Swift (4) and Go (13) to JS (377)
[22:53:48] <codesections> https://i.blackhat.com/USA-20/Wednesday/us-20-Edwards-The-Devils-In-The-Dependency-Data-Driven-Software-Composition-Analysis.pdf
[23:03:36] *** Joins: eseyman (~manu@lfbn-idf1-1-491-163.w86-242.abo.wanadoo.fr)
[23:04:38] <ugexe> yeah thats just dumb. a developer should have a good understanding of all of their dependencies and that isn't happening with 377 
[23:05:33] <ugexe> offloading that understanding to the first transitive dependency's author is a bad solution
[23:05:48] <ugexe> but hey then you dont have to care
[23:08:32] *** Quits: melezhik (~melezhik@c-73-32-143-85.hsd1.tx.comcast.net) (Quit: Ping timeout (120 seconds))
[23:13:17] *** Quits: Geth (~LizBot@ipv4-wenzperl.connected.by.freedominter.net) (Ping timeout: 264 seconds)
[23:13:19] *** Quits: TempIRCLogger (~RakuIRC@ipv4-wenzperl.connected.by.freedominter.net) (Ping timeout: 256 seconds)
[23:15:17] *** Quits: RakuIRCLogger (~RakuIRC@ipv4-wenzperl.connected.by.freedominter.net) (Ping timeout: 268 seconds)
[23:15:54] *** Quits: lizmat (~textual@ipv4-wenzperl.connected.by.freedominter.net) (Ping timeout: 268 seconds)
[23:17:43] *** Joins: lizmat (~textual@ipv4-wenzperl.connected.by.freedominter.net)
[23:18:03] *** Joins: RakuIRCLogger (~RakuIRC@ipv4-wenzperl.connected.by.freedominter.net)
[23:18:07] *** Joins: Geth (~LizBot@ipv4-wenzperl.connected.by.freedominter.net)
[23:21:40] *** Joins: Kaipi (~Kaiepi@156.34.44.192)
[23:23:12] *** Joins: childlikempress (~moon-chil@cardinal.elronnd.net)
[23:23:13] *** Joins: bingos_ (bitbucket@cabbage.bingosnet.org)
[23:23:39] *** Joins: gordonfish- (~gordonfis@user/gordonfish)
[23:24:40] *** Joins: simcop2387_ (~simcop238@perlbot/patrician/simcop2387)
[23:25:11] *** Joins: broquain1 (~dbrook@static.140.30.69.159.clients.your-server.de)
[23:25:13] *** Joins: moritz_ (~moritz@tina.perlgeek.de)
[23:25:51] *** Joins: bdju_ (~bard@user/bdju)
[23:26:02] *** Joins: avarab (avar@vm.nix.is)
[23:26:34] *** Quits: BinGOs (~bitbucket@user/bingos) (Killed (NickServ (GHOST command used by bingos_!bitbucket@cabbage.bingosnet.org)))
[23:26:49] *** bingos_ is now known as BinGOs
[23:27:00] *** Quits: BinGOs (bitbucket@cabbage.bingosnet.org) (Changing host)
[23:27:00] *** Joins: BinGOs (bitbucket@user/bingos)
[23:28:16] *** Quits: gordonfish (~gordonfis@user/gordonfish) (Killed (NickServ (GHOST command used by gordonfish-)))
[23:28:18] *** gordonfish- is now known as gordonfish
[23:30:37] <MasterDuke> andinus: btw, i have a patch for rakudo that fixes the MVM_oops you're seeing with fornax
[23:30:46] *** Quits: bdju (~bard@user/bdju) (*.net *.split)
[23:30:46] *** Quits: moritz (~moritz@tina.perlgeek.de) (*.net *.split)
[23:30:46] *** Quits: broquaint (~dbrook@static.140.30.69.159.clients.your-server.de) (*.net *.split)
[23:30:46] *** Quits: Kaiepi (~Kaiepi@156.34.44.192) (*.net *.split)
[23:30:46] *** Quits: discord-raku-bot (~RakuIRC@ip5f5ab74b.dynamic.kabel-deutschland.de) (*.net *.split)
[23:30:47] *** Quits: avar (~avar@vm.nix.is) (*.net *.split)
[23:30:47] *** Quits: moon-child (~moon-chil@cardinal.elronnd.net) (*.net *.split)
[23:30:47] *** Quits: simcop2387 (~simcop238@perlbot/patrician/simcop2387) (*.net *.split)
[23:30:47] *** simcop2387_ is now known as simcop2387
[23:32:56] *** Joins: Skarsnik (~Skarsnik@91-170-31-218.subs.proxad.net)
[23:46:23] *** Joins: melezhik (~melezhik@c-73-32-143-85.hsd1.tx.comcast.net)
[23:53:44] *** Quits: melezhik (~melezhik@c-73-32-143-85.hsd1.tx.comcast.net) (Quit: Ping timeout (120 seconds))
[23:56:33] *** childlikempress is now known as moon-child
