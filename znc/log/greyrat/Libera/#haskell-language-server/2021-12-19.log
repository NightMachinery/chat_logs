[00:29:48] *** Joins: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb)
[01:00:45] <anton-latukha[m]> * a picture as a whole
[01:00:56] <anton-latukha[m]> s/Yes - it is important detail to know. Until the review is posted as finished - all messages in it are not posted. And if review not finished - content gets aborted. That thankfully allows to refactor a review & responses and think several times at a picture a whole before posting./Yes - it is important detail to know. Until the review is posted as finished - all messages in it are not posted. And if review not
[01:00:56] <anton-latukha[m]> finished - content gets aborted. That thankfully allows to refactor a review & responses and think several times & reflect on a picture as a whole before posting./
[02:04:12] *** Quits: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb) (Quit: coot)
[02:18:25] *** Quits: Morrow (~quassel@bzq-110-168-31-106.red.bezeqint.net) (Quit: https://quassel-irc.org - Chat comfortably. Anywhere.)
[02:43:04] <jneira[m]> anton-latukha: what you think about using the same workflow for testing and caching in master?
[02:43:34] <jneira[m]> disabling test steps if the workflow is in master?
[02:44:14] <jneira[m]> it needs separate the build job from the test job
[02:45:38] <jneira[m]> in fact we could use the same build job (or very similar) for caching, build, test and bench
[02:45:51] <jneira[m]> * caching, build release, test
[04:03:55] <anton-latukha[m]> `caching` builds all targets. It means: `cabal v2-build all --enable-tests --enable-benchmarks`, using `testing` for it - would requre `testing` & all PRs to do that, or to use a separate build command when it gets ran in `master`. Also `caching` is workflow is required to run on every merge. So then `testing` would run on on every merge & tests get long time to pass, occupying the runners & the cache needs to be
[04:03:55] <anton-latukha[m]> supplied asap, because after the moment of merge & before cache is saved in `master` - there is no relevant `master` cache, so test suites would prolong periods without "the current cache". So to have efficient & fast CI loop - test suites need to be disabled when built on `master`. & at that stage ...
[04:05:27] <anton-latukha[m]> Also `caching` runs fault tolerant on `master` *idk* how to configura that beaviour, so workflow checks builds in PR, but fault-tolerant in `master`.
[04:05:49] <anton-latukha[m]> s/*idk*/_idk_/, s/configura/configure/, s/beaviour/behavior/
[04:11:54] <anton-latukha[m]> (because it is 02:41 at location - I made a willful decision to sleep 8) )
[05:09:34] *** Joins: Morrow (~quassel@bzq-110-168-31-106.red.bezeqint.net)
[08:58:29] *** Quits: adamCS (~adamCS@ec2-34-207-160-255.compute-1.amazonaws.com) (Ping timeout: 268 seconds)
[09:03:43] *** Joins: adamCS (~adamCS@ec2-34-207-160-255.compute-1.amazonaws.com)
[10:03:09] *** Quits: juhp (~juhp@128.106.188.82) (Quit: juhp)
[10:03:47] *** Joins: juhp (~juhp@128.106.188.82)
[11:56:00] *** Joins: coot (~coot@89-64-85-93.dynamic.chello.pl)
[13:11:14] <jneira[m]> we can use conditionals on the event/branch triggering the workflow in the required steps, including the concrete cabal command and in the continue-on.fail field 
[13:11:25] <jneira[m]> s/./-/
[13:46:36] *** Joins: libertyprime (~libertypr@203.211.76.58)
[14:58:29] *** Quits: hololeap (~hololeap@user/hololeap) (Remote host closed the connection)
[14:59:55] *** Joins: hololeap (~hololeap@user/hololeap)
[15:25:14] <anton-latukha[m]> Also I've got an idea. If the cache key is hit directly - the workflow can skip the build at all. So scheduled runs indeed become the runs that check if cache is present, and run the build only if is not present. Which makes workflow a couple of seconds in the normal case, which allows to run the workflow multiple times a day just to make sure main cache is kept in the pool at all times.
[15:32:35] <anton-latukha[m]> I also believe DHall would be a solution to deduplicate yaml config in the workflows. A lot in project setup/configuration can be deduplicated with DHall usage, but it would require people to understand DHall syntax & running regeneration before PRs, & even CI itself can check for it. Because DHall results are deterministic - the CI indeed can check that updated configuration is applied.
[15:39:47] *** Quits: coot (~coot@89-64-85-93.dynamic.chello.pl) (Remote host closed the connection)
[15:44:52] <jneira[m]> I would try to not introduce a preprocessor step and templates if possible
[15:45:37] <jneira[m]> exploring GH ways of sharing workflows once we extract the common denominator
[15:46:40] <jneira[m]> jneira[m]: although dhall-to-yaml could be a good option in this case
[15:50:54] *** Joins: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb)
[15:53:00] *** Quits: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb) (Remote host closed the connection)
[15:53:37] *** Quits: libertyprime (~libertypr@203.211.76.58) (Ping timeout: 240 seconds)
[16:06:48] *** Joins: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb)
[17:44:15] *** Quits: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb) (Remote host closed the connection)
[17:52:58] *** Joins: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb)
[18:23:36] *** Quits: coot (~coot@2a02:a310:e03f:8500:933a:39ca:ef4e:37cb) (Remote host closed the connection)
[20:19:05] <michaelpj[m]> Is there some other tool that we can lean on for this caching step? It seems like it's rapidly becoming extremely complicated. Maybe someone has written another GitHub action we can steal?
[20:27:01] *** Joins: coot (~coot@2a02:a310:e03f:8500:7248:52ac:817b:7fb4)
[20:28:56] *** Quits: x88x88x (~x88x88x@2001:19f0:5:39a8:5400:3ff:feb6:73cb) (Read error: Connection reset by peer)
[21:18:04] *** Quits: coot (~coot@2a02:a310:e03f:8500:7248:52ac:817b:7fb4) (Quit: coot)
[22:42:47] *** Joins: coot (~coot@2a02:a310:e03f:8500:7248:52ac:817b:7fb4)
[23:22:31] <jneira[m]> not sure if an action could help us as we have issues with the cache size limit and how to work gh cache scopes, and we have to make changes across several workflows to try hard avoid cache eviction
[23:23:43] <jneira[m]> best bet would be make bigger the cache (a good xmast present) or reduce the dependency set or the size of compiled outputs
[23:26:45] <jneira[m]> s/to//
[23:33:23] *** Quits: coot (~coot@2a02:a310:e03f:8500:7248:52ac:817b:7fb4) (Quit: coot)
[23:34:57] *** Quits: juhp (~juhp@128.106.188.82) (Ping timeout: 240 seconds)
[23:38:12] *** Joins: juhp (~juhp@128.106.188.82)
