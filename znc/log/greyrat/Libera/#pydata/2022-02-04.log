[00:11:07] *** Joins: trace987 (~trace@ip5f5ad14d.dynamic.kabel-deutschland.de)
[00:44:09] *** Quits: Pickchea (~private@user/pickchea) (Quit: Leaving)
[00:53:37] *** Quits: trace987 (~trace@ip5f5ad14d.dynamic.kabel-deutschland.de) (Remote host closed the connection)
[02:43:53] *** Quits: palasso (~palasso@user/palasso) (Remote host closed the connection)
[04:59:43] *** Quits: Arrinao (~Arrinao@85-70-61-102.rcd.o2.cz) (Read error: Connection reset by peer)
[09:07:20] *** Joins: Arrinao (~Arrinao@85-70-61-102.rcd.o2.cz)
[11:30:21] *** Joins: palasso (~palasso@user/palasso)
[12:43:36] *** Joins: trace987 (~trace@ip5b42976d.dynamic.kabel-deutschland.de)
[13:41:29] *** Quits: Arrinao (~Arrinao@85-70-61-102.rcd.o2.cz) (Read error: Connection reset by peer)
[15:15:40] <bluss> Evolver: .pkl is pickle?
[18:19:23] *** Joins: Arrinao (~Arrinao@85-70-61-102.rcd.o2.cz)
[18:49:52] *** Joins: Pickchea (~private@user/pickchea)
[19:11:35] *** Quits: Arrinao (~Arrinao@85-70-61-102.rcd.o2.cz) (Read error: Connection reset by peer)
[20:28:25] <Evolver> bluss: yes
[20:32:29] <mefistofeles> Evolver: if all that matters is file size of the serialized object, it seems like parquet is a good alternative format for it (from apache Hadoop)
[20:34:00] <Evolver> I will be happy to compare with parquet, but typically it helps when the data has duplicate values
[20:34:35] <Evolver> If a column has a lot of duplicate categories or values, then parquet is expected to help because it uses columnar storage.
[20:35:29] <Evolver> but my df doesn't have such duplicate values
[20:35:59] <Evolver> Worth comparing still.
[20:36:42] <bluss> I was thinking of zstd and maybe even a custom dictionary for zstd (that only matters if the individual files are small, but you have many of them)
[20:36:52] <Evolver> u right
[20:37:07] <Evolver> it could help with the indexes because my indexes are datetimes that get repeated across files
[20:37:15] <Evolver> but they're a tiny part of the total size
[20:37:43] <Evolver> The bulk of the size is used by the float columns that have absolutely nothing in common.
[20:38:13] <Evolver> There are some variable float encoders but they may not be lossless.
[20:38:47] <bluss> pickle sounds like a very inefficent representation to start with. But it's unfortunately one of the easiest to work with. Both parquet and hdf5 will refuse to serialize certain dataframes (depends on their column/row indexes)
[20:39:14] <Evolver> For an employer I never use pickle because it is Python specific
[20:39:16] <bluss> hm.. maybe the default hdf5 will do them all(?) I remember problems with hdf5 in the table variant..
[20:39:55] <Evolver> I have found ways to get around the parquet writing errors
[20:40:05] <Evolver> so I'm reasonably content with parquet as a format
[20:40:08] <Evolver> I have also used orc.
[20:42:19] <Evolver> This warrants a systematic comparison.
[20:44:01] <mefistofeles> I'd go for either parquet or feather
[20:44:21] *** Quits: Pickchea (~private@user/pickchea) (Quit: Leaving)
[20:45:09] <Evolver> whichever uses less space wins. no other criteria here.
[20:45:28] <Evolver> I mean assuming it takes under 10 seconds to write.
[20:46:13] <mefistofeles> yeah, but that depends on your data and types themselves
[20:46:26] <mefistofeles> but I'd say those two are good options to consider
[21:59:20] *** Quits: trace987 (~trace@ip5b42976d.dynamic.kabel-deutschland.de) (Remote host closed the connection)
[22:23:39] *** Joins: goldfish (~goldfish@user/goldfish)
[23:01:52] *** Joins: Arrinao (~Arrinao@85-70-61-102.rcd.o2.cz)
