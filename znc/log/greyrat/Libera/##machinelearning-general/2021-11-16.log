[00:09:11] *** Quits: brand0x (~brandon@user/brand0) (Ping timeout: 246 seconds)
[00:10:24] *** Joins: Codaraxis (~Codaraxis@user/codaraxis)
[00:11:17] *** Joins: brand0x (~brandon@user/brand0)
[00:27:05] *** Joins: Jong (~Jong@2620:10d:c090:400::5:4527)
[00:27:05] *** Quits: Bi1non (~8iIn0n@2a01:4b00:8e07:7900:aa4d:84b:c3c0:acbe) (Quit: Leaving)
[00:49:24] *** Quits: hygl (uid16621@tinside.irccloud.com) (Quit: Connection closed for inactivity)
[01:05:30] *** Quits: brand0x (~brandon@user/brand0) (Ping timeout: 256 seconds)
[01:07:22] *** Joins: Klinda (~superleag@user/klinda)
[01:07:37] *** Joins: brand0x (~brandon@user/brand0)
[02:07:42] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[02:08:51] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:08:58] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[02:27:12] *** Quits: palasso (~palasso@user/palasso) (Read error: Connection reset by peer)
[02:34:55] *** Joins: marcello42 (~mp@p200300dfaf07ff01fa2e52f98b4dfc80.dip0.t-ipconnect.de)
[03:36:42] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 265 seconds)
[04:09:07] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[04:40:16] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
[05:21:25] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[05:36:19] *** Quits: shoky (uuuggg@141.226.193.67) ()
[05:41:49] *** Joins: shoky (uuuggg@141.226.193.67)
[05:54:26] *** Quits: marcello42 (~mp@p200300dfaf07ff01fa2e52f98b4dfc80.dip0.t-ipconnect.de) (Quit: WeeChat 3.3)
[06:04:11] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[06:14:54] *** Joins: Karel (~Karel@ptr-9091p55l5qzdvorvtjt.18120a2.ip6.access.telenet.be)
[06:18:23] *** Quits: jinsun (~quassel@user/jinsun) (Ping timeout: 264 seconds)
[06:40:16] *** Joins: Gimble (~G1mble2@2a01:4b00:8e07:7900:aa4d:84b:c3c0:acbe)
[06:43:39] *** Joins: hygl (uid16621@tinside.irccloud.com)
[07:35:26] *** Joins: jinsun (~quassel@user/jinsun)
[07:44:40] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[07:46:00] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[07:50:01] *** Quits: Gimble (~G1mble2@2a01:4b00:8e07:7900:aa4d:84b:c3c0:acbe) (Quit: Leaving)
[08:10:04] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[08:10:59] *** Joins: orbisvicis (~orbisvici@user/orbisvicis)
[08:25:53] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 250 seconds)
[08:26:50] <orbisvicis> I have time-series data with 3 groups, relative size of any member of each group is roughly 1,3,7 (3 times the size of a member of the previous group, etc) but the absolute values vary over time. I'm doing 1d clustering using jenks. I'm streaming the data (processing as it becomes available) and chunking the data (since the values vary slowly over time). The last group (*7) is very rare and unlikely to show up. How would you find
[08:26:50] <orbisvicis> the number of groups?
[08:28:25] <orbisvicis> I need a number of groups (k) for jenks, but I can't test jenks with k of 2-3 since goodness variance of fit is always better with higher k
[08:33:28] <orbisvicis> not even sure jenks is best since I know the relative values
[08:52:17] *** Quits: jonasbits (~quassel@2a00:66c0:1:1::58) (Ping timeout: 264 seconds)
[08:53:24] *** Quits: hygl (uid16621@tinside.irccloud.com) (Quit: Connection closed for inactivity)
[08:53:38] *** Joins: jonasbits (~quassel@2a00:66c0:1:1::58)
[09:48:15] *** Joins: hygl (uid16621@tinside.irccloud.com)
[09:57:31] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[10:14:03] *** Joins: manti7 (~manti7@176.10.104.94)
[10:37:10] *** Joins: Gimble (~G1mble2@2a01:4b00:8e07:7900:aa4d:84b:c3c0:acbe)
[10:37:56] <Samian> Is one of the main differences between SVMs and neural networks that SVMs are linear and work by adding enough dimensions until a linear hyperplane becomes a good decision boundary, while neural nets are non linear?
[10:38:32] <Samian> so the difference is SMVs are linear only, and neural nets are non-linear?
[10:48:49] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[10:53:45] *** Quits: Gimble (~G1mble2@2a01:4b00:8e07:7900:aa4d:84b:c3c0:acbe) (Quit: Leaving)
[11:30:35] *** Joins: palasso (~palasso@user/palasso)
[11:36:58] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[11:39:06] *** Joins: SiegeLord (~sl@user/siegelord)
[12:07:37] *** Quits: causative (~halberd@user/causative) (Remote host closed the connection)
[12:10:07] *** Joins: causative (~halberd@user/causative)
[12:36:05] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Read error: Connection reset by peer)
[12:55:32] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi)
[13:14:25] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:15:53] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi)
[13:20:31] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi) (Read error: Connection reset by peer)
[13:21:47] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi)
[14:47:19] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi) (Read error: Connection reset by peer)
[14:48:33] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi)
[14:52:49] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi) (Read error: Connection reset by peer)
[14:54:04] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi)
[15:10:30] <HuntsMan> Samian: SVMs do not "add dimensions"
[15:11:35] <Samian> it maps input data to a high dimensional space. Excuse my language
[15:12:05] <Samian> in making an svm, some high dimensional space is arrived at and chosen to be used
[15:12:19] <HuntsMan> chosen?
[15:12:22] <HuntsMan> who chooses it?
[15:12:37] <Samian> whoever chose to use the svm
[15:13:52] <HuntsMan> yeah
[15:17:43] <Samian> HuntsMan  if someone understands neural networks and gradient descent, is there much reason to learn SVMs? I don't see it being used
[15:21:06] <HuntsMan> I still use SVMs
[15:21:37] <HuntsMan> you can even train an SVM with gradient descent and the hinge loss
[15:23:50] <Samian> hinge loss doesn't appear in Goodfellow's book :-(
[15:41:21] *** Joins: Klinda (~superleag@user/klinda)
[15:58:11] *** Quits: Codaraxis (~Codaraxis@user/codaraxis) (Remote host closed the connection)
[15:58:36] *** Joins: Codaraxis (~Codaraxis@user/codaraxis)
[16:05:53] *** Joins: marcello42 (~mp@p200300dfaf07ff01fa2e52f98b4dfc80.dip0.t-ipconnect.de)
[16:10:51] *** Joins: tomeaton17 (~tomeaton1@2a0c:5bc0:40:3e3a:69b3:9c8e:e1c9:574d)
[16:32:24] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi) (Read error: Connection reset by peer)
[16:33:48] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi)
[16:34:48] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[16:39:07] *** Quits: tomeaton17 (~tomeaton1@2a0c:5bc0:40:3e3a:69b3:9c8e:e1c9:574d) (Quit: Client closed)
[16:39:18] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi) (Read error: Connection reset by peer)
[16:40:39] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi)
[17:12:16] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi) (Read error: Connection reset by peer)
[17:13:36] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi)
[17:19:16] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0c0-17.dhcp.inet.fi) (Read error: Connection reset by peer)
[17:20:31] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi)
[17:22:40] <hodapp> SVMs are still pretty interesting as a kernel method in which one can in effect work with a linear classifier in a transformed space, without having to actually use that transformed space, which might even be infinite-dimensional
[17:22:54] <hodapp> which is more general than just SVMs
[17:23:30] <hodapp> and sometimes, you don't need or want a neural net
[17:25:12] <[itchyjunk]> inf dimensional vector spaces? how do computers work with those?
[17:26:39] <hodapp> they don't. that is the key of kernel methods: if there is a kernel which computes distance in that transformed space (which can be guaranteed mathematically given some broad constraints) then you don't actually have to *use* that space if distance is all you need
[17:27:18] <[itchyjunk]> hmm i guess i don't know what kernel means either :(
[17:28:08] <hodapp> as far as I remember, "kernel" here really just means the function which computes the distance between two data points in some transformed (maybe nonlinear) space
[17:28:26] <hodapp> there are a bunch of pre-made kernel functions, e.g. radial basis function
[17:29:53] <[itchyjunk]> so there is some function (kernel) that can computer this optimal hyperplane in some transformed space
[17:30:01] <[itchyjunk]> but we don't actually do the transformation?
[17:30:22] <[itchyjunk]> what happens then? the function gets transformed to our space where we use it?
[17:31:30] <hodapp> the kernel function doesn't compute the optimal hyperplane, it just computes distance; IIRC, the SVM algorithm itself is responsible for figuring out that decision boundary based on all the pairwise distances/labels
[17:32:46] <[itchyjunk]> ah okay
[17:33:21] *** Joins: tomeaton17 (~tomeaton1@2a0c:5bc0:40:3e3a:4d88:781e:9eb7:852b)
[17:33:49] <[itchyjunk]> but SVM still needs the kernel to compute the distances no? so if were not transforming out data to a new vector space, we must be transforming our fuction from some other space to the space we are working with?
[17:34:28] <[itchyjunk]> at least that sounds like one way how we can do the optimization without transforming out data
[17:36:45] <hodapp> AFAIK the decision boundary can be evaluated just by the distances
[17:37:00] <hodapp> so it requires no transformation
[17:40:41] <tomeaton17> what do you mean by distances? the length of the support vector?
[17:41:51] <[itchyjunk]> maybe distances between classes
[17:42:04] *** Joins: Gimble (~G1mble2@2a01:4b00:8e07:7900:258a:d8a:632b:a428)
[17:42:19] <hodapp> distances between any two points in the dataset
[17:42:50] <hodapp> guess I should say 'inner product', not distance
[17:42:56] *** Joins: Guest7758 (~Guest77@186.46.207.121)
[17:44:51] <tomeaton17> okay, I am not sure what the initial question or discussion was sorry I missed that.
[17:45:19] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[17:45:26] <[itchyjunk]> talking about svm in general
[17:48:37] <tomeaton17> okay well about computing distances then the kernel function returns the inner product of two points in z space
[17:49:37] <hodapp> right
[17:49:57] <[itchyjunk]> i was asking about this part
[17:50:00] <[itchyjunk]> hodapp> they don't. that is the key of kernel methods: if there is a kernel which computes distance in that transformed space (which can be guaranteed mathematically given some broad constraints) then you don't actually have to *use* that space if distance is all you need
[17:50:15] <[itchyjunk]> how was the kernel being used without a transform happening etc
[17:52:37] *** Joins: yeirr (~yeirr@user/yeirr)
[17:55:44] <tomeaton17> My understand was that the transform was included in the kernel
[17:56:34] <[itchyjunk]> oh i see, kernel is the transform + inner product? interesting
[17:56:54] <[itchyjunk]> i realize there are so many classes of functions out there i'll never even hear about
[17:57:08] <hodapp> yeah, it is equivalent to transform + inner product
[17:57:09] <[itchyjunk]> crazy to think few 100 years ago, there were only a dozen or so
[17:57:25] <hodapp> without necessarily doing each step explicitly
[17:58:15] <[itchyjunk]> does this kernel have any realtionship to what kernel is in math typically?
[17:58:29] <hodapp> I don't know what kernel is in math typically
[17:58:50] <tomeaton17> yes the important part is that it can be computed in time O(d) rather than depending on z space dimension
[17:59:00] <[itchyjunk]> kernel of a map is the set in pre image that gets mapped to 0's in the image
[17:59:09] <yeirr> kernel is the most confusing term in ml i
[17:59:15] <yeirr> i've encountered so far
[17:59:29] <yeirr> it means differently under different contexts
[17:59:31] <[itchyjunk]> what is d there? dimension of original space?
[17:59:47] <tomeaton17> yes
[18:00:00] <[itchyjunk]> oh i see
[18:00:53] <yeirr> there's kernel in math, gpu programming, classical ml, dl  and many obsured ones
[18:01:32] <hodapp> and OS kernels, of course!
[18:01:32] <[itchyjunk]> ah so not related to each other
[18:01:34] <yeirr> this branch of science needs better nomenclature and taxonomies
[18:01:34] <[itchyjunk]> i see
[18:02:04] <yeirr> they are related during implementation
[18:03:34] <tomeaton17> luckily you don't really need to understand kernels so much as it would be quite rare to create your own kernel function
[18:06:01] <[itchyjunk]> i'm more interested in understand how it works than the actual models that have practical use :P
[18:06:38] <hodapp> [itchyjunk]: you can implement your own crappy SVM pretty easily, I think
[18:06:55] <[itchyjunk]> it took me literal weeks to make a XOR approximating ANN :(
[18:07:02] <[itchyjunk]> i am no good with this stuff
[18:08:26] <tomeaton17> I would recommend amlbook.com it goes through the theory very well
[18:20:39] *** Quits: tomeaton17 (~tomeaton1@2a0c:5bc0:40:3e3a:4d88:781e:9eb7:852b) (Quit: Client closed)
[18:21:26] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[18:26:30] *** Quits: Guest7758 (~Guest77@186.46.207.121) (Ping timeout: 256 seconds)
[18:42:47] <yeirr> unfortunately..its all math if you're talking about ml specifically
[18:44:03] <yeirr> ranging from auto-differentiation, backprop, layers, optimizers and dozens more supporting the ecosystem
[18:44:29] <yeirr> its a grind honestly
[18:45:41] <yeirr> you can look through libraries like scikit-learn, pytorch and tensorflow implemented by experts
[18:46:01] <yeirr> its useful in getting up to speed
[18:49:07] *** Joins: Guest7772 (~Guest77@186.46.207.121)
[18:56:30] *** Quits: Klinda (~superleag@user/klinda) (Ping timeout: 256 seconds)
[18:58:11] <[itchyjunk]> why unfortunate? math is okay
[19:02:46] *** Quits: Guest7772 (~Guest77@186.46.207.121) (Ping timeout: 256 seconds)
[19:09:32] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 265 seconds)
[19:10:56] *** Joins: gareppa (~gareppa@user/gareppa)
[19:18:35] *** Quits: gareppa (~gareppa@user/gareppa) (Quit: Leaving)
[19:18:52] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[19:35:23] *** Joins: akevinhuang2 (~thekevinh@user/thekevinhuang)
[19:38:03] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 265 seconds)
[19:47:18] *** Joins: Klinda (~superleag@user/klinda)
[20:11:43] *** Quits: yeirr (~yeirr@user/yeirr) (Remote host closed the connection)
[20:26:46] *** Quits: marcello42 (~mp@p200300dfaf07ff01fa2e52f98b4dfc80.dip0.t-ipconnect.de) (Quit: WeeChat 3.3)
[20:29:00] *** Joins: marcello42 (~mp@p200300dfaf07ff01fa2e52f98b4dfc80.dip0.t-ipconnect.de)
[20:29:17] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Quit: Leaving)
[20:59:41] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[21:28:15] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-567343-143.dhcp.inet.fi) (Read error: Connection reset by peer)
[21:51:44] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[21:53:37] *** Quits: akevinhuang2 (~thekevinh@user/thekevinhuang) (Ping timeout: 250 seconds)
[21:59:30] <octav1a> Hello, looking for some advice / understanding about CNN layers. I'm just trying to understand how it works by making a simple-ish numpy version. When I create a pytorch conv2d with (3, 3, 3, 1, 1) (three channel in /out, kernsize 3, padding and stride 1) I see the weight matrix is initially set to shape (3, 3, 3, 3) By my initial understanding I would expect shape (3, 3, 3) (three channels, 3 in the X direction and 3 in the Y)  ?
[22:18:12] *** Joins: manuels (~manuel@80.211.205.88)
[22:42:56] <HuntsMan> octav1a: there are input channels and output channels
[22:47:40] *** Joins: ttkap (~ttkap@user/ttkap)
[23:04:56] *** Joins: defjam (~eb0t@90.209.246.132)
[23:41:00] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Quit: Leaving)
[23:56:35] <cslr> I'm looking for a cheap machine learning server to rent with many CPU cores/threads (>=64cores/128 threads). I don't use GPU right now so I don't need GPU.
[23:57:02] <cslr> Can somebody recommend a cheap server? I don't have much money.
[23:57:38] <dostoyevsky2> https://vast.ai/
[23:57:39] <SigmoidFroid> ⇒  Rent GPU Servers for Deep Learning and AI | Vast.ai
[23:58:32] *** Quits: orbisvicis (~orbisvici@user/orbisvicis) (Ping timeout: 256 seconds)
[23:58:42] <cslr> I don't need GPU as I'm developing software for normal laptops (C++) with no GPU requirements or realtime requirements for which python is no good.
