[00:08:38] *** Quits: thanas (~thanas@user/thanas) (Quit: ZNC - https://znc.in)
[00:12:10] *** Joins: thanas (~thanas@user/thanas)
[00:15:38] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 252 seconds)
[00:15:52] *** Joins: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net)
[00:35:09] *** Quits: Null_A (~null_a@2601:645:8700:2290:cc41:848b:cba0:322d) (Remote host closed the connection)
[00:35:34] *** aab_ is now known as Active8
[00:35:44] *** Joins: Null_A (~null_a@2601:645:8700:2290:cc41:848b:cba0:322d)
[00:40:24] *** Quits: Null_A (~null_a@2601:645:8700:2290:cc41:848b:cba0:322d) (Ping timeout: 276 seconds)
[00:45:32] *** Quits: tex (~dee@user/dix) (Ping timeout: 245 seconds)
[00:45:35] *** Quits: tang^ (~DoofusCan@2604:3d09:47c:f970:ac99:2bd8:f769:a098) (Quit: So as you can see from this flowchSQUIRREL!!)
[00:48:35] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[00:51:46] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[00:56:47] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 245 seconds)
[00:58:38] *** Quits: TomTom (uid45892@id-45892.charlton.irccloud.com) (Quit: Connection closed for inactivity)
[01:10:53] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[01:13:44] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[01:19:16] *** Joins: jazzy (~jaziz@2600:380:c174:6a3d:9430:49a9:10fc:d56c)
[01:21:32] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[01:23:29] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[01:25:31] *** Quits: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com) (Read error: Connection reset by peer)
[01:26:28] *** Joins: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com)
[01:28:42] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 240 seconds)
[01:37:28] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[01:49:26] *** Joins: greatgatsby (~greatgats@bras-base-toroon0411w-grc-52-142-114-106-7.dsl.bell.ca)
[01:51:02] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[01:51:35] *** Joins: pvdp (~Pieter@static-n58-105-183-94.rdl4.qld.optusnet.com.au)
[01:53:26] *** Quits: Ard1t (~ard1t@user/ard1t) (Quit: Nettalk6 - www.ntalk.de)
[01:56:49] *** Quits: AnapodoPsalidaki (~AnapodoPs@2a02:587:2910:6a35:1119:7c47:801e:9e4f) (Quit: Leaving)
[01:57:39] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[01:58:10] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[02:02:18] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 240 seconds)
[02:13:23] *** Quits: The_Loko (~The_Loko@86.127.235.231) (Quit: Leaving)
[02:18:50] *** Joins: Gustavo6046_ (~Gustavo60@user/gustavo6046)
[02:18:50] *** Quits: Gustavo6046 (~Gustavo60@user/gustavo6046) (Remote host closed the connection)
[02:20:36] *** Gustavo6046_ is now known as Gustavo6046
[02:23:24] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[02:26:34] *** Quits: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963) (Ping timeout: 240 seconds)
[02:27:13] *** Quits: esses (uid38151@id-38151.stonehaven.irccloud.com) (Quit: Connection closed for inactivity)
[02:27:38] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 252 seconds)
[02:33:17] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[02:36:34] *** Quits: aminvakil (~aminvakil@2a01:4f8:120:336b:4::1) (Quit: Ping timeout (120 seconds))
[02:36:59] *** Joins: aminvakil (~aminvakil@2a01:4f8:120:336b:4::1)
[02:39:59] *** Joins: jonfen (~jonfen@65-130-170-232.slkc.qwest.net)
[02:41:39] *** Joins: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963)
[02:48:22] *** Quits: aminvakil (~aminvakil@2a01:4f8:120:336b:4::1) (Quit: Ping timeout (120 seconds))
[02:48:48] *** Joins: aminvakil (~aminvakil@2a01:4f8:120:336b:4::1)
[02:50:53] *** Quits: thc202 (~thc202@user/thc202) (Quit: thc202)
[03:13:52] *** Joins: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net)
[03:18:24] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Ping timeout: 265 seconds)
[03:26:49] *** Joins: thiras (~thiras@user/thiras)
[03:29:07] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[03:30:02] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Remote host closed the connection)
[03:31:17] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[03:40:30] *** Joins: tang^ (~DoofusCan@2604:3d09:47c:f970:c423:cb12:d0d4:cd5f)
[03:44:21] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[03:52:20] *** Quits: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com) (Read error: Connection reset by peer)
[03:53:20] *** Joins: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com)
[03:59:19] *** Quits: beanzilla (~beanzilla@user/beanzilla) (Quit: ZNC 1.8.2 - https://znc.in)
[04:04:00] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[04:06:32] *** Joins: beanzilla (~beanzilla@user/beanzilla)
[04:09:42] <Lutin> Whaha! https://www.pedestrian.tv/online/porn-news-websites-vidme/
[04:11:16] *** Quits: notevil (~notevil@user/notevil) (Quit: ZNC 1.8.2 - https://znc.in)
[04:11:29] *** Joins: notevil (~notevil@user/notevil)
[04:13:47] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[04:14:41] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[04:20:40] *** Joins: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com)
[04:39:15] *** Joins: rewrit3 (~rewrit3@user/rewrit3)
[04:42:26] *** Quits: Sasazuka (~Sasazuka@user/sasazuka) (Quit: My MacBook has gone to sleep. ZZZzzzâ€¦)
[04:50:08] *** Quits: hposca (~hposca@node-1w7jr9phoke2tgwo5dsrqdhce.ipv6.telus.net) (Ping timeout: 255 seconds)
[04:52:50] *** Quits: coc0nut (~coc0nut@167.99.37.181) (Quit: ZNC 1.8.2 - https://znc.in)
[04:54:00] *** Joins: coc0nut (~coc0nut@167.99.37.181)
[04:54:44] *** Quits: Aayush (~Alvin@122.162.213.4) (Quit: Cya)
[04:55:23] *** Quits: tang^ (~DoofusCan@2604:3d09:47c:f970:c423:cb12:d0d4:cd5f) (Quit: So as you can see from this flowchSQUIRREL!!)
[05:00:15] *** Joins: Gustavo6046_ (~Gustavo60@user/gustavo6046)
[05:00:36] *** Quits: Gustavo6046 (~Gustavo60@user/gustavo6046) (Remote host closed the connection)
[05:01:16] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 252 seconds)
[05:02:12] *** Gustavo6046_ is now known as Gustavo6046
[05:08:00] *** Quits: goddard (~goddard@user/goddard) (Remote host closed the connection)
[05:08:20] *** Joins: goddard (~goddard@user/goddard)
[05:20:10] *** Joins: daMaestro (~damaestro@fedora/daMaestro)
[05:30:24] *** Quits: vlm (~vlm@user/vlm) (Quit: "")
[05:34:26] *** Joins: vlm (~vlm@user/vlm)
[05:36:59] *** Joins: PaulFranz (~pfranz@c-73-187-178-34.hsd1.pa.comcast.net)
[05:54:54] *** Quits: CombatVet (~c4@user/combatvet) (Ping timeout: 244 seconds)
[05:55:27] *** Joins: CombatVet (~c4@user/combatvet)
[06:07:48] *** Joins: artok (~azo@mobile-access-5d6ace-203.dhcp.inet.fi)
[06:13:26] *** Joins: Gustavo6046_ (~Gustavo60@user/gustavo6046)
[06:13:30] *** Quits: Gustavo6046 (~Gustavo60@user/gustavo6046) (Remote host closed the connection)
[06:15:11] *** Gustavo6046_ is now known as Gustavo6046
[06:15:16] *** Joins: oxum (~oxum@106.203.217.191)
[06:19:01] *** Quits: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com) (Read error: Connection reset by peer)
[06:19:58] *** Joins: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com)
[06:20:22] *** Quits: oxum (~oxum@106.203.217.191) (Ping timeout: 258 seconds)
[06:41:17] *** Quits: endigma (~endigma@134.41.133.214) (Remote host closed the connection)
[06:41:35] *** Joins: endigma (~endigma@134.41.133.214)
[06:41:42] *** Quits: ablutor (~quassel@wasscher.com) (Ping timeout: 268 seconds)
[06:41:42] *** Quits: endigma (~endigma@134.41.133.214) (Remote host closed the connection)
[06:43:23] *** Joins: ablutor (~quassel@wasscher.com)
[06:52:59] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[07:06:31] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Quit: Atum_)
[07:12:10] *** Quits: ablutor (~quassel@wasscher.com) (Ping timeout: 252 seconds)
[07:14:25] *** Quits: ultima (~ultima@23.81.113.229) (Ping timeout: 258 seconds)
[07:16:31] *** Quits: Bossi (~quassel@p4fc22308.dip0.t-ipconnect.de) (Ping timeout: 268 seconds)
[07:32:41] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[07:33:25] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[07:47:18] *** Joins: endigma (~endigma@134.41.133.214)
[07:47:48] *** Joins: L0j1k (~L0j1k@user/l0j1k)
[07:47:48] *** ChanServ sets mode: +o L0j1k
[07:48:30] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[07:49:20] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Remote host closed the connection)
[07:49:34] *** Joins: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d)
[07:55:06] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[07:59:18] *** Quits: lemonzest (~lemonzest@user/lemonzest) (Quit: Quitting)
[08:11:30] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[08:11:46] *** Joins: AtomicElephants (~AtomicEle@user/atomicelephants)
[08:17:06] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 265 seconds)
[08:17:17] *** Quits: artok (~azo@mobile-access-5d6ace-203.dhcp.inet.fi) (Ping timeout: 258 seconds)
[08:22:01] *** Joins: oxum (~oxum@106.203.217.191)
[08:26:51] *** Quits: oxum (~oxum@106.203.217.191) (Ping timeout: 250 seconds)
[08:39:41] *** Quits: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net) (Quit: My MacBook has gone to sleep. ZZZzzzâ€¦)
[08:44:19] *** Quits: pfeilmann (~pfeilmann@c3po.mahr.pw) (Read error: Connection reset by peer)
[08:44:51] *** Joins: pfeilmann (~pfeilmann@c3po.mahr.pw)
[08:46:01] *** Quits: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com) (Read error: Connection reset by peer)
[08:46:59] *** Joins: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com)
[08:48:58] *** Quits: Fossil (~Fossil@2a03:b0c0:0:1010::398:b001) (Ping timeout: 240 seconds)
[08:52:26] *** Joins: oxum (~oxum@106.203.217.191)
[08:56:44] *** Quits: oxum (~oxum@106.203.217.191) (Ping timeout: 255 seconds)
[08:58:22] *** Joins: fdan (~fdan@192.146.154.3)
[08:58:25] <fdan> hi there
[08:58:32] <fdan> how can i find the image layers for an image
[08:58:38] <fdan> i did docker inspect , can check the image layers
[08:58:47] <fdan> but im nt able to find it in the underlying host
[09:18:41] *** Joins: AnapodoPsalidaki (~AnapodoPs@2a02:587:2910:6a35:cd40:b885:2f0c:2928)
[09:27:45] *** Quits: daMaestro (~damaestro@fedora/daMaestro) (Quit: Leaving)
[09:29:58] *** Quits: Null_A (~null_a@2601:645:8700:2290:b5ac:63df:d679:fb5d) (Read error: Connection reset by peer)
[09:32:29] *** Joins: ultima (~ultima@23.81.113.229)
[09:34:30] *** Joins: Null_A (~null_a@2601:645:8700:2290:b978:3dca:9084:7b5f)
[09:38:13] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[09:39:28] *** Quits: fdan (~fdan@192.146.154.3) (Ping timeout: 246 seconds)
[09:41:12] *** Quits: bouncy_ (~ben@user/benoit) (Ping timeout: 265 seconds)
[09:43:30] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[09:44:20] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[09:44:44] *** Joins: thc202 (~thc202@user/thc202)
[09:49:15] *** Quits: Null_A (~null_a@2601:645:8700:2290:b978:3dca:9084:7b5f) ()
[09:49:22] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[09:54:40] *** Quits: ac5tin (~ac5tin@user/ac5tin) (Quit: WeeChat 3.2)
[09:55:24] *** Joins: nickjj_ (~nickjj@user/nickjj)
[09:56:23] *** Quits: AtomicElephants (~AtomicEle@user/atomicelephants) (Quit: Leaving)
[09:56:26] *** Quits: goddard (~goddard@user/goddard) (Ping timeout: 252 seconds)
[09:58:07] *** Quits: nickjj (~nickjj@user/nickjj) (Ping timeout: 265 seconds)
[09:58:54] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[10:03:08] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 252 seconds)
[10:35:18] *** Quits: dalan6 (~dalan@61-68-95-57.static.tpgi.com.au) (Read error: Connection reset by peer)
[10:35:53] *** Joins: dalan6 (~dalan@61-68-95-57.static.tpgi.com.au)
[10:37:04] *** Joins: vidbina (~vid@dynamic-077-011-068-015.77.11.pool.telefonica.de)
[10:40:37] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[10:40:38] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[10:46:27] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 265 seconds)
[10:53:59] *** Quits: c10l (~c10l@89.34.167.207) (Read error: Connection reset by peer)
[10:55:32] *** Quits: vidbina (~vid@dynamic-077-011-068-015.77.11.pool.telefonica.de) (Ping timeout: 255 seconds)
[10:56:13] *** Joins: c10l (~c10l@89.34.167.207)
[10:58:11] *** Quits: CombatVet (~c4@user/combatvet) (Ping timeout: 244 seconds)
[11:01:36] *** Joins: CombatVet (~c4@user/combatvet)
[11:13:43] *** Quits: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com) (Remote host closed the connection)
[11:14:40] *** Joins: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com)
[11:18:58] <L0j1k> fdan: you can do a 'docker inspect' on your container, then look for RootFS.Layers and you can find those hashes as directories under /var/lib/docker/image/zfs/layerdb/sha256/xxx where xxx is the hash. i'm using zfs so yours will be whatever FS you're using
[11:29:31] *** Quits: eramirez (~eramirez@2001:4453:367:0:7e13:9779:9980:5d72) (Remote host closed the connection)
[11:34:30] *** Quits: fedenix (~fedenix@gateway/tor-sasl/fedenix) (Remote host closed the connection)
[11:34:41] *** Joins: fedenix (~fedenix@gateway/tor-sasl/fedenix)
[11:52:04] *** Joins: TomTom (uid45892@id-45892.charlton.irccloud.com)
[11:53:02] *** Quits: zakame (~zakame@user/zakame) (Ping timeout: 252 seconds)
[11:55:09] *** Joins: zakame (~zakame@user/zakame)
[12:00:10] *** Joins: rageshkrishna (~rageshkri@136.185.187.57)
[12:05:08] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[12:05:49] *** Joins: vidbina (~vid@dynamic-046-114-037-029.46.114.pool.telefonica.de)
[12:09:30] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[12:18:08] *** Joins: Atque (~Atque@user/atque)
[12:24:39] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[12:41:03] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[12:46:00] *** Quits: jimmyb (~jimmyb@user/jimmyb) (Ping timeout: 258 seconds)
[12:46:44] *** Joins: Atque (~Atque@user/atque)
[12:47:09] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 258 seconds)
[12:49:08] *** Quits: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com) (Ping timeout: 252 seconds)
[12:54:22] *** Joins: bouncy_ (~ben@user/benoit)
[12:54:42] *** Quits: vidbina (~vid@dynamic-046-114-037-029.46.114.pool.telefonica.de) (Ping timeout: 240 seconds)
[13:06:42] *** Quits: rageshkrishna (~rageshkri@136.185.187.57) (Ping timeout: 240 seconds)
[13:06:51] *** Joins: vidbina (~vid@dynamic-077-011-068-015.77.11.pool.telefonica.de)
[13:13:13] *** Quits: jaskal (jaskal@user/jaskal) (Quit: leaving)
[13:20:53] *** Joins: jaskal (jaskal@user/jaskal)
[13:41:11] *** Joins: Lutin (~Lutin@user/lutin)
[13:54:39] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Remote host closed the connection)
[13:54:55] *** Joins: rewrit3 (~rewrit3@user/rewrit3)
[13:59:10] *** Quits: diverdude (~user@176-21-102-230-cable.dk.customer.tdc.net) (Ping timeout: 252 seconds)
[14:01:50] *** Quits: bouncy_ (~ben@user/benoit) (Ping timeout: 255 seconds)
[14:05:15] *** Joins: bouncy (~ben@user/benoit)
[14:09:36] *** Joins: mikeliuk (~mikeliuk@213.205.198.158)
[14:15:32] *** Joins: shokohsc (~shokohsc@161.88.195.77.rev.sfr.net)
[14:32:40] *** Joins: thiras (~thiras@user/thiras)
[14:52:33] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[14:56:47] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 245 seconds)
[15:11:10] *** Quits: AnapodoPsalidaki (~AnapodoPs@2a02:587:2910:6a35:cd40:b885:2f0c:2928) (Quit: Leaving)
[15:24:05] *** Quits: winstonsmith (~winstonsm@77.247.181.218) (Remote host closed the connection)
[15:24:21] *** Joins: winstonsmith (~winstonsm@109.201.152.168)
[15:27:50] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[15:46:50] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[15:50:20] *** Joins: Atque (~Atque@user/atque)
[15:59:39] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 265 seconds)
[16:09:22] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Remote host closed the connection)
[16:10:37] *** Joins: Fossil (~Fossil@2a03:b0c0:0:1010::398:b001)
[16:11:22] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[16:13:24] *** Joins: Lutin (~Lutin@user/lutin)
[16:17:19] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[16:21:15] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[16:24:11] *** Joins: alzgh (~alzgh@216.155.158.214)
[16:27:31] *** Joins: Atque (~Atque@user/atque)
[16:32:15] *** Quits: mikeliuk (~mikeliuk@213.205.198.158) (Quit: Connection closed)
[16:39:07] *** Joins: mikeliuk (~mikeliuk@213.205.198.158)
[16:47:00] *** Joins: cart_man (~rynot@185.160.60.121)
[16:48:00] <cart_man> Hi everyone. I am trying to write docker container stats to a file with commas sepereating the values. However I can not seem to actually get it to print and exit. Seems docker stat ust keeps the proc alive and refreshes every now and then
[16:48:41] <cart_man> Is there a way I can print the docker stat values ilke    CPU,MEM    example  20%, 450   etc?
[16:49:12] <cart_man> at the moment I have -> docker stats --all --format "{{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}" zealous_wilbur
[16:51:08] *** Quits: vidbina (~vid@dynamic-077-011-068-015.77.11.pool.telefonica.de) (Ping timeout: 252 seconds)
[16:58:00] *** Joins: fdan (~fdan@192.146.154.3)
[16:58:16] <fdan> hi there
[16:58:17] <fdan> how can i find the image layers for an image,i did docker inspect , can check the image layers but im nt able to find it in the underlying host
[16:58:26] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[16:59:01] *** Quits: Helikopt1 (~Helikopte@infracloud.lcnaud.fr) (Quit: WeeChat 2.8)
[16:59:10] *** Joins: Helikoptere (~Helikopte@infracloud.lcnaud.fr)
[17:04:22] *** Quits: maret (~textual@195.12.158.102) (Read error: No route to host)
[17:06:25] *** Quits: fdan (~fdan@192.146.154.3) (Ping timeout: 246 seconds)
[17:07:18] *** Joins: drant (~drant@2a05:f480:1c00:d82::)
[17:08:00] <cart_man> Ok the answer to my question is --no-stream
[17:09:27] *** Joins: rsx (~dummy@ppp-188-174-150-109.dynamic.mnet-online.de)
[17:10:01] *** Joins: sebastianos (~sebastian@user/sebastianos)
[17:10:47] *** Joins: fdan (~fdan@192.146.154.3)
[17:10:52] <fdan> hi there
[17:10:53] <fdan> how can i find the image layers for an image,i did docker inspect , can check the image layers but im nt able to find it in the underlying host
[17:10:56] <cart_man> So now I have that. I notice doing {{.MemUsage}} gives me xxxx/yyyy ... how can I just get Mem used without the / ?
[17:11:42] <cart_man> fdan: Do you mean the OS that the original container pulls before it adds stuff on with dockerfile?
[17:12:07] <fdan> cart_man no from the place where i install docker
[17:12:14] <fdan> in the /var/log/docker
[17:12:20] <fdan> i dont see the image layers
[17:12:24] <cart_man> oh ok sorry I dont know
[17:14:53] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 255 seconds)
[17:14:56] <L0j1k> fdan: look for the image layer hsahes in 'docker inspect' under RootFS.layers and each of those hashes will be a directory under /var/lib/docker/image/zfs/layerdb/sha256/xxx where xxx is the hash (i'm using zfs tho, yours will depend on your FS)
[17:15:51] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[17:16:51] <fdan> ok let me check
[17:18:22] *** Joins: maret (~textual@195.12.158.102)
[17:20:19] *** Joins: Lutin (~Lutin@user/lutin)
[17:20:35] *** Joins: tang^ (~DoofusCan@2604:3d09:47c:f970:8d13:b68:18df:3319)
[17:21:37] *** Quits: qilx (~quassel@62.201.21.8) (Ping timeout: 258 seconds)
[17:22:27] *** Joins: qilx (~quassel@dynamic-109-81-210-171.ipv4.broadband.iol.cz)
[17:24:02] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[17:24:25] *** Quits: mikeliuk (~mikeliuk@213.205.198.158) (Quit: Connection closed)
[17:34:53] <fdan> L0j1k zfs is the file system?
[17:34:57] <fdan> in my case i have overlay2
[17:36:47] <L0j1k> then use overlay2 instead :)
[17:36:58] *** Joins: Atque (~Atque@user/atque)
[17:37:02] <fdan> L0j1k may i know why you use zfs
[17:37:42] <L0j1k> well, because it is a superior filesystem in almost every way, but it's also a lot more complex
[17:37:50] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 272 seconds)
[17:38:22] <fdan> L0j1k does it implement UFS 
[17:38:30] <wez> What ever happened to Andrew FS?
[17:39:03] <wez> L0j1k: Sadly ZFS is owned by Oracle now :(
[17:41:48] *** Joins: XSDNMN (~XSDNMN@2601:449:8400:7f0:44cd:8210:de34:8e2a)
[17:43:10] <fdan> L0j1k https://pasteboard.co/KcvlP05.png
[17:43:18] <fdan> i dont see any hashes in rootFS
[17:44:35] <L0j1k> wez: no, that's not true. check out zfs on linux, there is some minor difference in license but it's fine
[17:44:41] <L0j1k> it isn't like mysql or something
[17:44:55] <wez> L0j1k: Is like like MariaDB?
[17:45:01] <wez> s/like/it/
[17:45:15] <wez> In that it is opensources version of something owned by Oracle?
[17:45:18] <L0j1k> sort of, look at zfs on linux
[17:45:39] <L0j1k> it is not owned by oracle
[17:45:43] <L0j1k> the only question is around distribution of the binary
[17:45:50] <L0j1k> which canonical has done for some years
[17:46:36] <L0j1k> fdan: it's under overlay2/sha256/xxx
[17:47:30] <L0j1k> v2.x of zfs on linux is i would say as good as or better than anything i used on freebsd
[17:47:38] <programmerq> wezâ–¸ oh man, I haven't thought about andrew fs in a long long time. That was supposed to be this globally namespaced / distributed filesystem, right?
[17:47:49] <L0j1k> aloha programmerq o7
[17:47:50] *** Quits: fdan (~fdan@192.146.154.3) (Quit: Client closed)
[17:47:55] <programmerq> yo!
[17:48:15] *** Joins: fdan (~fdan@192.146.154.3)
[17:48:36] <fdan> [18:43:09] <fdan> L0j1k https://pasteboard.co/KcvlP05.png
[17:48:36] <fdan> [18:43:17] <fdan> i dont see any hashes in rootFS
[17:48:44] <fdan> sorry my connection was weak
[17:48:48] <L0j1k> fdan: you're missing /sha256/ in the directory
[17:48:53] <wez> o7
[17:48:56] <L0j1k> overlay2/sha256/<hash>
[17:49:30] <fdan> -su: cd: sha256: No such file or directory
[17:50:53] <L0j1k> oh whoops
[17:50:55] <programmerq> fdanâ–¸ why are you wanting to access the layer directly? What problem are you wanting to solve?
[17:51:06] <L0j1k> overlay2/layerdb/sha256/<hash>
[17:51:12] <fdan> programmerq im just seeing where is where
[17:51:22] <L0j1k> if you poke around in there you'll find it
[17:51:24] <L0j1k> :)
[17:51:28] <programmerq> a non-driver-specific way to get to the layer is to do 'docker save <image>' > image.tar
[17:51:42] <programmerq> then in that resulting image.tar, there are more .tar files-- one for each layer
[17:51:42] <fdan> L0j1k not yet -su: cd: layerdb: No such file or directory
[17:51:53] <programmerq> and it works the same no matter what storage driver is being used.
[17:52:12] <fdan> programmerq oh i didnt know that there will be more .tar files for each layer
[17:52:14] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[17:52:15] <programmerq> all the drivers have their own layout
[17:52:18] <L0j1k> i'd def do that docker save haha :)
[17:52:26] <ada_> L0j1k: eyy
[17:52:32] <L0j1k> ay!! aloha ada :)
[17:52:40] <ada_> aloha :)
[17:52:44] *** Quits: cart_man (~rynot@185.160.60.121) (Ping timeout: 252 seconds)
[17:54:02] *** Quits: thc202 (~thc202@user/thc202) (Ping timeout: 255 seconds)
[17:54:29] *** Quits: BT (~BT@user/lightstalker) (Ping timeout: 268 seconds)
[17:54:58] *** Joins: lightstalker (~BT@user/lightstalker)
[17:55:35] <programmerq> L0j1kâ–¸ you should fly out to NC and hang out with ada_ and me
[17:55:48] <L0j1k> oh yeah i'd love that
[17:56:18] <L0j1k> i didn't think about that but yeah i'd definitely like to do that. i'm gonna think about that
[17:57:22] *** Joins: esses (uid38151@id-38151.stonehaven.irccloud.com)
[17:57:23] <ada_> L0j1k: that would be so amazing!
[17:57:26] *** Joins: Atque (~Atque@user/atque)
[17:58:32] <programmerq> ada_ you're back in raleigh, right? we should hang out too
[17:58:45] <L0j1k> it would be awesome! i have a number of chat friends in the area
[17:59:18] <ada_> programmerq: yes
[17:59:43] <ada_> programmerq: wanna go to a show toinght?  https://www.discodonniepresents.com/event/2021-0723-durham-thefruit
[18:00:04] <L0j1k> oh i would haha in a heartbeat
[18:00:51] *** Joins: thc202 (~thc202@user/thc202)
[18:02:42] <ada_> L0j1k: <3
[18:03:18] *** Quits: Icedream (~icedream@hzn-b.serverkomplex.de) (Quit: A lol made me boom.)
[18:04:28] *** Quits: maret (~textual@195.12.158.102) (Ping timeout: 252 seconds)
[18:07:57] *** Quits: jazzy (~jaziz@2600:380:c174:6a3d:9430:49a9:10fc:d56c) (Ping timeout: 250 seconds)
[18:08:12] *** Quits: fdan (~fdan@192.146.154.3) (Quit: Client closed)
[18:08:18] *** Joins: Icedream (~icedream@hzn-b.serverkomplex.de)
[18:11:47] *** Joins: fdan (~fdan@192.146.154.3)
[18:13:24] <fdan> programmerq is it possible to use aws ebs volumes as the rootFS for docker
[18:15:21] <programmerq> fdanâ–¸ yup. it's pretty common. Any block device type should work as long as it's running a backing filesystem that docker supports with one of its storage drivers.
[18:15:45] <fdan> what kind of file systems should the block device have
[18:16:01] *** Joins: BT (~BT@user/lightstalker)
[18:16:12] *** Quits: lightstalker (~BT@user/lightstalker) (Ping timeout: 252 seconds)
[18:17:12] <programmerq> fdanâ–¸ for overlay2 storage driver, ext4 or xfs with d_type true https://docs.docker.com/storage/storagedriver/overlayfs-driver/#prerequisites
[18:17:16] <programmerq> for zfs, zfs
[18:17:25] <programmerq> for btrfs, btrfs
[18:17:44] <programmerq> aufs is also a choice, but is basically phased out since most distros don't support it in favor of overlay2
[18:18:29] <programmerq> aufs needs the kernel modules, and the docs only specify that aufs doesn't work with the following backing filesystems: aufs, btrfs, encryptfs
[18:18:48] <programmerq> most common is ext4 + overlay2
[18:18:54] <programmerq> or xfs + overlay2
[18:21:31] *** Quits: tang^ (~DoofusCan@2604:3d09:47c:f970:8d13:b68:18df:3319) (Quit: So as you can see from this flowchSQUIRREL!!)
[18:23:04] *** Quits: fdan (~fdan@192.146.154.3) (Ping timeout: 246 seconds)
[18:23:53] *** Joins: fdan (~fdan@192.146.154.3)
[18:23:58] <fdan> sure programmerq
[18:24:24] <fdan> how can i configure docker to use ebs volumes? do have that in the docs?
[18:26:39] <ada_> fdan: you would need an ebs volume driver
[18:26:49] <fdan> and how to get that?
[18:27:17] <ada_> https://aws.amazon.com/blogs/compute/amazon-ecs-and-docker-volume-drivers-amazon-ebs/
[18:27:33] <programmerq> ada_â–¸ that concert sounds like fun. Do I need to buy a ticket online?
[18:27:58] <ada_> programmerq: I don't think they're going to sell out, but I would buy one ahead of time
[18:28:01] *** Joins: TomyWork (~TomyLobo@p200300e80f133c000c96cf69ee97f5a9.dip0.t-ipconnect.de)
[18:28:08] <ada_> programmerq: the fruit is an _awesome_ venue, too
[18:29:01] <ada_> L0j1k: I may be in sri lanka in January.  do you like psytrance music?  I can't remember.  if so you should come to Atman with us :) 
[18:29:32] <wez> reiserfs is what wife killers use right?
[18:30:17] *** Joins: kikijiki1 (~Thunderbi@user/kikijiki)
[18:32:49] *** Quits: rsx (~dummy@ppp-188-174-150-109.dynamic.mnet-online.de) (Quit: rsx)
[18:33:06] *** Quits: kikijiki (~Thunderbi@user/kikijiki) (Ping timeout: 240 seconds)
[18:33:06] *** kikijiki1 is now known as kikijiki
[18:37:23] *** Quits: kikijiki (~Thunderbi@user/kikijiki) (Ping timeout: 252 seconds)
[18:40:26] <fdan> @ada_ is there a simple documentation without cloud formation.,etc?
[18:40:54] <fdan> do i need to do docker plugin install rexray/ebs REXRAY_PREEMPT=true EBS_REGION=<AWS_REGION> --grant-all-permissions
[18:42:50] <ada_> fdan: you should only need to install the plugin and then use it
[18:43:01] <ada_> you don't need the cloudformation stuff
[18:43:17] <ada_> https://docs.docker.com/engine/extend/EBS_volume/
[18:43:25] <ada_> might be more concise
[18:43:36] <wez> fdan: CloudFormation is easy, it's just YAML, it makes redploying the same thing easier
[18:43:51] <wez> IaC is what you should be using
[18:45:08] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[18:49:33] <fdan> wez im ok with tf. but not with cf :)
[18:50:04] <wez> fdan: As long as you are using one of them 
[18:50:16] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 265 seconds)
[18:58:25] *** Quits: fdan (~fdan@192.146.154.3) (Ping timeout: 246 seconds)
[19:01:29] *** Quits: pvdp (~Pieter@static-n58-105-183-94.rdl4.qld.optusnet.com.au) (Remote host closed the connection)
[19:06:48] *** Joins: pertho (~pertho@pertho.net)
[19:08:11] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[19:08:38] <pertho> hi.. got a question.. co-worker has docker for mac, and has his $HOME/.ssh exported to the container with grpcfuse on /root/.ssh type fuse.grpcfuse (ro,relatime,user_id=0,group_id=0,allow_other,max_read=1048576) and yet the ownership on /root/.ssh instead of being owned by root, is owned by his Mac user id (501) and group is set to 20.. there any way to fix this? 
[19:09:09] <pertho> I cannot replicate on my Mac / Docker for Mac (same version 3.5.2)
[19:09:45] <pertho> when he unchecks grpcfuse to use the legacy ossx sharing.. problem goes away but it's super slow
[19:10:02] <pertho> is it some obscure grpcfuse setting on his mac doing this?
[19:13:12] *** Joins: vidbina (~vid@dynamic-077-011-068-015.77.11.pool.telefonica.de)
[19:13:16] <programmerq> the osxfs stuff does automatic mapping of uid based on the uid of the process accessing the file in the docker desktop VM. I don't think grpcfuse has any such functionality.
[19:14:46] <pertho> programmerq: if he switches to osxfs method, it correctly maps ownership to root:root in the container.. but if he checks the box to use grpcfuse.. it maps his user id (501) and group (20) into the container's /root/.ssh and it breaks  the permissions
[19:15:25] <pertho> it's like the user_id=0 and group_id=0 are being ignored when the volume is mounted in the container
[19:16:10] *** Joins: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net)
[19:17:59] <pertho> docker compose under volumes: section has this: '- "~/.ssh:/root/.ssh:ro"
[19:18:12] <programmerq> correct, osxfs will make it appear that the ownership of the file is the same user as the thing accessing it. this means that if it is accessed as 0:0 in the container, it'll appear as 0:0
[19:18:17] <pertho> works on everyone else's Mac in the company but not this one person's.. it's so strange
[19:18:29] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[19:18:46] <pertho> programmerq: yeah but osxfs seems a heck of a lot slower than grpcfuse
[19:18:48] <programmerq> most file sharing mechanisms don't do this. they either pass through the numeric uid of the shared files/folders (nfs, vboxsf, others)
[19:18:52] <programmerq> that is true.
[19:19:04] <programmerq> osxfs in particular does poorly with lots and lots of small files.
[19:19:06] <pertho> yeah I'm aware of nfs and vbox, vagrant, etc doing these
[19:19:32] <pertho> is there some hidden grpcfuse setting buried somewhere?
[19:19:44] <programmerq> or in the case of a sharing mechanism that isn't aware of unix style permissions like cifs, the client will have to pick something to use for numeric uid when it does the mount.
[19:19:44] *** Joins: rageshkrishna (~rageshkri@136.185.187.57)
[19:19:53] <programmerq> as far as I am aware, there is no setting for grpcfuse that does this.
[19:20:19] <programmerq> I could be wrong of course
[19:23:27] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 245 seconds)
[19:28:30] *** Joins: hposca (~hposca@node-1w7jr9phoke2vrgk4227u1bpc.ipv6.telus.net)
[19:34:10] *** Quits: hposca (~hposca@node-1w7jr9phoke2vrgk4227u1bpc.ipv6.telus.net) (Ping timeout: 256 seconds)
[19:36:48] <pertho> everything in $HOME/.docker is identical (except the paths of course.. since home directory paths are slightly different)
[19:43:50] *** Quits: Timvde (~tim@towely.vdeynde.com) (Remote host closed the connection)
[19:51:53] <jonfen> i am trying to wrap my head around dns in compose files, is anyone familar?
[19:57:06] <ada_> jonfen: what problem are you trying to solve
[20:00:21] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[20:02:53] <jonfen> ada_: https://github.com/docker/compose/issues/8441
[20:03:10] <ada_> jonfen: I think I replied to you the other day.  this behaviuor is expected
[20:03:24] <jonfen> i am sorry, i missed the response
[20:03:35] <ada_> jonfen: you should not expect to see your upstream nameservers in /etc/resolv.conf inside the container.  the docker engine uses those addresses as a forwarding nameserver
[20:03:38] <jonfen> why is the default behavior different for docker-compose vs docker?
[20:03:41] <ada_> jonfen: it isn't
[20:03:56] <jonfen> try my example :)
[20:04:02] <ada_> jonfen: the difference is with containers attached to the default 'bridge' network vs user-defined networks
[20:04:08] <ada_> compose implicitly attaches containers to user-defined networks
[20:06:09] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 265 seconds)
[20:06:32] <jonfen> ada_: nslookup <ip_in_question> <dns_server_you_want_to_use>
[20:06:39] <jonfen> i was hoping for something like that
[20:06:47] <ada_> you can still do that
[20:06:55] <ada_> you just won't see your nameservers listed in /etc/resolv.conf
[20:07:23] <ada_> it is expected to see only 127.0.0.11 as your nameserver inside your contianer when attached to a user-defined network.  the docker engine intercepts outgoing dns requests and sends them to your upstream nameservers you specified in your dns setting.
[20:07:31] <ada_> you could still nslookup a specific server, that works just fine
[20:07:54] <jonfen> i am writing a python script, so i was going to do it in python, but https://docs.python.org/3/library/socket.html#socket.gethostbyaddr uses the container's dns -- which isn't complete
[20:08:11] <ada_> just make a dns query
[20:08:17] <ada_> the docker engine will forward it to the right place
[20:08:45] <ada_> you could use 127.0.0.11 or read from /etc/resolv.conf programmatically
[20:12:11] <jonfen> ada_: example?
[20:13:26] <ada_> im not sure what I can provide
[20:13:29] <ada_> I don't have python code ready
[20:13:45] <ada_> if you make a getaddrinfo() call, and it's not a container, docker engine forwards it to your upstream nameserver
[20:14:30] <ada_> but I can tell you that this bug report is not a bug
[20:16:39] *** Joins: divine (~divine@2001:470:8247:1::31)
[20:21:24] *** Joins: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[20:25:45] *** Joins: Bossi_ (~quassel@p4fc2272c.dip0.t-ipconnect.de)
[20:25:54] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[20:40:25] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[20:48:44] *** Quits: ColdKeyboard (~ColdKeybo@user/coldkeyboard) (Ping timeout: 252 seconds)
[21:03:32] *** Quits: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net) (Remote host closed the connection)
[21:04:03] *** Joins: withered_wolf (~withered_@1437135-v103.1561-static.stchilaa.metronetinc.net)
[21:15:38] *** Quits: momomo (~momomo@user/momomo) (Ping timeout: 255 seconds)
[21:16:21] *** Joins: agowa338 (~agowa338@p200300fb0f023f00d0c8a5bd518d59d8.dip0.t-ipconnect.de)
[21:20:03] <jonfen> ada_: fair enough
[21:21:27] *** Joins: hposca (~hposca@node-1w7jr9phoke2v1ekkbsawouqo.ipv6.telus.net)
[21:24:27] *** Quits: finsternis (~X@23.226.237.192) (Remote host closed the connection)
[21:30:56] *** Joins: tang^ (~DoofusCan@2604:3d09:47c:f970:5570:f1f6:be0d:fc27)
[21:46:35] *** Joins: Sasazuka (~Sasazuka@user/sasazuka)
[21:50:55] *** Joins: Windy (~windy@user/windy)
[21:52:11] <Windy> got a weird issue.  i have a compose stack running in Docker Swarm - it's set with restart_policy: condition: none, but for some reason when the nodes are rebooted the service runs again
[21:53:34] <ada_> Windy: can you share a "docker service inspect" for your affected service
[21:53:44] <ada_> and probably show your compose file in the same paste
[21:54:39] <Windy> sure, might take me a couple minutes to sanitize it, but i'll try not to remove the important bits
[22:07:49] <Windy> https://pastebin.com/CmN2taJv
[22:08:23] <Windy> for reference, this app runs periodically, the script executes then dies, so the service is normally present but with 0 containers
[22:09:07] *** Joins: oxum (~oxum@122.181.34.214)
[22:12:21] <Windy> ada_: maybe restart_policy has no effect on that behavior.  seems like it would be best to have the service scale to 0 after execution, but not sure how i'd go about orchestrating that
[22:13:48] *** Quits: oxum (~oxum@122.181.34.214) (Ping timeout: 252 seconds)
[22:19:12] <ada_> Windy: so what I'd probably venture is that this may be a side effect of swarm scheduling
[22:19:24] <ada_> swarm has a concept of tasks, and your service desires 1 task to exist
[22:19:31] <ada_> the container runs and exits, but still occupies a task slot
[22:19:49] <ada_> upon reboot, the container no longer exists, but you still desire 1 task to exist because of your # of replicas specified
[22:19:56] <ada_> so, the engine is going to make sure the task exists
[22:20:10] <ada_> which causes a container to be created and run once and then exit; but not restart because of the restart policy
[22:22:21] <Windy> yeah, ok, that's kind of what I was starting to think.  somehow if i could have it run "docker service scale app=0" after the container exited, and then scale to 1 at the next run that would do it
[22:25:08] <Windy> scaling to 1 at runtime is easy, since we're already using a batch script fired from a Jenkins job
[22:25:35] *** Parts: pertho (~pertho@pertho.net) ()
[22:26:09] *** Joins: Brainium (~brainium@user/brainium)
[22:45:22] *** Quits: agowa338 (~agowa338@p200300fb0f023f00d0c8a5bd518d59d8.dip0.t-ipconnect.de) (Quit: Leaving)
[22:52:03] <Windy> thanks for taking a look though!
[22:52:10] *** Joins: factor2 (~factor@c-66-30-67-217.hsd1.ma.comcast.net)
[22:52:18] *** Parts: sshine_ (~simon@hubris.eta.solutions) ()
[22:53:15] <ada_> for sure
[22:53:59] *** Quits: c10l (~c10l@89.34.167.207) (Read error: Connection reset by peer)
[22:55:36] *** Quits: factor (~factor@c-66-30-67-217.hsd1.ma.comcast.net) (Ping timeout: 252 seconds)
[22:55:36] *** factor2 is now known as factor
[22:56:15] *** Joins: c10l (~c10l@89.34.167.207)
[22:56:23] *** Quits: TomyWork (~TomyLobo@p200300e80f133c000c96cf69ee97f5a9.dip0.t-ipconnect.de) (Quit: Leaving)
[22:58:18] *** Quits: vidbina (~vid@dynamic-077-011-068-015.77.11.pool.telefonica.de) (Ping timeout: 240 seconds)
[23:09:32] *** Joins: beencubed (~beencubed@209.131.238.248)
[23:10:57] *** Quits: rageshkrishna (~rageshkri@136.185.187.57) (Ping timeout: 245 seconds)
[23:16:08] *** Quits: incognito (~relativit@user/incognito) (Ping timeout: 252 seconds)
[23:20:54] *** Quits: aidalgol (~aidalgol@user/aidalgol) (Ping timeout: 252 seconds)
[23:20:54] *** Quits: AgentK (~AgentK@user/agentk) (Ping timeout: 252 seconds)
[23:21:16] *** Quits: chemsmith (~chemsmith@user/chemsmith) (Ping timeout: 252 seconds)
[23:22:18] *** Joins: chemsmith (~chemsmith@user/chemsmith)
[23:24:45] *** Joins: AgentK (~AgentK@user/agentk)
[23:25:42] *** Joins: aidalgol (~aidalgol@user/aidalgol)
[23:33:22] *** Quits: luvalon1 (~luva@178.239.173.200) (Ping timeout: 252 seconds)
[23:33:39] *** Joins: luvalon1 (~luva@178.239.173.200)
[23:34:28] *** Joins: nerdcore (~mike@nerdcore.net)
[23:36:38] <nerdcore> I am curious: How do folks in the Docker community deal with version pinning and security vs stability? My team has been asked to deploy an application from some seemingly outdated Dockerfile with out of date base images pinned by minor version, and I'm unsure whether to edit the Dockerfile and diverge versions or what; Any thoughts?
[23:38:00] <nerdcore> and further to this point, how often do folks typically update base images and relaunch containers to obtain security updates? daily/weekly, or on security announcements, or something else?
[23:41:39] <Windy> i suspect there's 'best practice' and then there's what actually happens
[23:42:12] <nerdcore> sure; I'm open to learning best practice and whats done in the wild :)
[23:42:25] <nerdcore> i gather aa lot of folks just blindly download and run Dockerfiles and this scares me
[23:42:58] <nerdcore> but diverging versions is also against the Docker paradigm as I understand it, so not sure if that's something people typically do or not
[23:43:13] <ada_> nerdcore: how did you manage it in your software projects?
[23:43:51] <nerdcore> ada_: I haven't yet; that's why I'm here asking these questions, so I can choose how to manage it in my software projects. How do YOU manage it in YOUR software projects? :)
[23:44:02] <ada_> I don't develop software
[23:44:39] <nerdcore> well I'm looking at a Dockerfile a client is asking us to deploy, and it seems to be based on out-of-date base images from docker hub; wondering what others might do in such a situation
[23:44:50] <ada_> nerdcore: update the image tag to what's currently available
[23:44:54] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[23:44:57] *** Quits: shokohsc (~shokohsc@161.88.195.77.rev.sfr.net) (Quit: Ping timeout (120 seconds))
[23:45:05] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[23:45:08] *** Joins: jkovac12 (~jkovac1@user/jkovac1)
[23:45:10] <nerdcore> ada_: and version compatibility be damned? :)
[23:45:14] <ada_> i mean, I don't really see it as much different from doing this task in other domains
[23:45:14] <nerdcore> im ok with that hehe
[23:45:23] <ada_> if your version compatibility requires some library version, you have your answer
[23:45:41] *** Joins: shokohsc (~shokohsc@161.88.195.77.rev.sfr.net)
[23:45:43] <ada_> package repos track dependencies, so rely on the package repos to make sure you have the right versions of Things installed
[23:45:50] *** Quits: chemsmith (~chemsmith@user/chemsmith) (Ping timeout: 252 seconds)
[23:46:10] <nerdcore> I see it as different from other domains insofar as Dockerfiles are typically designed to be a complete deployment from the ground up, rather than something like a source code compile where you add one piece of software to an existing system
[23:46:13] *** Quits: jkovac1 (~jkovac1@user/jkovac1) (Read error: Connection reset by peer)
[23:46:14] *** jkovac12 is now known as jkovac1
[23:46:20] *** Joins: thiras (~thiras@user/thiras)
[23:46:23] *** Joins: chemsmith (~chemsmith@user/chemsmith)
[23:46:23] <ada_> it's not a complete deployment
[23:46:27] <ada_> it's just a fancy way to run A Process
[23:46:49] <ada_> so ... how are you making sure you have modern versions of process that you use right now?
[23:47:03] <ada_> generally, you rely on your package manger to update things for you, and it doest teh dependency management
[23:47:04] <nerdcore> we use Ubuntu and run apt updates daily
[23:47:27] *** Quits: thiras (~thiras@user/thiras) (Remote host closed the connection)
[23:47:32] <ada_> some people use tools like watchtower to check dockerhub and pull new versions of the images they use
[23:47:42] <ada_> but its still up to you to re-build your images with the new sources
[23:47:44] <nerdcore> that is helpful thanks
[23:47:48] <ada_> most people are going to handle this in a CI/CD process
[23:49:08] <ada_> I would not suggest using the :latest tag, because you don't really get much control over what gets delivered to you
[23:49:36] <ada_> but yeah, I approach this the way I approach other application updates.  If I'm running mysql, and a new version comes out, if it's not a security release I'm probably not going to upgrade unless it has some killer feature I require
[23:50:16] <ada_> so, in my dockerfile, I might install mysql 5.foo.bar and until I get an announcement that 5.foo.baz comes out with a CVE update, then I just leave it alone
[23:51:00] <nerdcore> right of course. We are looking at a couple different situations such as a home-grown docker container we might develop from scratch and how we will manage that, but separately how we should manage the security implications when a client asks us to deploy some 3rd party Dockerfile into their environment
[23:51:58] <nerdcore> ada_: how do you find out about new MySQL? Can you monitor the Docker Hub page for the mysql base image in an automated/programmatic fashion?
[23:52:31] <nerdcore> or is there some sort of `docker check-for-base-image-updates` style command?
[23:53:36] *** Quits: iota_pi (uid100059@tooting.irccloud.com) (Quit: Connection closed for inactivity)
[23:55:40] <ada_> you can query the dockerhub API
[23:55:49] <ada_> "newer" is a hard question to answer though
[23:56:03] <ada_> like, if I push 5.4.1, then I push 4.7.8, which is considered the "newest" image?
[23:56:19] <ada_> so you'd have to figure out how th author numbers their releases, and then go from there
[23:57:08] *** Joins: thiras (~thiras@user/thiras)
[23:57:14] <nerdcore> makes sense thanks
[23:57:15] <programmerq> nerdcoreâ–¸ I just wrote this SO answer the other day which is largely similar: https://stackoverflow.com/questions/68284582/how-to-find-availability-of-newer-versions-of-docker-images-in-docker-compose/68488168#68488168
[23:57:36] *** Quits: dodo (~dodo@user/dodo) (Quit: dodo)
[23:57:46] <programmerq> tldr; docker images themselves don't provide any sort of versioning per se. individual image authors may choose to use a tag naming scheme that corresponds with semantic versioning, but there's no guarantee.
[23:59:01] *** Joins: dodo (~dodo@user/dodo)
[23:59:52] <programmerq> there *is* https://github.com/containrrr/watchtower which will look at the local image tag and if there's a new one on the registry, it'll try to recreate the container with the same params but the copy of the image on the repository. Useful for running tags like :mainline or :latest that image authors regularly overwrite as updates are available.
