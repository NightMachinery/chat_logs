[00:28:08] *** Quits: cocomo (~cocomo@111.119.188.27) (Quit: leaving)
[00:38:40] <Batzy> in julia am I able to do an operation on a vector which i want to remove mutable where i get that vector back but without a certain element
[00:39:06] <Batzy> so i want a  function which passes in a vector and an element, returns the vector without the element without mutating the original vector
[00:40:05] <mikko> sure, the methods that don't end with ! do not mutate the original
[00:40:32] <Batzy> WHY IS JULIA SO GOOD
[00:40:34] <mikko> eg. filter!() mutates but filter() does not
[00:40:59] <Batzy> man, my advisor really wants to use cython instead
[00:41:03] <Batzy> but julia seems so perfect
[00:41:58] <Batzy> mikko: so how can i delete element from vector? deleteat seems to only work with !
[00:44:04] <mikko> oh you want to remove by index, i'm not sure if there's a method for that.. i would probably use list comprehension
[00:44:26] <Batzy> filter works actually
[00:44:39] <Batzy> in my case index==value
[01:02:06] <Batzy> ugh i wish there was modular matrix inversion
[01:18:15] *** Quits: lunik1 (~lunik1@37.205.14.203) (Quit: Ping timeout (120 seconds))
[01:18:30] *** Joins: lunik1 (~lunik1@37.205.14.203)
[01:22:47] <Batzy> im trying to get abstract types working too
[01:24:30] *** Quits: lunik1 (~lunik1@37.205.14.203) (Quit: Ping timeout (120 seconds))
[01:24:44] *** Joins: lunik1 (~lunik1@37.205.14.203)
[01:32:58] <Batzy> why is mod of a negative number returning a negative number?
[01:37:34] <Batzy> ok the mod() function works but i have to use a map to do it elementwise
[01:37:34] <Batzy> whatever
[01:38:25] <Batzy> okay so how am i able to pass in a matrix with abstract type?
[01:38:30] <Batzy> as a function arg?
[01:38:40] <Batzy> I want both UInt128 and Int to work
[01:41:03] *** Quits: lunik1 (~lunik1@37.205.14.203) (Quit: Ping timeout (120 seconds))
[01:41:18] *** Joins: lunik1 (~lunik1@37.205.14.203)
[01:54:07] <Batzy> https://bpa.st/MOAQ does anyone see why this works for an Int64 matrix but the same matrix casted to a UInt128 is outputting the wrong answer/
[01:54:16] <Batzy> it's gaussian elimination for modular matrix inverse
[01:57:34] <Batzy> yeah casting back and forth for UInt128 doesn't seem to be working lol
[01:58:24] <Batzy> halp
[01:59:42] <Batzy> ok julia is actually doing some really weird shit
[02:06:16] <Batzy> for some reason typecasting is randomly mutating my variables too?
[02:16:36] <Batzy> im just goign to do another algorithm i think
[02:18:51] <energizer> cast?
[02:21:53] <Batzy> yeah like a type cast
[02:22:07] <Batzy> julia was just doing extremely weird shit so im trying a different algorithm now
[02:22:15] <Batzy> of course in this algorithm, i need to find cofactor matrices
[02:22:19] <Batzy> i have no idea how to do that
[02:23:34] <Batzy> eh maybe i do
[02:38:37] <Batzy> i feel like there was a slick syntax in julia to skip a value in a range
[02:38:40] <Batzy> but i cant find it
[02:41:13] <dTal> you mean like 1:2:10?
[02:42:12] <Batzy> dTal: but that goes 1 3 5 etc
[02:42:21] <Batzy> i want like 1 3 4 5 6 7
[03:05:46] <Batzy> no?
[03:07:49] <Batzy> ok i got it with filters
[03:16:26] <Batzy> so julia seems incapable of doing modular matrix inverses
[03:16:26] <Batzy> lol
[03:16:34] <Batzy> correct algorithms just dont work for some reason
[03:23:39] <Batzy> https://bpa.st/OKRQ anyone see anything wrong with this? im testing it on a matrix thats known to be invertible
[03:45:50] *** Quits: kmh (~kmh@2a00:6020:5004:6800:4be6:856d:7108:d738) (Quit: Leaving)
[04:00:50] *** Joins: kmh (~kmh@2a00:6020:5004:6800:d56f:5ec4:ddd1:e31d)
[04:27:11] *** Quits: kmh (~kmh@2a00:6020:5004:6800:d56f:5ec4:ddd1:e31d) (Remote host closed the connection)
[04:27:22] *** Joins: kmh (~kmh@2a00:6020:5004:6800:d56f:5ec4:ddd1:e31d)
[04:43:46] <adigitoleo> Batzy: 1) consider ::AbstractMatrix dispatch, 2) are modulus and cofactor globals? I can't run the function
[04:44:29] <Batzy> adigitoleo: they are.. well cofactor is a function
[04:44:42] <Batzy> why abstract matrix, why dispatch?
[04:44:50] <Batzy> also what is dispatch
[04:45:07] <adigitoleo> you asked about how to dispatch on a more general type, I'm saying there's AbstractMatrix for that
[04:46:35] <adigitoleo> anyway more important to get it working, is it possible to share the cofactor and modulus definitions as well?
[04:46:46] <Batzy> well i just mean the entries of the matrix could have 2 different types
[04:46:54] <Batzy> modulus is just some constant  value
[04:47:11] <Batzy> adigitoleo: well you think the algorithm is doable right
[04:47:17] <Batzy> like this can be used to find modular matrix inverse
[04:47:33] <adigitoleo> for the entries, you could use Integer type if you want something that covers both UInts and Ints
[04:47:43] <Batzy> im on a question to show my advisor julia >> cython
[04:47:55] <Batzy> I have UInt128
[04:47:55] <adigitoleo> should be doable yes
[04:48:00] <Batzy> ok let me share
[04:49:15] <adigitoleo> re: UInt128, see for yourself: UInt128 <: Integer (true) and Int <: Integer (true)
[04:49:37] <Batzy> https://bpa.st/UP5Q
[04:49:53] <Batzy> I was using Int
[04:49:53] <Batzy> :)
[04:49:55] <Batzy> oop
[04:50:23] <Batzy> do i just type like function(a::Matrix{T<:Integer}) then?
[04:50:25] <Batzy> in the definition?
[04:52:10] <Batzy> Frankly i have no idea how to do that but w.e
[04:53:10] <Batzy> adigitoleo: for the matrix a = [47574334 86189764; 89731733 324253445] my inverse's first entry is 298639 which is clearly wrong
[04:53:23] <adigitoleo> I think it's by adding where T<:Integer at the end
[04:53:51] <Batzy> oh i see
[04:54:08] <Batzy> perfect
[04:54:48] <Batzy> mod and invmod i think return Int64s
[04:54:57] <Batzy> so idk if that's fucking with anything
[04:55:07] <Batzy> either way im testing with int64's
[04:56:09] <Batzy> im a little mad though because this should be working
[04:56:33] <Batzy> im also a little mad ill have to write a bunch of converts
[04:58:00] <adigitoleo> you're 1 / det(a) is super small
[04:58:08] <adigitoleo> s/you're/your
[04:58:26] <Batzy> well im not doing 1/det(a)
[04:58:37] <Batzy> it's inverse of det(a) mod whatever
[04:58:43] <Batzy> im multiplying by that
[04:58:45] <adigitoleo> I'm asking if it is for sure invertible
[04:58:50] <Batzy> it is
[04:58:55] <Batzy> im getting it from sympy 
[04:58:58] <Batzy> so i know the inverse
[04:59:31] <Batzy> let me check if it's computing the det inverse correctly
[05:01:23] <Batzy> adigitoleo: yes it's getting that correctly
[05:04:07] <Batzy> the inverse's first entry should be 860016
[05:04:36] <Batzy> also apparently in julia type casting changes values
[05:04:37] <Batzy> lmao
[05:05:26] <Batzy> because that last map actually produces something inequivalent
[05:05:27] <Batzy> lol
[05:06:03] <Batzy> terrific, wow im really infuriated by that actually
[05:06:20] <Batzy> ok no i think it's fine actually
[05:08:18] <Batzy> adigitoleo: any ideas?
[05:08:39] <adigitoleo> I feel like convert is not the way to go, I haven't had to use other numeric types though so I'm not sure
[05:11:57] <Batzy> what else am i supposed to do
[05:12:00] <Batzy> julia doc says to do this
[05:12:57] <Batzy> you think that's what's messing it up?
[05:17:26] <Batzy> julia feels oddly unreliable
[05:17:35] <adigitoleo> I think you should promote evereything to BigInt upfront
[05:17:51] <adigitoleo> convert(Matrix{Bigint}, a)
[05:18:08] <Batzy> ok
[05:18:11] <Batzy> the rest seems fine?
[05:18:28] <adigitoleo> I'll play with it some more, it's an interesting problem that I haven't encountered yet honestly
[05:18:43] <adigitoleo> still pretty new to Julia myself tbh
[05:18:49] <Batzy> adigitoleo: the issue is i still need convert
[05:18:51] <Batzy> for determinant
[05:19:41] <Batzy> still the wrong answer!
[05:19:45] <Batzy> the same exact wrong answer!
[05:19:47] <Batzy> bah!!!!!!
[05:21:16] <Batzy> i tried writing an algorithm with gaussian elimination
[05:21:19] <Batzy> again just fucking weird issues
[05:21:24] <Batzy> numbers are being randomly wrong
[05:32:33] <adigitoleo> I got a different answer using BigInts, will send a snippet soon
[05:46:20] <adigitoleo> https://x0.at/aHIx.txt
[05:46:58] <adigitoleo> https://x0.at/7wDo.txt
[05:49:00] <adigitoleo> Batzy: It's still not the answer you wanted I think? but using https://x0.at/aHIx.txt I get https://x0.at/7wDo.txt
[05:49:32] <adigitoleo> got to run for lunch, but try the function with a simpler matrix maybe
[06:45:49] <Batzy> adigitoleo: yeah that's not that inverse
[06:47:00] <adigitoleo> btw there's a typo line 9, s/A/a
[06:47:42] <Batzy> that's ok
[06:47:46] <Batzy> i dont think it's a type conversion issue
[06:47:50] <Batzy> maybe the algorithm isnt doing something wrong
[06:49:43] <Batzy> really makes me lose confidence in julia
[06:50:23] <adigitoleo> does numpy have modular matrix inverse?
[06:51:15] <Batzy> no actually
[06:51:17] <Batzy> im using sympy though
[06:51:26] <Batzy> i know it's not mistaken
[06:51:30] <Batzy> checked against wolfram and stuff
[06:52:54] <Batzy> adigitoleo: http://devdoc.net/python/sympy-1.0/_modules/sympy/matrices/matrices.html
[06:52:57] <Batzy> soruce for inv_mod is here
[06:53:45] <Batzy> maybe ill use what they do
[06:54:23] <adigitoleo> yeah that's what I was thinking
[06:54:47] <Batzy> they take
[06:55:08] <Batzy> det(A)^(phi(m)-1) mod m
[06:55:24] <Batzy> what is that? fermat's little?
[06:55:32] <Batzy> oh no that's an euler theorem isn't it
[06:56:54] <adigitoleo> yes to the second one, can't recall exactly what tho
[06:57:04] <Batzy> yeah
[06:57:07] <Batzy> the theorem is like
[06:57:12] <Batzy> a^phi(m) = 1 mod m
[06:57:27] <Batzy> okay, yeah i actually dont think i need this
[06:57:32] <Batzy> all my moduli are of the form p^k
[06:58:55] <Batzy> i also have inv_mod
[06:58:56] <Batzy> invmod?
[07:01:15] <adigitoleo> btw the function I sent can reproduce their doctest examples
[07:01:52] <adigitoleo> so I still think it's something fishy going on with overflow maybe
[07:03:33] <adigitoleo> anyway I guess best to verify what you need
[07:04:13] <adigitoleo> happy to help out if I can
[07:07:44] <Batzy> i just wrote up what they have
[07:07:45] <Batzy> still
[07:07:46] <Batzy> wrong
[07:07:47] <Batzy> answers
[07:07:51] <Batzy> same one as always
[07:07:53] <Batzy> im going to kill myself
[07:13:18] <Batzy> adigitoleo: yeah it's an overflow
[07:13:25] <Batzy> if i change to UInt128s suddenly i get different answers
[07:15:14] <adigitoleo> strange, I thought BigInt was for arbitrary size, I guess it's worth tracking down where things diverge?
[07:43:35] *** Quits: Sofia (~sofia@user/sofia) (Ping timeout: 240 seconds)
[07:56:21] *** Joins: Sofia (~sofia@user/sofia)
[08:01:40] <adigitoleo> Batzy: wait, doing M' in julia is adjoint not adjugate
[08:01:55] <adigitoleo> we want adjugate though right
[08:02:36] <adigitoleo> Oh nvm that's what you have cofactor func for
[08:15:30] <adigitoleo> Batzy: I don't understand why the use adjugate here: https://github.com/sympy/sympy/blob/master/sympy/matrices/inverse.py#L180
[08:15:57] <adigitoleo> they've already done the determinant dance, surely it's meant to be jsut adjoint
[08:17:53] <adigitoleo> that's aside from the pointless list comprehension
[08:27:57] <adigitoleo> Batzy: not sure what you checked in wolfram, maybe this isn't the inverse you're looking for: https://x0.at/RuFv.txt
[08:36:04] <Batzy> the list comprehension isn't pointless
[08:36:13] <Batzy> they're doing element-wise modulus
[08:36:27] <adigitoleo> should already broadcast tho no?
[08:36:37] <adigitoleo> isn't it a numpy array under the hood
[08:37:15] <Batzy> adigitoleo: im not sure, but i dont think so
[08:37:29] <Batzy> adigitoleo: also are you accusing sympy of doing matrix inverses incorrectly?
[08:37:31] <Batzy> in inv_mod?
[08:37:47] <Batzy> we can do a few tests :)
[08:37:49] <Batzy> with wolfram
[08:38:03] <Batzy> adigitoleo: p*A + I will always be invertible mod p^k
[08:38:06] <Batzy> so you can use that
[08:39:14] <Batzy> adigitoleo: if you're right you should be very pleased with yourself btw
[08:39:17] <Batzy> im about to check
[08:40:21] <Batzy> hm wolfram is giving 
[08:40:23] <Batzy> not good answers
[08:43:50] <Batzy> adigitoleo: dude wolfram doesnt do modular inverses correctly :\
[08:44:06] <Batzy> i think wolfram finds the inverse first, then does a reducton mod n but that simply won't work
[08:45:08] <Batzy> let me try one of their widgets
[08:45:11] <adigitoleo> yeah it doesn't seem like a super well-known operation, still odd that they didn't get someone to check it
[08:46:09] <Batzy> adigitoleo: ok i did it on wolfram
[08:46:17] <Batzy> sympy gives the transpose of wolfram's answer
[08:46:18] <Sofia> Hello world. I'm interested in optimizing compilers and have noted Julia is very competitive, especially with the metaprogramming based optimizations. Is there a list of these such tools?
[08:50:20] <Batzy> adigitoleo: actually i think it still works oddly enough?
[08:50:41] <Batzy> for the matrices im using i think it's equivalent
[08:50:47] <Batzy> either way julia is completely getting it wrong
[08:53:25] <Batzy> adigitoleo: casting it all to big int fixes it
[08:53:26] <Batzy> lol
[08:54:17] <adigitoleo> Batzy: and it agrees with sympy now?
[08:54:37] <Batzy> adigitoleo: yes, i checked wolframs answer and the wolfram answer is actually wrong
[08:54:47] <Batzy> so the wolfram answer is the transpose of the the right answer
[08:54:54] <Batzy> what a rollercoaster
[08:55:02] <Batzy> adigitoleo: ill try it with some bigger numbers
[08:55:05] <adigitoleo> could you send the julia snippet, I'm still confused why sympy is using adjugate
[08:55:56] <Batzy> it works!
[08:56:05] <Batzy> adigitoleo: adjugate matrix is fine
[08:56:34] <Batzy> adigitoleo: im doing adjugate too, but i take the cofactor matrix at a different point
[08:57:52] <Batzy> adigitoleo: https://bpa.st/IJPQ
[08:57:58] <Batzy> cofactor code hasn't changed at all
[09:12:10] <adigitoleo> Batzy: Oh, it is actually adjugate, OK, got confused there. adjoint is just wrong
[09:12:45] <adigitoleo> Batzy: But their list comprehension is still pointless, haha, the modulo definitely broadcasts
[09:52:55] <Batzy> im not sure what you mean
[09:53:22] <ninjin> f(x) == [f(y) in x]
[09:53:30] <ninjin> Is my guess.
[11:02:20] <adigitoleo> Batzy: does this work for the larger number as well? It's probably how I'd do it: https://x0.at/11UO.txt
[11:40:14] *** Quits: Batzy (~quassel@user/batzy) (Quit: https://quassel-irc.org - Chat comfortably. Anywhere.)
[11:41:17] *** Joins: Batzy (~quassel@user/batzy)
[12:17:31] <mikko> 00:02 <Batzy> why is mod of a negative number returning a negative number?
[12:17:38] <mikko> that is unfortunately something that varies between languages
[12:18:30] <mikko> (-5) % 3 is 1 in haskell and python, but -2 in C and julia
[12:19:25] <mikko> although julia's rem() function takes a rounding parameter that you can use
[12:20:25] <mikko> rem(-5, 3, RoundDown) == 1
[12:25:47] <ninjin> Sofia: Not sure if there is a good list of tools out there as I am not really in the same area. What I do know is that such tools do exist.
[12:26:32] <ninjin> What I am most familiar with is the GPU codegen, but only superficially.
[12:27:32] <ninjin> I do know that there is plenty of low-hanging fruit on the Julia compiler itself though and that they are understaffed. So patches would most likely be welcomed with open arms.
[12:33:02] <Sofia> I'm actually researching towards making my own optimizing compiler, but I will be using Julia as one of the benchmark targets. Alongside Rust and Lean 4. Either way I might find some contributions to make to Julia. If my project is successful, it could potentially be an LLVM alternative.
[12:33:24] <Sofia> I mean for Julia to use as a backend.
[12:35:05] <mikko> you mean for compiling from LLVM IR into native code? iirc julia uses fairly standard llvm optimizations, i haven't looked into it personally though
[12:35:06] <Sofia> ninjin: Speaking of, I noticed JuliaGPU has Nvidia, AMD, and oneAPI. I was expecting to see Vulkan. Is the GPUArray compiling to SPIR-V for the GPU targets? I have seen some instances of SPIR-V mention but don't know what is active or relevant for each of these targets.
[12:35:56] <Sofia> mikko: I mean as an alternative to LLVM, as in, not using its IR. IIUC Julia has its own IR which it translates to LLVM-IR. If my project is successful, it could be another target for Julia's IR.
[12:36:38] <ninjin> Sofia: I have only really looked at it for Nvidia, sadly. But the people over in #GPU are *very* friendly. Sadly that requires jumping onto Slack where there are orders of magnitute more people than on IRC.
[12:38:09] <adigitoleo> Sofia: This is way out of my area, but you might be interested in https://github.com/JuliaLabs/Cassette.jl
[12:41:46] <ninjin> This is the coolest stuff I have been involved in that involved the compiler: https://github.com/FluxML/Hydra.jl
[12:42:13] <Sofia> adigitoleo: Huh, that "tagging" sounds a lot like taint tracking. I read that right?
[12:42:16] <ninjin> If only I had more competent MSc/BSc students with compiler experience in the pipeline. >.<
[12:42:34] <Sofia> ninjin: #GPU on slack or #GPU in here is bridged to slack?
[12:42:46] <ninjin> No bridge sadly Sofia.
[12:42:51] * Sofia nods
[12:42:56] <ninjin> Not even for this channel.
[12:43:13] <Sofia> I don't want slack, but may eventually set it up. Thanks for the ref.
[12:43:42] <ninjin> Yeah, Julia is the only Slack I use. There just is no other choice really.
[12:47:49] <Sofia> Hydra is yet another tiny macro (for the user) doing a tiny subset of Halide. I'd be REALLY nice to see something more like Halide applied to Julia.
[12:48:21] <Sofia> https://halide-lang.org/
[12:49:00] <Sofia> Alongside Taichi, DiffTaichi and QuanTaichi and while I'm dropping names, Herbie and Pherbie. All very nice tools. :)
[12:49:17] <Sofia> Though I think Julia might have the derivatives under control.
[12:50:51] <Sofia> Halide separates computation from scheduling. Taichi separates computation from representation, promoting sparse representations while your computations are specified "against" dense arrays. Both may be carefully tuned with a tiny bit of code. 
[12:53:27] <Sofia> Metatheory.jl uses E-graphs for optimizations. My compiler has E-graphs at its core, using rewrite rules synthesized using Ruler from an evaluator and syntax generator. https://github.com/JuliaSymbolics/Metatheory.jl 
[12:53:55] <Sofia> I am only targeting RISC-V for now. Will also target WebAssembly and likely also SPIR-V. 
[12:54:47] <Sofia> s/I'd/It'd/
[12:57:16] <Sofia> ninjin: Any chance you're familiar with Julia's -> SPIR-V magic?
[12:59:47] <energizer> Sofia: i assume you're aware of LoopVectorization.jl and Tullio.jl?
[13:00:19] <Sofia> Yes and no. Or maybe, might ring a bell but I don't have it in my list.
[13:01:48] <ninjin> Sadly no Sofia, I am familiar with things on a very high level as an AI researcher with a little bit on an engineering bend.
[13:01:58] <Sofia> For context, I don't use Julia yet. I just recognize it as the strongest competition for benchmarking against.
[13:03:06] <energizer> what is Ruler?
[13:03:26] <Sofia> Alright. IIUC GPUArray.jl is the key detail there, along with the specific target's integrations.
[13:03:43] <Sofia> energizer: https://arxiv.org/abs/2108.10436 + https://github.com/uwplse/ruler
[13:04:53] <energizer> cool
[13:06:20] <energizer> also look at the JuliaFolds github org
[13:07:22] <Sofia> My language will be most like Lean 4, using a richer type theory based on GrTT and multi-stage metaprogramming. All surface syntax mapped to the core calculus directly through user-defined syntax constrained by the profunctor laws. This is necessary and sufficient for tools written for the core calculus to become tools for the surface language for free without the risk of abstractions leaking. See
[13:07:24] <Sofia> Justin Pombro's 'syntax resugaring' for more on that if interested.
[13:08:24] <Sofia> Folds, nice. Thanks.
[13:18:58] <adigitoleo> Sofia: sadly I don't know much about taints or any of this. Your work sounds super cool though. Probably not your focus but as a science guy I'm always on the lookout for not only the array stuff but also related visualisation backends, e.g. https://github.com/datoviz/datoviz
[13:27:07] <energizer> adigitoleo: what do you think of makie 
[13:27:21] <energizer> er pressed enter too early
[13:28:56] <energizer> adigitoleo: what do you think of makie? it seems very impressive currently https://lazarusa.github.io/BeautifulMakie/ but i'm worried about how it will fare without thread safety
[13:30:12] <adigitoleo> I have tried makie very briefly, but it was missing some things I needed at the time (cartography) so I haven't checked back in a while
[13:30:23] <energizer> it has cartography now
[13:30:49] <energizer> well i guess it depends what that means but there is GeoMakie
[13:34:01] <adigitoleo> interesting, I vaguely remember trying it but I'll have to check again. I definitely hope that makie can overtake the other plotting patchwork
[13:36:17] <energizer> actually just noticed the roadmap has "late 2022?: redesigned internal architecture for multithreading and distributed environments (still a work-in-progress)"
[13:37:24] <energizer> (that's datoviz)
[13:38:51] <adigitoleo> yeah I've got my eye on it because 1) Vulkan and 2) I can't stand VTK lol
[13:39:25] <adigitoleo> even though I've only just entered into HPC stuff recently, from the start it grated with me
[13:45:38] * Sofia managed to misread visualization as virtualization and had to re-read all that :P
[13:47:59] <Sofia> adigitoleo: Visualizations are certainly in scope. My motivating domains are cryptography and graphics. Both are really interesting optimization targets. Cryptography cares about information flow and leakage through side channels. Graphics is forgiving with approximations and frequently involves smooth continuious functions which can be approximated using anytime algorithms. 
[13:50:58] <Sofia> So at least when rendering implicit surfaces, I have my own N-dimensional rendering strategy in mind I'd like to implement and benchmark. So it would all at least be useful for benchmarking against and API design and coverage if I generalize the visualization goal.
[13:51:58] <Sofia> I've saved all the links. Thanks everyone. 
[13:52:16] * Sofia shall call the night. Happy hacking. o/
[14:13:58] *** Joins: furrymcgee (~devuan@cgn-213-196-210-225.nc.de)
[15:32:02] *** Quits: adigitoleo (~adigitole@115.69.38.75) (Quit: adigitoleo)
[15:33:15] *** Joins: adigitoleo (~adigitole@115.69.38.75)
[17:08:29] *** Joins: MajorBiscuit (~MajorBisc@2a02:a461:129d:1:193d:75d8:745d:e91e)
[17:09:12] *** Quits: MajorBiscuit (~MajorBisc@2a02:a461:129d:1:193d:75d8:745d:e91e) (Client Quit)
[17:18:10] *** Joins: aaii (~aaii@user/aaii)
[17:19:36] *** Joins: MajorBiscuit (~MajorBisc@c-001-031-019.client.tudelft.eduvpn.nl)
[17:21:17] *** Joins: aaii_ (~aaii@user/aaii)
[17:23:57] *** Quits: aaii (~aaii@user/aaii) (Ping timeout: 252 seconds)
[17:27:08] *** Quits: aaii_ (~aaii@user/aaii) (Ping timeout: 268 seconds)
[17:32:35] *** Quits: MajorBiscuit (~MajorBisc@c-001-031-019.client.tudelft.eduvpn.nl) (Quit: WeeChat 3.4)
[17:37:07] *** Joins: Major_Biscuit (~MajorBisc@c-001-024-004.client.tudelft.eduvpn.nl)
[17:37:39] *** Quits: Major_Biscuit (~MajorBisc@c-001-024-004.client.tudelft.eduvpn.nl) (Client Quit)
[17:37:49] *** Joins: Major_Biscuit (~MajorBisc@c-001-024-004.client.tudelft.eduvpn.nl)
[19:55:15] *** Quits: Sofia (~sofia@user/sofia) (Ping timeout: 240 seconds)
[20:10:05] *** Joins: Sofia (~sofia@user/sofia)
[21:55:25] *** Quits: notzmv (~zmv@user/notzmv) (Ping timeout: 240 seconds)
[23:18:35] *** Quits: Sofia (~sofia@user/sofia) (Ping timeout: 240 seconds)
[23:33:49] *** Joins: Sofia (~sofia@user/sofia)
