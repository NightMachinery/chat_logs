[00:07:49] *** Quits: gh34 (~textual@cpe-184-58-181-106.wi.res.rr.com) (Quit: Textual IRC Client: www.textualapp.com)
[01:23:26] *** Joins: BSaboia (~bsaboia@177.37.198.11)
[01:26:43] *** Quits: BSaboia (~bsaboia@177.37.198.11) (Client Quit)
[01:36:49] *** Joins: BSaboia (~bsaboia@177.37.198.11)
[01:43:46] *** Quits: BSaboia (~bsaboia@177.37.198.11) (Quit: This computer has gone to sleep)
[01:52:15] *** Joins: gschanuel4 (~gschanuel@user/gschanuel)
[01:55:12] *** Joins: smeeagle (~smeeagle@2a00:23c8:9611:e600:e84c:31fa:24ac:a92f)
[01:56:12] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Ping timeout: 268 seconds)
[01:56:13] *** gschanuel4 is now known as gschanuel
[02:09:28] *** Quits: smeeagle (~smeeagle@2a00:23c8:9611:e600:e84c:31fa:24ac:a92f) (Quit: smeeagle)
[03:00:34] *** Quits: umask077 (~Thunderbi@pool-108-54-173-91.nycmny.fios.verizon.net) (Quit: umask077)
[03:00:58] *** Joins: umask077 (~Thunderbi@pool-108-54-173-91.nycmny.fios.verizon.net)
[03:25:35] *** Quits: hiredman (~hiredman@frontier1.downey.family) (Ping timeout: 264 seconds)
[04:06:49] *** Joins: hiredman (~hiredman@frontier1.downey.family)
[07:25:24] *** Joins: gschanuel5 (~gschanuel@user/gschanuel)
[07:28:52] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Ping timeout: 256 seconds)
[07:28:52] *** gschanuel5 is now known as gschanuel
[08:02:34] *** Joins: mbuf (~Shakthi@122.174.170.222)
[08:36:26] *** Quits: mbuf (~Shakthi@122.174.170.222) (Quit: Leaving)
[09:30:18] *** Joins: mbuf (~Shakthi@122.174.170.222)
[10:25:06] *** Joins: smeeagle (~smeeagle@2a00:23c8:9611:e600:f867:24e8:4d46:7059)
[10:56:03] *** Joins: gschanuel5 (~gschanuel@user/gschanuel)
[11:00:00] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Ping timeout: 260 seconds)
[11:00:00] *** gschanuel5 is now known as gschanuel
[13:03:05] *** Joins: dirtwash (~dirtwash@2a01:4f8:c010:b7e6::1)
[13:03:45] <dirtwash> hi, i had this issue before that i run out of shards. but i dont understand why...it barely uses 10gb of data and all tutorials say I should add more data nodes? so I need to run multiple ES instances on the same machine or what?
[13:03:45] *** Joins: Tenchi[m] (~tenchim]@2001:470:69fc:105::f42)
[13:03:45] *** Joins: Eryn83FL_Matrix[ (~eryn-1983@2001:470:69fc:105::7c74)
[13:03:45] *** Joins: DX099 (~dx099@2001:470:69fc:105::21ce)
[13:03:55] <dirtwash> I fail to see how that makes sense if its using so little storage
[13:04:04] *** Quits: dirtwash (~dirtwash@2a01:4f8:c010:b7e6::1) (Changing host)
[13:04:04] *** Joins: dirtwash (~dirtwash@user/dirtwash)
[13:10:56] <beorn_> "run out of shards" how?
[13:14:06] <dirtwash> beorn_: it said im at 999/1000
[13:14:22] <beorn_> Who is "it"?
[13:14:25] <dirtwash> no idea how..im adding data contiously, but its very little data
[13:14:36] <dirtwash> plain simple logging of nginx logs, 1 index per day
[13:14:51] <dirtwash> so I dont understand it either
[13:15:04] <beorn_> Pastebin the exact error message please.
[13:16:04] <dirtwash> [validation_exception]: Validation Failed: 1: this action would add [2] shards, but this cluster currently has [999]/[1000] maximum normal shards open
[13:16:25] <beorn_> Looks like you should take a look at https://discuss.elastic.co/t/how-to-increase-the-shards-limit-in-elasticsearch-7-1/204865 
[13:16:55] <dirtwash> yea i already increased the limit but its a workaround, everybody says its not good too many too many shards per node
[13:17:02] <dirtwash> but I dont understand how 3GB of data is using 1000 shards
[13:17:13] <beorn_> The amount of data is irrelevant
[13:17:32] <beorn_> If you have 5 shards per index, and 1 replica, every index will create 10 shards.
[13:17:33] <dirtwash> well how does it correlate
[13:17:43] <dirtwash> I see
[13:17:52] <dirtwash> i have whatever the defaults are, i didnt touch any settings
[13:17:55] <beorn_> So even if you have 1 log entry on a given day, it will still spin off 10 shards.
[13:18:16] <dirtwash> so what do I do, 1 shard per index and 0 replica?
[13:18:59] <beorn_> You could 1) merge older indices into fewer shards, 2) reindex into accumulating indices (e.g. monthly), or 3) delete old indices.
[13:19:18] <beorn_> Look into the "shrink" mechanism provided with Elasticsearch.
[13:19:25] <dirtwash> found this https://opster.com/guides/elasticsearch/data-structuring/elasticsearch-reduce-shards/
[13:19:27] <dirtwash> will read
[13:19:30] <dirtwash> beorn_:thank you
[13:19:40] <beorn_> No problem :-)
[13:20:42] <beorn_> In one of my environments, I shrink indices after a few days, and then accumulate those into a monthly index after a few more days.
[13:21:08] <beorn_> And finally delete the monthly indices after a year or so.
[13:21:08] <dirtwash> is removing replica a bad idea? its a single node and not important data
[13:21:19] <beorn_> You don't need replicas on a single node
[13:21:26] <dirtwash> good i just disabled them
[13:23:48] *** Quits: smeeagle (~smeeagle@2a00:23c8:9611:e600:f867:24e8:4d46:7059) (Quit: smeeagle)
[14:19:09] *** Joins: hqdruxn08_ (~hqdruxn08@cpe-66-68-166-214.austin.res.rr.com)
[14:23:37] *** Quits: hqdruxn08__ (~hqdruxn08@cpe-66-68-166-214.austin.res.rr.com) (Ping timeout: 268 seconds)
[14:30:12] *** Joins: BSaboia (~bsaboia@177.37.198.11)
[15:22:37] *** Quits: kelo_ (~atanyukev@mx7.olek.im) (Quit: leaving)
[15:22:52] *** Joins: kelo (~atanyukev@mx7.olek.im)
[15:56:55] *** patstoms_ is now known as patstoms
[16:00:15] *** Joins: esro (~esro@nat1.krakowskiinternet.pl)
[18:10:54] *** Quits: Bossi (~quassel@p5dc59f87.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[18:11:54] *** Quits: esro (~esro@nat1.krakowskiinternet.pl) (Quit: Leaving)
[18:28:28] *** Quits: BSaboia (~bsaboia@177.37.198.11) (Quit: This computer has gone to sleep)
[18:34:20] *** Joins: BSaboia (~bsaboia@177.37.198.11)
[18:35:10] *** Joins: gschanuel0 (~gschanuel@user/gschanuel)
[18:38:44] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Ping timeout: 260 seconds)
[18:38:44] *** gschanuel0 is now known as gschanuel
[18:47:20] *** Joins: rsx (~dummy@ppp-188-174-135-237.dynamic.mnet-online.de)
[19:03:29] *** Quits: BSaboia (~bsaboia@177.37.198.11) (Quit: This computer has gone to sleep)
[19:13:02] *** Quits: rsx (~dummy@ppp-188-174-135-237.dynamic.mnet-online.de) (Quit: rsx)
[19:20:38] *** Joins: esro (~esro@nat1.krakowskiinternet.pl)
[19:38:20] *** Joins: BSaboia (~bsaboia@177.37.198.11)
[19:45:45] *** Quits: BSaboia (~bsaboia@177.37.198.11) (Quit: This computer has gone to sleep)
[20:12:58] *** Joins: BSaboia (~bsaboia@177.37.198.11)
[20:20:32] *** Quits: mbuf (~Shakthi@122.174.170.222) (Quit: Leaving)
[21:08:06] *** Quits: BSaboia (~bsaboia@177.37.198.11) (Quit: This computer has gone to sleep)
[21:33:56] *** Joins: BSaboia (~bsaboia@177.184.132.202)
[21:42:45] *** Quits: BSaboia (~bsaboia@177.184.132.202) (Quit: This computer has gone to sleep)
[21:45:12] *** Joins: BSaboia (~bsaboia@177.184.132.202)
[21:56:45] *** Quits: BSaboia (~bsaboia@177.184.132.202) (Quit: This computer has gone to sleep)
[22:01:37] *** Joins: BSaboia (~bsaboia@177.184.132.202)
[22:15:10] *** Quits: lilgopher (~textual@2601:241:8000:38f0:6d00:a92d:3e37:105d) (Quit: Textual IRC Client: www.textualapp.com)
[22:18:19] *** Quits: BSaboia (~bsaboia@177.184.132.202) (Quit: This computer has gone to sleep)
[23:16:03] *** Joins: Bossi (~quassel@p4fc22aa4.dip0.t-ipconnect.de)
[23:17:27] *** Quits: esro (~esro@nat1.krakowskiinternet.pl) (Quit: Leaving)
