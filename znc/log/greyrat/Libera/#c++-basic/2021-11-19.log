[00:33:00] *** Quits: gehn (~gehn@user/gehn) (Quit: Leaving)
[00:37:20] *** Quits: byku (~byku@host-92-10-130-64.as13285.net) (Quit: WeeChat 2.8)
[00:59:27] *** Joins: vdamewood (~vdamewood@fedora/vdamewood)
[01:31:31] *** Quits: Inline (~Inline@2a02:908:1252:7a80:6adf:f25e:1e5f:74e1) (Ping timeout: 250 seconds)
[01:32:39] *** Quits: CodeMouse92 (~CodeMouse@user/codemouse92) (Quit: Oh freddled gruntbuggly | Thy micturations are to me | As plurdled gabbleblotchits | On a lurgid bee)
[01:41:33] *** Joins: smallville7123 (~smallvill@cpe-172-193-72-46.qld.foxtel.net.au)
[01:52:45] *** Quits: RoKenn (~RoKenn@user/rokenn) (Quit: NSA proxy service interrupted)
[02:05:11] *** Quits: dld (~dld@2601:197:600:1210:9992:b3e2:af1a:fbc2) (Ping timeout: 264 seconds)
[02:07:06] *** Joins: dld (~dld@98.217.187.57)
[02:19:30] *** Quits: spaceangel (~spaceange@ip-62-245-71-160.net.upcbroadband.cz) (Remote host closed the connection)
[03:15:22] *** Joins: geli (~gelignite@55d4c92e.access.ecotel.net)
[03:18:06] *** Quits: magla (~gelignite@55d4e1d7.access.ecotel.net) (Ping timeout: 268 seconds)
[03:20:03] *** Joins: magla (~gelignite@55d46975.access.ecotel.net)
[03:21:48] *** Quits: geli (~gelignite@55d4c92e.access.ecotel.net) (Ping timeout: 268 seconds)
[04:05:25] *** Joins: Colt (~Colt@user/colt)
[04:10:23] *** Quits: magla (~gelignite@55d46975.access.ecotel.net) (Quit: Stay safe!)
[04:55:29] *** Joins: lionkor (~lionkor@beammp/staff/lionkor)
[04:56:03] *** Quits: lkor (~lionkor@beammp/staff/lionkor) (Ping timeout: 250 seconds)
[05:02:17] *** Quits: wasd (~wasd@user/axis) (Remote host closed the connection)
[05:21:42] *** Quits: AmR (~AmREiSa@41.36.24.35) (Quit: Konversation terminated!)
[05:31:46] *** Quits: PJBoy (~PJBoy@user/pjboy) (Ping timeout: 265 seconds)
[05:43:57] *** Joins: CalamityBlue (~CalamityB@cpe-108-185-144-94.socal.res.rr.com)
[05:49:49] *** Quits: lionkor (~lionkor@beammp/staff/lionkor) (Ping timeout: 268 seconds)
[06:03:28] <CalamityBlue> has anyone seen a good benchmark of different STL data containers? I've seen that priority queues are significantly faster than multimaps, and I was wondering if there's a good place to reference how different STL containers perform at stuff like insertion/iteration/deletion
[06:03:32] *** Joins: aleko1[m] (~aleko1@2001:470:69fc:105::1:1016)
[06:03:32] *** Joins: ormaaj (~ormaaj@user/ormaaj)
[06:03:32] *** Joins: hnOsmium0001[m] (~hnosmium0@2001:470:69fc:105::15d4)
[06:03:32] *** Joins: merit (~merit@2001:470:69fc:105::5ae)
[06:07:18] <smallville7123> is this safe to do? https://gist.github.com/mgood7123/4b042a21bd849652ae1c09fc317fa11b
[06:12:37] <Alipha> CalamityBlue: eh, it doesn't make sense to compare the stl containers that way because each has a different purpose (except the ordered and unordered_* containers can be used more or less interchangeably). Or are you looking for a big-O comparison of them?
[06:15:54] <Alipha> CalamityBlue: https://medium.com/@rodrigues.b.nelson/choosing-wisely-c-containers-and-big-oh-complexity-64f9bd1e7e4c has a chart, though the flowchart is probably more useful
[06:16:15] <CalamityBlue> Alipha: well I'm moreso interested in a comparison of how they all match up within each big-O range, i.e. like priority queues vs multisets, because they're got the same big-O complexity but one is faster than the other
[06:22:46] *** Quits: unixpro1970 (~unixpro19@c-73-181-185-205.hsd1.wa.comcast.net) (Ping timeout: 260 seconds)
[06:25:20] <Alipha> CalamityBlue: well, for that particular example, if you were to use a multiset like a priority queue, then erase would be O(1) because you'd only be erasing from the front, which is faster than erasing from an arbitrary location
[06:26:05] <Alipha> Though the fact a std::priority_queue uses a std::vector internally, the cache friendliness could be a huge factor
[06:28:07] <Alipha> Honestly, if there is some benchmark out there, it would be rather meaningless because you really have to benchmark your specific scenario. The amount and shape of data, the relative frequency of how much you use various operations, etc, all have drastic impacts on performance
[06:33:05] <CalamityBlue> Alipha: I don't mean to ask about big-O complexities, since those are already in the docs, I mean something along the lines of this, to see how different containers that ostensibly have the same big-O complexity perform relative to each other https://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html
[06:33:11] *** Joins: unixpro1970 (~unixpro19@c-73-181-185-205.hsd1.wa.comcast.net)
[06:45:03] *** Joins: ferdna (~ferdna@user/ferdna)
[07:13:18] *** Joins: PJBoy (~PJBoy@user/pjboy)
[07:22:18] *** Quits: PJBoy (~PJBoy@user/pjboy) (Ping timeout: 268 seconds)
[08:11:54] *** Quits: X-Scale (~ARM@46.50.7.119) (Ping timeout: 256 seconds)
[08:12:03] *** Joins: X-Scale` (~ARM@165.201.137.78.rev.vodafone.pt)
[08:13:53] *** X-Scale` is now known as X-Scale
[08:16:53] *** Quits: ferdna (~ferdna@user/ferdna) (Quit: Leaving)
[09:08:48] *** Joins: Guest77 (~Guest77@2600:1700:e2a0:13f0:7108:7faf:daad:5f1f)
[09:10:04] <Guest77> I have a general question about c++ startup/shutdown time.
[09:10:37] <Guest77> I have a small program that reads in a fairly large file (17GB) and stores it in a vector<string>.
[09:11:19] <Guest77> The program takes about 40 sec to read in the entire file (about 49 million lines) and then print the first three lines (as a sanity check).
[09:11:57] <Guest77> What surprised me is that after printing the sanity check, the program takes an additional 19 sec to terminate and pass control back to the command line.
[09:12:42] <Guest77> Considering the sanity check is the last instruction in main{} I would have expected the program to end immediately, not take another ~20 sec to "shutdown".
[09:13:08] <Guest77> Can anyone explain why this is the case?  Is there also a hidden X sec startup time I'm not aware of?
[09:28:44] <Alipha> Guest77: the std::vector gets destroyed, which in turn destroys each of the std::strings, and each std::string individually frees its memory
[09:31:11] <Alipha> Guest77: if this vector is a local variable, then calling std::exit instead I believe would bypass executing the destructors of any local variables (but not global variables)
[09:32:13] <Alipha> It's generally not advisable to bypass destructors because they may do important things, but in this case, it would cause all the memory to be returned to the OS at once instead of each freed individually
[09:32:54] <smallville7123> you would probably use fread/read and friends instead of reading the entire file in one go
[09:33:32] <smallville7123> eg read up to the first line and print it, then read up to the second line and print it, and so on
[09:33:54] <smallville7123> unless the lines are EXTREMELY long
[09:34:13] <smallville7123> ( like 200MB or so per line )
[09:34:48] <smallville7123> or if the entire 17 GB file is just a single line with no line breaks between the start and the end
[09:35:22] <smallville7123> (for some insane reason) that will kill most text editors
[09:35:57] <smallville7123> (since they usually read up to '\n')
[09:36:33] * smallville7123 can hear screaming of text editors trying to read a 3 GB single-line file
[09:38:26] <Guest77> Alipha: std::exit worked, it exits immediately after the sanity check now. Thanks!  I agree bypassing destructors is normally bad, but I'm assuming it
[09:38:46] <Guest77> 's ok since all my program needs to do at this point is terminate?
[09:40:33] <Guest77> smallville7123: I probably should have clarified, it reads each line at a time, parses the fields, and stores each field in a vector<string> (so one vector for each field, one entry per line)
[09:40:53] <smallville7123> yes
[09:41:06] <Alipha> Guest77: it is still unusual to keep that much in memory at one time
[09:41:17] <smallville7123> but it only READS 3 lines if it is to PRINT 3 lines, right?
[09:41:47] <Alipha> Guest77: though are you the guy with 1TB of RAM?
[09:42:30] *** Quits: hsv-2 (~weechat@gateway/tor-sasl/hsv-2) (Quit: WeeChat 3.3)
[09:43:16] <Guest77> smallville7123  It's a little dumb right now.  I read the whole file and then print the first three entries at the end.
[09:43:41] *** Joins: hsv-2 (~weechat@gateway/tor-sasl/hsv-2)
[09:43:45] <smallville7123> Guest77: is this the soul purpose of the program?
[09:43:56] <smallville7123> eg to emulate "cat"
[09:45:00] <Guest77> smallville7123 No, I'm very new to c++ and trying to learn how to efficiently read in and parse a large file.
[09:46:24] <smallville7123> parse it how?
[09:46:32] <Guest77> Alipha: guilty as charged :P.  The plan is to eventually numerically encode the data so that each unique string only needs to be stored once, but I haven't gotten that far yet.
[09:47:32] <Guest77> Alipha: at which point some sort of streaming approach might work
[09:47:47] <smallville7123> try searching how to parse huge text files in C
[09:49:20] <smallville7123> also try looking at VIM, it can open very large text files instantly and search through them insanely quickly
[09:50:50] <Guest77> smallville7123: Its a large file with 4 fields the majority of which are variable width (some entries quite large).  I have things working pretty well now with a combo of std::getline into a temp string and then emplace_back into the vector<string>
[09:51:06] <Guest77> ~40 sec for a 17 GB file seems reasonable to me
[09:51:46] <smallville7123> it could be made faster by extracting the fields into parsable info
[09:52:52] <smallville7123> eg if a field is say STRING: "rheh", then parse :STRING:" then grab "rheh" and store it into say info[index].string = "rheh";
[09:53:03] <smallville7123> or similar
[09:53:32] <smallville7123> instead of say vector.pushBack("STRING: \"rheh\"");
[09:53:59] <smallville7123> or vector.push_back(readLine());
[09:55:14] <smallville7123> also if the data can be represented as a int then you could use info[index].integer = strod(INT_STRING_DATA, NULL); where INT_STRING_DATA may be "3526"
[09:55:18] <smallville7123> and so on
[09:55:46] <smallville7123> that way you dont need to allocate a string for the int string
[09:56:29] <smallville7123> tho it depends heavily on what your field's represent
[09:57:01] <Guest77> Alipha: You did mention this being a lot of data.  Is there any point where I should start getting concerned and store the  vector (housekeeping bit) on the heap instead of the stack?
[09:57:26] <smallville7123> if your fields represent strings than you could optimize it by allocating a big string for each field and serialize to string
[09:58:21] <smallville7123> eg info[index] = createString(readline(), readline(), readline(), readline()); // create 1 allocation for storing 4 strings
[09:58:41] <smallville7123> these strings should be delimited with NULL '\0'
[09:59:16] <smallville7123> eg createString("hello", "world"); // "hello\0world\0";
[09:59:34] <Guest77> smallville7123 The fields are mostly strings.  Two are fixed width and one is variable (and varies quite a bit).  Tab is the field separator and so far I have been handling the parsing with `std::getline(fin, temp, '\t');`
[10:00:23] <Guest77> then `field1.emplace_back(instring);` and repeating the process for each field.
[10:00:37] <Guest77> where filed1 is a vector<string>
[10:00:56] <Guest77> * `field1.emplace_back(temp);`
[10:01:12] <smallville7123> Guest77: so each string field is delimited by tab?
[10:01:22] <Guest77> smallville7123: yes
[10:01:28] <smallville7123> eg "A\tB" results in "A", "B" ?
[10:01:35] <smallville7123> as 2 seperate strings ?
[10:01:36] <Guest77> Yes
[10:01:52] <smallville7123> then the above would be helpful
[10:02:35] <smallville7123> assuming you can split, and assuming 3 string fields, and assuming 2 tabs in each field
[10:02:49] <smallville7123> then you have a total of 6 fields
[10:03:05] <smallville7123> which means 6 allocations per field group
[10:03:42] <smallville7123> you can reduce this to a single allocation per field group by merging (serializing) the 6 fields into 1 field delimited by '\0' s
[10:05:38] <smallville7123> eg Field[0] = "NAME\0Mayor\tSmith\0\0Job\0Mayor\0\0Age\050\0\0City\0NewYork";
[10:05:45] <Guest77> smallville7123: what exactly is index[info] in your example?
[10:06:57] <smallville7123> eg Field[0] = "NAME\0Mayor\0Smith\0\0Job\0Mayor\tHusband\0\0Age\050\0Deceased\0\0City\0New\0York";  *
[10:07:49] <smallville7123> (a tab \t is the same length as a \0 so they can be used interchangably as an identifier to where tabs are located)
[10:08:03] <smallville7123> eg "Mayor\tSmith"
[10:09:13] <smallville7123> it would look something like this, with \0 marking the end of a field, and \0\0 marking the end of a field section:     Field[0] = "NAME\0Mayor\0Smith\0\0Job\0Mayor\tHusband\0\0Age\050\0Deceased\0\0City\0New\0York\0";  *
[10:09:34] <smallville7123> assuming fields NAME, Job, Age, and City
[10:10:50] *** Joins: krishnac (~krishnac@c-67-188-37-26.hsd1.ca.comcast.net)
[10:11:01] <smallville7123> note that you DO NOT want to allocate a 17 GB string and read all data into it
[10:11:19] <smallville7123> as 1. most users have 4 to 8 GB memory
[10:12:05] <smallville7123> and 2. reading 17 GB is probably not as effiecent when you do not need to display/process all of it
[10:12:30] *** Quits: krishnac (~krishnac@c-67-188-37-26.hsd1.ca.comcast.net) (Remote host closed the connection)
[10:13:23] <smallville7123> a good example is this
[10:13:27] <smallville7123> "when the user said they were willing to handle up to 100 MiB chunks at a time, then requests passed that on to pyopenssl, and then pyopenssl used cffi.new to allocate a 100 MiB buffer to hold the incoming data. But most of the time, there wasn't actually 100 MiB ready to read on the connection; so pyopenssl would allocate this large buffer, but then would only use a small part of it"
[10:14:10] <smallville7123> which in the case of 17 GB, will make a BIG difference
[10:15:48] <smallville7123> eg assuming your printing your data and allowing for scrolling, it is ineffiecent to read and then print ALL 17 GB of data
[10:15:52] <smallville7123> instead, read, process, and print only 30 lines at a time assuming you can only fit 30 lines on the display
[10:16:20] <smallville7123> also assume each line can be 80 characters long
[10:16:30] <smallville7123> each line on the display*
[10:17:58] <smallville7123> if a field is longer than 80 characters then it would be best to apply the same approach to the lines as well, eg only read 80 characters and then simply scroll as needed, shifting the start offset of where you read the characters from
[10:19:07] <smallville7123> this way you only consume a very small amount of memory but can display as much data as it will let you read
[10:19:38] <smallville7123> (eg 32 bits can only read up to 4 GB of memory)
[10:19:55] <smallville7123> (tho 64 bit can up to 1 PT)
[10:20:25] <smallville7123> Guest77: 
[10:31:14] <Guest77> smallville7123  The goal is not to display the data, and I need to do a fairly complicated bit of analysis with it.  This unfortunately requires I hold all of the data in memory, although it could be compressed (ie say every occurrence of "John" is 1, "Sally" is 2, ect. then just store arrays of numbers for each entry. There is essentially a
[10:31:15] <Guest77> dictionary of several thousand elements that each field can be composed of.)
[10:34:12] <Guest77> smallville7123 and while I could store each line in a std::string, I will need to parse it eventually, so I'm not sure how much performance I would gain with the single string approach vs just parsing as its read in and using multiple strings.
[11:04:27] *** Joins: night_wulfe_ (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[11:08:00] *** Quits: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 268 seconds)
[11:17:17] *** Quits: artok (~azo@mobile-access-bcee1d-81.dhcp.inet.fi) (Quit: drive to work)
[11:46:28] *** Joins: night_wulfe (~wulfe@cpe-174-103-156-213.cinci.res.rr.com)
[11:50:20] *** Quits: Guest77 (~Guest77@2600:1700:e2a0:13f0:7108:7faf:daad:5f1f) (Quit: Client closed)
[11:50:22] *** Quits: night_wulfe_ (~wulfe@cpe-174-103-156-213.cinci.res.rr.com) (Ping timeout: 260 seconds)
[11:50:33] *** Joins: Guest77 (~Guest77@2600:1700:e2a0:13f0:7108:7faf:daad:5f1f)
[11:51:13] *** Quits: Guest77 (~Guest77@2600:1700:e2a0:13f0:7108:7faf:daad:5f1f) (Client Quit)
[12:31:29] *** Joins: PJBoy (~PJBoy@user/pjboy)
[12:31:37] *** Joins: kylese (~kylese@p5dd8b8a7.dip0.t-ipconnect.de)
[13:10:26] *** Joins: lionkor (~lionkor@beammp/staff/lionkor)
[13:15:38] *** Quits: unixpro1970 (~unixpro19@c-73-181-185-205.hsd1.wa.comcast.net) (Ping timeout: 256 seconds)
[13:17:38] *** Joins: karonto (~karonto@dynamic-002-211-085-157.2.211.pool.telefonica.de)
[13:21:13] *** Joins: teut (~teut@182.68.64.194)
[13:21:27] <teut> https://godbolt.org/z/e58rTzMnP  how to destructure the  vertex?
[13:21:32] <teut> its 3 values
[13:21:48] <teut> like      auto [v, u, count]
[13:25:34] *** Joins: unixpro1970 (~unixpro19@c-73-181-185-205.hsd1.wa.comcast.net)
[14:04:54] <smallville7123> wtf how does this make any sense? https://gist.github.com/mgood7123/41050f3e7e931fa43078621f983b7526
[14:26:14] *** Joins: magla (~gelignite@55d403ec.access.ecotel.net)
[14:55:52] <teut>  can someone tell why its not printing?  https://godbolt.org/z/31Yv58TKf
[15:02:45] *** Quits: karonto (~karonto@dynamic-002-211-085-157.2.211.pool.telefonica.de) (Quit: Leaving)
[15:05:32] <Svitkona> teut, graph is empty when you try to index it in the for loop
[15:08:17] <teut> ya,  that was one issue, thanks
[15:38:28] *** Quits: teut (~teut@182.68.64.194) (Ping timeout: 256 seconds)
[15:43:59] *** Joins: Guest17 (~Guest17@wificampus-098247.grenet.fr)
[15:53:46] *** Quits: Guest17 (~Guest17@wificampus-098247.grenet.fr) (Ping timeout: 256 seconds)
[16:47:13] *** Joins: teut (~teut@182.68.64.194)
[17:13:42] *** Quits: lionkor (~lionkor@beammp/staff/lionkor) (Ping timeout: 260 seconds)
[17:27:50] *** Quits: teut (~teut@182.68.64.194) (Ping timeout: 256 seconds)
[17:37:32] *** Joins: lionkor (~lionkor@beammp/staff/lionkor)
[17:43:30] *** Quits: Colt (~Colt@user/colt) (Quit: Leaving)
[17:43:54] *** Joins: Inline (~Inline@2a02:908:1252:7a80:9b56:dca:ba79:dc9b)
[17:44:00] *** Joins: CodeMouse92 (~CodeMouse@user/codemouse92)
[18:52:14] *** Quits: smallville7123 (~smallvill@cpe-172-193-72-46.qld.foxtel.net.au) (Ping timeout: 260 seconds)
[18:56:27] *** Joins: NiD27 (~nid27@49.205.151.232)
[19:01:09] *** Quits: hsv-2 (~weechat@gateway/tor-sasl/hsv-2) (Ping timeout: 276 seconds)
[19:09:48] *** Joins: teut (~teut@182.68.64.194)
[19:17:55] *** Joins: AmR (~AmREiSa@41.36.24.35)
[19:20:28] *** Quits: NiD27 (~nid27@49.205.151.232) (Quit: Leaving)
[19:22:48] <teut> https://godbolt.org/z/EYTTW8xbs
[19:23:09] <teut> can someone tell the problem with the overloaded operator?
[19:25:52] <teut> oh the std
[19:26:18] <teut> nope something else is there
[19:26:40] <Alipha> teut: you can't destructure a std::vector, and since you're asking to do so, it sounds like you shouldn't use a std::vector<int>, but instead make a custom struct, `struct Vertex { int v, u, count; };`
[19:26:53] <teut> ya ok I get that,
[19:27:02] <teut> what about current problem?
[19:27:25] <teut> initialization before << ?
[19:27:49] <Alipha> Your current problem takes more effort to answer, so I haven't looked at it yet :-)
[19:28:08] <teut> no no ,   https://docs.microsoft.com/en-us/cpp/standard-library/overloading-the-output-operator-for-your-own-classes?view=msvc-170
[19:28:13] <teut> I just tried this
[19:33:12] *** Joins: hsv-2 (~weechat@gateway/tor-sasl/hsv-2)
[19:47:47] *** Quits: hsv-2 (~weechat@gateway/tor-sasl/hsv-2) (Remote host closed the connection)
[19:48:07] *** Joins: hsv-2 (~weechat@gateway/tor-sasl/hsv-2)
[19:54:04] <teut> operator  not operand
[20:10:38] *** Quits: Inline (~Inline@2a02:908:1252:7a80:9b56:dca:ba79:dc9b) (Ping timeout: 265 seconds)
[20:36:05] *** Quits: very_sneaky (~very_snea@user/very-sneaky/x-7432109) (Ping timeout: 264 seconds)
[20:52:40] *** Joins: X-Scale` (~ARM@83.223.248.137)
[20:52:57] *** Quits: X-Scale (~ARM@165.201.137.78.rev.vodafone.pt) (Ping timeout: 256 seconds)
[20:54:37] *** X-Scale` is now known as X-Scale
[21:06:15] <Alipha> teut: oh, i forgot about your question. Are you still having issues? heh
[21:07:21] <teut>  https://ibb.co/vHX2ym0
[21:07:28] <teut> can you help solve this?
[21:08:03] <teut> https://ibb.co/HGWNVSJ
[21:08:07] <teut> test case
[21:08:29] <teut> oh ya one problem specific to c++
[21:08:48] <teut> INT_MAX + 5  results an underflow
[21:09:23] <teut> so my graph has uses INT_MAX for nodes edges which are not connected
[21:10:02] <teut> now I need to update edge weight when graph[i, k]  + graph[k, j] <graph[i, j]
[21:10:16] <teut> this results in underflow
[21:34:29] *** Quits: teut (~teut@182.68.64.194) (Quit: Client closed)
[21:59:28] *** Joins: teut (~teut@182.68.64.194)
[21:59:28] <teut> https://godbolt.org/z/MMW33Kxch
[22:00:26] <teut> can someone fix that  2000?
[22:00:33] <teut> I have used it for INF
[22:00:43] <teut> for that small test case
[22:01:35] *** Joins: Inline (~Inline@2a02:908:1252:7a80:4744:6074:ea58:ae8a)
[22:06:25] *** Quits: Inline (~Inline@2a02:908:1252:7a80:4744:6074:ea58:ae8a) (Remote host closed the connection)
[22:09:02] *** Joins: Inline (~Inline@2a02:908:1252:7a80:4744:6074:ea58:ae8a)
[22:11:15] *** Quits: Inline (~Inline@2a02:908:1252:7a80:4744:6074:ea58:ae8a) (Remote host closed the connection)
[22:25:29] *** Joins: Inline (~Inline@2a02:908:1252:7a80:4744:6074:ea58:ae8a)
[22:37:35] *** Joins: krishnac (~krishnac@c-67-188-37-26.hsd1.ca.comcast.net)
[22:38:06] *** Quits: krishnac (~krishnac@c-67-188-37-26.hsd1.ca.comcast.net) (Remote host closed the connection)
[22:54:14] *** Joins: meator (~meator@user/meator)
[23:25:16] *** Joins: spaceangel (~spaceange@ip-62-245-71-160.net.upcbroadband.cz)
[23:28:48] *** Joins: Colt (~Colt@user/colt)
[23:48:27] *** Quits: unixpro1970 (~unixpro19@c-73-181-185-205.hsd1.wa.comcast.net) (Remote host closed the connection)
[23:50:03] <teut> in recursion, stack frames are put onto the stack , and that happens at runtime
[23:50:21] <teut> why do classes instances need to be put in heap?
[23:50:48] *** Joins: unixpro1970 (~unixpro19@c-73-181-185-205.hsd1.wa.comcast.net)
[23:53:04] <teut> or why not store recursion stack frames in heap to prevent overflow of memory?
