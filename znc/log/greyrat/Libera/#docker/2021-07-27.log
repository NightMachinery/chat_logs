[00:04:57] <pi0> @programmerq thank you very much! is this channel connected to discord also?
[00:08:48] *** Quits: sudomann (~sudomann@c-73-133-131-19.hsd1.md.comcast.net) (Quit: Lost terminal)
[00:11:43] *** Quits: alzgh (~alzgh@216.155.158.214) (Ping timeout: 246 seconds)
[00:12:49] *** Quits: lemonzest (~lemonzest@user/lemonzest) (Quit: Quitting)
[00:13:11] <programmerq> nope. I am not a fan of cross-services bridges.
[00:13:17] <programmerq> nobody has ever asked for it
[00:13:26] <programmerq> not that I'm aware of, anyway.
[00:23:04] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[00:23:29] *** Joins: maret (~textual@193.37.255.236)
[00:23:46] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[00:24:19] <pi0> gotcha
[00:24:32] <pi0> i love it tho! i was considering using lxc or something
[00:24:37] <pi0> but since docker is open source as well
[00:24:49] <pi0> and pretty much industry leader / standard
[00:28:22] <pi0> for cotainers you use docker for vm's which do you recommend?
[00:29:12] *** Quits: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks) (Ping timeout: 244 seconds)
[00:29:17] <ada_> the one you're the most comfortable administrating
[00:31:31] *** Joins: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx)
[00:32:02] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[00:32:16] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[00:34:06] <programmerq> this is the way. I use proxmox for homelab. I'm partial to vmware for onprem stuff at work, but my current gig is almost all aws.
[00:34:23] <programmerq> I've also run vmware in the homelab too
[00:41:24] *** Quits: maret (~textual@193.37.255.236) (Quit: maret)
[00:43:11] *** Quits: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx) (Remote host closed the connection)
[00:45:05] *** Quits: Haxxa (~Haxxa@89nnjg0xckz9ggn6r5xm.ip6.superloop.com) (Quit: Haxxa flies away.)
[00:46:02] <pi0> oh wait promox handles both vms and containers?
[00:46:11] <pi0> i am still learning 
[00:46:22] <pi0> but right now i downloaded qemu along with virt-manager
[00:46:34] <pi0> i could be using promox instead of virt-manager?
[00:47:07] <programmerq> proxmox is a distro/gui. it uses qemu-kvm under the hood and it can manage lxc containers, giving an experience similar to VMs.
[00:47:10] *** Joins: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx)
[00:47:37] <pi0> ah i see
[00:47:38] *** Joins: Haxxa (~Haxxa@122.199.45.186)
[00:53:42] <pi0> is there much diff between lxc and docker
[00:53:45] <pi0> like in the scripts?
[00:56:30] <tabakhase> yes, and no and yes...
[00:56:48] <tabakhase> early docker used lxc internally ;-)
[00:57:24] <tabakhase> so kinda yes, but "docker" is a stack of like 13 different things, so also very much "no"
[00:59:40] <ada_> the end result is the same; run a process in a sandbox.  but the implementations are different
[01:06:18] *** Joins: thiras (~thiras@user/thiras)
[01:09:29] *** Quits: queeq_ (~queeq@user/queeq) (Ping timeout: 250 seconds)
[01:14:53] *** Quits: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx) (Remote host closed the connection)
[01:25:06] *** Quits: bradley (~bradley@user/bradley) (Quit: Bye.)
[01:27:17] *** Quits: pi0 (~default@neee.ga) (Quit: leaving)
[01:29:30] *** Joins: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx)
[01:34:32] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[01:37:08] <zamba> is there a way to make sure that doing 'up' doesn't automatically pull the latest image even if the tag is set to latest?
[01:37:17] <zamba> so that it has to be a manual step to actually update?
[01:38:00] *** Joins: pi0 (~default@neee.ga)
[01:38:05] <pi0> sorry my session timed out
[01:38:20] <pi0> did someone respond to my question regarding diff between lxc or docker
[01:40:39] *** Quits: c10l (~c10l@89.34.167.207) (Remote host closed the connection)
[01:41:02] *** Joins: c10l (~c10l@89.34.167.207)
[01:42:02] *** Quits: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net) (Ping timeout: 258 seconds)
[01:47:46] <jochum> pi0: havent seen the question, but LXD -> Full OS, Docker -> app container
[01:48:05] <jochum> pi0: all LXD containers start with /sbin/init
[01:48:27] <jochum> zamba: hey there :) watchtower does that for you
[01:48:48] *** Joins: finsternis (~X@23.226.237.192)
[01:49:06] <jochum> zamba: just make sure you pin a major version or something (whatever you need)
[01:50:37] <jochum> pi0: I'm a big fan of Proxmox
[01:50:54] <akik> jochum: do you use ceph with it?
[01:52:51] <pi0> jochum: i think i understand
[01:53:07] <pi0> i have a puppy system, not sure if i have the resources for a full blown proxmox
[01:53:21] <jochum> akik: not yet, once i get my boss to use proxmox we hill have CEPH and CephFS though
[01:54:23] <jochum> akik: on the lab ZFS with a standby using ZFS send
[01:55:21] <akik> jochum: i was mainly interested whether ceph automatically extends to new cluster nodes
[01:56:27] <jochum> akik: best is to join ##proxmox and ask there :)
[01:56:31] *** Joins: brickfat (~brickfat@user/brickfat)
[02:05:57] *** Quits: rpthms (~rpthms@user/rpthms) (Remote host closed the connection)
[02:06:18] *** Quits: c10l (~c10l@89.34.167.207) (Remote host closed the connection)
[02:06:33] *** Joins: c10l (~c10l@89.34.167.207)
[02:09:31] *** Joins: rpthms (~rpthms@user/rpthms)
[02:12:47] *** Quits: brickfat (~brickfat@user/brickfat) (Quit: Leaving)
[02:13:59] *** Quits: Bardon (~Bardon@user/Bardon) (Ping timeout: 255 seconds)
[02:15:39] *** Joins: sveinse (~sveinse@7.92-221-150.customer.lyse.net)
[02:20:08] <sveinse> Is there a way to associate a bridged network with an macvlan? That is, opened port bind to an alternate ip from what the server is running on? 
[02:23:54] <jochum> sveinse: explain you want Container->bridge->macvlan->internet?
[02:25:52] <jochum> Container->bridge->macvlan-port->macvlan->internet?
[02:27:24] <sveinse> jochum: I want to run multiple containers in a bridged network as per usual, but when the -p port option is used, it will be binding to another IP than the server's normal IP
[02:28:18] *** Joins: Bardon (~Bardon@user/Bardon)
[02:29:07] <sveinse> Hmm. In ipv6 a host may have multiple ips. Perhaps the same is possible in ipv4 and then setup docker to bind a specific bridged network to on of the alternate ips?
[02:29:41] <jochum> sveinse: ahh you want to publish ports on different ips?
[02:29:48] <sveinse> yes
[02:30:48] <jochum> sveinse: i googled that for ya https://forums.docker.com/t/can-i-change-the-default-ip-from-0-0-0-0-when-binding/30358
[02:30:59] <jochum> sveinse: that should work on IPv4
[02:32:13] <sveinse> jochum: I have seen that, but it changes the bind port for everything in docker, not only a few containers, doesn't it?
[02:32:58] <jochum> sveinse: are you talking about private root server or some larger network?
[02:33:28] <jochum> and how many containers?
[02:33:37] <sveinse> jochum: neither
[02:34:08] <sveinse> jochum: 2-5 in each bridge, 2 bridges
[02:34:22] <jochum> 10 Containers, how much "changes" on them?
[02:34:23] *** Quits: rewrit3 (~rewrit3@user/rewrit3) (Quit: rewrit3)
[02:34:31] <jochum> so additions and so?
[02:34:41] <sveinse> not much, really
[02:34:52] *** Quits: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx) (Remote host closed the connection)
[02:36:04] <jochum> Personaly i would do this WITHOUT "port binding" but with static ips for the containers AND some sort of firewall NAT in front
[02:36:34] <jochum> Its some work but easy for 10 of them
[02:36:56] <sveinse> That is what bridge essentially is, isn't it?
[02:37:10] <jochum> sveinse: a bridge is something like a switch/hub
[02:37:24] <jochum> each bridge is independent
[02:37:39] <jochum> as long you don't pass a link
[02:38:35] <sveinse> Yes, but the bridges gets its own IP network, when -p used, the package is masqueraded externally to come from the docker host server with conntrack and everything, so hence, there is a NAT running there
[02:38:41] <jochum> So in most cases you have a bridge as "software" switch or many switches to split networks
[02:39:25] <jochum> ofc those packages are masqueraded because coming from the bridge
[02:39:38] <jochum> But its Container->INET what about INET->Container
[02:40:11] <jochum> that heavy lifting is done by docker in iptables
[02:40:13] <sveinse> That follows as per usual NAT traversal (as the internet router will)
[02:40:18] <sveinse> exactly
[02:41:01] <sveinse> But that masquerade setup is using the IP4 from the docker host and I was wondering if I would bind it to other IPs, specific to one bridge network
[02:41:09] <jochum> sveinse: you could if you have a good firewall disable all NAT on docker
[02:42:25] <sveinse> How do you proxy a docker network out on a physical NIC?
[02:43:30] <sveinse> Macvlan kinda does that, except it gives each container an individual LAN ip
[02:44:56] <pi0> is it possible to use virt-manager to run docker images 
[02:45:04] <pi0> i know it can use lxc
[02:45:49] <jochum> sveinse: this is all unofficial hackish shit I did: docker network create --ipam-driver=null 
[02:46:29] <jochum> sveinse: brctl addif <newbr0> <link-of-host_eth0>
[02:47:15] <jochum> jochum: then containers either get an ip from LAN or you assign one, like on a normal vm/host
[02:48:43] <jochum> sveinse: another easier to maintain solution is docker-in-docker
[02:48:59] <sveinse> hmm
[02:49:40] <sveinse> Well, I need to experiment a bit more later
[02:49:48] <jochum> gl
[02:49:57] *** Joins: useful_idiot2 (~useful_id@66.115.142.101)
[02:50:06] <jochum> Sorry for not having a clear and easy solution for ya.
[02:50:30] <jochum> sveinse: maybe ping programmer q he knows 10 times more about docker than I do.
[02:51:48] *** Quits: useful_idiot (~useful_id@154.3.42.32) (Ping timeout: 252 seconds)
[02:51:48] *** useful_idiot2 is now known as useful_idiot
[02:51:52] <sveinse> Consider this usecase: nginx front-end binding to 80, 443 and a backend set of containers sharing a bridged network. Say you want to start another nginx + backend containers, but you don't want to share the same bridge as before. Obviously you cannot reuse 80 and 443 for the second nginx, so why not bind it to another IP. That is what I'm contemplating.
[02:51:58] <sveinse> jochum: no worries, thanks.
[02:53:02] *** Quits: vidbina (~vid@dynamic-089-014-139-231.89.14.pool.telefonica.de) (Ping timeout: 256 seconds)
[02:53:25] <sveinse> Perhaps my explaination was XY-ish :D sorry if that was the case
[02:54:02] <jochum> sveinse: hmm why not go for another proxy in front? :)
[02:54:21] <jochum> sveinse: for example damn whats name was it...
[02:54:53] <jochum> Traefik
[02:54:54] <sveinse> In this case I want them to be completely separate. But I would guess you're looking for SNI
[02:55:47] <jochum> sveinse: that thing you've seen changes the binding for a single container.
[02:55:54] <jochum> <jochum> sveinse: i googled that for ya https://forums.docker.com/t/can-i-change-the-default-ip-from-0-0-0-0-when-binding/30358
[02:56:05] <jochum> NOW we have a solution :)
[02:56:45] <sveinse> baah, of course. How stupid. jochum thanks!
[02:58:18] *** Quits: TheSilentLink (~TheSilent@user/thesilentlink) (Ping timeout: 240 seconds)
[02:59:54] <Lutin> Damn I'm so lazy lately
[03:02:39] <jochum> sveinse: I belive all of that is possible with docker-compose , have a look into it. Makes  things much more easy to maintain 
[03:03:10] <sveinse> jochum: yes it is. I feel stupid I missed that little obvious detail
[03:03:21] <jochum> with compose you do: cd projectA; docker-compose up -d; cd ../projectB; docker-compose up -d
[03:03:24] *** Joins: TheSilentLink (~TheSilent@user/thesilentlink)
[03:03:44] <sveinse> jochum: jep, I'm using docker-compose a lot. Thanks.
[03:04:01] <jochum> sveinse: I feel stupid because of going in so complex "solutions" :)
[03:04:17] *** Joins: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx)
[03:04:26] *** Joins: useful_idiot3 (~useful_id@154.3.42.62)
[03:06:22] *** Quits: useful_idiot (~useful_id@66.115.142.101) (Ping timeout: 258 seconds)
[03:06:23] *** useful_idiot3 is now known as useful_idiot
[03:13:57] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[03:14:42] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[03:16:21] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[03:16:37] *** Joins: tang^ (~DoofusCan@2604:3d09:47c:f970:a40d:c13d:f70c:8156)
[03:17:49] <jochum> Lutin: i know lazy days....
[03:18:09] <jochum> Lutin: I've had 3 of them now :) Happily I'm in vaccation
[03:23:55] <Lutin> jochum nice! My whole life a vacation :P
[03:23:55] *** Quits: shokohsc (~shokohsc@161.88.195.77.rev.sfr.net) (Read error: Connection reset by peer)
[03:24:58] *** Joins: shokohsc (~shokohsc@161.88.195.77.rev.sfr.net)
[03:25:32] *** Quits: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net) (Ping timeout: 258 seconds)
[03:29:44] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[03:30:01] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[03:35:40] <sveinse> heh, I think I might have opened another pandoras box: Getting ipv6 packages into an nginx container :D
[03:38:30] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[03:40:31] <jochum> hahaha
[03:41:06] <jochum> sveinse: i don't know the current state of IPv6 and docker
[03:42:33] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[03:42:39] *** Joins: brickfat (~brickfat@user/brickfat)
[03:43:24] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[03:48:15] *** Quits: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net) (Ping timeout: 265 seconds)
[04:00:37] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[04:00:52] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[04:02:58] *** Quits: polymorphic (~polymorph@066-169-158-111.res.spectrum.com) (Read error: Connection reset by peer)
[04:03:19] *** Joins: polymorphic (~polymorph@066-169-158-111.res.spectrum.com)
[04:04:08] <sveinse> hmm. Probably the easiest way around is to NAT ipv6 (ewww), https://github.com/robbertkl/docker-ipv6nat
[04:04:49] <sveinse> I absolutely recognize that the makers of docker has a challenge with ipv6, given that the current design is so ipv4-centric
[04:05:09] <sveinse> Or should I say NAT centric
[04:05:59] <jochum> back to the LAN approach ? ;P
[04:06:09] <sveinse> heh
[04:06:16] <jochum> sveinse: you could also have LAN Bridge + 2 Backend bridges
[04:07:47] <sveinse> Actually, I was hoping on tackling this one separate from the multiple bridge networks thing above.
[04:08:57] <jochum> sveinse: if you have a external firewall it easy
[04:09:02] <sveinse> But its a request I've had for a long time: Can we get weblogs from nginx ipv6 traffic? Which one can't due to the docker proxy
[04:09:55] <jochum> Not sure never did that, but I think you can have that LAN-Bridge in docker-compose too.
[04:10:05] <jochum> I mean reference it
[04:11:31] <sveinse> Docker proxy listens on ipv6, so ipv6 connectivity has always been there. But it changes the request from TCP6 to TCP4, so the container doesn't see the ipv6 attributes
[04:12:32] <sveinse> After reading about it, I have arrived at that NAT on ipv6 is probably the solution which is most docker-like and would create the least amount of fuzz
[04:14:44] <jochum> :-)
[04:15:40] *** Quits: yaalon (~yaalon@189.202.79.36.cable.dyn.cableonline.com.mx) ()
[04:27:37] *** Quits: hposca (~hposca@node-1w7jr9phoke2uuevlzoqmsdks.ipv6.telus.net) (Ping timeout: 245 seconds)
[04:29:31] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[04:36:46] *** ihaveamac_ is now known as ihaveamac
[04:36:50] *** Quits: Trieste (T@user/pilgrim) (Ping timeout: 258 seconds)
[04:39:54] *** Quits: tex (~dee@user/dix) (Ping timeout: 276 seconds)
[04:41:33] *** Quits: Atum_ (~IRC@user/atum/x-2392232) (Quit: Atum_)
[04:44:03] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[04:44:11] *** Quits: brickfat (~brickfat@user/brickfat) (Quit: Leaving)
[04:44:49] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[05:07:26] *** Joins: dmalteseknight (~dmaltesek@user/dmalteseknight)
[05:13:27] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[05:13:42] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[05:24:10] *** Joins: daMaestro (~damaestro@fedora/daMaestro)
[05:27:35] <sveinse> Close, but yet so far (alt: no cigar). I managed to get ipv6 connectivity from the host into the container via the docker proxy, but the proxy keeps rewriting the source IP for some reason, so I can not log the source of the ipv6 request in the container. Oh well. Time for a break.
[05:30:18] *** Joins: jazzy (~jaziz@2600:380:8644:8168:ad48:50e6:fb3f:d642)
[05:30:20] *** Quits: vlm (~vlm@user/vlm) (Quit: "")
[05:31:16] <jochum> sveinse: theres no chance to have the IPV6 source in nginx
[05:32:18] <sveinse> jochum: please explain
[05:32:33] <jochum> sveinse: you know how nat works?
[05:32:42] *** Joins: vlm (~vlm@user/vlm)
[05:33:13] <jochum> Lets say SNAT: Box -> router -> inet
[05:33:22] <sveinse> Yes, and you don't need to rewrite the source addr on incoming packages, only the dest addr. That's how I can see the real world ipv4 addrs in nginx, right?
[05:33:50] <jochum> sveinse: yeah
[05:34:14] <jochum> sveinse: but when you switch protocols things change
[05:34:38] <jochum> Howto pack a 128 Byte IPV6 in a 32Byte IPV4 field
[05:35:36] <sveinse> never going via ipv4 here. I have LAN (ipv6) <-> bridge (ipv6) <-> container (ipv6)
[05:35:54] <sveinse> And it works, except the rewrite
[05:35:55] <jochum> sveinse: ye but container is IPv4 only, right?
[05:36:01] <sveinse> nope
[05:36:04] <jochum> ohh
[05:36:14] <sveinse> that is what I managed to do
[05:36:29] <sveinse> with no particular hacks, just out of box docker
[05:36:34] <jochum> great :)
[05:36:38] * jochum loves hacks
[05:37:21] <jochum> I believe in the end you will have a fine multibridge solution :)
[05:41:06] <sveinse> Its a known issue. ...open from 2015 - so extremely forward leaning there
[05:41:55] <jochum> linke it please
[05:41:57] <jochum> link
[05:42:04] <sveinse> https://github.com/moby/moby/issues/17666
[05:42:45] <jochum> The IPv6 Ticket
[05:45:43] <sveinse> I need to research if docker proxy has some options for this
[05:45:47] <sveinse> ...tomorrow
[05:47:35] <jochum> gl and gn8
[05:47:50] <sveinse> thx
[06:05:59] *** Joins: sed (~sed@pool-100-7-38-116.rcmdva.fios.verizon.net)
[06:28:34] *** Quits: metah4ck3r (~meta@user/metah4ck3r) (Quit: WeeChat 3.2)
[06:31:50] *** Joins: metah4ck3r (~meta@user/metah4ck3r)
[06:32:44] *** Joins: HaMsTeRs (~mx@121.202.70.46)
[06:35:54] *** Quits: HaMsTeRs (~mx@121.202.70.46) (Client Quit)
[06:36:17] *** Joins: HaMsTeRs (~mx@121.202.70.46)
[06:36:46] *** Quits: Sasazuka (~Sasazuka@user/sasazuka) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[06:38:35] *** Quits: thiras (~thiras@user/thiras) (Ping timeout: 255 seconds)
[06:39:10] *** Quits: dmalteseknight (~dmaltesek@user/dmalteseknight) (Quit: WeeChat 3.2)
[06:45:09] *** Joins: Raito_Bezarius (~Raito@wireguard/tunneler/raito-bezarius)
[06:56:20] *** Joins: compuguy3 (ahall@user/compuguy)
[06:58:07] *** Quits: compuguy (ahall@user/compuguy) (Read error: Connection reset by peer)
[06:58:07] *** compuguy3 is now known as compuguy
[07:14:38] *** Quits: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net) (Read error: Connection reset by peer)
[07:20:56] *** Quits: OPK (~OPK@user/opk) (Ping timeout: 252 seconds)
[07:24:55] *** Joins: Atque (~Atque@user/atque)
[07:30:14] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[07:30:16] *** Quits: Brainium (~brainium@user/brainium) (Quit: Konversation terminated!)
[07:30:34] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[07:40:42] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[07:41:25] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[07:43:55] *** Joins: alsotang (~DoofusCan@2604:3d09:47c:f970:7023:75e0:da:6207)
[07:46:19] *** Quits: daMaestro (~damaestro@fedora/daMaestro) (Quit: Leaving)
[07:46:38] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[07:46:54] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[07:47:09] *** Quits: alsotang (~DoofusCan@2604:3d09:47c:f970:7023:75e0:da:6207) (Client Quit)
[07:47:42] *** Quits: tang^ (~DoofusCan@2604:3d09:47c:f970:a40d:c13d:f70c:8156) (Ping timeout: 256 seconds)
[07:51:43] *** Joins: OPK (~OPK@user/opk)
[07:56:09] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[07:57:23] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[08:01:28] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[08:02:17] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[08:02:22] *** Quits: PeGaSuS (BlackHole@user/pegasus) (Quit: ZNC 1.9.x + BlackHole v2 - https://znc.in)
[08:03:21] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Read error: Connection reset by peer)
[08:03:21] *** Joins: PeGaSuS (BlackHole@user/pegasus)
[08:03:35] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[08:16:12] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[08:16:21] *** Quits: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net) (Ping timeout: 276 seconds)
[08:16:52] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[08:18:41] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[08:23:59] *** Joins: HaMcHi (~mx@182.153.55.162)
[08:25:41] *** Quits: HaMsTeRs (~mx@121.202.70.46) (Ping timeout: 255 seconds)
[08:25:48] *** Joins: Atque (~Atque@user/atque)
[08:31:00] *** Quits: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net) (Ping timeout: 265 seconds)
[08:31:50] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[08:33:40] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[08:33:55] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[08:44:05] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[08:44:51] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[08:47:32] *** Joins: HaMsTeRs (~mx@14-0-175-242.static.pccw-hkt.com)
[08:48:04] *** Quits: HaMcHi (~mx@182.153.55.162) (Ping timeout: 268 seconds)
[08:58:04] *** Quits: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net) (Ping timeout: 265 seconds)
[09:01:32] *** Joins: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks)
[09:02:40] *** Quits: HaMsTeRs (~mx@14-0-175-242.static.pccw-hkt.com) (Read error: Connection reset by peer)
[09:07:03] *** Joins: HaMsTeRs (~mx@121.203.229.184)
[09:13:45] *** Joins: glik22 (~glik22@99-36-164-253.lightspeed.snjsca.sbcglobal.net)
[09:21:06] *** Quits: nerdcore (~mike@nerdcore.net) (Ping timeout: 240 seconds)
[09:30:10] *** Quits: c10l (~c10l@89.34.167.207) (Quit: The Lounge - https://thelounge.chat)
[09:30:29] *** Joins: c10l (~c10l@89.34.167.207)
[09:46:17] *** Quits: setesat40 (~setesat@94.4.156.220) (Quit: The Lounge - https://thelounge.chat)
[09:46:43] *** Joins: setesat40 (~setesat@94.4.156.220)
[09:48:15] *** Joins: maret (~textual@193.37.255.238)
[09:53:59] *** Quits: HaMsTeRs (~mx@121.203.229.184) (Quit: Konversation terminated!)
[10:04:12] *** Quits: maret (~textual@193.37.255.238) (Quit: maret)
[10:06:04] *** Joins: maret (~textual@193.37.255.238)
[10:07:40] *** Joins: lemonzest (~lemonzest@user/lemonzest)
[10:18:37] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[10:18:52] *** Joins: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99)
[10:18:57] *** Quits: Null_A (~null_a@2601:645:8700:2290:98e5:42b9:7e41:db99) (Remote host closed the connection)
[10:31:24] *** Quits: Zelec (~Zelec@135-23-82-85.cpe.pppoe.ca) (Quit: The Lounge - https://thelounge.chat)
[10:31:49] *** Joins: Zelec (~Zelec@135-23-82-85.cpe.pppoe.ca)
[10:42:16] *** Quits: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks) (Remote host closed the connection)
[10:42:40] *** Joins: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks)
[10:47:29] *** Parts: pi0 (~default@neee.ga) ()
[10:50:15] *** Quits: TheBigK (~quassel@user/thebigk) (Quit: bye bye)
[10:54:06] *** Quits: c10l (~c10l@89.34.167.207) (Read error: Connection reset by peer)
[10:56:14] *** Joins: c10l (~c10l@89.34.167.207)
[10:57:45] *** Joins: vidbina (~vid@dynamic-046-114-032-097.46.114.pool.telefonica.de)
[10:58:25] *** Quits: d33ceee (~decent@ip68-10-104-81.hr.hr.cox.net) (Ping timeout: 265 seconds)
[10:58:41] *** Joins: HaMsTeRs (~mx@121.203.229.184)
[11:03:07] *** Quits: sed (~sed@pool-100-7-38-116.rcmdva.fios.verizon.net) (Ping timeout: 268 seconds)
[11:08:57] *** Joins: d33cee (~decent@ip68-10-104-81.hr.hr.cox.net)
[11:25:31] *** Quits: HaMsTeRs (~mx@121.203.229.184) (Quit: Konversation terminated!)
[11:31:18] *** Quits: r0bby (r0bby@user/r0bby) (Ping timeout: 272 seconds)
[11:33:15] *** Joins: r0bby (r0bby@user/r0bby)
[11:36:13] *** Joins: TheBigK (~quassel@user/thebigk)
[11:39:08] *** Joins: fibsifan (~quassel@dynamic-078-054-038-225.78.54.pool.telefonica.de)
[11:40:45] *** Joins: tex (~dee@user/dix)
[11:42:24] *** Quits: holbrode (sid339826@id-339826.tinside.irccloud.com) (Ping timeout: 276 seconds)
[11:44:08] *** Quits: sa (sid1055@2a03:5180:f::41f) (Ping timeout: 255 seconds)
[11:44:32] *** Joins: holbrode (sid339826@id-339826.tinside.irccloud.com)
[11:45:35] *** Joins: sa (sid1055@id-1055.tinside.irccloud.com)
[11:47:36] *** Joins: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks)
[11:50:41] *** Quits: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks) (Ping timeout: 244 seconds)
[11:54:18] *** Quits: i64 (i64@gateway/vpn/protonvpn/i64) (Ping timeout: 268 seconds)
[11:56:00] *** Joins: kikijiki (~Thunderbi@user/kikijiki)
[11:56:13] *** Joins: i64 (i64@gateway/vpn/protonvpn/i64)
[12:13:27] *** Joins: thc202 (~thc202@user/thc202)
[12:13:56] *** Quits: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks) (Ping timeout: 244 seconds)
[12:15:57] *** Joins: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks)
[12:16:40] <erhandsome> i have a dockerfile compile kernel module and modprobe it, but when 'modprobe module_name' it shows modprobe: ERROR: could not insert 'module_name': Operation not permitted, but works on some machine, it's there any settings i didn't noticed may cause this?
[12:16:50] *** Joins: DevAntoine (~DevAntoin@195.101.183.21)
[12:16:52] <DevAntoine> Hello
[12:18:10] <DevAntoine> Using Alpine Linux it seems that when I run a basic sh shell doing "docker-compose exec sh" the /etc/profile file is not loaded.
[12:18:13] <DevAntoine> How can I fix that?
[12:20:05] <DevAntoine> It seems that it's because it's a non interactive shell and if I add the -l flag it'll be interactive.
[12:20:14] <DevAntoine> Which it does.
[12:20:25] <DevAntoine> But I'm lost at the interactive/non interactive shell here
[12:22:42] *** Quits: vidbina (~vid@dynamic-046-114-032-097.46.114.pool.telefonica.de) (Ping timeout: 276 seconds)
[12:22:56] <gordonjcp> DevAntoine: so pass -l to sh?
[12:23:07] <gordonjcp> DevAntoine: why sh instead of ash?
[12:23:38] <DevAntoine> gordonjcp: you're right, ash, not sh
[12:23:57] <DevAntoine> gordonjcp: yes I'd like to set it as the default when running "docker-compose exec", is that even doable?
[12:26:20] *** Quits: mohabaks_ (~mohabaks@gateway/tor-sasl/mohabaks) (Ping timeout: 244 seconds)
[12:30:11] *** Joins: Lutin (~Lutin@user/lutin)
[12:30:37] <gordonjcp> I think it is, isn't it?
[12:32:30] *** Quits: alicef (~none@gentoo/developer/alicef) (Quit: install gentoo)
[12:32:45] <gordonjcp> DevAntoine: what are you trying to do, and what is it doing now that is different from what you want?
[12:33:19] *** Joins: alicef (~none@gentoo/developer/alicef)
[12:41:28] *** Quits: alicef (~none@gentoo/developer/alicef) (Quit: install gentoo)
[12:43:42] *** Joins: alicef (~none@gentoo/developer/alicef)
[12:52:26] *** Joins: vidbina (~vid@dynamic-089-014-050-093.89.14.pool.telefonica.de)
[12:57:51] *** Quits: vidbina (~vid@dynamic-089-014-050-093.89.14.pool.telefonica.de) (Quit: vidbina)
[12:58:12] *** Joins: vidbina (~vid@dynamic-089-014-050-093.89.14.pool.telefonica.de)
[12:59:06] <DevAntoine> gordonjcp: I'm trying to load a .sh when someone run OR exec a shell.
[12:59:25] <DevAntoine> In docker-compose I added "tty: true" but it doesn't change anything.
[12:59:39] <DevAntoine> This message makes me wonder if it's used: https://github.com/docker/compose/issues/5741#issuecomment-588119345
[13:02:09] *** Quits: momomo (~momomo@user/momomo) (Ping timeout: 265 seconds)
[13:06:00] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 252 seconds)
[13:06:41] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[13:13:17] *** Joins: keypushe- (keypusher@user/keypusher)
[13:13:22] *** Quits: keypusher (keypusher@user/keypusher) (Ping timeout: 240 seconds)
[13:16:13] *** keypushe- is now known as keypusher
[13:16:46] *** Joins: alzgh (~alzgh@216.155.158.214)
[13:21:25] *** Joins: keypushe- (keypusher@user/keypusher)
[13:21:47] *** Quits: keypusher (keypusher@user/keypusher) (Ping timeout: 245 seconds)
[13:21:54] <DevAntoine> btw gordonjcp, I'm not sure what the "-l" flag does.
[13:22:03] <DevAntoine> It makes no sense to enable an interactive shell
[13:22:18] <DevAntoine> By definition if I run "docker-compose exec service ash" I've got an interactive shell.
[13:24:26] <DevAntoine> a,d by default there's a TTY when running exec.
[13:24:28] <gordonjcp> yup
[13:24:42] *** keypushe- is now known as keypusher
[13:24:47] <gordonjcp> DevAntoine: so you're trying to run something like a profile script, something that runs when the user "logs in"?
[13:24:59] <DevAntoine> gordonjcp: that's right
[13:25:35] <DevAntoine> I can make it work with "ash -l" but I don't understand why.
[13:25:46] <DevAntoine> And I'd like to be the default when doing "docker-compose exec ash"
[13:25:53] <gordonjcp> yeah
[13:26:17] <gordonjcp> when you pass ash to it like that, you're actually telling it "run this command"
[13:26:19] <gordonjcp> so
[13:26:38] <gordonjcp> you'd need to create your docker container with a config for ash in it somewhere
[13:31:29] <DevAntoine> I don't understand: "run" this command" means I'll have a shell prompt. So to me, it's an interactive shell.
[13:31:43] <DevAntoine> I think the difference may come from login / non-login shell
[13:32:04] <DevAntoine> I'm not sure what should I do regarding "you'd need to create your docker container with a config for ash in it somewhere"
[13:39:47] *** Quits: mei (~mei@user/mei) (Read error: Connection reset by peer)
[13:40:09] *** Joins: mei (~mei@user/mei)
[13:43:30] <DevAntoine> Oh ok, I've rtfmed https://unix.stackexchange.com/a/46856/355337 about interactive, non-interactive, login, non-login shell
[13:43:33] <DevAntoine> It was really instructive
[13:43:52] <DevAntoine> So, in my case I'm in a interactive, non-login shell.
[13:59:05] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 250 seconds)
[13:59:35] *** Joins: artok (~azo@mobile-access-567357-119.dhcp.inet.fi)
[14:00:12] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[14:10:36] *** Quits: artok (~azo@mobile-access-567357-119.dhcp.inet.fi) (Quit: faa)
[14:16:10] *** Joins: simplicity (~yti@user/simplicity)
[14:24:39] *** Quits: svm_invictvs_ (~svm_invic@user/svm-invictvs/x-6696469) (Ping timeout: 252 seconds)
[14:28:47] *** Joins: svm_invictvs (~svm_invic@user/svm-invictvs/x-6696469)
[14:39:10] *** Quits: doc (~doc@user/doc) (Quit: Things to do)
[14:39:51] *** Quits: simplicity (~yti@user/simplicity) (Quit: leaving)
[14:44:53] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 252 seconds)
[14:45:30] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[14:49:58] *** Joins: rewrit3 (~rewrit3@user/rewrit3)
[14:56:26] *** Joins: candyman1 (~candyman@user/candyman)
[14:56:36] *** Quits: candyman1 (~candyman@user/candyman) (Client Quit)
[14:56:57] *** Joins: candyman1 (~candyman@user/candyman)
[14:57:10] *** Joins: TomyWork (~TomyLobo@p200300e80f133c0065813425a42d3549.dip0.t-ipconnect.de)
[15:07:30] *** Quits: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com) (Remote host closed the connection)
[15:08:14] *** Joins: terrorjack (~terrorjac@ec2-54-95-39-30.ap-northeast-1.compute.amazonaws.com)
[15:14:35] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 265 seconds)
[15:15:00] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[15:22:23] *** Joins: simplicity (~yti@user/simplicity)
[15:30:57] *** Joins: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks)
[15:40:36] *** Quits: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks) (Ping timeout: 244 seconds)
[15:42:17] *** Joins: mohabaks (~mohabaks@gateway/tor-sasl/mohabaks)
[15:52:31] *** Quits: alzgh (~alzgh@216.155.158.214) (Ping timeout: 246 seconds)
[15:52:39] *** Quits: vidbina (~vid@dynamic-089-014-050-093.89.14.pool.telefonica.de) (Ping timeout: 258 seconds)
[15:57:51] *** Quits: Gustavo6046 (~Gustavo60@user/gustavo6046) (Ping timeout: 276 seconds)
[15:58:15] *** Quits: sveinse (~sveinse@7.92-221-150.customer.lyse.net) (Quit: leaving)
[15:58:31] *** Joins: vidbina (~vid@dynamic-089-014-050-093.89.14.pool.telefonica.de)
[16:02:27] *** Joins: Gustavo6046 (~Gustavo60@user/gustavo6046)
[16:08:11] *** Quits: simplicity (~yti@user/simplicity) (Quit: leaving)
[16:11:08] *** Joins: simplicity (~yti@user/simplicity)
[16:15:05] *** Joins: oxum (~oxum@122.181.34.214)
[16:18:18] *** Quits: oxum (~oxum@122.181.34.214) (Remote host closed the connection)
[16:34:42] *** Quits: qilx (~quassel@dynamic-109-81-210-171.ipv4.broadband.iol.cz) (Ping timeout: 256 seconds)
[16:35:07] *** Joins: qilx (~quassel@62.201.21.8)
[16:38:08] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[16:38:28] *** Joins: gschanuel (~gschanuel@user/gschanuel)
[16:42:01] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 250 seconds)
[16:43:05] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[16:52:37] *** Quits: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963) (Ping timeout: 245 seconds)
[16:52:56] *** Joins: Code_Bleu (~Code_Bleu@user/code-bleu/x-6939963)
[17:13:07] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 268 seconds)
[17:13:45] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[17:17:16] *** Joins: oxum (~oxum@122.181.34.214)
[17:18:55] <zamba> how can i use docker-compose override to lock the image to a specific version?
[17:19:05] *** Quits: oxum (~oxum@122.181.34.214) (Remote host closed the connection)
[17:19:17] *** Joins: oxum (~oxum@122.181.34.214)
[17:25:30] <ada_> zamba: write an override file that only specifies the image:  key and use the -f flag to feed it into docker-compose
[17:25:50] <ada_> https://docs.docker.com/compose/extends/
[17:26:35] *** Quits: oxum (~oxum@122.181.34.214) (Remote host closed the connection)
[17:32:09] <zamba> if i have this command: docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.secrets.yml -p netbox-docker-test up
[17:32:31] <zamba> then each docker-compose file will override whatever's "matching" in the previous ones?
[17:33:52] *** Joins: thiras (~thiras@user/thiras)
[17:35:43] <ada_> yep
[17:36:29] <zamba> sweet :)
[17:36:49] <zamba> can i disable a service with a override file as well?
[17:36:57] <ada_> no, I don't think so
[17:37:04] *** Quits: lemonzest (~lemonzest@user/lemonzest) (Quit: Quitting)
[17:37:09] <ada_> well, you could set replicas to 0 i suppose
[17:37:19] <ada_> but you can't "knock out" a key that's written in a higher file
[17:38:12] <gordonjcp> zamba: no, override files just add, they don't replace
[17:38:17] <gordonjcp> it's kind of annoying, and a bit misnamed
[17:38:29] <zamba> but setting replicas: 0 would have the same effect?
[17:39:07] <zamba> hm, no, it didn't
[17:39:17] <zamba> WARNING: Some services (postgres) use the 'deploy' key, which will be ignored. Compose does not support 'deploy' configuration - use `docker stack deploy` to deploy to a swarm.
[17:39:32] <ada_> yeah you're using the replicas key under deploy, which is only for swarm
[17:39:45] <ada_> you'd move your replicas key out of underneath "deploy" since you're not targeting swarm
[17:40:36] <ada_> you know, i actually don't know if that works with plain containers and compose
[17:40:51] <ada_> I guess I haven't tried before, but I was imagining it might do the same thing 
[17:41:24] <ada_> but with swarm since the container is abstracted out, the service object could exist even with 0 containers;  but I guess with regular containers API and compose, that probably doesn't work
[17:44:35] *** Joins: lemonzest (~lemonzest@user/lemonzest)
[17:44:36] *** Joins: varaindemian (~varaindem@86.124.78.162)
[17:46:34] <wez> Is swarm equiv to k8s?
[17:46:42] <wez> Or are they solving different problems?
[17:47:45] <ada_> they try to solve the same problem
[17:48:03] <ada_> orchestrating containers across a cluster of machines
[17:48:13] <ada_> very different designs, though
[17:49:43] <wez> Hmmmmm, I am trying to decide which one to use.  I currently either manage each machine that runs docker seperately or use AWS ECS Fargate.
[17:50:43] <ada_> swarm has a smaller API, but k8s is more flexible
[17:51:06] <ada_> k8s also has way more moving parts
[17:51:41] <wez> Yeah, I have read that k8s has a steep learning curve
[17:52:03] <wez> Not sure if the hosted offerings by Azure, AWS, etc.. would make that easier.
[17:52:21] <ada_> learning the objects and the relationships between them would be the same
[17:52:27] <ada_> managed services make putting up the cluster easier
[17:52:41] <ada_> bootstrapping a k8s cluster is kind of a royal pain
[17:53:32] <ada_> check out k3s, k0s, kurl.sh, minikube
[17:53:44] <ada_> all projects to help stand up a kubernetes cluster in various ways
[17:54:07] <ada_> managed services are great if you don't want to have to do the maintenance yourself
[17:54:46] <programmerq> wez what sort of deploy usecases are you wanting to do? if swarm's features fit your usecase, then there's no reason to go through the kubernetes learning curve.
[17:55:03] <programmerq> or at least you can put off learning kube until you come across some requirements that necessitate it.
[17:55:25] <programmerq> and moving from swarm to kube is usually pretty straightforward if it should ever come to that.
[17:57:04] <programmerq> I've found myself using kompose to generate kubernetes objects from a docker-compose.yaml file, which I'm very very familiar with already since it works with swarm and non-swarm docker setups I've used for years before I got serious about learning kube.
[18:01:46] <wez> Features like failover I am interested in, if a machine that makes up a cluster goes down, restoring the containers that were running on it to another physical machine.  I also want a scheduling platform to launch instance that perform a task then exit at a specified time(s) (cron like).
[18:02:57] <wez> I do like AWS's offering however I run into issues with clients that say "We can't use AWS, we are an X house".
[18:06:28] *** Joins: alzgh (~alzgh@216.155.158.214)
[18:09:15] <ada_> wez: swarm has autohealing, but it doesn't have scheduled jobs.  there are ways to simulate scheduled jobs in swarm, however.
[18:09:53] *** Quits: varaindemian (~varaindem@86.124.78.162) (Quit: Client closed)
[18:10:01] <ada_> I think there's a PR out there to add a Job type to swarm but idk if it's been pulled in yet
[18:10:05] *** Quits: travisghansen (~travisgha@192.74.130.86) (Quit: The Lounge - https://thelounge.github.io)
[18:11:00] <ada_> kubernetes has Job and CronJob types built in.  in swarm, I usually just make a fairly empty container that has my job/script in it and run it with an appropriately long restart delay, which usually gets the job done
[18:11:07] *** Joins: travisghansen (~travisgha@192.74.130.86)
[18:11:41] <Lutin> ada_ :D How is life ?
[18:12:17] <ada_> been busy lately
[18:12:45] <Lutin> ada_ which is good!
[18:12:48] <programmerq> wez: do you need that failover to mount persistent storage on the arbitrary hosts that the workloads could run on? swarm can handle an nfs mount okay, but kubernetes' storage interface is much more robust. kube can handle nfs mounts or block mounts and has tons of storage drivers.
[18:13:01] <Lutin> as long it are not men... they keep you busy
[18:13:16] <Lutin> We talk the same about women so don't worry :D
[18:13:35] <ada_> Lutin: no worries there, I don't date men lol
[18:13:49] <Lutin> ada_ :D You made my day :D
[18:13:58] <Lutin> can I make yours ? 
[18:14:13] <ada_> deliver this presentation for me and you'll make my day
[18:14:46] <Lutin> ada_ sure, I think I can :)
[18:14:58] <Lutin> oh you moved that one over to yesterday and today indeed
[18:15:05] <Lutin> heh still same effect :P
[18:18:03] *** Joins: sveinse (~sveinse@2a01:799:55e:1000:c23f:d5ff:fe69:c6fb)
[18:19:55] <sveinse> YEAH! I finally made ipv6 work the way I need in my nginx container! Had to use experimental and ip6tables settings
[18:20:38] <sveinse> Looking forward to the day using ipv6 in docker is a smooth ride :D
[18:22:13] *** Joins: gschanuel6 (~gschanuel@user/gschanuel)
[18:22:25] *** Quits: gschanuel (~gschanuel@user/gschanuel) (Read error: Connection reset by peer)
[18:23:54] <wez> programmerq: Yeah, some are from Windows servers / DFS
[18:24:44] <programmerq> dfs mounts should be similar to nfs mounts from swarm's point of view. orchestrating block storage across hosts gets tricky in swarm.
[18:25:58] <programmerq> basically, you have the service definition with --mount that has everything needed to make the mount syscall. The host must have nfs client libraries. There's an example in this section of the docs: https://docs.docker.com/storage/volumes/#choose-the--v-or---mount-flag
[18:26:09] <programmerq> --mount 'type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=local,volume-opt=type=nfs,volume-opt=device=<nfs-server>:<nfs-path>,"volume-opt=o=addr=<nfs-address>,vers=4,soft,timeo=180,bg,tcp,rw"'
[18:26:20] <programmerq> it'd be a similar setup for dfs I imagine
[18:27:42] <programmerq> block devices can get tricky because it's usually more than *just* calling mount somewhere. you'd need an extra component to attach the storage, enforce that it's really detached, create the filesystem if needed. swarm just doesn't have that built in. there are some third party projects that help fill the gap, but most users with that usecase and the developers on the storage provider side have
[18:27:44] <programmerq> long moved on to kubernetes.
[18:29:31] *** Joins: momomo (~momomo@user/momomo)
[18:30:00] <gordonjcp> programmerq: can you recommend a good, up-to-date, at least vaguely aware of best practices guide for getting started with kubernetes?
[18:33:25] <ada_> there are some interactive guides on the kubernetes docs site
[18:33:58] <ada_> https://kubernetes.io/docs/tutorials/kubernetes-basics/
[18:34:21] <ada_> minikube is pretty easy to use to get started, it runs kube in a little vm
[18:38:00] <Lutin> gordonjcp yoyo we are making a pdns proxy so we can manage seperate powerdns instances with traeik/acme :)
[18:43:54] <gordonjcp> Lutin: interesting
[18:44:23] <gordonjcp> Lutin: not used powerdns, sounds good though
[18:47:15] *** Quits: maret (~textual@193.37.255.238) (Ping timeout: 265 seconds)
[18:52:37] <Lutin> gordonjcp yeah it's pretty finished. We are setting up a DNS service. PowerDNS is great but there are too many options possible
[18:57:07] *** Joins: onelegend (onelegend@wireguard/tunneler/onelegend)
[18:59:46] *** gordonjcp-lounge is now known as gordonjc1
[19:02:02] <zamba> how can i detach after running docker-compose up?
[19:02:47] *** Quits: simplicity (~yti@user/simplicity) (Quit: leaving)
[19:02:50] <zamba> ah, ^P + ^Q
[19:07:03] *** Joins: simplicity (~yti@user/simplicity)
[19:29:25] *** Joins: lilgopher (~textual@c-73-51-174-246.hsd1.il.comcast.net)
[19:29:54] *** Quits: jazzy (~jaziz@2600:380:8644:8168:ad48:50e6:fb3f:d642) (Ping timeout: 240 seconds)
[19:32:24] *** Joins: oxum (~oxum@122.181.34.214)
[19:37:03] *** Quits: oxum (~oxum@122.181.34.214) (Ping timeout: 252 seconds)
[19:44:05] *** Quits: tex (~dee@user/dix) (Read error: Connection reset by peer)
[19:45:30] *** Joins: Null_A (~null_a@2601:645:8700:2290:74e6:d8ab:910e:9608)
[19:54:40] *** Joins: tang^ (~DoofusCan@2604:3d09:47c:f970:bd9a:83d:2d57:dee4)
[20:06:31] *** Joins: oxum (~oxum@122.181.34.214)
[20:09:51] *** Joins: hposca (~hposca@node-1w7jr9phoke2t9sajlv8zcl7c.ipv6.telus.net)
[20:13:05] *** Quits: Lutin (~Lutin@user/lutin) (Quit: Lutin)
[20:20:18] *** Quits: Bossi (~quassel@p4fc2272c.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[20:22:09] *** Joins: Bossi (~quassel@p4fc22c5c.dip0.t-ipconnect.de)
[20:25:03] *** Quits: Null_A (~null_a@2601:645:8700:2290:74e6:d8ab:910e:9608) (Remote host closed the connection)
[20:25:31] *** Quits: oxum (~oxum@122.181.34.214) (Ping timeout: 268 seconds)
[20:25:38] *** Joins: Null_A (~null_a@2601:645:8700:2290:74e6:d8ab:910e:9608)
[20:26:56] *** Quits: greatgatsby (~greatgats@bras-base-toroon0411w-grc-52-142-114-106-7.dsl.bell.ca) (Quit: Leaving)
[20:28:28] *** Quits: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com) (Quit: WeeChat 3.0.1)
[20:29:46] *** Quits: Null_A (~null_a@2601:645:8700:2290:74e6:d8ab:910e:9608) (Ping timeout: 240 seconds)
[20:37:53] *** Joins: Atum_ (~IRC@user/atum/x-2392232)
[20:43:23] *** Joins: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com)
[20:49:53] *** Joins: Null_A (~null_a@2601:645:8700:2290:5520:ed9a:24da:3d0c)
[20:58:47] <aesthetikx> What is the latest wisdom on how to use the dreaded 'tag', especially in the context of a feature branch based CI workflow?
[20:59:00] <aesthetikx> e.g. tag branch name, tag commit hash, both?
[20:59:10] <ada_> aesthetikx: depends on what you want 
[20:59:20] *** Quits: DevAntoine (~DevAntoin@195.101.183.21) (Remote host closed the connection)
[20:59:50] <ada_> aesthetikx: commit sha as a tag is great, also tag git-tag and branch names if you want that too
[20:59:51] <aesthetikx> basically I am looking for test-build-cache performanc
[20:59:54] <ada_> a tag is cheap as free
[21:00:14] <ada_> the same layers can be referenced by multiple tags so choosing which tags to apply should be nearly free in terms of time
[21:00:22] <aesthetikx> basically at the beginning of each build I want to pull down either the current branch or master if it doesnt exist yet
[21:00:45] <aesthetikx> I'm not even deploying docker containers just using it for the build and test pipeline anyway
[21:00:58] <aesthetikx> right ok so I can have multiple tags
[21:01:28] <aesthetikx> but for people who are actually deploying containers, I hear 'latest' is a bad idea?
[21:01:41] <ada_> I build images and tag then with commit sha - then your entire build history lines up with commit msgs.  then, if I have a feature branch that I'm working on, I might also tag with branch name. then, if I am ready to make a release, I might git-tag that commit with a version number, and I also tag based on git-tags
[21:01:55] <aesthetikx> yeah that makes sense
[21:01:56] <ada_> its not "bad" you just have to understand what it does and the image author has to be diligent 
[21:02:12] <ada_> :latest carries no context, so its simply, the most recently build image that didn't have a more specific tag specified
[21:02:38] <aesthetikx> right, is there any risk of deploying stale images too? I guess that would be setup depenednet
[21:02:45] <ada_> depends on how you measure risk
[21:04:26] <ada_> theres no version control to images, or in the regsitry, so yeah, if you're relying on re-using teh same tag over and over, there is some "risk" associated with not having the most up to date image _at the tag specified_ but you could always query the registry to see if there's a newer manifest for your tag
[21:04:37] <ada_> so it's really up to the author to decide how to version their images using tags appropriately
[21:04:45] <aesthetikx> understood, thanks for the info
[21:14:00] *** Joins: Lutin (~Lutin@user/lutin)
[21:21:26] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 265 seconds)
[21:22:02] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[21:31:27] <Nothing4You> what option do i have to change the owner of a folder declared as volume after it was declared as volume path? specifically i want to change the owner after i FROM the original container which declares VOLUME
[21:32:26] *** Quits: Maxattax (~max@50-195-160-193-static.hfc.comcastbusiness.net) (Ping timeout: 252 seconds)
[21:34:27] <programmerq> Nothing4You▸ unfortunately, this is a limitation of the docker builder. You basically need to fork the FROM image and remove the VOLUME instruction.
[21:36:11] <Nothing4You> even more of a pain :(
[21:41:03] *** Joins: Sasazuka (~Sasazuka@user/sasazuka)
[21:42:56] *** Joins: oxum (~oxum@106.203.221.241)
[21:44:08] *** Quits: linsux (~metbsd@user/linsux) (Read error: Connection reset by peer)
[21:45:58] <tabakhase> depending on "what", changing that path and just ignoring thevolume may be simplest (mysql or such, flipping the datadir is pretty simple)
[21:46:41] <tabakhase> dunno if one can from scratch; copy / / --from=... :D but that seems even uglier...
[21:47:09] <tabakhase> (and youd still have to manually carry over things like entry/cmd/env...)
[21:47:32] *** Quits: oxum (~oxum@106.203.221.241) (Ping timeout: 268 seconds)
[21:47:45] *** Joins: andrzejv (~andrzejv@78-56-77-187.static.zebra.lt)
[21:48:02] *** Joins: linsux (~metbsd@user/linsux)
[21:53:11] *** Joins: goldfish (~goldfish@user/goldfish)
[21:57:06] *** Joins: Maxattax (~max@50-195-160-193-static.hfc.comcastbusiness.net)
[22:10:45] *** Quits: candyman1 (~candyman@user/candyman) (Remote host closed the connection)
[22:24:48] *** Quits: fossdd (~fossdd@sourcehut/user/fossdd) (Ping timeout: 252 seconds)
[22:25:41] *** Joins: fossdd (~fossdd@sourcehut/user/fossdd)
[22:35:06] *** Quits: goldfish (~goldfish@user/goldfish) (Ping timeout: 256 seconds)
[22:43:27] *** Joins: goldfish (~goldfish@user/goldfish)
[22:53:55] *** Quits: c10l (~c10l@89.34.167.207) (Read error: Connection reset by peer)
[22:56:09] *** Quits: Atque (~Atque@user/atque) (Quit: ...)
[22:56:11] *** Joins: c10l (~c10l@89.34.167.207)
[23:07:58] *** Quits: vidbina (~vid@dynamic-089-014-050-093.89.14.pool.telefonica.de) (Ping timeout: 256 seconds)
[23:11:52] *** Parts: onelegend (onelegend@wireguard/tunneler/onelegend) (WeeChat 2.8)
[23:12:01] *** Joins: onelegend (onelegend@wireguard/tunneler/onelegend)
[23:19:29] *** Quits: tang^ (~DoofusCan@2604:3d09:47c:f970:bd9a:83d:2d57:dee4) (Quit: So as you can see from this flowchSQUIRREL!!)
[23:22:08] *** Joins: tang^ (~DoofusCan@2604:3d09:47c:f970:bd9a:83d:2d57:dee4)
[23:30:38] *** Quits: goldfish (~goldfish@user/goldfish) (Ping timeout: 256 seconds)
[23:43:16] *** Quits: TomyWork (~TomyLobo@p200300e80f133c0065813425a42d3549.dip0.t-ipconnect.de) (Quit: Leaving)
[23:52:36] *** Joins: brickfat (~brickfat@user/brickfat)
[23:55:38] *** Parts: brickfat (~brickfat@user/brickfat) ()
