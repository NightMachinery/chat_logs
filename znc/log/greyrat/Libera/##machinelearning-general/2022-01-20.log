[00:02:24] *** Quits: jlrnick (~josephler@2a01cb040a15940039e6d2167b1d3fd8.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[00:17:28] *** Joins: marcello42 (~mp@2001:1a81:13ee:e600:7579:d375:3cfe:86fd)
[00:33:40] *** Quits: trace987 (~trace@ip5b42976d.dynamic.kabel-deutschland.de) (Remote host closed the connection)
[00:48:58] *** Quits: marcello42 (~mp@2001:1a81:13ee:e600:7579:d375:3cfe:86fd) (Read error: Connection reset by peer)
[00:51:09] *** Joins: spaceseller (~spacesell@31.147.205.13)
[00:54:06] *** Quits: georgios (~georgios@user/georgios) (Quit: Konversation terminated!)
[01:10:52] *** Joins: marcello42 (~mp@2001:1a81:13ee:e600:7579:d375:3cfe:86fd)
[01:15:12] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[01:17:57] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 240 seconds)
[01:36:58] *** Joins: trace987 (~trace@ip5b42976d.dynamic.kabel-deutschland.de)
[01:44:21] *** Quits: marcello42 (~mp@2001:1a81:13ee:e600:7579:d375:3cfe:86fd) (Read error: Connection reset by peer)
[02:02:08] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[02:02:46] *** Quits: mantiX (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:03:23] *** Joins: marcello42 (~mp@2001:1a81:13ee:e600:7579:d375:3cfe:86fd)
[02:09:15] *** Joins: sinaowolabi_ (~SinaOwola@41.58.56.106)
[02:09:52] *** Quits: marcello42 (~mp@2001:1a81:13ee:e600:7579:d375:3cfe:86fd) (Read error: Connection reset by peer)
[02:13:15] *** Quits: Donitz (~Donitz@88-115-149-215.elisa-laajakaista.fi) (Ping timeout: 256 seconds)
[02:14:37] *** Joins: marcello42 (~mp@2001:1a81:13ee:e600:2429:f440:aff0:8b70)
[02:15:27] *** Joins: Donitz (~Donitz@88-115-149-215.elisa-laajakaista.fi)
[02:47:03] *** Joins: namkeleser (~namkelese@124.188.192.57)
[02:50:05] *** Quits: Codaraxis__ (~Codaraxis@user/codaraxis) (Ping timeout: 256 seconds)
[03:01:25] *** Quits: sinaowolabi_ (~SinaOwola@41.58.56.106) (Ping timeout: 256 seconds)
[03:10:04] *** Quits: palasso (~palasso@user/palasso) (Read error: Connection reset by peer)
[03:14:32] *** Joins: sinaowolabi_ (~SinaOwola@160.152.135.50)
[03:42:37] *** Quits: marcello42 (~mp@2001:1a81:13ee:e600:2429:f440:aff0:8b70) (Ping timeout: 240 seconds)
[03:44:55] *** Joins: marcello42 (~mp@2001:1a81:1204:8a00:ea0c:2047:6cd8:7a12)
[03:50:34] *** Quits: marcello42 (~mp@2001:1a81:1204:8a00:ea0c:2047:6cd8:7a12) (Ping timeout: 268 seconds)
[03:53:34] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[03:56:48] *** Quits: sinaowolabi_ (~SinaOwola@160.152.135.50) (Ping timeout: 250 seconds)
[04:09:19] *** Joins: sinaowolabi_ (~SinaOwola@160.152.104.223)
[04:18:06] *** Joins: [_] (~itchyjunk@user/itchyjunk/x-7353470)
[04:19:37] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Ping timeout: 240 seconds)
[04:27:00] *** [_] is now known as [itchyjunk]
[04:27:24] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) ()
[06:31:05] *** Quits: namkeleser (~namkelese@124.188.192.57) (Ping timeout: 256 seconds)
[07:01:33] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[07:03:20] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[08:21:57] *** Quits: sinaowolabi_ (~SinaOwola@160.152.104.223) (Ping timeout: 240 seconds)
[08:36:32] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[08:37:53] *** Quits: defjam (~eb0t@90.203.120.248) (Read error: Connection reset by peer)
[08:43:42] *** Joins: defjam (~eb0t@90.209.247.249)
[09:21:23] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[09:45:13] *** Joins: derelict (~derelict@user/derelict)
[10:13:36] *** Joins: manti7 (~manti7@176.10.104.94)
[10:25:04] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[10:29:27] *** Joins: sinaowolabi_ (~SinaOwola@160.152.104.223)
[11:13:05] *** Joins: palasso (~palasso@user/palasso)
[11:15:29] *** Joins: spaceseller (~spacesell@31.147.205.13)
[11:34:33] *** Joins: marcello42 (~mp@2001:1a81:1204:8a00:ea0c:2047:6cd8:7a12)
[11:46:59] *** Quits: _flood (flooded@gateway/vpn/protonvpn/flood/x-43489060) (Remote host closed the connection)
[11:47:19] *** Joins: _flood (flooded@gateway/vpn/protonvpn/flood/x-43489060)
[11:52:20] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[12:00:37] *** Quits: _flood (flooded@gateway/vpn/protonvpn/flood/x-43489060) (Ping timeout: 240 seconds)
[12:23:56] *** Joins: jlrnick (~josephler@2a01cb040a15940015fe0f818448252a.ipv6.abo.wanadoo.fr)
[12:24:36] *** Quits: spaceseller (~spacesell@31.147.205.13) (Quit: Leaving)
[12:30:02] <lericson> could somebody explain to me why people bother with stuff like GELU?
[12:30:09] <lericson> https://arxiv.org/abs/1606.08415v4
[12:30:38] <SigmoidFroid> ⇒  [1606.08415v4]
[12:30:54] <lericson> in their own reported numbers, on a toy problem, there is only an advantage in some particular settings and it complicates the network immensely (error functions in activations??)
[12:31:41] <lericson> they also managed to make their vector plots so dense they don't render properly on my computer :|
[12:32:29] <lericson> am i crazy? they show time and time again that validation score is unchanged?
[12:32:48] <lericson> > For the numerous datasets evaluated in this paper, the GELU exceeded the accuracy of the ELU and ReLU consistently, making it a viable alternative to previous nonlinearities.
[12:32:52] <lericson> no it didn't?
[12:42:27] <dostoyevsky2> lericson: I wonder if GELU is related to stochastic rounding
[12:45:52] <lericson> that's what i call my diet
[13:28:20] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Ping timeout: 256 seconds)
[13:32:07] *** Quits: harshhpareek (~harshhpar@c-67-188-113-15.hsd1.ca.comcast.net) (Ping timeout: 256 seconds)
[13:46:56] <xs> lericson: was the article published somewhere that is peer reviewed?
[13:48:00] <xs> if it was somewhere with openreview (in 2016, probably only iclr i would guess), you can look at what the reviewrs thought
[13:49:16] <xs> note: gpt uses gelus, and subsequently bert and gpt3 did too
[13:49:21] <xs> so you are likely missing something
[13:49:22] <lericson> yes that's my point
[13:50:33] <lericson> the thing is, i'm seeing transformers without GELUs too and they seem to work just fine
[13:52:41] <xs> DL is full of folklore of what works best. sometimes choices are only slightly above the seed noise
[13:54:52] <lericson> yes, and i think especially activation functions
[13:55:13] <lericson> there are better and worse such things i think
[13:55:18] <lericson> the ReZero paper i found convincing for example
[13:55:23] <lericson> this one, i'm just left scratching my head?
[13:55:44] <lericson> and weren't we all supposed to be using leaky relus anyway?
[14:01:19] <lericson> on an entirely unrelated note, i would love a tool in tensorboard or so where you could say "linearize the curve at cursor" so you can estimate asymptotes
[14:01:36] <lericson> like it'd just draw a tangent along the curve where your mouse cursor is
[14:21:24] <lericson> i also have another question, in transformers you typically alternate cross and self attention -- but why?
[14:24:26] *** Quits: jerome- (~jerome@88.173.24.196) (Ping timeout: 256 seconds)
[14:24:47] *** Joins: jerome- (~jerome@88.173.24.196)
[14:41:14] *** Joins: `Tim (~zenguin@user/zenguin)
[15:01:58] *** Quits: Scarecr0w (scarecr0w@user/scarecr0w) (Ping timeout: 250 seconds)
[15:15:28] <hodapp> they show faster, smoother convergence and lower error in that paper for every test I am looking at
[15:18:35] <hodapp> also, all of the 'standard' transformers I see are just self-attention
[15:26:34] *** Joins: sinaowolabi__ (~SinaOwola@160.152.104.223)
[16:03:55] <taeaad> This is a bit odd: rmse: 92118.4
[16:04:21] <taeaad> Usually it starts at around 15 or sometimes 70...
[16:04:38] <taeaad> I guess it could be that the data is less sparse than usual
[16:14:26] <dostoyevsky2> taeaad: what value does your model converge to eventually?
[16:19:47] <lericson> hodapp: not on validation after 50 epochs, no
[16:19:57] <lericson> which is the crucial number if you ask me
[16:20:07] <lericson> who cares if it got slightly lower training error or got there ten epochs earlier
[16:26:08] <taeaad> dostoyevsky Eh should be around 4 or 5, but could be I subsetted in a nonrepresentative way...
[16:28:11] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Read error: Connection reset by peer)
[16:28:38] <hodapp> lericson: early stopping certainly cares; training will finish sooner
[16:29:17] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[16:32:02] <hodapp> I don't know why you'd arbitrarily pick 50 epochs but in almost every test I am looking at, the test error (not training) is *also* converging faster
[16:32:40] <lericson> what about figure 2
[16:32:53] *** Quits: sinaowolabi__ (~SinaOwola@160.152.104.223) (Ping timeout: 256 seconds)
[16:33:16] *** Quits: jlrnick (~josephler@2a01cb040a15940015fe0f818448252a.ipv6.abo.wanadoo.fr) (Remote host closed the connection)
[16:33:37] <hodapp> you wouldn't just arbitrarily take the model at 50 epochs, that is not some magical number; you do early stopping
[16:33:54] <lericson> the cifar results (fig 6) are also a wash
[16:34:21] *** Joins: Codaraxis__ (~Codaraxis@user/codaraxis)
[16:34:26] <hodapp> test error converges in ~5 epochs in figure 2
[16:34:53] <hodapp> In figure 6 if you arbitrarily take epoch 50 it is still lower in test error for
[16:34:55] <hodapp> GeLU
[16:35:22] <lericson> by like .1%
[16:36:10] <dostoyevsky2> taeaad: might be interesting to check if the data was normalized correctly and initialization weights where also calculated correctly
[16:36:41] <lericson> and fig 2 shows elu winning out
[16:36:51] <lericson> suggesting that the results may not be as robust as you would hope
[16:37:06] <lericson> oh actually that's after overfitting
[16:37:54] <hodapp> that is when they purposely set dropout very high
[16:39:49] <lericson> and 3.3: > We train each network five times per learning rate, and the median test set error is 12.57% for the GELU, 12.67% for the ReLU, and 12.91% for the ELU.
[16:39:56] <lericson> a whooping .4%
[16:42:46] *** Joins: jlrnick (~josephler@2a01cb040a15940015fe0f818448252a.ipv6.abo.wanadoo.fr)
[16:42:51] <hodapp> this is still consistently exceeding the others in accuracy, which is what they claim, and the faster convergence in both train and test is still there
[16:43:48] <hodapp> which is also an asset if you have ever worked in ML and dealt with novel architectures or training schedules
[16:45:13] <lericson> what are you implying here?
[16:45:16] <dostoyevsky2> https://a2c.fyi/1606.08415 <- interesting that they also perform better when dropout is applied.  As I'd guessed that GELU just add another form of regularization
[16:45:16] <SigmoidFroid> ⇒  a2c [1606.08415] Gaussian Error Linear Units (GELUs)
[16:46:50] <lericson> dostoyevsky2: i think they motivate the use of the cdf precisely with dropout
[16:48:02] <lericson> i say motivate but that is a bit strong
[16:48:50] *** Quits: sinaowolabi_ (~SinaOwola@160.152.104.223) (Read error: No route to host)
[16:51:04] <lericson> i remain sceptical, and i suspect that you'd see very little difference in practice while introducing significantly more computation
[16:52:27] *** Joins: sinaowolabi_ (~SinaOwola@160.152.104.223)
[16:54:00] <hodapp> faster and smoother convergence definitely pays off in practice.
[16:55:59] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Quit: Leaving)
[16:58:36] <dostoyevsky2> I'd be interested to see how GELU performs against leaky ReLU... also how does it perform in residual architectures?
[17:00:32] <dostoyevsky2> I guess a lot of things that ML folks see as folklore are based on the fact that Math's real numbers are represented as IEEE floating point and all the oddities that those approximations bring
[17:01:43] <hodapp> section 3.5 tests a residual network
[17:02:26] <hodapp> I don't think floating point error especially at 32-bit/64-bit accounts for really any of this
[17:03:30] <hodapp> a lot of it is spoken of as folklore because there is simply no good theory that gives meaningful predictions or explanations
[17:04:36] <hodapp> the noise in the data is almost always going to swamp any quantization error from floats
[17:06:15] <dostoyevsky2> hodapp: GELU prevents dead weights, because there was not enough space to represent the result.  No need for folklore
[17:07:29] <dostoyevsky2> But so do residual architectures
[17:09:23] *** Joins: _flood (flooded@gateway/vpn/protonvpn/flood/x-43489060)
[17:16:16] <hodapp> 'folklore' might be an overly harsh term but a lot of things in ML are designed around "here is how we want it to work" and not "here is what we can provide strong evidence for it working like"
[17:20:32] <hodapp> a lot of it might come with strong *empirical* evidence for the results itself - which pushes it past folklore - but still not into the territory of having strong evidence to explain those results
[17:20:44] *** Joins: marcello1 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3)
[17:21:29] *** Quits: marcello42 (~mp@2001:1a81:1204:8a00:ea0c:2047:6cd8:7a12) (Ping timeout: 268 seconds)
[17:31:39] <dostoyevsky2> I think everyone in ML should at least have read this book: https://dl.acm.org/doi/10.1145/103162.103163  .. It might take a rocket scientist to figure out how to write it, but it doesn't take one to read it...
[17:31:41] <SigmoidFroid> ⇒  What every computer scientist should know about floating-point arithmetic | ACM Computing Surveys
[17:32:32] <dostoyevsky2> But in ML I just often get the feeling that people are surprised you can't just take a math formula, use it in Python and not understand why it wouldn't work
[17:34:44] <dostoyevsky2> And why in practice architectures perform better when it doesn't make any sense mathematically
[17:44:18] *** Quits: marcello1 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3) (Ping timeout: 268 seconds)
[17:49:00] <hodapp> yeah, understanding the limitations of floats definitely helps, but things like overflow and underflow and quantization error present in their own peculiar ways
[18:09:48] *** Joins: clime (~clime@78-80-115-5.customers.tmcz.cz)
[18:11:15] <clime> what's the difference regarding content between #machinelearning and #machinelearning-general channel?
[18:12:16] <hodapp> #machinelearning just redirects here
[18:12:50] <hodapp> for ##machinelearning, I wouldn't know, as I'm banned from there - but when I *was* there it was 95% just bots repeating news headlines
[18:17:25] <mefistofeles> dostoyevsky2: a whole book on floating point arithmetic? wow
[18:18:36] <hodapp> getting things right with floats can be very difficult
[18:19:17] <hodapp> there's a reason most libs just dump things onto BLAS/LAPACK when they can, and that a whole lot of COBOL routines are still in use in some sectors
[18:20:50] <mefistofeles> yeah, I understand that, but a whole book sounds excessive, unless you want to really in your path on becoming an expert on these things or a BLAS/LAPACK or similar maintainer :P
[18:23:27] <clime> right, i mean ##machinelearning, sries. Ok, thanks for the info
[18:26:21] <hodapp> sure. feel free to hang around here - we aren't going to ban you for criticizing or for being somewhat off-topic
[18:26:35] <hodapp> nor send you threatening messages
[18:27:18] <clime> sounds like it must be pretty rough channel there
[18:28:12] <mefistofeles> clime: say that again... SAY THAT AGAIN....
[18:28:19] <mefistofeles> clime: just joking... welcome xD
[18:28:30] <clime> thx ;)
[18:28:31] <hodapp> the person who owns it has... very strong opinions
[18:28:38] <hodapp> also, very loud tantrums
[18:29:47] <hodapp> back at freenode he instituted a policy that he would permaban you for being in ##machinelearning-general, or for even mentioning it in ##machinelearning
[18:30:20] <clime> k, heh, i just asked there: "Is this channel for more technical discussion than ##machinelearning-general?"
[18:30:40] <clime> maybe, i will get banned too...
[18:31:13] <hodapp> maybe so
[18:31:52] <hodapp> you might also get an earful about dostoyevsky
[18:32:37] *** Joins: georgios (~georgios@user/georgios)
[18:35:10] <clime> let's see...
[18:37:53] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi)
[19:01:47] *** Joins: marcello1 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3)
[19:05:18] *** Quits: marcello1 (~mp@2001:1a81:1204:8a00:3991:cbbc:67fd:83b3) (Read error: Connection reset by peer)
[19:07:27] *** Joins: Codaraxis_ (~Codaraxis@user/codaraxis)
[19:10:17] *** Quits: Codaraxis__ (~Codaraxis@user/codaraxis) (Ping timeout: 240 seconds)
[19:30:00] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[19:52:12] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-56734f-193.dhcp.inet.fi) (Read error: Connection reset by peer)
[19:57:45] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[20:04:37] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[20:21:30] *** Joins: harshhpareek (~harshhpar@c-67-188-113-15.hsd1.ca.comcast.net)
[20:26:05] *** Quits: harshhpareek (~harshhpar@c-67-188-113-15.hsd1.ca.comcast.net) (Client Quit)
[20:29:17] *** Joins: harshhpareek (harshhpare@c-67-188-113-15.hsd1.ca.comcast.net)
[20:32:11] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[20:39:52] <dostoyevsky2> mefistofeles: The book has 44 pages... I think if you learn C you should also learn about assembly, so in same vein: If you work relies on floating-point you should learn about it, as it will greatly affect you anyhow, so you really can just choose to understand it or not ;-)
[20:40:46] <mefistofeles> dostoyevsky2: fair enough :P
[20:47:06] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[20:49:17] <dostoyevsky2> And yeah, a lot of those things played a role when then implemented libraries such as BLAS or just relying on old fortran libraries (like they do in R)... but just having a better understanding that it's very easy in FP for two non-zero real numbers like `a*b' to have a result of 0 (which would never happen in Math) and having some intuition on how quickly it can happen can already
[20:49:23] <dostoyevsky2> help you a lot
[20:50:11] <mefistofeles> yes, totally agree, just clarifying that I wasn't implying the opposite :)
[21:06:20] *** Joins: marcello42 (~mp@2001:1a81:1204:8a00:ea0c:2047:6cd8:7a12)
[21:07:17] *** Quits: sinaowolabi_ (~SinaOwola@160.152.104.223) (Ping timeout: 240 seconds)
[21:30:01] *** Quits: ns12 (~ns12@user/ns12) (Quit: bye)
[21:30:47] *** Joins: ns12 (~ns12@user/ns12)
[22:00:25] *** Joins: sinaowolabi_ (~SinaOwola@160.152.104.223)
[22:21:57] *** Quits: clime (~clime@78-80-115-5.customers.tmcz.cz) (Ping timeout: 256 seconds)
[22:22:07] *** Joins: SiegeLord (~sl@user/siegelord)
[22:40:10] *** Joins: clime (~clime@78-80-115-5.customers.tmcz.cz)
[22:47:42] *** Quits: jlrnick (~josephler@2a01cb040a15940015fe0f818448252a.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[23:44:32] <clime> Has anyone here tried some machine learning with Rust possibly rust+tensorflow (i.e. using https://github.com/tensorflow/rust)? I am thinking from Python to Rust at some point for execution speed mainly but not sure yet.
[23:44:33] <SigmoidFroid> ⇒  GitHub - tensorflow/rust: Rust language bindings for TensorFlow
[23:45:02] <clime> *thinking about switching from Python to Rust...
[23:45:03] <hodapp> basically all of your execution speed is going to be in Tensorflow's hands
[23:45:26] <hodapp> so the only real reason to switch there is "because you like Rust" or "because you need Rust for some other reason"
[23:46:01] <clime> ye that's likely true although there is always going be to be some code on top of tensorflow as well for handling data etc.
[23:46:11] <clime> hodapp: your are likely right
[23:46:28] <hodapp> probably even most of your loaders are going to be Tensorflow modules unless you are doing something custom
[23:47:05] <hodapp> let us know if you try this though. I don't do much Tensorflow, and while I do some Rust I don't do ML in it
[23:47:14] <hodapp> I did Tensorflow in Go and it was...... ugly
[23:47:35] <clime> you mean modules like: https://www.tensorflow.org/api_docs/python/tf/data/experimental/load ?
[23:47:36] <SigmoidFroid> ⇒  tf.data.experimental.load | TensorFlow Core v2.7.0
[23:47:51] <hodapp> yeah, something likely to be already implemented in TF
[23:48:50] <hodapp> really Julia interests me more for ML but I've not gotten back to that for awhile...
[23:48:56] <clime> right, main reason is that I am struggling with Python and Pandas especially...
[23:49:42] <clime> i mean i've been coding in Python for a long time now some of it professionally but some of these Pandas constructs are really messing up with my head sometimes.
[23:49:47] <hodapp> huh, like what?
[23:50:07] <clime> and I try to write clean code in Python but it just still seems messy and hard to orient in, idk why
[23:51:04] <hodapp> clean code around ML in general can be very though
[23:52:13] <clime> idk just only, e.g.: df_batch.loc[df_batch['date'].between(starting_position_row['date'], row['date'])] but probably it can get worse
[23:52:51] <clime> these df references inside df[...] are really unusual for me
[23:53:09] <hodapp> have you used things like R & dplyr?
[23:54:03] <clime> also overall .loc[] construct has so many options what can be inside there and diffs between df[], df.loc[], df.iloc[] are sometimes puzzling...now it's better but few months ago it was a real fight
[23:54:16] <clime> hodapp: R & dplyr, not yet
[23:55:31] <hodapp> indexing in general is... a pain to do right.It's ugly in xarray too
[23:56:19] <hodapp> pandas puts a lot of effort into getting the correct semantics around indexing, and xarray is based on pandas (but sort of extends pandas dataframes to the multidimensional case)
[23:56:31] <clime> (i especially hate those SettingWithCopyWarnings, now i also get them mostly but yeah takes a bit of time, otherwise Pandas is great)
[23:56:58] <hodapp> but in general I'm not aware of anything that does dataframes all that much better, especially when it comes to handling indexing correctly
[23:57:34] <hodapp> the only thing I've seen that does it nicer than Pandas is dplyr in R and I'm not sure if it does indexing right
[23:57:54] <clime> okay
[23:58:29] <clime> i was thinking PostgreSql can be good at handling "dataframes" :)
[23:58:57] <hodapp> hmm. SQL DBs are great for relational data, dataframes are sorta kinda relational but not really
