[00:03:10] *** Quits: sinaowolabi__ (~SinaOwola@160.152.117.193) (Ping timeout: 260 seconds)
[01:30:28] *** Quits: basti_ (~basti@ip-84-119-8-195.unity-media.net) (Quit: leaving)
[01:32:05] *** Joins: Klinda (~superleag@user/klinda)
[01:42:30] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 260 seconds)
[01:56:31] *** Joins: CaCode (~CaCode@user/cacode)
[02:15:14] <Samian> Klinda I take it you watched that youtube video?
[02:16:43] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:17:51] <Klinda> no didn't have 3 hours free
[02:19:03] <mefistofeles> Klinda: it is 1.5 hours at 2x
[02:19:04] <mefistofeles> :P
[02:26:02] <Klinda> 2x I don't understand anything
[02:26:10] <Klinda> I just fixed my gradient
[02:26:17] <Klinda> and got 91% of accuracy
[02:27:34] <Klinda> I am seeing some lesson of this youtuber https://www.youtube.com/watch?v=aeM-fmcdkXU
[02:27:36] <SigmoidFroid> ⇒  L8.8 Softmax Regression Derivatives for Gradient Descent - YouTube, 00:19:39
[02:28:34] <Klinda> tomorrow I have to study distributed systems, idk when I have time to spare these 3 hours
[02:30:11] <Klinda> math
[02:30:13] <Klinda> gradient descent
[02:30:16] <Klinda> sigmoid
[02:30:20] <Klinda> yeah.. a lot of maths
[02:30:26] <Klinda> probability etc  etc
[02:30:38] <Klinda> better a bachelor's degree on math than cs
[02:31:57] <Klinda> theta, theta1, theta asumption 1 always
[02:31:59] <Klinda> offset
[02:32:08] <Klinda> best line
[02:34:47] <Klinda> because you have to understand things from scratch, I mean I did a bachelor's degree on some of ml stuffs alone, in master's a lot of concepts, it's not just use just two library functionsss as I did a newbie bachelor's
[02:34:58] <Klinda> thesis*
[02:39:02] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[02:39:27] <HuntsMan> most people that do ML have a bachelors in CS, think about that
[02:40:19] <Klinda> maybe they are good
[02:40:44] <HuntsMan> they can also study this stuff at the bachelor level
[02:40:47] <HuntsMan> your math is just rustyt
[02:40:53] <Klinda> I mean I know I am a noob ahah so yeah that's a common guy here
[02:40:55] <HuntsMan> this is very basic, I have done much more complex stuff
[02:41:25] <Klinda> I need time to learn stuffs
[02:41:31] <HuntsMan> yes
[02:41:39] <HuntsMan> and a good lecturer :D
[02:41:44] <Klinda> ahahaha
[02:41:51] <Klinda> a mirage
[02:42:00] <Klinda> always done things alone
[02:42:05] <Klinda> with indians
[02:42:14] <Klinda> indians... I mean the best ones in youtube
[02:42:45] <mefistofeles> Klinda: my suggestion is to try not to learn this stuff from video lectures
[02:42:52] <mefistofeles> that is, if you are considering significant learning
[02:43:14] <Klinda> and from where ?
[02:43:53] <mefistofeles> books I guess
[02:43:58] <mefistofeles> or tutorials
[02:44:21] <HuntsMan> mefistofeles: depends on the video lecture, it can be a good explainer
[02:44:21] <Klinda> books I don't think has cross entropy gradient with softmax
[02:44:30] <HuntsMan> they do :)
[02:45:17] <mefistofeles> HuntsMan: sure, but this have been studied several times, you get less significant learning from video lectures
[02:45:42] <mefistofeles> that said, it doesn't mean you should avoid them, sometimes the video lectures have better material than any other non-video lecture out there
[02:46:52] <Samian> Klinda when the pandemic first started I watched this video every day for a month
[02:49:01] <Klinda> and what happened Samian?
[02:49:40] <Klinda> mefistofeles: I just hate accademic stuff formulas, cause sometimes I get lost cause I don't know how to apply them in real-case
[02:50:01] <mefistofeles> Klinda: yeah, knowledge is not trivial, you should work hard to understand things
[02:50:05] <mefistofeles> tha tis, if you really want to
[02:51:52] <Klinda> I am a type of guy who can stay there learning things for hours but in the end didn't accomplish any result
[02:52:14] <mefistofeles> ah yeah, most of us are like that, I'd say
[02:52:34] <mefistofeles> society should reward people that know stuff more than it does, but well, hard to change it
[02:52:37] <mefistofeles> :P
[02:52:58] <Samian> Can anyone see this?
[02:52:59] <Samian> https://jspaint.app/#local:73bc5fbf36aca
[02:53:00] <SigmoidFroid> ⇒  JS Paint
[02:53:03] <mefistofeles> I'm pretty sure I've helped people becoming rich, in critical stages of their career
[02:53:04] <Samian> I want to train that by hand
[02:53:12] <mefistofeles> but here I am, talking to strangers in poor mans land IRC
[02:53:13] <mefistofeles> :P
[02:53:59] <mefistofeles> Samian: train what exactly?
[02:54:03] *** Quits: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de) (Ping timeout: 256 seconds)
[02:54:06] <Samian> do you see it?
[02:54:14] <mefistofeles> Samian: I don't see your drawings
[02:54:18] <mefistofeles> just the blank canvas
[02:54:24] <Samian> darn
[02:54:40] <mefistofeles> oh wait
[02:54:49] <mefistofeles> I think it works somehow...
[02:54:55] <Samian> cool there's an upload to imgur button
[02:54:56] <Samian> https://imgur.com/KLdCkwB
[02:54:57] <SigmoidFroid> ⇒  Imgur: The magic of the Internet
[02:55:18] <Klinda> tell me a book who just type the gradient of cross entropy of softmax
[02:55:38] <Klinda> the vectorized form!
[02:55:56] <Klinda> imagine learning the not vectorized form ahaha
[02:57:14] <Samian> URL:https://i.imgur.com/palTcvz.png
[02:57:25] <SigmoidFroid>  ⇒  (PNG) No title 15K
[02:57:59] <mefistofeles> it is very interesting how all these things have analogies in many fields, even for centuries
[02:58:07] <mefistofeles> such as stat mech partition function with softmax
[02:59:47] <Samian> out_1 = (in_1*w_1 + in_2*w_6)*w_4   +    (in_1*w_5 + in_2*w_2)*w_7
[03:00:14] <mefistofeles> mathb.in
[03:00:15] <mefistofeles> :P
[03:01:18] <Klinda> now I am doing the naive bayess for example and seems more easy
[03:01:19] <Samian> let's say we want to backpropg using in_1=5 and out_1=3 as ground truth
[03:02:10] <Samian> and in_2 = 2
[03:02:26] <Samian> 3 = (5*w_1 + 2*w_6)*w_4   +    (5*w_5 + 2*w_2)*w_7
[03:02:59] <Samian> let's say all weights are initialized to 0.5
[03:03:40] <Samian> (5*0.5 + 2*0.5)*0.5 + (5*0.5 + 2*0.5)*0.5  = 3.5
[03:04:08] <Samian> so y_true = 3.  y_pred = 3.5
[03:04:55] <Samian> suppose we're doing mean square error.
[03:05:32] <Samian> sqrt( (3 - 3.5)^2) = 3-3.5  = 0.5
[03:05:44] <Samian> so the loss value is 0.5
[03:06:21] <Samian> we can replace 3 with the nueral net
[03:06:40] <Samian>  (5*w_1 + 2*w_6)*w_4   +    (5*w_5 + 2*w_2)*w_7   -   3.5     =    0.5
[03:07:28] <Samian> I think we want to compute the derivative, and than plug in 0.5 for all the w's as the point we want to find the tangential hyperplane?
[03:07:52] <mefistofeles> Samian: you should really write a mathb.in doc for that
[03:08:05] <mefistofeles> you will even have something to easily share in the future
[03:08:10] <Samian> you mean write in latex?
[03:08:16] <mefistofeles> well, or whatever works
[03:08:31] <Samian> I learned latex 10 years ago. It was a pain to type in.
[03:08:55] <Samian> http://mathb.in/67173
[03:08:56] <SigmoidFroid> ⇒  MathB.in - Math pastebin with LaTeX and Markdown support
[03:09:29] <Samian> I was hoping it updates/syncs everone looking at a certain url. but it doesn't
[03:09:36] <Samian> google docs does that though
[03:10:14] <Samian> let's say we want to update w_1.  That means we find the derivative with respect to w_1
[03:11:06] <Samian> I think this is the derivative for w_1: (5 + 2*w_6)*w_4   +    (5*w_5 + 2*w_2)*w_7   -   0    =    0
[03:11:09] <Samian> Is that right?
[03:25:28] <Samian> nope it not
[03:25:44] <HuntsMan> derivative of what?
[03:26:46] <Samian> actually, I realize I need to compute partial derivatives
[03:28:33] <Samian> I need to create a vector of partial derivatives;  [w_1 partial derivative,   w_2 partial derivative,   w_3 partial derivative,   w_4 partial derivative,   w_5 partial derivative,   w_6 partial derivative]
[03:29:03] <Samian> I need to fill that out with the actual partial derivatives
[03:29:51] <Samian> w_1 partial derivative = 5*w_4
[03:30:21] <Samian> w_2 partial derivative = 2*w_7
[03:30:39] <Samian> w_3 is unused for out_1
[03:32:27] <Samian> w_4 partial derivative = 5+2*w_6;    w_5 pd = 5*w_7;   w_6 pd = 2*w_6;   w_7 pd = (5*w_5 + 2*w_2)
[03:33:08] <Samian> correction.  w_6 pd = 2*w_4
[03:34:15] <Samian> now I plug in 0.5 for all the weights since that I decided to initialize them to
[03:44:52] <Samian> this is easy. It all clicks now
[03:51:41] *** Joins: ykska (~ykska@182.226.37.172)
[03:57:26] *** Joins: akevinhuang2 (~thekevinh@user/thekevinhuang)
[03:59:48] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 268 seconds)
[04:35:18] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[04:50:24] *** Joins: trace987 (~trace@ip5b429941.dynamic.kabel-deutschland.de)
[05:32:16] *** Joins: CaCode_ (~CaCode@user/cacode)
[05:35:11] *** Quits: CaCode (~CaCode@user/cacode) (Ping timeout: 264 seconds)
[05:46:09] *** Joins: Coldblackice (~c@user/coldblackice)
[06:13:09] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[06:34:25] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) ()
[06:57:04] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[06:59:25] *** Quits: akevinhuang2 (~thekevinh@user/thekevinhuang) (Ping timeout: 256 seconds)
[07:36:26] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Quit: Leaving)
[07:36:44] *** Joins: akevinhuang (~thekevinh@user/thekevinhuang)
[07:46:37] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[08:07:06] *** Quits: CaCode_ (~CaCode@user/cacode) (Ping timeout: 260 seconds)
[09:04:00] *** Joins: sinaowolabi__ (~SinaOwola@160.152.173.38)
[09:24:06] *** Quits: sinaowolabi__ (~SinaOwola@160.152.173.38) (Ping timeout: 260 seconds)
[09:36:29] *** Joins: sinaowolabi__ (~SinaOwola@169.159.115.54)
[10:01:02] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[10:04:27] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[10:06:59] *** Quits: akevinhuang (~thekevinh@user/thekevinhuang) (Ping timeout: 256 seconds)
[10:15:21] *** Quits: stkrdknmibalz (~test@rrcs-75-87-43-226.sw.biz.rr.com) (Ping timeout: 268 seconds)
[10:22:45] *** Quits: sinaowolabi__ (~SinaOwola@169.159.115.54) (Ping timeout: 268 seconds)
[10:36:56] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[10:39:18] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Remote host closed the connection)
[10:45:53] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[11:19:17] *** Joins: manti7 (~manti7@176.10.104.94)
[12:11:50] *** Joins: CaCode (~CaCode@user/cacode)
[12:13:44] *** Quits: CaCode (~CaCode@user/cacode) (Remote host closed the connection)
[12:14:03] *** Joins: CaCode (~CaCode@user/cacode)
[12:14:03] *** CaCode is now known as netester
[12:14:22] *** Quits: netester (~CaCode@user/cacode) (Remote host closed the connection)
[12:14:52] *** Joins: netester (~CaCode@user/cacode)
[12:28:19] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[13:00:46] *** netester is now known as CaCode
[13:02:08] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[13:47:46] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[13:51:47] *** Joins: jlrnick (~josephler@2a01cb040a159400e928261421e00c3c.ipv6.abo.wanadoo.fr)
[14:33:40] *** Joins: Klinda (~superleag@user/klinda)
[15:01:20] <Klinda> morning
[15:02:44] *** Quits: Samian (~s@user/samian) (Quit: My MacBook has gone to sleep. ZZZzzz…)
[15:05:49] *** Quits: jlrnick (~josephler@2a01cb040a159400e928261421e00c3c.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[16:05:25] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[16:37:10] *** Quits: Klinda (~superleag@user/klinda) (Ping timeout: 260 seconds)
[16:48:54] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[17:00:20] *** Quits: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi) (Quit: Leaving)
[17:19:00] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 268 seconds)
[17:32:48] <twainwek> morning
[17:37:43] *** Joins: Klinda (~superleag@user/klinda)
[17:39:10] *** Joins: Donitz (~Donitz@88-115-149-152.elisa-laajakaista.fi)
[17:56:32] *** Joins: `Tim (~zenguin@user/zenguin)
[19:33:20] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[19:49:24] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[19:50:31] *** Quits: ykska (~ykska@182.226.37.172) (Ping timeout: 245 seconds)
[19:59:28] *** Joins: palasso (~palasso@user/palasso)
[20:10:14] *** Joins: Hunts (~hunts@p4fc9aa32.dip0.t-ipconnect.de)
[20:11:03] *** Quits: HuntsMan (~hunts@p4fc9a453.dip0.t-ipconnect.de) (Ping timeout: 256 seconds)
[20:24:39] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[20:38:16] *** Joins: jlrnick (~josephler@2a01cb040a159400e928261421e00c3c.ipv6.abo.wanadoo.fr)
[20:47:32] *** Joins: sinaowolabi (~SinaOwola@102.134.114.1)
[21:09:11] *** Joins: StormGlass (~CraigDurh@2604:2d80:af07:5300:accd:fbab:7513:2aa0)
[21:15:37] *** Joins: MirrorGate (~CraigDurh@2604:2d80:af07:5300:accd:fbab:7513:2aa0)
[21:18:59] *** Quits: StormGlass (~CraigDurh@2604:2d80:af07:5300:accd:fbab:7513:2aa0) (Ping timeout: 264 seconds)
[21:20:41] *** Quits: MirrorGate (~CraigDurh@2604:2d80:af07:5300:accd:fbab:7513:2aa0) (Quit: Konversation terminated!)
[21:46:05] *** Joins: Codaraxis_ (~Codaraxis@user/codaraxis)
[21:49:39] *** Quits: Codaraxis (~Codaraxis@user/codaraxis) (Ping timeout: 256 seconds)
[21:54:00] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[22:23:03] *** Joins: Samian (~s@user/samian)
[23:25:15] *** Joins: SiegeLord (~sl@user/siegelord)
[23:34:53] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[23:49:47] *** Quits: sinaowolabi (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
