[00:00:35] <[itchyjunk]> dostoyevsky2, sure. we (humans) see information encoded in nature in certain ways but we might not be seeing some of it due to evolutionary constrants.
[00:00:43] <dostoyevsky2> In pure mathematics someone had the idea to see formulas as numbers... was it Goedel?  And then do math on those numbers
[00:01:16] <[itchyjunk]> Neural networks are (imho) a different way to check for encoded information and if they do find something we can't obviously see, it opens a novel way to explore "reality"
[00:01:28] <[itchyjunk]> Godel saw proofs as numbers
[00:01:50] <dostoyevsky2> so he kind of invented a very linear embedding... for proofs
[00:02:08] <[itchyjunk]> well, he discovered a meta language to talk about proofs
[00:02:33] <[itchyjunk]> and that language told him about how some formal systems can be inconstant or incomplete etc
[00:02:49] <dostoyevsky2> hmmm..
[00:02:59] <[itchyjunk]> And a sufficiently expressive system is either consistant or incomplete
[00:03:19] <hodapp> godel's proof was monumental, but turing's proof is equivalent and far easier to follow
[00:03:26] <hodapp> godel's work is hard to follow
[00:03:39] <[itchyjunk]> turings proof doesn't give you incompleteness directly
[00:03:46] <[itchyjunk]> you need to show equivalence to formal systems
[00:04:02] <[itchyjunk]> so sure you have decision problem in computational model
[00:04:07] <[itchyjunk]> but how do you bridge that over to math?
[00:05:26] <dostoyevsky2> [itchyjunk]: are there decision problems in pure math?  I always felt that since you don't have external data, those decision problems don't exist
[00:05:52] <[itchyjunk]> turing machine <=> lambda calculus
[00:06:02] <[itchyjunk]> decision problem <=> incompleteness for formal system
[00:06:32] <[itchyjunk]> so decision problem in math is the idea that there are some "truths" out there that you can never proove within the framework of the formal system you want to work in
[00:08:25] <dostoyevsky2> hodapp: so is that an example of turing reinventing goedel's proof in a novel way that was easier to understand?
[00:08:58] <[itchyjunk]> you should compare turing's work with church's work, imho
[00:09:15] <dostoyevsky2> never actually heard of church
[00:09:19] <[itchyjunk]> comparing turings work with godels work doesn't fit in as well without extra stuff
[00:09:27] <[itchyjunk]> you've not heard of lambda calculus?
[00:10:01] <hodapp> dostoyevsky2: it's more that Church, Turing, and Godel all had proofs that ended up being equivalent, but establishing that equivalence was also itself something of a monumental result
[00:11:27] <hodapp> dostoyevsky2: "Godel, Escher, Bach" by Hofstadter is one of the more interesting looks at this and its implications, but in a more accessible way, and "I Am A Strange Loop" (I think) too
[00:12:01] <dostoyevsky2> [itchyjunk]: well, I learned a bit about lambda calculus when learning Lisp... but never from a math point of view
[00:12:35] <dostoyevsky2> hodapp: oh yeah, I read that like 20 years ago...
[00:13:31] <hodapp> there are countless results in math, though, where it turns out that solving some math problem is equivalent to solving the Halting Problem
[00:13:39] <hodapp> it comes up everywhere
[00:14:11] <dostoyevsky2> well, the way around it in pure Mathematics is that you can always define a space where the halting problem doesn't exist
[00:15:03] <dostoyevsky2> or rather spaces where you can work around that...  the question which spaces solve which halting problem
[00:15:13] <hodapp> none of them solve it, some of them just don't have it
[00:16:01] <hodapp> there are programming languages that do not suffer from the halting problem either, and it turns out that even if you disallow unbounded recursion (for instance) you can still allow all kinds of other recursion that may end up being as much as you need
[00:16:27] <[itchyjunk]> dostoyevsky2, church invented lambda calculus
[00:16:34] <[itchyjunk]> dostoyevsky2, church was turings advisor
[00:16:45] <hodapp> e.g. see https://www.jucs.org/jucs_10_7/total_functional_programming/jucs_10_07_0751_0768_turner.pdf
[00:16:46] <SigmoidFroid> ⇒  Total Functional Programming D.A.Turner (Middlesex University, UK) d.a.turner@mdx.ac.uk Abstract: The driving idea of functional programming is to make programming more closely related to mathematics. A program in a functional language such as Haskell or Miranda consists of equations which are both computation rules an (140K)
[00:17:32] <hodapp> Coq, Idris, and Agda are (I think) total functional, and avoid universality by avoiding things like unbounded recursion or loops
[00:17:36] <[itchyjunk]> dostoyevsky2, its not about "space"
[00:17:40] <[itchyjunk]> its about a formal system.
[00:18:04] <[itchyjunk]> A "sufficiently strong" system can either be incomplete or inconsistent.
[00:18:19] <[itchyjunk]> Since you always want consistance, you always have incomplete system.
[00:18:39] <[itchyjunk]> so systems where you do solve halting problems become non-interesting for various reason
[00:18:56] <hodapp> Homotopy Type Theory & Univalent Foundations disagree
[00:19:25] <dostoyevsky2> [itchyjunk]: are imaginary numbers incomplete or inconsistent?
[00:19:49] <[itchyjunk]> "<hodapp> there are programming languages that do not suffer from the halting problem either,"
[00:19:50] <hodapp> imaginary numbers do not themselves comprise any sort of formal system
[00:19:52] <[itchyjunk]> which language is this?
[00:20:08] <[itchyjunk]> complex numbers are part of a formal system.
[00:20:11] <[itchyjunk]> set theoritic.
[00:20:27] <hodapp> [itchyjunk]: AFAIK, Coq, Idris, Agda, and Lean have this property, and the language described in that paper (though I'm not sure it was implemented) does
[00:20:28] <[itchyjunk]> dostoyevsky2 and set theory is incomplete.
[00:20:42] <[itchyjunk]> hodapp, those are all dependently typed
[00:20:56] <[itchyjunk]> and type systems are incomplete
[00:20:59] <dostoyevsky2> hodapp: yeah, I see them as just a space where you can do math involving infinite numbers...
[00:22:51] <[itchyjunk]> I asked around, dependently typed system do suffer from halting problem
[00:23:06] <[itchyjunk]> "infinity" isn't the problem
[00:23:08] <dostoyevsky2> and isn't Peter Scholze obsessed with prime-based spaces for similar reasons?
[00:24:44] <[itchyjunk]> dostoyevsky2, "sufficienty strong" generally encodes primes
[00:24:52] <[itchyjunk]> you want to be able to encode natural numbers
[00:24:58] <hodapp> [itchyjunk]: afaik, that only applies when it allows types to depend on values whose computation can itself suffer from halting problem
[00:25:04] <[itchyjunk]> you can some up with a system with addition and substraction which is complete.
[00:25:09] <hodapp> which doesn't apply when the language itself doesn't allow it
[00:25:11] <[itchyjunk]> but you can't encode the idea of "primes"in it
[00:27:43] <[itchyjunk]> hmm
[00:27:55] <[itchyjunk]> this is beyond me
[00:28:06] <[itchyjunk]> hopefully i can download it to my head via AI someday
[00:28:21] <dostoyevsky2> Just define a supercomputer where there is no beyond
[00:28:55] <dostoyevsky2> Zombocom spaces
[00:29:11] <[itchyjunk]> i think "spaces" is the wrong term.
[00:29:27] <[itchyjunk]> Sure, this type of problem comes up when dealing with some spaces
[00:29:43] <[itchyjunk]> like infinite dimensional vector spaces is where you encounter axiom of choice cropping up
[00:29:53] <[itchyjunk]> and you can then try to tie it back to completeness and what not
[00:29:57] *** Quits: Colt (~Colt@user/colt) (Quit: Leaving)
[00:30:03] <[itchyjunk]> but it's fundamentally about your formal system
[00:30:15] <[itchyjunk]> you start with a collection of axioms.
[00:30:20] <[itchyjunk]> are these axioms "strong enough" ?
[00:30:39] <[itchyjunk]> if yes -> incomplete probably or inconsistant
[00:31:57] <hodapp> there are also some weird corners when it comes to "universality" (when it refers to Turing completeness/equivalence)
[00:32:25] <hodapp> "weakly universal" and "semi-weakly universal" are a thing, where it is like 'this system (typically an automata) can express the halting problem, but only if you help it out a bit or make certain allowances'
[00:32:47] <hodapp> e.g. rule 110 automata is universal, but seemingly only if you are allowed to use an infinite background pattern
[00:33:22] <hodapp> there are a few papers - I think by people like Liesbeth De Mol - trying to establish where the actual boundary is at which you switch from non-universal to universal, and it's... weird
[00:33:48] <dostoyevsky2> [itchyjunk]: is that really how it works?  Didn't e.g. Poincaré just invent a notion of self similarity and then Einstein applied this to physical phenomenons on Earth and was able to infer equations for the universe...  To my understanding Poincaré's idea didn't have a lot purpose when he invented it
[00:34:07] <[itchyjunk]> they were all working on the same formal system.
[00:34:36] <[itchyjunk]> Generally, "spaces" like hilbert, vector, topological etc in math have been built up on set theoritic foundation.
[00:34:46] <[itchyjunk]> Which set theory? that's a different topic since there are dozens.
[00:35:09] <[itchyjunk]> so once you pick your foundational axioms, the things you build on top are always dependent on those
[00:35:29] <[itchyjunk]> pick ZF instead of ZFC and you lose the axiom "all vector spaces have a basis"
[00:35:49] <[itchyjunk]> you can still prove things in ZF but you don't get to assume you have basis just because you have a vector space etc
[00:36:10] <[itchyjunk]> axiom of choice <=> all vector spaces have basis
[00:36:13] <dostoyevsky2> [itchyjunk]: well that surely would get closer to my question of how Mathematicians decided on what spaces would make sense... and not just them being random ideas which may stick from time to time
[00:36:47] <[itchyjunk]> you pick some axioms and anything that satisfies those axioms makes sense for that "space"
[00:36:53] <hodapp> [itchyjunk]: check out HoTT/UF if you get a chance; a lot is beyond me but, IIRC, it re-makes those foundations based around something closer to type theory
[00:37:00] <[itchyjunk]> the reason specific spaces appear more is because of the usefulness
[00:37:17] <[itchyjunk]> hodapp, yes i've seen some HoTT stuff
[00:37:32] <[itchyjunk]> there is a guy in math who studies it so i hear about it here and there
[00:39:08] <[itchyjunk]> I've only recently started learning about some of this CS stuff though
[00:39:24] <[itchyjunk]> but i found type theory to be kinda interesting
[00:39:48] <hodapp> I have had a long-running side project that has been about looking at connections between chaos theory and computability, which is a little-explored topic
[00:40:20] <dostoyevsky2> hodapp: transformers are all about self-similarity, though
[00:40:37] <hodapp> I found one textbook that approached chaos theory from the standpoint of computability, and it's from ~30 years ago and not very well-known... emailed the author (Joseph McCauley) about it and he confirmed that he knew of no one besides Palmore (a colleague) that had taken the approach they did
[00:41:25] <[itchyjunk]> ah, well wolframs approach would incompass differential equations as computational model
[00:41:40] <[itchyjunk]> He even talks about it in lex fridman podcast.
[00:41:57] <[itchyjunk]> What does a storm computer and is it intelligence from a computational standpoint?
[00:45:09] <hodapp> dostoyevsky2: not really in any comparable sense; chaos isn't about self-similarity, though it turns out that chaotic behavior does by definition form a fractal in phase space
[00:46:13] <dostoyevsky2> hodapp: if it's fractal then it needs to be self-similar...
[00:47:28] <hodapp> that is not a requirement of a fractal
[00:48:25] *** Quits: sinaowolabi_ (~SinaOwola@160.152.24.180) (Ping timeout: 256 seconds)
[00:49:59] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:68c6:1034:8cf9:5a2d)
[00:50:43] <dostoyevsky2> any examples of fractals which are not self-similar?
[00:53:37] <hodapp> a whole lot of what people like Mandelbrot and Musgrave studied was simply systems that had detail at every scale (or at a wide range of scales) but not necessarily self-similarity
[00:54:03] *** Joins: brand0 (~brandon@user/brand0)
[00:54:11] <[itchyjunk]> https://www.stsci.edu/~lbradley/seminar/fractals.html
[00:54:12] <SigmoidFroid> ⇒  Fractals - Chaos & Fractals
[00:54:31] <[itchyjunk]> "fractals" don't have one particular definition either, unfortunately
[00:58:32] <hodapp> someone on my team did post a paper weeks back about how SGD (and similar) can sometimes have consequences from chaotic behavior in training, too
[00:58:55] <hodapp> 'chaotic' in the sense of sensitivity to tiny changes in input
[00:59:08] <hodapp> https://arxiv.org/abs/2111.05803 here we go
[00:59:14] <SigmoidFroid> ⇒  [2111.05803] Gradients are Not All You Need  | RadboU/Donders Institute for Brain/Google -> Part of the reason gradient descent works in neural networks is due to over parameterization (Kawaguchi, 2016) and known weights prior/initialization, which is often not possible in simulated systems have
[01:00:28] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:68c6:1034:8cf9:5a2d) (Quit: I have gone to sleep. ZZZzzz…)
[01:00:43] *** Joins: sinaowolabi_ (~SinaOwola@41.58.248.215)
[01:02:17] <hodapp> my interest was more like: what is the simplest (from a computational standpoint) system that exhibits the same sort of chaotic and fractalian behavior as something like the Mandelbrot or Julia fractal
[01:02:29] <dostoyevsky2> .oO( So someday in 2021-11 SigmoidFroid wasn't banned on arxiv.org and could generate some summaries for the current papers... )
[01:03:14] <hodapp> Mandelbrot & Julia of course are extremely simple to write but when you get into the actual computations they end up generating complexity fairly quickly and they sort of hide it away under things like complex-number multiplication
[01:04:14] <hodapp> and it's known already that (for instance) one can encode the halting problem into the initial conditions of the Julia fractal, and there are some strong parallels between escape-time fractals, dynamics that produce chaos, and the iteration that is sufficient to produce Turing-complete systems
[01:04:22] <dostoyevsky2> hodapp: aren't most random number generators chaotic systems?
[01:05:27] <dostoyevsky2> or anything cryptographic really
[01:05:44] <hodapp> per proper "chaos", I don't see how they could be; their starting conditions aren't continuous so there is no way you can bring them arbitrarily-close and demonstrate that they still diverge
[01:07:20] <hodapp> and - for instance - PRNGs are still periodic but designed for some nice statistical properties and very long periods
[01:09:08] <dostoyevsky2> I guess turning things into noise is different from chaos
[01:09:22] <hodapp> yes
[01:10:38] <hodapp> though chaos is thought to be one mechanism (possibly *the* mechanism) by which noise and randomness are emergent in many natural processes
[01:13:06] <dostoyevsky2> My christmas present for #crypto was this GCM santa claus....  Some experiment with format preserving encryption: https://i.ibb.co/X7vJGN3/gcm-santa.png (vs the ecb penguin)
[01:13:17] <SigmoidFroid>  ⇒  (PNG) No title 334K
[01:36:08] *** Joins: C-Man (~C-Man@78.31.190.82)
[01:38:02] *** Quits: sinaowolabi_ (~SinaOwola@41.58.248.215) (Ping timeout: 240 seconds)
[01:52:25] *** Joins: sinaowolabi_ (~SinaOwola@160.152.24.180)
[02:01:26] *** Quits: manti7 (~manti7@176.10.104.94) (Quit: WeeChat 3.3)
[02:22:46] *** Quits: `Tim (~zenguin@user/zenguin) (Remote host closed the connection)
[02:31:46] *** Quits: palasso (~palasso@user/palasso) (Read error: Connection reset by peer)
[02:33:31] *** Quits: etolier (~somewhere@122-199-44-127.ip4.superloop.com) (Quit: No Ping reply in 180 seconds.)
[02:34:55] *** Joins: etolier (~somewhere@122-199-44-127.ip4.superloop.com)
[02:40:17] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Remote host closed the connection)
[02:52:36] *** Quits: Sheilong (uid293653@id-293653.ilkley.irccloud.com) (Quit: Connection closed for inactivity)
[03:18:06] *** Quits: clime (~clime@78-80-113-95.customers.tmcz.cz) (Quit: WeeChat 3.3)
[03:23:20] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:68c6:1034:8cf9:5a2d)
[03:50:45] *** Joins: `Tim (~zenguin@user/zenguin)
[03:52:27] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:68c6:1034:8cf9:5a2d) (Quit: I have gone to sleep. ZZZzzz…)
[04:45:37] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[05:13:19] *** Quits: georgios (~georgios@user/georgios) (Ping timeout: 268 seconds)
[05:52:53] *** Quits: x88x88x (~x88x88x@149.28.53.172) (Read error: Connection reset by peer)
[06:21:34] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[06:24:44] *** Joins: SiegeLord (~SiegeLord@user/siegelord)
[06:58:57] *** Quits: gist__ (~jack@94.134.88.166) (Ping timeout: 240 seconds)
[06:59:19] *** Joins: gist__ (~jack@94.134.88.122)
[07:21:02] *** Quits: sinaowolabi_ (~SinaOwola@160.152.24.180) (Ping timeout: 240 seconds)
[07:59:53] *** Quits: `Tim (~zenguin@user/zenguin) (Quit: Leaving)
[08:35:02] *** Quits: gist__ (~jack@94.134.88.122) (Ping timeout: 240 seconds)
[08:36:20] *** Quits: jonasbits (~quassel@2a00:66c0:1:1::58) (Remote host closed the connection)
[08:42:09] *** Joins: jonasbits (~quassel@2a00:66c0:1:1::58)
[09:05:47] *** Quits: SiegeLord (~SiegeLord@user/siegelord) (Read error: Connection reset by peer)
[09:09:24] *** Joins: SiegeLord (~sl@user/siegelord)
[09:24:57] *** Quits: mefistofeles (~mefistofe@user/mefistofeles) (Ping timeout: 240 seconds)
[09:34:48] *** Joins: spaceseller (~spacesell@31.147.205.13)
[09:36:03] *** Quits: spaceseller (~spacesell@31.147.205.13) (Remote host closed the connection)
[09:36:46] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[09:44:43] *** Joins: sinaowolabi_ (~SinaOwola@160.152.24.180)
[10:12:34] *** Joins: manti7 (~manti7@176.10.104.94)
[10:20:10] *** Joins: jlrnick (~josephler@2a01cb040a1594007dfc79c9afe919a1.ipv6.abo.wanadoo.fr)
[10:35:17] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 240 seconds)
[10:50:06] *** Joins: manti7 (~manti7@p57ba338d.dip0.t-ipconnect.de)
[11:11:36] *** Joins: sinaowolabi__ (~SinaOwola@102.134.114.1)
[11:18:17] *** Quits: manti7 (~manti7@p57ba338d.dip0.t-ipconnect.de) (Ping timeout: 240 seconds)
[11:18:50] *** Joins: manti7 (~manti7@176.10.104.94)
[11:36:06] *** Joins: palasso (~palasso@user/palasso)
[11:39:37] *** Quits: sinaowolabi_ (~SinaOwola@160.152.24.180) (Ping timeout: 240 seconds)
[11:54:06] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[11:56:29] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[11:56:31] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 256 seconds)
[11:58:35] *** Joins: manti7 (~manti7@p57ba338d.dip0.t-ipconnect.de)
[12:03:14] *** Quits: manti7 (~manti7@p57ba338d.dip0.t-ipconnect.de) (Ping timeout: 250 seconds)
[12:08:51] *** Quits: jim (~jim@about/linux/staff/jim) (Remote host closed the connection)
[12:10:08] *** Joins: manti7 (~manti7@95.216.139.244)
[12:15:31] *** Quits: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470) (Read error: Connection reset by peer)
[12:20:53] *** Joins: jim (~jim@about/linux/staff/jim)
[12:27:06] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Remote host closed the connection)
[12:28:26] *** Joins: Klinda (~superleag@user/klinda)
[12:29:05] <Klinda> hello the number of iterations of gradient descent depends on the learning rate? is it true?
[12:29:46] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[12:58:27] *** Joins: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi)
[13:03:04] <stefan-_> Klinda, yes, that is correct
[13:14:00] *** Quits: AbleBacon (~AbleBacon@user/AbleBacon) (Read error: Connection reset by peer)
[13:17:20] <Hunts> Klinda: if you take smaller steps, you need more steps to reach the goal
[13:25:42] *** Quits: SiegeLord (~sl@user/siegelord) (Quit: WeeChat 2.8)
[13:38:46] *** Quits: Klinda (~superleag@user/klinda) (Quit: Konversation terminated!)
[13:39:13] <lericson> great, then let the LR approach infinity and we need one step!
[13:39:25] <lericson> we solved DL guys, time to pack it up and go home
[14:09:08] <Hunts> as if that was the only problem in DL :)
[14:13:37] <lericson> i'd say we'd solve a lot of problems if we could optimize nets in a single step
[14:13:45] <lericson> architecture search would be way more feasible
[14:52:02] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 240 seconds)
[14:52:37] *** Quits: sinaowolabi__ (~SinaOwola@102.134.114.1) (Ping timeout: 240 seconds)
[15:05:02] *** Joins: sinaowolabi_ (~SinaOwola@160.152.101.40)
[15:05:35] *** Joins: gist__ (~jack@94.134.88.122)
[15:06:00] *** Quits: sinaowolabi_ (~SinaOwola@160.152.101.40) (Max SendQ exceeded)
[15:06:26] *** Joins: sinaowolabi_ (~SinaOwola@160.152.101.40)
[15:12:32] *** Joins: clime (~clime@78-80-113-95.customers.tmcz.cz)
[15:17:57] *** Quits: manti7 (~manti7@95.216.139.244) (Ping timeout: 240 seconds)
[15:20:41] *** Joins: manti7 (~manti7@176.10.104.94)
[15:25:22] *** Joins: Sheilong (uid293653@id-293653.ilkley.irccloud.com)
[15:30:17] *** Joins: sobobobobo (~sobobobob@85-76-84-182-nat.elisa-mobile.fi)
[15:42:04] *** Joins: `Tim (~zenguin@user/zenguin)
[15:57:17] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 240 seconds)
[15:59:22] *** Joins: manti7 (~manti7@95.216.139.244)
[16:29:13] *** Quits: sobobobobo (~sobobobob@85-76-84-182-nat.elisa-mobile.fi) (Quit: Client closed)
[16:42:45] *** Quits: jlrnick (~josephler@2a01cb040a1594007dfc79c9afe919a1.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[16:49:36] *** Joins: Klinda (~superleag@user/klinda)
[16:49:52] <Klinda> I did the exam Hunts
[16:50:13] <Klinda> I made a mistake on fighthing high variance and the other I don't know
[16:50:25] <Klinda> others*
[16:51:01] <Klinda> I said you should reduce the regularization
[16:51:08] <Klinda> but you should incresce right?
[16:51:23] <Klinda> I said correcty you have to add samples
[17:14:25] *** Quits: sinaowolabi_ (~SinaOwola@160.152.101.40) (Ping timeout: 256 seconds)
[17:16:03] *** Joins: georgios (~georgios@user/georgios)
[17:26:21] <dostoyevsky2> Klinda: Variance like: you train it N times and the final loss is always very different?
[17:28:55] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[17:30:00] *** Joins: sobobobobo (~sobobobob@85-76-84-182-nat.elisa-mobile.fi)
[17:37:24] *** Joins: jlrnick (~josephler@2a01cb040a15940021ad9cb3998b49f5.ipv6.abo.wanadoo.fr)
[17:38:01] <hodapp> regularization typically reduces variance
[17:38:10] <hodapp> often at the cost of worse bias
[17:47:30] *** Quits: jlrnick (~josephler@2a01cb040a15940021ad9cb3998b49f5.ipv6.abo.wanadoo.fr) (Ping timeout: 268 seconds)
[17:47:41] <dostoyevsky2> or just use a better optimizer?
[17:51:49] *** Quits: manti7 (~manti7@95.216.139.244) (Ping timeout: 240 seconds)
[17:53:21] *** Joins: manti7 (~manti7@176.10.104.94)
[17:56:16] *** Quits: sobobobobo (~sobobobob@85-76-84-182-nat.elisa-mobile.fi) (Quit: Client closed)
[17:56:30] *** Joins: hygl (uid16621@id-16621.tinside.irccloud.com)
[18:00:48] *** Joins: sobobobobo (~sobobobob@85-76-84-182-nat.elisa-mobile.fi)
[18:19:24] *** Joins: mefistofeles (~mefistofe@user/mefistofeles)
[18:30:30] *** Joins: AbleBacon (~AbleBacon@user/AbleBacon)
[18:56:42] *** Quits: sobobobobo (~sobobobob@85-76-84-182-nat.elisa-mobile.fi) (Quit: Client closed)
[19:20:49] <dostoyevsky2> it's neat that I could replace my map with a sync.Map to avoid data races, but there doesn't seem a sync.Array or the like... only sync.Pool... maybe that'll do
[19:24:33] <mefistofeles> dostoyevsky2: Go?
[19:24:46] <dostoyevsky2> oh, wrong channel ;)
[19:25:07] <mefistofeles> :P
[19:25:20] <dostoyevsky2> wanted to ask in #go-nuts
[19:25:27] <mefistofeles> dostoyevsky2: yes, go!
[19:25:28] <mefistofeles> xD
[19:40:57] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 240 seconds)
[19:50:28] *** Joins: manti7 (~manti7@176.10.104.94)
[19:56:41] *** Joins: jlrnick (~josephler@2a01cb040a1594005593f371f5be80da.ipv6.abo.wanadoo.fr)
[20:06:09] *** Quits: hygl (uid16621@id-16621.tinside.irccloud.com) (Quit: Connection closed for inactivity)
[20:16:16] *** Quits: DnzAtWrk (~DnzAtWrk@mobile-access-6df0a8-31.dhcp.inet.fi) (Read error: Connection reset by peer)
[20:26:36] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 268 seconds)
[20:39:30] *** Joins: shoky_ (uuuggg@141.226.193.67)
[20:39:33] *** Quits: shoky (uuuggg@141.226.193.67) (Ping timeout: 268 seconds)
[20:43:13] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06)
[20:57:47] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06) (Quit: I have gone to sleep. ZZZzzz…)
[21:11:24] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 250 seconds)
[21:15:53] *** Joins: sinaowolabi_ (~SinaOwola@160.152.101.40)
[21:16:49] *** Quits: sinaowolabi_ (~SinaOwola@160.152.101.40) (Max SendQ exceeded)
[21:17:15] *** Joins: sinaowolabi_ (~SinaOwola@160.152.101.40)
[21:18:12] *** Quits: sinaowolabi_ (~SinaOwola@160.152.101.40) (Max SendQ exceeded)
[21:18:38] *** Joins: sinaowolabi_ (~SinaOwola@160.152.101.40)
[21:18:59] *** Joins: [itchyjunk] (~itchyjunk@user/itchyjunk/x-7353470)
[21:25:33] *** Malvolio is now known as Guest1315
[21:25:33] *** Quits: Guest1315 (~Malvolio@user/malvolio) (Killed (strontium.libera.chat (Nickname regained by services)))
[21:26:07] *** Joins: Malvolio (~Malvolio@user/malvolio)
[21:26:34] *** Quits: shoky_ (uuuggg@141.226.193.67) ()
[21:32:15] *** Joins: shoky (uuuggg@141.226.193.67)
[21:32:32] *** Quits: sinaowolabi_ (~SinaOwola@160.152.101.40) (Ping timeout: 240 seconds)
[21:36:44] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06)
[21:38:42] *** Joins: manti7 (~manti7@176.10.104.94)
[21:40:57] *** Joins: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net)
[21:41:01] *** Parts: revolve (~u0_a227@cpc100838-bagu15-2-0-cust672.1-3.cable.virginm.net) ()
[21:43:38] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06) (Quit: I have gone to sleep. ZZZzzz…)
[21:44:49] *** Joins: sinaowolabi_ (~SinaOwola@102.134.114.1)
[21:47:37] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06)
[21:53:14] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06) (Quit: I have gone to sleep. ZZZzzz…)
[21:53:58] *** Joins: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06)
[21:56:32] *** Quits: medium_cool (~medium_co@2605:a601:a9aa:f800:2447:e568:11bd:7d06) (Client Quit)
[21:59:34] <Klinda> I didn't really understand what variance is..
[22:00:14] <Klinda> all I know is that high bias means like that the hypotesis you do can't express well the training data
[22:00:30] <Klinda> so you add some features
[22:00:39] <Klinda> make the model more complex
[22:00:46] <Klinda> but you have to pay attention to not overfit
[22:00:51] <Klinda> you should be in the middle
[22:01:29] <Klinda> to test your model you can use the validation set and that's it
[22:02:17] <Klinda> you try with different polynomial features
[22:04:07] <Klinda> the thing is... math is an important role to machine learning, so if you are not very confident to it, you can study ml, but don't really have all clear in your mind
[22:07:22] <[itchyjunk]> Variance was a function right?
[22:07:27] <[itchyjunk]> i can't remember what it was now
[22:09:07] <Klinda> all I understand is that you add samples and the variance reduce
[22:09:08] <Klinda> xD
[22:09:46] <mefistofeles> Klinda: there's irreducible error, though
[22:10:05] <Klinda> yes you can't do anything about that
[22:10:12] <mefistofeles> [itchyjunk]: it's second central moment of a distribution
[22:10:57] <mefistofeles> it's more of a functional, but I don't think it's even that
[22:11:16] <mefistofeles> because it takes distributions rather than functions as input
[22:16:16] <clime> Does anyone has a good knowledge of pandas here? https://bpa.st/POEQ - it just seems like behavior with lists inside cells is broken or idk. I would like to understand what is happening here
[22:16:18] <SigmoidFroid> ⇒  View paste POEQ
[22:18:29] <clime> i mean i can just use .append() or .extend() instead of += but this is headbump anyway
[22:22:35] <dostoyevsky2> clime: Don't you need to use a pd.Series instead of an array inside pandas?   I think numpy, pandas, pytorch, tf don't like the native python data types very much and have their own replacement
[22:25:52] <hodapp> variance has a formal definition but in ML it's used in a more hand-wavy way (even if the formal definition also works fine) to refer to how closely it fits the underlying training data - and so higher variance can occur when differences in the training distribution, or in the way it's sampled, produce much different models
[22:26:36] *** Joins: theseb (~theseb@47-220-214-54.cnrocmta03.res.dyn.suddenlink.net)
[22:27:16] <theseb> Jupyter notebook newb question....How come I have to keep restarting the kernel to rerun cells?
[22:27:31] <hodapp> you'd have to look at what it's dumping to console and see why the kernel is dying
[22:27:37] <theseb> (I'm doing SageMaker demo in AWS notebook)
[22:27:55] <theseb> hodapp: I *am* having lots of errors in my Python code I'm debugging
[22:27:56] <mefistofeles> clime: hmm, I do agree, there's some inconsisten behavior with that
[22:28:04] <mefistofeles> *inconsistent
[22:28:10] <theseb> hodapp: do i have to restart the kernel everytime i have a python syntax error?
[22:28:17] <clime> dostoyevsky2: idk, i would expect to be able to use [] as a standard python type, I guess
[22:28:19] <hodapp> uh, no
[22:28:32] <clime> mefistofeles: thanks
[22:28:34] <hodapp> I've had kernels going for weeks/months regardless of how many syntax errors I threw at it
[22:29:03] *** Quits: Malvolio (~Malvolio@user/malvolio) (Quit: it wasn't gonna turn into another winter of '22)
[22:29:34] <Klinda> hodapp: I still didn't understand the definition of variance
[22:32:42] <mefistofeles> Klinda: what don't you get? More specifically, or what are you having trouble with?
[22:33:04] <clime> if pandas told me: "hey, we don't support lists", that would be fine too"
[22:33:20] <Klinda> mefistofeles: when a model suffer from high variance?
[22:36:19] <mefistofeles> Klinda: not sure I understand the question, I guess you are asking about overfitting?
[22:36:26] <mefistofeles> and yes, it's related to variance
[22:37:28] *** Joins: Malvolio (~Malvolio@user/malvolio)
[22:44:07] *** Quits: manti7 (~manti7@176.10.104.94) (Ping timeout: 268 seconds)
[22:46:32] *** Joins: manti7 (~manti7@p57ba338d.dip0.t-ipconnect.de)
[22:47:06] <hodapp> I don't think a single model itself can exactly suffer from high variance; it's more about a particular training process tending to be high-variance when the models it produces are compared to each other
[22:47:08] <dostoyevsky2> theseb: Is your syntax error in a cell or in the python code you are loading?
[22:47:34] <dostoyevsky2> theseb: for a cell you can just re evaluate the cell after fixing the error (shift-enter)
[22:48:28] <hodapp> clime: pandas 'supports' all kinds of things but if you go through really any of their documentation, tutorials, or examples they steer you heavily towards just numeric/date/strings in cells
[22:48:51] <mefistofeles> yeha, also pandas API is infamously weird
[22:48:54] <mefistofeles> *yeah
[22:49:28] <dostoyevsky2> theseb: for included code you can enable the reload magics... `% reload_ext autoreload' and `% autoreload 2' on top of the first cell in the document, and then evaluate the cell too
[22:53:44] *** Quits: xs (~xs@user/xs/x-9591474) (Killed (gold.libera.chat (Nickname regained by services)))
[22:53:58] *** Joins: xs_ (~xs@user/xs/x-9591474)
[22:57:27] <Klinda> hodapp: can you do some real examples?
[22:58:07] <Klinda> mefistofeles: overfitting I think is more about bias?
[22:58:48] <Klinda> I mean the generalization errore can be decompose by: variance + bias^2 + irreducible error
[22:58:54] <Klinda> *error
[23:00:29] <mefistofeles> Klinda: no, overfitting is related to both bias and variance, since they both contribute to the error
[23:00:32] <mefistofeles> Klinda: https://imgur.com/a/DvOknOr
[23:00:33] <SigmoidFroid> ⇒  Imgur: The magic of the Internet
[23:00:41] <hodapp> Klinda: consider a hypothetical algorithm which does nothing but perfectly memorize every training example
[23:01:00] <hodapp> train one on one half of your data, train another on the other half of your data
[23:01:04] <mefistofeles> Klinda: check that, the first model (linear) is very low variance, but not that accurate... whereas the second has a higher variance
[23:02:07] <stefan-_> interesting, pre-training word embeddings on the train/test dataset (aclImdb) gives a boost of 4% in sentiment classification
[23:02:27] <stefan-_> see: https://arxiv.org/abs/2009.04007
[23:02:31] <mefistofeles> now, imagine you even add more variance... like say yo trace a line between EVERY data point, that's really high variance but probably not accurate, in terms of error
[23:02:42] <hodapp> both these models are going to get 100% training accuracy, but generalize very poorly - but also, they're going to vary a lot from each other, as much as the underlying distribution does
[23:02:55] <SigmoidFroid> ⇒  [2009.04007]
[23:03:22] <hodapp> now, consider another algorithm which literally does not even train at all, it just ignores the data and returns a constant value
[23:03:54] <hodapp> this has no variance whatsoever, because it is always giving you the same results regardless of what it's trained on
[23:04:14] <hodapp> but its bias will be very high
[23:05:13] <mefistofeles> there is always a tradeoff (at least fundamentally) between bias and variance, you tweak these two to get the lower possible error
[23:06:36] <hodapp> and you'll often see that bagging is a way to reduce variance, without increasing bias (compared to a single model, not bagged)
[23:06:53] *** Quits: sinaowolabi_ (~SinaOwola@102.134.114.1) (Ping timeout: 256 seconds)
[23:07:17] <hodapp> boosting (I think) has this interesting ability to reduce bias without increasing variance
[23:11:15] *** Quits: manti7 (~manti7@p57ba338d.dip0.t-ipconnect.de) (Ping timeout: 268 seconds)
[23:13:53] <Klinda> ah understand so the variance is like compare the training set with a val/test set and see that the performace vary a lot ?
[23:14:36] <Klinda> with different val/test sets
[23:14:37] <Klinda> ?
[23:15:17] <hodapp> not exactly; a gap in performance between train and test can be underfitting (which often indicates higher bias) *or* due to overfitting (which often indicates higher variance)
[23:16:45] <Klinda> what the gap between train and test is not overfitting? like 80% train a 50% test?
[23:17:04] <Klinda> what?*
[23:18:11] <Klinda> ah no sorry I understand wrong
[23:18:13] <Klinda> now I understand
[23:18:31] <Klinda> hodapp: to fight high variance you should add more samples
[23:18:32] <Klinda> right?
[23:19:33] <Klinda> reduce the complexity of the model and so on
[23:20:06] <Klinda> instead to fight high bias you should add more features and so on
[23:23:48] <Klinda> so the bias is the training error and the variance is the gap between train and val/test set
[23:31:43] *** Quits: Jong (~Jong@2620:10d:c090:400::5:78f7) (Quit: Textual IRC Client: www.textualapp.com)
[23:36:12] <mefistofeles> Klinda: as long as the sampling is done correctly? Sure
[23:36:30] <mefistofeles> you can sample and add bias (sampling the same thing over and over again)
[23:36:51] <Klinda> if you add more exaples you fight variance but you add bias?
[23:37:21] <mefistofeles> I mean, if your sampling is not good, you can add bias and basically shoot yourself in the foot
[23:37:34] <Klinda> ah ok
[23:37:47] <Klinda> well I didn't do a really machine learning work
[23:37:55] <Klinda> I just know the theory xD
[23:38:05] <mefistofeles> that's fine
[23:38:24] <mefistofeles> we are all learning this, this is in constant evolution :P
[23:39:03] <Klinda> next semester I will do a hard deep learning course
[23:39:51] *** Parts: octav1a (~quassel@173.195.145.98) (~Called off to Sentinal the Sand Temple~)
