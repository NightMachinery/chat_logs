[00:04:13] *** Quits: Brucio-61 (~Brucio-59@2001:638:504:20e6:6ac3:87e0:e782:f3fc) (Ping timeout: 260 seconds)
[00:04:15] *** Quits: scymtym (~user@2001:638:504:20e6:6ac3:87e0:e782:f3fc) (Ping timeout: 260 seconds)
[00:21:11] *** Joins: scymtym (~user@ip-094-114-248-079.um31.pools.vodafone-ip.de)
[00:21:17] *** Joins: Brucio-61 (~Brucio-61@ip-094-114-248-079.um31.pools.vodafone-ip.de)
[00:51:32] *** Quits: mgl (~mgl@cpc87455-finc19-2-0-cust234.4-2.cable.virginm.net) (Ping timeout: 250 seconds)
[00:53:26] *** Joins: kevingal (~quassel@37.228.201.223)
[01:00:24] *** Joins: LispyLights (~LispyLigh@user/lispylights)
[01:03:35] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Ping timeout: 240 seconds)
[01:08:43] *** LispyLights is now known as Aurora_v_komose
[01:08:45] *** Aurora_v_komose is now known as Aurora_v_kosmose
[01:13:40] <shka> https://www.youtube.com/watch?v=Y4xlRjxDPRg
[01:13:40] -ixelp- Kolo (Black Hole Sun Remix) - YouTube
[01:13:43] <shka> goodnight
[01:29:36] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 272 seconds)
[01:31:29] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[01:32:35] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Ping timeout: 240 seconds)
[01:34:37] *** Joins: Aurora_v_kosmose (~LispyLigh@user/lispylights)
[01:39:56] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 245 seconds)
[01:42:01] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[01:43:23] *** Quits: jeosol (~jeosol@user/jeosol) (Quit: Client closed)
[02:07:12] *** Quits: cosimone (~user@93-44-187-176.ip98.fastwebnet.it) (Quit: ERC (IRC client for Emacs 27.1))
[02:11:11] *** Quits: shka (~herr@109.231.3.55) (Ping timeout: 260 seconds)
[02:22:57] <hayley> I got an answer.
[02:23:00] <hayley> "It is part of type-i so the issue should go there"
[02:23:03] <hayley> ...
[02:32:35] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Ping timeout: 240 seconds)
[02:33:46] *** Joins: Aurora_v_kosmose (~LispyLigh@user/lispylights)
[02:40:26] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Remote host closed the connection)
[02:40:44] *** Joins: Aurora_v_kosmose (~LispyLigh@user/lispylights)
[02:47:55] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Ping timeout: 240 seconds)
[02:51:45] *** Joins: Aurora_v_kosmose (~LispyLigh@user/lispylights)
[03:10:01] <mfiano> I need help with the "naming things" problem.
[03:11:11] <mfiano> I want to make a class called "graph" that wraps the actual graph library's graph object with additional domain-specific metadata slots. Having a slot called "graph" in the "graph" class is not very good. Any suggestions?
[03:12:40] <hayley> Call the slot %backing-graph?
[03:13:08] <mfiano> Hmm, that could work. Or "%handle" maybe?
[03:13:18] <moon-child> the-real-mccoy
[03:14:57] <mfiano> First day back to coding after wearing my network admin hat for a month and I feel lost.
[03:17:43] <ck_> just do "netcat" but leave the "net" off. should buff right out
[03:21:09] *** Quits: mala (~mala@user/malaclyps) (Read error: Connection reset by peer)
[03:22:49] *** Joins: mala (~mala@user/malaclyps)
[03:27:16] * moon-child pets cat
[03:28:05] * ck_ adds that to "useless use of cat" list
[04:11:21] * edgar-rft cats pet -> error: file or directory not found
[04:36:53] *** Quits: random-nick (~random-ni@87.116.176.196) (Ping timeout: 250 seconds)
[04:36:57] <moon-child> where in the spec does it say literal data cannot be mutated?
[04:37:23] <moon-child> in particular: obviously, I cannot say (setf (car '(1)) 2).  But what about (setf (car '#.(list 1)) 2)?
[04:38:17] <moon-child> it seems like the answer might be maybe.  But then there's (defparameter *x* (list 1)) (setf (car '#.*x*) 2)
[04:38:26] <hayley> http://www.lispworks.com/documentation/HyperSpec/Body/s_quote.htm "The consequences are undefined if literal objects (including quoted objects) are destructively modified."
[04:38:27] -ixelp- CLHS: Special Operator QUOTE
[04:38:47] <hayley> I'd guess not, as you have quoted the value of *x*.
[04:39:39] <moon-child> hmm, wow
[04:40:16] <moon-child> my reading is that the simple act of referring to *x* as data makes any subsequent mutations of it undefined
[04:40:47] <moon-child> and I mean, it kinda makes sense, given a naive interner, but still--wow
[04:41:11] * hayley makes the one-more-re-nightmare book by copying from the Netfarm book
[05:15:38] <kakuhen> name things the way you want
[05:15:50] <kakuhen> if you violate a package lock just shadow whatever conflicted
[05:18:28] * hayley adds documentation boilerplate and now one-more-re-nightmare is 4,152 LOC
[05:28:08] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-08-70-29-36-27.dsl.bell.ca)
[05:35:19] <kakuhen> write code that writes documentation
[05:36:58] * hayley uploaded an image: (95KiB) < https://libera.ems.host/_matrix/media/r0/download/matrix.org/emtENPUkdUYkXYEwdHVZHlSC/Screenshot_2022-03-31_12-06-51.png >
[05:37:02] <hayley> Sure, I'll come up with code that writes this.
[05:38:29] <hayley> You could use the one-more-re-nightmare linter to test equality of Boolean expressions. But you probably shouldn't.
[05:40:20] <hayley> (lambda (x) (one-more-re-nightmare:first-match "¬¨(a$|$b)&¬¨(¬¨(a$)&¬¨($b))" x)) ‚áí "This expression is impossible to match."
[05:40:35] <moon-child> omrn 2: the revenge of SAT
[05:45:07] <hayley> moon-child: It'd be called two-more-re-nightmares, but I used that name for the project of implementing gilberth's compiler.
[05:49:30] <neominimum> Used copy of CLtL2 arrived and wow, he thicc boi
[05:50:19] <hayley> "In other words, the linter is used to prove $\overline a \land \overline b \Rightarrow \overline{a \lor b}$...we think."
[06:09:17] <hayley> No, I got the arrow the wrong way around in that quote. But basically that.
[06:14:19] *** Quits: pjb (~pjb@user/pjb) (Ping timeout: 260 seconds)
[06:22:20] *** Joins: Inline (~Inline@p200300cd470e1300a84f1effc05ae732.dip0.t-ipconnect.de)
[06:25:03] *** Quits: Inline__ (~Inline@p200300cd470a540051f7a3750a30b01e.dip0.t-ipconnect.de) (Ping timeout: 260 seconds)
[06:27:34] <hayley> https://applied-langua.ge/projects/one-more-re-nightmare/Linting_and_warnings.html#%28part.__.This_expression_is_impossible_to_match__%29
[06:27:34] -ixelp- 3 Linting and warnings
[06:50:11] <drakonis> neominimum: does the index really have kludges listed?
[06:50:28] <drakonis> hayley: which regex does this implement?
[06:50:35] <hayley> POSIX with funny syntax.
[06:50:49] <hayley> See https://applied-langua.ge/projects/one-more-re-nightmare/Interface.html#%28part._.Syntax%29
[06:50:49] -ixelp- 2 Interface
[06:52:12] <drakonis> clearly not compatible with the lowest common denominator...
[06:52:56] <hayley> PCRE? That'd be right. But intentionally so, since there is no established syntax for RE intersection and negation.
[06:53:12] <clothespin> are there regular process locks in sbcl?
[06:53:50] <hayley> Mutexes? Yes. But use Bordeaux threads and BT:MAKE-LOCK to be portable.
[06:55:00] <drakonis> https://docs.raku.org/language/regexes you also have this as an option
[06:55:00] -ixelp- Regexes
[06:55:18] <drakonis> but it depends a lot on raku's object model
[06:55:47] <drakonis> it also has some very... interesting stuff in it compared to perl's regex
[06:56:35] <drakonis> https://en.wikipedia.org/wiki/Raku_rules if you dont want to go through the writings that led to it
[06:58:49] <drakonis> https://design.raku.org/S05.html
[06:58:50] -ixelp- Synopsis 5: Regexes and Rules
[06:58:51] <drakonis> here
[07:00:15] <drakonis> its so strange that it was originally written in 2002 but nobody beyond raku has implemented it
[07:19:39] <neominimum> drakonis: away from home rn, but I'll check it out when I get back.
[07:20:12] <drakonis> it is a fundamental improvement over regex
[07:33:21] *** Quits: kevingal (~quassel@37.228.201.223) (Remote host closed the connection)
[07:40:59] <hayley> I think I have a slightly nicer way to do theorem proving with Mealy machines. To show A and B are equivalent, generate the DFA for (A&B)|¬´A¬ª|¬´B¬ª. Now ¬´A¬ª and ¬´B¬ª will be dead if A = B.
[07:42:38] <hayley> Totally subjective if it's nicer than (A \ B) | (B \ A). But I suspect you can also show A ‚â§ B or B ‚â§ A if only one is dead.
[07:44:15] *** Quits: waleee (~waleee@2001:9b0:213:7200:cc36:a556:b1e8:b340) (Ping timeout: 256 seconds)
[07:46:35] *** Quits: Aurora_v_kosmose (~LispyLigh@user/lispylights) (Ping timeout: 240 seconds)
[07:50:19] *** Joins: semz_ (~semz@user/semz)
[07:52:44] *** Joins: Aurora_v_kosmose (~LispyLigh@user/lispylights)
[07:52:45] *** Quits: semz (~semz@user/semz) (Ping timeout: 256 seconds)
[08:00:35] <hayley> Oh, and, to encode a Boolean expression as a RE, encode a variable as a string with all but one character being "any character". e.g. X might be X$ and Y $Y. The usual Boolean operators map trivially to operators on REs.
[08:06:06] <drakonis> huh, neat.
[08:09:01] * hayley tries out her idea, and one-more-re-nightmare apparently does not agree the REs are equivalent.
[08:09:09] <hayley> But the DFA also gives you counter-examples, too.
[08:13:08] <hayley> Not only does it not work, the code generated is plain broken. /me scratches head
[08:14:45] <moon-child> this is really starting to look like a sat solver
[08:14:56] <hayley> Well, it is.
[08:15:37] <hayley> And I see my bug. SLIME's who-calls didn't track esrap actions, so my parse for $ produces an object representing \Sigma* rather than just \Sigma.
[08:16:55] <hayley> (one-more-re-nightmare:first-match "¬¨(a$|$b)" "ab") ‚áí #(0 -1) ; Hm, no.
[08:22:28] <hayley> Ah, I handled effects on the other side of a negation wrong.
[08:23:09] <hayley> moon-child: My trick here is that DFA generation, and thus linting, perform exhaustive search. So it is like a crappy SAT solver, in a way.
[08:23:28] <hayley> And note that DFA generation is also O(2^n).
[08:25:47] <hayley> Here is proving De Morgan's law: https://plaster.tymoon.eu/view/3011
[08:29:59] <sm2n> Regex is an NP-complete?
[08:30:04] <sm2n> s/an//
[08:30:12] <sm2n> Did not know that
[08:30:29] <sm2n> I guess it makes sense
[08:30:29] <hayley> Constructing a DFA takes non-polynomial time, yes.
[08:31:27] *** Joins: dave0 (~dave0@069.d.003.ncl.iprimus.net.au)
[08:32:01] <hayley> And backtracking moves non-polynomial time to runtime, though the Pike machine instead makes construction linear, and matching time proportional to NFA size √ó string length.
[08:39:10] <hayley> Actually, I wonder if derivative classes help with avoiding the worst case for SAT.
[08:43:33] <sm2n> I thought you don't need backtracking for regex?
[08:43:51] <hayley> You don't.
[08:43:58] <neominimum> drakonis: Heh, yes indeed, there is a rather substantial list of kludges in the index.
[08:44:42] <drakonis> pcre implements backtracking
[08:44:46] <neominimum> https://pasteboard.co/eUtlydgMdjoS.jpg
[08:44:46] -ixelp- cludges - Image on Pasteboard
[08:46:19] <sm2n> I was under the impression we were discussion regex as formally defined in the theory of computation
[08:46:31] <sm2n> s/discussion/discussing/
[08:46:53] <sm2n> Not whatever larry wall came up with
[08:46:53] <hayley> Yes, I mean the formal definition. You need at least quadratic runtime for backreferences, I believe.
[08:47:08] <hayley> OMRN does the formal definition + submatches.
[08:49:42] <qhong> Talking about back reference, I wonder if a language environment will be much easier to use if all references are bidirectional
[08:50:11] <qhong> CAM naturally support this, unfortunately it's dead for general computing by now
[08:50:26] <sm2n> So, in summary, regex with dfas is exponential complexity for compilation, and linear at runtime?
[08:50:46] <sm2n> qhong: what is CAM?
[08:51:08] <qhong> sm2n: Content addressable memory
[08:51:56] <qhong> I find that lots of time people are inventing specialized back references for some limited class of objects, and sometimes it can't be done and people lament
[08:52:38] <hayley> sm2n: Right.
[08:53:15] <sm2n> Cool
[08:53:39] <sm2n> qhong: do you have an example? I don't see why this would not be implementable in software.
[08:57:37] <qhong> sm2n: it is implementable, but I think either you implement it really slowly (by walking the whole memory), or you waste hell lot of memory (by keeping a list of referer per object)
[08:58:32] <hayley> I think there was one concurrent GC which used that sort of pointer-reversal to be able to copy an object with one or two references to it atomically (using transactional memory).
[08:58:51] <sm2n> if you're fine with asynchrony, you could piggyback on the GC
[08:59:01] <sm2n> it already has to chase all the references
[08:59:12] <hayley> https://www.researchgate.net/publication/254006304_The_Collie_A_wait-free_compacting_collector
[08:59:13] -ixelp- (PDF) The Collie: A wait-free compacting collector
[09:00:59] <qhong> sm2n: still walking "back" the reference chain will be orders of magnitudes slower than "forward". Each step require a GC. I think it's possible to expose some lower-level interface so you can schedule for walking a specific path at once into the GC, but that makes the interface much more cumbersome
[09:01:21] <qhong> while with CAM, walking forward and backward are basically the same, both just a single memory access
[09:02:17] <sm2n> I see
[09:02:28] <qhong> hayley: so if back reference helps with GC, maybe CAM will help a lot? IDK, not an expert at concurrent GC
[09:02:40] <hayley> Quite possible.
[09:03:12] <qhong> ah I can see a little bit. Forward pointers. With CAM you don't need it, you can just update referer directly
[09:03:31] <sm2n> hayley: Not related, but have you read the orca gc paper?
[09:03:39] <sm2n> I'm currently working through it
[09:04:20] <rotateq> hayley: Imagine it would be 'regular statements' instead of expressions. *shrug*
[09:04:30] <sm2n> this: <https://www.ponylang.io/media/papers/orca_gc_and_type_system_co-design_for_actor_languages.pdf>
[09:04:30] <Bakerposting> OMG!! Ponies!!!
[09:04:37] <sm2n> hahaha
[09:05:15] <moon-child> some web competitor from way back in the day--I can never remember its name--had as a selling point that hyperlinks went both ways
[09:05:38] <hayley> "Namely, Orca relies on the absence of race conditions in order to avoid read/write barriers" Interesting statement in the abstract, because the idea is that a concurrent GC introduces new races with the mutator. The mutator can still be race-free and still require synchronisation with the GC.
[09:05:43] <sm2n> moon-child: xanadu
[09:05:49] <moon-child> yeah, that
[09:06:24] <sm2n> hayley: it's a bit weird
[09:06:36] <sm2n> it appears to work like distributed gc
[09:06:58] <sm2n> you have simple mark/sweep per actor, and deferred refcounts across actors
[09:07:42] <sm2n> and the reference capability type system apparently makes this work, so that you don't need the actors to sync up when gcing
[09:07:43] <hayley> Doligez-Leroy much? Though collecting the shared/mutable space is not non-blocking.
[09:08:09] <sm2n> gc is per-actor, and blocks the actor it runs on, so not really fully concurrent
[09:08:40] <hayley> "Various designs for segregated heaps have been explored in the literature. Domani et al. (2002) introduced a collector that segregates between thread-local objects and shared objects" God dammit, DL would have been a better citation, because they also use the type system to segregate heaps.
[09:08:42] <moon-child> one actor can run concurrently with another actor's gc
[09:08:59] <hayley> Immutable objects live in per-thread heaps, whereas mutable objects live in shared.
[09:09:05] <moon-child> moreover, if you scale actor counts rather than sizes, then heap size is bounded, and so is pause time for any given actor
[09:09:18] <sm2n> yup
[09:10:03] <sm2n> their thing seems to be that they wanted erlang but with shared memory and local mutation
[09:10:35] <moon-child> hmm, isn't shared memory kind of a problem for erlang?
[09:10:39] <moon-child> I read that somewhere
[09:10:49] <moon-child> you need it for perf reasons sometimes, and have to manage it manually
[09:10:53] <hayley> Joe and Robert insisted that it was only a problem in theory IIRC.
[09:10:57] <sm2n> You're probably thinking of the shared binary thing
[09:11:03] <hayley> The only exception is the mnesia database though.
[09:11:48] <sm2n> point is that actors all live in the same heap unlike erlang, 
[09:11:58] <sm2n> message passes don't need to copy
[09:12:37] <hayley> You can't avoid copying for distribution across separate address spaces though.
[09:13:07] <sm2n> Yeah, I don't think they actually have a distributed erlang equivalent, though I might be wrong.
[09:13:27] <hayley> DL even has to copy, if you make a reference from a mutable object (in the global heap) to an immutable one (in a local heap). But immutable objects have no EQ in ML, so they only need to copy, and it's not too slow.
[09:24:41] <gilberth> Oops, everything is white outside. First snow this year. Beautiful!
[09:25:50] <moon-child> first snow ... at the end of march?
[09:26:28] <gilberth> Yes, this happens.
[09:26:38] *** Quits: tyson2 (~user@bras-base-toroon0624w-grc-08-70-29-36-27.dsl.bell.ca) (Remote host closed the connection)
[09:26:41] <sm2n> is germany in the southern hemisphere? I thought it wasn't, but maybe I am mistaken
[09:27:36] <moon-child> :D
[09:27:40] <gilberth> I can't remember we had snow otherwise this year. And snow at this time of the year isn't that uncommon.
[09:27:59] <hayley> "Figure 22 shows the footprint ‚Äî maximum resident set size ‚Äî obtained using a core count of 64 for all the benchmarks, except for the heavyRing benchmark which was obtained using depth 16 for the tree being passed around" Well, your results are bogus if you don't fix resident set size, because tracing GC costs are inversely proportional to heap size.
[09:29:00] <gilberth> Anyhow, it looks beautiful. I like snow.
[09:30:43] *** Quits: rotateq (~user@p200300e787143500a6f16de1b1c2e823.dip0.t-ipconnect.de) (Quit: ERC (IRC client for Emacs 27.2))
[09:31:47] <hayley> This paper so far has just confused me. The same thing on barriers is repeated on page 8: "At any point, if an actor may write to an object, then no other actor can read from or write to this object‚Äôs fields. Thus, ORCA can avoid write barriers and tracing needs no synchronisation."
[09:31:57] <hayley> Again, the GC introduces races that barriers fix, not the mutator.
[09:32:17] <hayley> So not having races in the language doesn't help per se. I guess the trick is the rest of the paper.
[09:33:10] <gilberth> moon-child: I was thinking. A GATHERING macro (I would call it GATHERING-BIND) does need to know which kind of accumulation is actually used. It could be specified as being dynamic while saying once you MINIMIZE say, you can't COLLECT anymore ad vice versa.
[09:33:32] <gilberth> Recognizing that only one kind of gathering is used could be seen as an optimization.
[09:34:11] <moon-child> why does GATHERING need to know what accumulation is used?
[09:34:29] <moon-child> append/collect need to maintain a tail pointer, but gather can make one for all vars, and if you never append or collect, you don't use it
[09:35:42] <gilberth> Yes, this is my idea. And have flags like #:IS-MINIMIZING.123, #:IS-COLLECTING.123 etc. Even the dumb CCL compiler will recognize that e.g. #:IS-COLLECTING.123 always is NIL.
[09:38:01] <gilberth> We besides COLLECT, APPEND, NCONC, MINIMIZING, MAXIMIZING, and perhaps summing and counting do we need? I am not so sure about the latter even.
[09:38:31] <moon-child> hmm.  Here is a problem
[09:38:46] <gilberth> Maybe collecting a vector? I often use WITH-OUTPUT-TO-STRING to collect a string from bits and pieces.
[09:38:52] <moon-child> one nice thing about loop is I can say (loop repeat 0 sum 27) and get back 0
[09:39:38] <moon-child> 'collecting a vector?'  yes, and you need two variants: to collect a vector and to append a vector
[09:40:12] <gilberth> Well, what I hate though is (loop repeat 0 minimize 27) => 0. I really hate that, it should be NIL.
[09:40:20] <moon-child> that is implementation defined
[09:40:27] <gilberth> Even worse.
[09:40:28] <moon-child> min/max, that is
[09:40:30] <moon-child> indeed
[09:40:37] <moon-child> sum being 0 can be counted upon
[09:40:48] <moon-child> fwiw it is nil in sicl loop
[09:41:06] <moon-child> really, it hsould be infinity, not nil, but there is no infinity
[09:42:58] <gilberth> Yes, but I can live with sum being 0. We could, if one cares make it so that you could say e.g. (GATHERING-BIND ((X :initial-value 0)) (loop repeat 0 (sum-into x 27)) ...) or sth. I am not too concerned about NIL being returned when nothing is summed. idk. This all isn't perfect.
[09:44:02] <moon-child> empty sum being 0 is correc
[09:44:03] <gilberth> moon-child: Even infinity wouldn't be convenient. Suppose I compute a bounding box. There is a difference in "no bounding at all" and a bounding box of size zero. I would prefer NIL.
[09:44:10] <moon-child> because 0 is identity for +
[09:44:34] <gilberth> Yes, it is. And NIL is an identity for APPEND e.g.
[09:45:13] <gilberth> Anyhow, I could live with making specifying the kind of gathering optional.
[09:46:07] <gilberth> When you don't say anything, it's NIL. Or go with :INITIAL-VALUE like REDUCE.
[09:47:29] <gilberth> The latter would be really useful for something which is collecting with VECTOR-PUSH-EXTEND. You may have an initial buffer, that's handy to use.
[09:48:36] <hayley> gilberth: An irrelevant thought: we say that, say, copying GC is O(live) and mark-sweep is O(heap) since it sweeps too. The Immix GC is like mark-sweep, but with no free lists. So the sweep is still O(heap) but it's just diddling some bits in some bitmaps, which takes much less time (presumably). So having a small constant factor for the O(heap) part means that the O(live) for copying isn't so important.
[09:48:56] <moon-child> obtw, regarding what you mentioned yesterday wrt declarations
[09:49:12] <moon-child> I would make type declarations, at least, be built in, like with loop.  And you can use those to generate initialisers
[09:49:40] <moon-child> (gathering-bind ((x :type fixnum)) ...)
[09:49:46] <gilberth> Yes, and declarations may not come from macros, so you don't need a code walker.
[09:49:50] <gilberth> Nope.
[09:50:00] <moon-child> rather than (gathering-bind ((x :initial-value 0)) (declare (type fixnum x)))
[09:50:23] <gilberth> Have (gathering x (loop ...) (declare (type fixnum x)) ...) Set.
[09:50:39] <gilberth> No need for extra syntax here.
[09:51:01] <gilberth> I mean you also don't say (multiple-value-bind ((x :type fixnum)) ....), do you?
[09:51:15] <moon-child> no, but I do say (loop with x fixnum ...)
[09:51:26] <gilberth> Because it is Interlisp.
[09:51:48] <gilberth> I am about to craft something CLish.
[09:56:30] <moon-child> hmm, here is one counterargument
[09:56:46] <moon-child> it is dangerous if I can say (gathering (x) (setf x ...) ...)
[09:57:06] <gilberth> Why should that be possible at all?
[09:57:09] <moon-child> so instead make x a symbol macro, so you can only read it; can't write it, without going through an explicit accumulator
[09:57:29] <gilberth> I don't even want to read it.
[09:57:53] <moon-child> why?
[09:58:04] <gilberth> X simply is not a lexical variable in the "init" of (gathering-bind vars init ...)
[09:59:20] <gilberth> moon-child: If I want, I face spaghetti code. You are then free to use your own lexical variables for accumulation.
[10:00:11] <gilberth> I mean, that COLLECT. I don't want that you could mess with the immediate result. What happens if you say (COLLECT-INTO X) (POP X)? That's a mess IMHO.
[10:00:29] <moon-child> that's why I suggested to only allow reading, but not writing
[10:00:50] <moon-child> that said, for collect/append, there are complications, as you could mutate the conses pointed-to
[10:00:53] <gilberth> And this is why I intend to call it GATHERING-BIND to parallel MULTIPLE-VALUE-BIND and DESTRUCTURING-BIND.
[10:01:23] <gilberth> moon-child: (COLLECT-INTO X 42) (NCONC X (LIST 'HAH)) ?
[10:02:57] <gilberth> I plan to offer (GATHERING-BIND X (LOOP ... DO (COLLECT-INTO X #\A)) (TYPE SIMPLE-STRING X) ...) Midfly the [hidden] buffer is not a simple string.
[10:03:13] <gilberth> * (DECLARE #) ;rather
[10:03:49] <moon-child> so x is a list of characters, and then you parse the declaration and magically turn it into an array?
[10:03:51] <moon-child> I do not like that
[10:04:25] <gilberth> We have sequences. I could say that before binding whatever is there is COERCEed.
[10:05:02] <gilberth> If you insist I could go with a COLLECT-INTO-VECTOR. But then I don't like that.
[10:05:11] <moon-child> why?
[10:06:31] <gilberth> Because I may change my mind later. The job of the inner form is about to yield values and not being concerned whether it's a vector or a list. I like the "sequence" abstract data type.
[10:07:23] <moon-child> suppose I APPEND a vector
[10:07:40] <moon-child> does that produce an improper list, or a sequence the final elements of which are the final elements of the vector?
[10:07:42] <gilberth> APPEND is for lists.
[10:07:56] <moon-child> so append is only for lists, but collect is for arbitrary sequences?
[10:08:03] <moon-child> so, do I get append-into-vector?
[10:08:45] <moon-child> also: what I really want is collect-major-cell.  So e.g. I could collect-major-cell some 2-d arrays and get as a result a 3-d array
[10:08:49] <gilberth> moon-child: Yes. Actually I believe CL should have a VECTOR-APPEND and VECTOR-APPEND-EXTEND function. I miss that.
[10:09:09] * hayley wonders why GitHub still thinks that one-more-re-nightmare is 100% CL, when she added some CSS and TeX stuff for documentation.
[10:09:43] <kakuhen> hardcore "unix" philosophy lost again 
[10:09:56] <kakuhen> apparently 9front uses a literal shell script to host their http server
[10:09:57] <hayley> gilberth: Anyways, here's another manual: https://applied-langua.ge/projects/one-more-re-nightmare/
[10:09:58] -ixelp- The one-more-re-nightmare Book
[10:09:58] <gilberth> moon-child: I never wanted that kind of collecting. Roll your own. I mean we cannot be 100% general here, we just need to cover the most common cases. LOOP also isn't.
[10:10:17] <kakuhen> no inputs get sanitized so you can access the entire filesystem with enough ..%2F in your URL
[10:10:50] <kakuhen> e.g. http://fqa.9front.org/..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f/rc/bin/rc-httpd/rc-httpd
[10:10:53] <hayley> That's common for stupid web servers. But I guess shell makes it more fun.
[10:11:31] <moon-child> http://fqa.9front.org/..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f/etc/passwd   awwww
[10:11:34] * hayley struggles to grab /etc/passwd
[10:12:03] * moon-child wonders if a race bug will do the trick
[10:12:10] <kakuhen> its plan9
[10:12:15] <kakuhen> use /etc/user or something
[10:12:20] <kakuhen> http://fqa.9front.org/..%2f..%2f/etc/users/
[10:13:12] <moon-child> no way, are those plaintext passwords?
[10:13:13] <kakuhen> http://fqa.9front.org/..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f/sys/lib/tls/
[10:13:28] <kakuhen> i assume theyre using the usual cipher for passwd
[10:13:45] <kakuhen> thankfully their exposed tls folder doesnt give you read access to their ssl certs
[10:13:50] <moon-child> I found a 'b1tchb00ts'
[10:13:59] <moon-child> pretty sure that's not hashed
[10:14:00] * hayley notes that the password files are of variable size
[10:14:36] <moon-child> yes, that was the first tip.  But could just be a weird hash function
[10:15:05] <kakuhen> maybe they are plaintext
[10:15:17] <kakuhen> hash functions must be bloat or something
[10:15:28] <kakuhen> i mean... a shell script httpd.... what the hell
[10:15:44] * hayley uploaded an image: (241KiB) < https://libera.ems.host/_matrix/media/r0/download/matrix.org/moXlKpbuyVmxHovZYYssltlJ/faa-committee.jpeg >
[10:15:48] <kakuhen> i wouldn't put it past them to store plaintext passwords √† la Emacs' .authinfo
[10:19:12] <moon-child> hayley: took me a bit to parse 'The <first> group', maybe clearer as 'The <nth> group'
[10:19:20] <moon-child> also 'There are no characters that are simultaneously a and b.'
[10:19:21] <hayley> Right.
[10:19:24] <moon-child> should be b and c?
[10:19:34] <hayley> Quite possible.
[10:19:40] * hayley bikeshedding
[10:19:55] <hayley> The Netfarm book had blue and green highlights, this book has red and...blue? Not quite as harmonius.
[10:24:25] <qhong> moon-child: which family though? cyclic groups? alternating groups?
[10:25:44] * moon-child smacks qhong around a bit with an abelian grape
[10:26:17] <kakuhen> i think its the free category generated by "nth"
[10:26:48] <kakuhen> so group in this case is a group(oid)
[10:27:02] <kakuhen> or maybe a group object of the category generated by nth
[10:27:09] <dave0> hayley: when you time things and print it as nanoseconds per op, do you just count seconds and divide by the number of iterations or is there some nuance to it that i don't know?
[10:27:22] <hayley> That's what I do.
[10:27:29] <dave0> oh cool thanks
[10:35:22] *** Joins: Lycurgus (~juan@98.4.112.204)
[10:36:43] <dave0> bureau of meteorology says 90% chance of rain tomorrow in my town wollongong
[10:38:35] <Lycurgus> does wollongong have radar? if so you can see how that spreads over the day
[10:39:27] <Lycurgus> radar plus predictive models igess
[10:43:32] <dave0> oh i dunno i'll have a loop
[10:43:35] <dave0> look*
[10:44:38] *** Joins: rotateq (~user@ip-078-094-235-194.um19.pools.vodafone-ip.de)
[10:45:28] <dave0> http://www.bom.gov.au/products/IDR033.loop.shtml
[10:46:09] <rotateq> hey dave0 :)
[10:46:19] <dave0> hi rotateq :-)
[10:49:58] <rotateq> i arrived at work
[10:51:30] *** Joins: pjb (~pjb@user/pjb)
[10:52:02] <Lycurgus> that radar isn't as useful as the ones here (USA) as it doesn't have time control
[10:52:39] <Lycurgus> https://www.wgrz.com/radar like this
[10:53:58] <dave0> cool there's a dragbar
[10:54:04] * hayley upgrades REGRIND to check for out of bounds registers.
[10:55:14] <hayley> (one-more-re-nightmare:all-matches "¬´((R))*¬ª¬´(¬¨O)¬ª¬´(X)*¬ª" "CWKTBHMXOYO") ‚áí (#(0 11 0 -1 0 11 11 11) #(11 11 11 11 11 11 11 11))
[10:59:20] <Lycurgus> rotateq, in a lisp shop?
[11:00:23] <Lycurgus> if there is such a thing anymore
[11:00:55] <hayley> Worse, the interpreter and compiler disagree on what the result should be. The bug is only in the compiler.
[11:00:59] <Lycurgus> besides the commercial implementations ofc
[11:01:17] <Lycurgus> sounds like sbcl
[11:02:08] <qhong> hayley: sbcl: nuke the interpreter so compiler has no bug anymore
[11:02:10] *** Joins: treflip (~user@user/treflip)
[11:02:28] <hayley> I mean my regex interpreter vs compiler.
[11:02:39] <contrapunctus> lol qhong 
[11:06:12] *** Quits: mala (~mala@user/malaclyps) (Quit: ZNC 1.6.1 - http://znc.in)
[11:06:25] *** Joins: mala (~mala@user/malaclyps)
[11:11:46] <rotateq> Lycurgus: yes we serve parentheses for breakfast :D
[11:12:09] <Lycurgus> ah, good for you!
[11:13:14] <rotateq> okay no, sadly not and never can be. or i have to get smarter
[11:14:43] *** Quits: Oddity (~Oddity@user/oddity) (Ping timeout: 256 seconds)
[11:15:48] <Lycurgus> that's the ballad of the sad shop, which is all of em in my book
[11:16:45] <rotateq> or working at a company like Franz Inc for doing special projects
[11:24:25] *** Quits: Lycurgus (~juan@98.4.112.204) (Quit: Exeunt)
[11:26:37] <hayley> Aha, my unification was broken, and unified things it wasn't meant to.
[11:26:59] <hayley> "Don't care for X ~ X" Well, ain't you wrong, that's important because it removes the opportunity for something wrong to be unified later.
[11:32:36] <rotateq> hayley: in cl-ppcre the regexes are compiled, right?
[11:32:44] <hayley> To chains of closures.
[11:32:48] <hayley> So, not really.
[11:32:51] <rotateq> i really have to dig into that lib seriously now
[11:32:55] <rotateq> ah hm
[11:37:05] <hayley> Hm, two in a row of these bugs have been due to trying to unify the history in \alpha nodes. The assigments look backward somehow.
[11:38:03] <hayley> Yeah, it's just due to unifying that. They're the right way around, but the actual effect that SIMILAR thinks exists just doesn't exist.
[11:42:11] <hayley> And, in turn, I can avoid that by not generating new tags for the history part of NULLABLE, I think.
[11:49:02] *** Joins: shka (~herr@109.231.3.55)
[11:56:22] <moon-child> 'These GCC and ICC outcomes would not be correct with respect to a concrete semantics, and so to make the existing compiler behaviour sound it is necessary for this program to be deemed to have undefined behaviour'
[11:56:25] <moon-child> c standardization in a nutshell
[12:04:56] <hayley> Aha, it'd also unify incorrectly, since I only checked if the "to" side was already unified with another "from", whereas I didn't check if the "from" side had another "to".
[12:09:55] <moon-child> g
[12:09:58] <hayley> Now I have a weird performance regression. The Xorg.0.log test is slower, but only when there are no submatches. It's faster with submatches.
[12:10:23] <hayley> I don't like that. Telling people that submatches are negative cost would make me sound crazy.
[12:12:44] <qhong> what's everyone's opinion on argument evaluation order?
[12:13:09] <moon-child> probably left to right
[12:13:14] <moon-child> more interesting is arguments vs function
[12:13:14] <qhong> Not specifying it is cleaner (more functional?) and makes implementation easier, but I've wrote too much CL and I do find left to right handy
[12:13:24] <hayley> You're probably being too clever if it's important. But you should still define it.
[12:13:35] <qhong> moon-child: in my F-expr language it has to be function first
[12:13:41] <qhong> to decide whether to evaluate the arguments
[12:13:54] <moon-child> not specifying order is senseless unless you actually don't have effects, in which case it doesn't matter
[12:14:12] <qhong> the easiest thing to do is right to left, because map-reverse is easier than map
[12:14:39] <qhong> (and this always have to be hand coded in the VM core, so I want it to be as simple as possible)
[12:14:52] <moon-child> simplify the semantics, not the implementation
[12:15:50] <qhong> sure. And unspecified is unacceptable, so it leaves me with left-to-right 
[12:16:26] <qhong> Btw I recall someone mentioned Japanese people may have an affinity to Lisp, and I think I get the reason now
[12:16:47] <hayley> (parse-regular-expression "\"[0-9]+x[0-9]+\"") ‚áí "[0-8]["[0-8]]*x[0-8][x[0-8]]*"
[12:16:57] <hayley> Now, what's wrong with this picture?
[12:17:32] <qhong> Japanese language is strongly head-final, therefore strongly left-branching. This matches the structure of CONSes represented by S-expr perfectly
[12:22:59] *** Joins: lisp123 (~lisp123@143.238.171.43)
[12:26:19] *** Joins: Oddity (~Oddity@user/oddity)
[12:28:45] <kakuhen> i check lispcafe one last time for the night and i see this, nice
[12:29:07] <kakuhen> i will note that in actual conversation you may have some phrases "out of order" so to speak
[12:29:23] <kakuhen> the simplest example i have in mind is ‰Ωï„Åì„Çå
[12:29:47] <kakuhen> but theoretically, that is correct, what you said
[12:36:31] <hayley> gilberth: Well, on the precedence of OR and AND, I'd say that we write our disjunctive normal form like ab + cd. So AND is tighter than OR.
[12:36:57] <moon-child> imagine having operator precedence
[12:37:00] * moon-child hides
[12:37:55] * hayley is in trouble again with her parser
[12:38:10] <hayley> It used to parse ab+ like (?:ab)+ which is wrong. Now it won't parse ab+ :(
[12:41:02] * hayley reads the POSIX spec for help
[12:51:36] *** Quits: shka (~herr@109.231.3.55) (Quit: Konversation terminated!)
[12:51:52] *** Joins: shka (~herr@109.231.3.55)
[12:54:12] *** Joins: cosimone (~user@2001:b07:ae5:db26:c24a:d20:4d91:1e20)
[13:00:38] <hayley> Now it works, and I get to celebrate my 200th commit to one-more-re-nightmare.
[13:01:17] <rotateq> :3
[13:05:17] <rotateq> nice
[13:05:36] <rotateq> i see it
[13:15:26] *** Joins: mgl (~mgl@cpc87455-finc19-2-0-cust234.4-2.cable.virginm.net)
[13:17:05] *** Quits: treflip (~user@user/treflip) (Remote host closed the connection)
[13:26:27] <dave0> does 7.1 nanoseconds for RET sound right?
[13:27:19] <hayley> I'd have to check what function call overhead is, but IIRC it's smaller than 7ns on my machine. It might be easier to say, if you know how many cycles that is.
[13:28:11] <hayley> the-cost-of-nothing claims...970 picoseconds for a function call and return with no arguments on SBCL.
[13:28:26] *** Quits: X-Scale (~ARM@231.197.28.37.rev.vodafone.pt) (Ping timeout: 272 seconds)
[13:28:42] <hayley> Some serious superscalar execution at play, I guess.
[13:29:34] <moon-child> yeah, 7.1 ns sounds way expensive
[13:29:53] * dave0 grumbles
[13:29:56] <moon-child> unless you are abusing the return stack and triggering mispredicts
[13:41:44] <rotateq> when it will be fs we need a femtoseconds spectroscopy camera
[13:41:55] *** Quits: Oddity (~Oddity@user/oddity) (Ping timeout: 246 seconds)
[13:45:01] <rotateq> i said "nooo not again installing VS Code in this 3rd virtual machine" and now the teacher said I'm the assistent when others need help with that :D
[13:47:39] <rotateq> hayley: they call it editor :D
[13:50:56] <moon-child> cycles are not gonna get that short
[13:51:09] <moon-child> you have the speed of light to worry about
[13:51:34] *** Joins: treflip (~user@user/treflip)
[13:52:40] <moon-child> clock cycle synchronizes the entire cpu.  CPU is ~1cm.  light femptosecond is ~1e-5cm
[13:53:41] <moon-child> (and electricity is not quite as fast as light)
[13:53:51] <moon-child> also have heat dissipation to worry about
[14:05:25] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 250 seconds)
[14:06:06] <rotateq> of course it was just a joke
[14:07:41] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[14:24:20] <gilberth> Oh, Zilog responded, they will correct the manual. I am surprised.
[14:25:48] <rotateq> is Zilog a manufacturer of electronic devices/chips?
[14:26:16] <hayley> Yes.
[14:27:09] <gilberth> The Z80 was and still is a thing.
[14:28:39] <rotateq> ahh
[14:29:48] <rotateq> someone I knew from here sometimes repaired Z80s for a local museum as for example the thin broad cable for the keyboard got broken at some point
[14:31:01] <selwyn> you can get femtosecond laser pulses
[14:31:11] <gilberth> Wow, he repaired the chip? Must be quite some fancy equipment that he has.
[14:31:35] <hayley> What was wrong with the manual?
[14:32:38] <gilberth> hayley: The encoding given for the RR instruction. I noticed while hacking my assembler and disassembler.
[14:33:12] <hayley> Z80?
[14:33:19] <gilberth> Yep.
[14:34:11] <hayley> Congratulations!
[14:34:44] <rotateq> gilberth: no not the chips :D
[14:34:53] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 250 seconds)
[14:34:59] <rotateq> just the cables, shortened them
[14:35:09] <rotateq> Sollbruchstelle
[14:35:28] <gilberth> It's a bit annoying that the Z80 mnemonics are completely different from Intels, although the Z80 is binary compatible to the 8080.
[14:35:40] <moon-child> I found a typo in the intel manual.  Didn't bother reporting though
[14:35:56] <moon-child> also 2 bugs in nasm and 1 in gas
[14:36:05] <gilberth> moon-child: I was bored.
[14:36:14] <moon-child> reported the first nasm bug, no response.  So I did not bother reporting anything else
[14:37:07] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[14:37:12] <gilberth> I do not bother reporting gcc bugs anymore. I reported a few together with the most minimal code which would expose the bug and never got a response. Seems like they don't care.
[14:39:12] <selwyn> shka: it seems that you can generate soft x rays with most of the properties of lasers using some clever non linear optics
[14:39:31] <selwyn> https://aip.scitation.org/doi/10.1063/1.5041498
[14:39:32] -ixelp- Apparatus for soft x-ray table-top high harmonic generation: Review of Scientific Instruments: Vol 89, No 8
[14:40:34] <moon-child> I sometimes wonder, if even assemblers like nasm and gas that have many eyes on them have simple bugs like this, what are the implications for people trying to make formal models of instruction sets?
[14:40:36] <rotateq> Intel makes errors? :D
[14:40:43] <moon-child> this is why I wish the processor vendors would release their own formal models
[14:41:15] <moon-child> (formal models--mm0, ccomp, there was also one in cl)
[14:42:19] <gilberth> I only noticed because my disassembler generator noticed that the encoding for RR given is the same as for RL.
[14:42:47] <gilberth> And how to verify that everything is correct?
[14:43:06] <moon-child> how indeed!
[14:44:26] <gilberth> You could verify against a different assembler, but that other assembler could also have copied mistakes in the CPU manual. You would need to write a test suite to for the whole CPU. Easy with the 8080, but for AMD64?! Have fun.
[14:45:23] <moon-child> a half-written article begins 'it seems like every few years somebody publishes a paper: "we did differential testing of a bunch of (dis)assemblers and concluded they were all garbage"'
[14:45:46] <moon-child> it remains true
[14:45:53] <gilberth> And yes, for my 8080 I checked against Digital Research's assembler. I done that some 30 years ago. :-)
[14:46:09] <moon-child> look up 'sandsifter' for the closest anyone's come to exhaustive testing of an amd64
[14:47:41] <moon-child> my pet project (one of many) is to build an exhaustive disassembler.  I could probably verify that every byte sequence can be decoded, and that any two decodings which alias are flagged as such (e.g. vendor-specific instructions).  But that still doesn't ensure that the decodings are actually correct :/
[14:47:47] *** Joins: dec0d3r (~dec0d3r@2001:8003:480a:e00:e07:e7c3:7efc:ed0f)
[14:47:56] <gilberth> Anyhow, I would at least attempt to cross check. It should be easy to generate a text file of all encodings exercised. Assemble with yours and assemble with theirs. Binaries should match.
[14:48:17] <gilberth> When binaries don't match, you found an error. Either in your assembler, or in theirs.
[14:48:41] <gilberth> Does protect you from documentation errors though.
[14:48:47] <gilberth> * Does NOT
[14:48:49] <moon-child> indeed
[14:48:58] <moon-child> I fuzz-test my assembler against nasm
[14:49:05] <moon-child> has definitely caught some bugs
[14:49:18] <gilberth> I mean there must sure be instructions that aren't actually used by anyone. :)
[14:50:00] <dave0> yeah bcd crap
[14:50:23] <gilberth> I am sure some will use that. Banks, maybe?
[14:50:32] <moon-child> dave0: gone in amd64!
[14:50:47] <dave0> moon-child: \o/
[14:50:59] <gilberth> moon-child: Even in 8086 mode?
[14:51:05] <moon-child> no
[14:58:53] *** Joins: OlCe (~user@lfbn-nic-1-449-48.w90-116.abo.wanadoo.fr)
[15:08:25] <selwyn> twitch is getting rid of its desktop app
[15:09:43] *** Quits: lisp123 (~lisp123@143.238.171.43) (Quit: Leaving...)
[15:10:53] <selwyn> torn between saying good riddance to electron nonsense and upset that i will have to use the browser
[15:23:32] <shka> selwyn: sorry, i can't understand the article :D
[15:23:51] <selwyn> i can't either tbh
[15:24:10] <shka> what has gas pressure to do with laser pumping anyway
[15:24:47] <shka> https://en.wikipedia.org/wiki/Gas_dynamic_laser
[15:24:52] <shka> ok, so this is a thing
[15:25:02] <shka> lasers are weird
[15:25:11] <selwyn> which page?
[15:25:16] <shka> abstract
[15:26:37] <selwyn> oh, this is just high harmonic generation
[15:26:38] <shka> this article is well above my level, won't benefit from reading it
[15:26:58] <selwyn> its different from a gas dynamic laser
[15:28:11] <selwyn> pumping laser light into such a gas cell drives the electrons very far away from their nucleii
[15:28:54] <selwyn> however, they rapidly accelerate back where they came from as the electric field changes direction, and recombine with the nucleus
[15:29:14] <selwyn> in the process, they emit laser light that is at some multiple of the frequency of the driving laser field
[15:30:10] <selwyn> i suspect that it has to be a gas otherwise electron electron interactions might become significant
[15:30:35] <selwyn> and if its not high pressure you just won't get very much out of the process
[15:30:35] <shka> ok, so it is just charge moving against the electric field, got it
[15:30:56] <selwyn> the highly oscillating electric field that you get inside a laser field
[15:31:10] <selwyn> i was lectured on this by one of the authors of the paper
[15:31:19] <selwyn> but hhg didn't appeal to me that much
[15:31:29] <selwyn> it is not an area that theorists can contribute to very much imo
[15:32:52] <dave0> i timed call/ret and i get 1.5 nanos per call/ret pair, that sounds a lot better
[15:35:13] *** Joins: random-nick (~random-ni@87.116.167.125)
[15:35:46] <selwyn> it seems that you don't have to use gases
[15:36:12] <selwyn> but noble gases are the best candidates
[15:40:32] <shka> selwyn: well, you kinda ionize the gas in process 
[15:40:34] <shka> so...
[15:44:02] <selwyn> ?
[15:44:13] <selwyn> well
[15:44:36] <selwyn> yeah, i suppose it is sort of ionised
[15:45:06] *** Joins: tyson2 (~user@bras-base-toroon0624w-grc-08-70-29-36-27.dsl.bell.ca)
[15:45:11] <selwyn> you can't really say that the electron is bound while an external field is driving it far away
[15:46:39] <selwyn> they are interested in using these x rays for fancy spectroscopy and investigating quantum processes that we can't observe otherwise
[16:00:02] *** Joins: razetime (~quassel@117.193.2.164)
[16:07:18] <ck_> I wonder what other kinds of things you could amplify by stimulating some emissions
[16:07:22] <ck_> must be a universal concept
[16:15:26] *** Joins: kevingal (~quassel@149.157.108.66)
[16:15:59] <dave0> i did more tests.. i think blowing the instruction cache makes things 20x slower
[16:21:27] *** Quits: kevingal (~quassel@149.157.108.66) (Ping timeout: 260 seconds)
[16:24:19] *** semz_ is now known as semz
[16:25:47] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 272 seconds)
[16:26:53] <selwyn> stimulated emission comes from atomic physics, where you stimulate some transition between electron levels to release light of a specific wavelength
[16:27:12] <selwyn> however, this proposal to stimulate nuclear fission using lasers looks strikingly familiar
[16:27:17] <selwyn> even if the physics is quite different
[16:27:36] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[16:27:42] <ck_> it also works globally and in construction
[16:28:06] <ck_> amplified reinforcement dykes by stimulated (by tsunami) emission of radiation (by particle dispersal)
[16:37:03] *** Quits: aeth (~aeth@user/aeth) (Ping timeout: 260 seconds)
[16:38:52] *** Joins: aeth (~aeth@user/aeth)
[16:41:31] <dave0> i'm getting mixed messages from RET ... by itself, it seems to take a very predictable time of 7 nanoseconds ... if i use the stack as a list of pointers aka return oriented program... both predictable and unpredictable it's the same... for small or large lists it's the same...
[16:43:03] <gilberth> Isn't seven nanoseconds quite a lot?
[16:43:15] <dave0> lists of `call`s to `ret`s is must faster than the ROP but if it blows out the instruction cache call/ret is 20x slower
[16:43:22] <dave0> gilberth: yeah i think it is
[16:43:28] <dave0> but it's extremely stable
[16:44:01] <gilberth> So then mispredicted all the time. Congrats!
[16:44:13] <dave0> if i pruposely randomize the calls to the functions it's still 7 nanos
[16:44:21] <dave0> yeah it's really weird
[16:45:11] <dave0> i get wild variations for things that "emulate" ret, such as   pop rax / jmp rax
[16:45:43] <dave0> they are really fast if cpu can predict it.. as soon as i randomize it's 5x slower
[16:46:00] <dave0> weird
[16:46:27] <gilberth> Yes, I very much suspect that the CPU predicts a RET to lead it back to the last CALL. That would explain why all those RETs are the same speed.
[16:47:11] <dave0> with 1,000 `call`s to the same `ret` it takes 1.5 nanos for the pair
[16:47:21] <dave0> but if i make it 10,000 it's 20x slower
[16:47:39] <dave0> but that's gotta be blowing out the icache
[16:48:17] <dave0> call is 5 bytes times 10,000 which is ~50k
[16:48:25] <gilberth> Are that 10,000 different "subroutines"?
[16:48:33] <dave0> nope, same ret every time
[16:48:51] <gilberth> What is blowing the cache then?
[16:49:04] <dave0> the list of call instructions
[16:49:30] <dave0> .rept 10000 / call func / .endr
[16:49:32] <gilberth> So you have 10,000 CALLs. I see.
[16:50:57] <gilberth> This explains a few things. How tiny is the icache?
[16:51:14] <dave0> i'm not sure how to find out
[16:51:23] *** Joins: phantomics (~phantomic@71-218-248-44.hlrn.qwest.net)
[16:51:24] <hayley> lstopo
[16:51:39] <hayley> But you might blow your BTB which is not easy to find the size of.
[16:52:45] <gilberth> Anyhow, this explains why when I have BC interpreter with just a dozen or so different instructions to dispatch on, I easily can get 800MIPS, while when I add more instructions to the byte code ISA, it quickly gets slower.
[16:54:43] <dave0> the speed difference is dramatic
[16:55:30] <dave0> it's a benchmark so eh
[16:55:41] <dave0> hardly realistic
[16:57:36] <selwyn> unfortunately my partner didn't get accepted to the hackathon
[16:57:38] *** Joins: kevingal (~quassel@hamilton-nat.nuim.ie)
[16:59:50] *** Quits: kevingal (~quassel@hamilton-nat.nuim.ie) (Remote host closed the connection)
[16:59:57] <gilberth> Why not, what happened?
[17:01:01] <selwyn> they just rejected her application
[17:01:24] <gilberth> Yes, but why? Do you know?
[17:01:26] <selwyn> it is annoying because they said they would prioritise applications of partners of people already accepted
[17:04:00] <selwyn> gilberth: rejection seems to be generic
[17:04:33] <selwyn> it is also annoying because i will have to find a partner who tolerates me writing it in common lisp
[17:04:43] <selwyn> and i don't really want to participate otherwise
[17:20:36] <shka> go lisp, or go home
[17:23:39] <selwyn> exactly
[17:23:55] <selwyn> anyway, my task today is to learn about this adaptive nearest neighbour algorithm
[17:24:05] <selwyn> is it in statistical learning? i couldn't see it
[17:24:49] <shka> selwyn: random forest is, in a nutshell, adaptive nearest neighbour
[17:25:31] <shka> but that's not important right now
[17:25:56] <shka> selwyn: the real crux is how you want to represent game state
[17:28:09] <selwyn> ok
[17:32:58] <shka> selwyn: representation is the key to everything in data science 
[17:33:10] <shka> and 90% of programming at large
[17:44:26] <selwyn> right
[17:45:36] <selwyn> its tough because the arrangement of static defences is important
[17:48:06] <shka> yeah
[17:48:06] *** Quits: luis (~luis@lisp/luis) (Ping timeout: 252 seconds)
[17:48:24] <selwyn> well, that can be today's task
[17:54:02] *** Quits: tyson2 (~user@bras-base-toroon0624w-grc-08-70-29-36-27.dsl.bell.ca) (Remote host closed the connection)
[18:22:10] *** Quits: treflip (~user@user/treflip) (Remote host closed the connection)
[18:32:12] *** Quits: rotateq (~user@ip-078-094-235-194.um19.pools.vodafone-ip.de) (Remote host closed the connection)
[18:42:40] *** Quits: ec (~ec@gateway/tor-sasl/ec) (Quit: ec)
[18:51:00] <gilberth> You would believe that void foo (void) { foo(); } would be an infinite loop? It isn't, when I compile that with clang, it makes foo() return doing nothing.
[18:51:42] <gilberth> I wonder when for(;;); will be compiled to just nothing.
[18:54:42] <pjb> gilberth: foo(); should be an infinite loop.  The argument that it may overflow the stack and therefore be an undefined behavior would be falacious.
[18:55:15] <semz> "It may not be what you wanted, but at least it does the wrong thing really fast!"
[19:00:26] *** Joins: tyson2 (~user@cpef492bf758151-cm688f2e29d8d0.cpe.net.cable.rogers.com)
[19:01:07] <gilberth> pjb: I believe there is nothing in ISO C that states that a stack must overflow eventually. Or that one must be used at all.
[19:01:49] <gilberth> I mean, TCO is not forbidden.
[19:11:38] *** Joins: Jacobis9000 (~jonaholuf@host31-54-71-0.range31-54.btcentralplus.com)
[19:12:29] <Jacobis9000> Hello! I am watching a lecture for CS50 (I lost a lot of progress because I did not complete the course in time, and now I must go back) and they are covering 'singly linked lists'. It seems they work very similar to the cons cell and lists of Lisp!
[19:13:05] <Jacobis9000> The first time I studied this, I did not know anything about Lisp, so the connection did not occur to me
[19:13:13] <Jacobis9000> the lecture is covering C
[19:16:00] <shka> Jacobis9000: lisp lists are literally single linked lists 
[19:42:17] *** Joins: waleee (~waleee@2001:9b0:213:7200:cc36:a556:b1e8:b340)
[19:42:46] *** Quits: tyson2 (~user@cpef492bf758151-cm688f2e29d8d0.cpe.net.cable.rogers.com) (Remote host closed the connection)
[19:47:59] *** Joins: tyson2 (~user@cpef492bf758151-cm688f2e29d8d0.cpe.net.cable.rogers.com)
[19:53:26] <dave0> gilberth: i think the c standard says a compiler can assume all loops eventually terminate
[19:53:48] <dave0> not 100% sure, it was an old discussion in #c
[19:55:46] <dave0> https://stackoverflow.com/questions/2178115/are-compilers-allowed-to-eliminate-infinite-loops
[19:55:46] -ixelp- c - Are compilers allowed to eliminate infinite loops? - Stack Overflow
[19:55:53] <gilberth> dave0: Oh dear. This is funny with regard to the halting problem.
[19:55:53] <dave0> it's stack overflow so meh
[19:57:06] <gilberth> Heh. I'll prepare fresh coffee, you want some, too?
[19:57:26] <dave0> maybe in a couple hours.. it's nearly 2:30am here and i should sleep
[19:57:46] <gilberth> Ah yes, time zones.
[19:58:29] <dave0> i got distracted by a pdf 'A fork() in the road" which basically aruges that fork was a hack, scales terribly, poor security, became complicated as computers grew, etc. etc.
[20:01:43] <selwyn> not being able to use lisp interactively is really annoying
[20:04:26] <gilberth> dave0: How fast is your VM implementation? When I do this minimal skeleton <http://clim.rocks/gilbert/mini-vm-skeleton.s>, which more or less is just dispatch, I measure 2800MIPS on a 3.9GHz Xeon.
[20:05:25] <gilberth> That's the base line, I mean, you can't go faster, can you?
[20:15:48] <dave0> it looks fast
[20:16:07] *** Joins: Oddity (~Oddity@user/oddity)
[20:17:10] <dave0> it looks like forth token threading
[20:17:25] <gilberth> Yep, I wanted to see what the baseline is. Actually I was surprise that the Z80 emulator I found, which is written in assembly, emulates a Z80 which would be clocked at 12GHz or so.
[20:17:41] <gilberth> dave0: Kind of, yes.
[20:18:03] <gilberth> 8080 is easy. Each opcode is just one byte.
[20:18:17] <dave0> gilberth: i am worfed by c, i cannot think in terms of other languages
[20:18:38] <dave0> to me, this looks like an array of function pointers.. no tail call elimination
[20:18:59] *** Quits: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475) (Ping timeout: 260 seconds)
[20:19:01] <dave0> rather my solution would be an array of function pointers
[20:20:34] <gilberth> Well, the "add %bl,%al" is the emulated instruction. Think of i_add_c, i_add_d ... following. The dispatch is inlined at the tail, but that doesn't make a lot of difference. When I say i_add_b: add %bl, %al ; jmp d I measure the same.
[20:20:45] *** Joins: mon_aaraj (~MonAaraj@user/mon-aaraj/x-4416475)
[20:24:10] <gilberth> dave0: The jump table is a table of functions, only that each function tail calls to the next.
[20:27:34] *** Joins: ec (~ec@gateway/tor-sasl/ec)
[20:29:58] <dave0> i haven't timed any of the forth interpreters
[20:30:08] <dave0> of my*
[20:30:37] <dave0> oh no april fools day
[20:31:00] <gilberth> Not yet. At least here. I am safe for a few hours.
[20:34:06] *** Quits: tyson2 (~user@cpef492bf758151-cm688f2e29d8d0.cpe.net.cable.rogers.com) (Remote host closed the connection)
[20:39:31] *** Joins: rotateq (~user@p200300e787143500a6f16de1b1c2e823.dip0.t-ipconnect.de)
[20:53:49] <dave0> ok sleep time cya later
[20:54:07] <gilberth> Take care then.
[20:54:32] <dave0> byes gilberth 
[20:54:58] *** Quits: dave0 (~dave0@069.d.003.ncl.iprimus.net.au) (Quit: dave's not here)
[21:03:15] <selwyn> gilberth: are you expecting many jokes and pranks to be played on you
[21:04:36] <selwyn> are java generics compile time?
[21:07:19] <ck_> hmm
[21:07:23] <ck_> "yes, because JIT"
[21:13:25] *** Quits: razetime (~quassel@117.193.2.164) (Quit: https://quassel-irc.org - Chat comfortably. Anywhere.)
[21:13:28] <gilberth> selwyn: No, I don't for there isn't anybody around who would play pranks on me.
[21:13:44] <shka> selwyn: yes
[21:13:52] <shka> selwyn: they are like C++ templates
[21:14:07] <shka> even turing complete as it turns out 
[21:21:29] *** Quits: phantomics (~phantomic@71-218-248-44.hlrn.qwest.net) (Read error: Connection reset by peer)
[21:22:59] <selwyn> what?
[21:24:26] <selwyn> wow
[21:26:07] <ck_> https://stackoverflow.com/questions/1032602/template-ing-a-for-loop-in-c
[21:26:07] -ixelp- optimization - Template-ing a 'for' loop in C++? - Stack Overflow
[21:28:14] <gilberth> Anyhow, when I consider that the emulator I have has a Z80 at 3000x the orignal speed and that those were useful systems, I wonder. Take a 8 core machine that is 24,000 times. As people need to sleep and think before they type, I could perhaps easily put 100,000 users onto one box :-) Perhaps even more.
[21:29:44] <gilberth> How much bandwidth would that be for say 9600bps for each user? About 1Gbps. Also feasible.
[21:34:22] <edgar-rft> I don't have the impression that anybody sleeps or thinks before typing on irc :-)
[21:35:06] <gilberth> People don't exactly type all the time.
[21:35:37] <ck_> said like a libra
[21:35:58] <ck_> (get it?)
[21:37:32] <shka> selwyn: yes
[21:37:47] <shka> https://arxiv.org/abs/1605.05274
[21:38:17] <shka> luckly, you can't really do recursive programming the way you do in C++ so at least it is not useful in practice 
[21:38:35] <sham1> shka: except that Java's parametric polymorphism has type erasure, so unlike with C++ templates, you can't really reason about the actual types you're generic over until runtime, and even then you need reflection
[21:39:10] <shka> oh, interesting 
[21:39:31] <shka> but at least it won't slow the compiler down to a crawl i guess
[21:39:44] <sham1> That it won't
[21:41:43] <sham1> Although the real problem with that in C++ is that so much stuff is crammed into the header files, and thus all the things have to be analysed and such for Every. Single. Translation unit
[21:43:23] <sham1> But yeah. In C++, std::vector<int> knows that it will contain ints, while with Java's ArrayList<Integer>, the compiler will certainly warn and even error out in certain cases when you're trying to do things other than Integers, but as far as the class itself is concerned, you might as well just be passing Objects
[21:43:42] <selwyn> happily this makes life much easier in abcl
[21:44:36] <sham1> Because ArrayList is ArrayList<T>, and that's basically the same as ArrayList<T extends Object>, and so T cannot be inspected at compile-time and one can't really know what it actually is
[21:45:04] <sham1> Kinda like how in Go one has interface{}, or I suppose with their parametric polymorphism system, Any
[21:46:48] <shka> cool
[21:47:02] <shka> it is arguable more OO approach 
[21:48:04] <sham1> With all the arguments about invariance, covariance and contravariance that come with it. Microsoft, for their part, saw this and thus managed to avoid Java's mistakes with C# in this regard
[21:54:42] <sham1> 3
[22:02:47] *** Quits: dec0d3r (~dec0d3r@2001:8003:480a:e00:e07:e7c3:7efc:ed0f) (Quit: Leaving)
[22:04:31] *** Quits: Jacobis9000 (~jonaholuf@host31-54-71-0.range31-54.btcentralplus.com) (Ping timeout: 245 seconds)
[22:24:11] <ck_> 2
[22:26:30] *** Joins: tyson2 (~user@cpec4411eab32aa-cm64777d9876e0.sdns.net.rogers.com)
[22:28:01] <gilberth> 4/3
[22:58:31] <rotateq> sham1: I bet the functional programming experts they had employed in the Cambridge research group saw this :D Peyton-Jones is out there since November now after many years
[23:01:59] <rotateq> but okay I don't know. it's just sometimes not so nice if very competent individuals/experts/geniuses working at a company make some real progress or discover something new useful, and then this company claims "we did it"
[23:02:16] <rotateq> or even worse, think they can "own" algorithms or mathematics
[23:11:09] *** Joins: phantomics (~phantomic@71-33-133-243.hlrn.qwest.net)
[23:21:48] *** Quits: mgl (~mgl@cpc87455-finc19-2-0-cust234.4-2.cable.virginm.net) (Ping timeout: 250 seconds)
[23:22:31] *** Quits: tyson2 (~user@cpec4411eab32aa-cm64777d9876e0.sdns.net.rogers.com) (Remote host closed the connection)
[23:27:28] <kakuhen> is anyone here familiar with knot theory
[23:28:11] <kakuhen> im having a hard time showing wikipedia's definition of kaufmann bracket agrees with the one given in Bar-Natan's 2002 notes on Khovanov homology
[23:30:44] <rotateq> I'm out with such abstract things.
[23:39:02] <selwyn> not in detail
[23:53:07] <sham1> knot in detail
[23:53:19] *** Quits: rogersm (~rogersm@90.166.180.250) (Quit: Leaving...)
[23:53:48] <kakuhen> unknot territory for you
[23:55:14] <rotateq> sham1: :D
[23:55:36] <rotateq> "the 'k' is silent, k?"
